[
    {
        "source": "RSD",
        "name": "seiscomp",
        "description": "SeisComP is a widely distributed seismological software package for real-time monitoring of earthquakes and other seismic events and provides for data acquisition, processing, distribution and interactive analysis. It has been developed within the GEOFON Program by Helmholtz Centre Potsdam GFZ German Research Centre for Geosciences and gempa GmbH with contributions from other international partners.\n\nThe software system consists of several automatic and interactive modules working separately to process data and to analyze seismicity. The software design ensures high robustness and a flexibility to respond to new developments within the SeisComP community.\n\n*User group*\n\nSeismologists and earthquake specialists at many hundreds of data and research centers as well as governmental agencies and companies world-wide benefit from this open source software by its reliability, performance, automation, usability and maintenance.\n\n## Features\nIn general, SeisComP includes several features as:\n\n- Data acquisition, recording and quality control\n- Real-time data exchange and processing\n- Automatic and interactive event detection and location and magnitude calculation\n- Interactive determination of focal mechanisms\n- Network status monitoring and issuing event alerts\n- Easy access to relevant information about stations, waveforms and recent earthquakes\n- Python interface for developing custom scripts and modules\n\nWithin SeisComP each module has particular tasks. There are data acquisition and automatic processing modules, which allow to provide, store and process waveform data or event parameters in real-time or offline. GUI modules show waveforms and processing results and allow user interactions. Users can view the latest events on a map with relevant event parameters or the seismogram of a station in a helicorder plot. Events can be selected for further processing as determining magnitudes and focal mechanisms.\n\nIn order to manipulate inventories and provide access to the data base several inventory and utility modules for managing those issues are included to SeisComP. The user-friendly documentation provides comprehensive descriptions to all concepts, modules and extensions for the current release. Community user have the possibility to contribute to the documentation.",
        "url": "#",
        "get_started_url": "https://www.seiscomp.de/",
        "repoDoi": "10.5880/GFZ.2.4.2020.003",
        "amountPublications": {},
        "createdAt": "2025-12-05T12:10:43.602Z",
        "mainPaper": {
            "doi": "10.5880/gfz.2.4.2020.003",
            "title": "The SeisComP seismological software package",
            "journal": "GFZ Data Services",
            "dateReleased": "2008-05-01T00:00:00.000Z",
            "abstract": "",
            "citationsArray": []
        },
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "10.5880/GFZ.2.4.2020.003",
            "openCitations": "10.5880/GFZ.2.4.2020.003",
            "dataCite": "10.5880/GFZ.2.4.2020.003",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "RSD",
        "name": "hybridmt",
        "description": "The hybridMT is MATLAB/shell environment software package for the seismic moment tensor inversion in time domain. The package allows to estimate fault plane parameters and the balance between volumetric and non-volumetric strain in the seismic source using different decomposition schemes. The moment tensors of earthquakes forming a cluster can be further refined using hybrid moment tensor approach that suppresses effects related to path, sensor and site effects. The methodology is optimized for earthquake data recorded by regional-to-local seismic networks as well as for acoustic emission activity.\n\n*User group*\n\nInternational users in the field of seismology as well as geosciences in general.\n\n## Features\n- Inversion of first P-wave amplitude data for unconstrained full, deviatoric and double-couple moment tensors\n- Suppression of propagation, site and sensor-related effects through hybrid seismic moment tensor approach\n- Uncertainty assessments using resampling approach\n- Shell command / MATLAB wrapper calls\n- Graphical beach-ball output (SVG, PNG, EPS, PDF) and flexible ASCII export\n\nThe moment tensor inversion may be performed directly in the shell environment (by a dedicated command-line tool) or conveniently through the MATLAB wrapper functions. The package is supplemented with extensive documentation, tutorials, and a dedicated website.\n\nImproved quality of seismic moment tensors by hybrid moment tensor approach:\n\n![](https://)![](https://s3.desy.de/hackmd/uploads/upload_a64e4f402c67c2818f343d27cf071599.gif)\n",
        "url": "#",
        "get_started_url": "https://www.induced.pl/software/hybridmt",
        "repoDoi": "10.1785/0220150251",
        "amountPublications": {},
        "createdAt": "2025-12-05T12:10:43.602Z",
        "mainPaper": {
            "doi": "10.1785/0220150251",
            "title": "",
            "journal": "",
            "abstract": ""
        },
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "10.1785/0220150251",
            "openCitations": "10.1785/0220150251",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "RSD",
        "name": "seisbench",
        "description": "[![PyPI - License](https://img.shields.io/pypi/l/seisbench)](https://github.com/seisbench/seisbench/blob/main/LICENSE) [![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/seisbench/seisbench/main_push.yml?branch=main)](https://github.com/seisbench/seisbench) [![Read the Docs](https://img.shields.io/readthedocs/seisbench)](https://seisbench.readthedocs.io/en/latest/) [![PyPI](https://img.shields.io/pypi/v/seisbench)](https://pypi.org/project/seisbench/) [![Python 3.9](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/release/python-390/) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5568813.svg)](https://doi.org/10.5281/zenodo.5568813)\n\nThe Seismology Benchmark collection (*SeisBench*) is an open-source python toolbox for\nmachine learning in seismology. It provides a unified API for accessing seismic datasets and both training and applying machine learning algorithms to seismic data. SeisBench has been built to reduce the overhead when applying or developing machine learning techniques for seismological tasks.\n\nTo get started, try one of our [example notebooks](https://seisbench.readthedocs.io/en/stable/pages/examples.html). For more detailed information on SeisBench check out the [SeisBench documentation](https://seisbench.readthedocs.io/)\n\n## Usage examples\n\n### Managing datasets and benchmark data\n\nThe SeisBench `data` module provides everything around data management.\nThis includes a common data format, tools to compile datasets in this format, and pre-compiled benchmark datasets that are hosted in the SeisBench repository.\n\nEach dataset consists of two parts, the metadata and associated seismic waveforms. Waveforms are modeled as dataframes and are flexible in terms of their columns. Here is an example metadata for a dataset.\n\n![Example metadata](https://munchmeyer.de/data/helmholtz_software/data_metadata.png)\n\nTo each row in the metadata, there is an associated waveform. Here is an example three-component waveform trace.\n\n![Example waveform](https://munchmeyer.de/data/helmholtz_software/data_waveforms.png)\n\nInternally, waveforms are stored in hdf5-files. This allows managing large quantities of data while at the same time achieving high throughput rates for the training of deep learning models.\n\nFor further details, check the dataset basics notebook [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/01a_dataset_basics.ipynb) or the creating a dataset notebook [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/03b_creating_a_dataset.ipynb).\n\n### Training ML models\n\nThe SeisBench `model` module contains a wide variety of deep learning models for processing seismic waveforms, for example, for event detection, phase picking, denoising and depth estimation.\nTo efficiently train these models, SeisBench offers custom, modular training pipelines with the `generate` module.\nThese convert the data from the datasets into the input format for each model, provide the correct label format and can also facilitate data augmentation.\nHere's an example of a simple generator pipeline.\n\n![Example generator pipeline](https://munchmeyer.de/data/helmholtz_software/generator.png)\n\nFor more details, check out the notebook on generator pipelines [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/01c_generator_pipelines.ipynb). To learn how to train a deep learning model from scratch using SeisBench datasets and generator pipelines, try the training phasenet notebook [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/03a_training_phasenet.ipynb).\n\n### Deploying ML models\n\nSeisBench bridges the gap between deep learning developers and seismological practitioners by allowing easy deployment of SeisBench models. To this end, SeisBench comes with a large collection of pretrained models and with a unified interface for all models. This interface allows to directly apply the models to all common data formats of seismic data. Here is an example showing the waveforms of an earthquake in Chile together with the predictions from PhaseNet implemented in SeisBench.\n\n![PhaseNet example](https://munchmeyer.de/data/helmholtz_software/phasenet.png)\n\nThis example notebook [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/01b_model_api.ipynb) provides details on how to deploy trained SeisBench models.\n\nSeisBench can also be integrated into larger event detection pipelines. The notebook on creating a seismicity catalog [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/03c_catalog_seisbench_gamma.ipynb) walks users through the steps of phase picking and association on the challenging 2014 Iquique earthquake sequence in Northern Chile. This workflow allows to compile highly complete earthquake catalogs as visualised below.\n\n![2014 Iquique sequence](https://munchmeyer.de/data/helmholtz_software/chile_eqs.png)\n\n## References\n\n* Woollam, J., Münchmeyer, J., Tilmann, F., Rietbrock, A., Lange, D., Bornstein, T., ... & Soto, H. (2022). SeisBench—A toolbox for machine learning in seismology. Seismological Research Letters, 93(3), 1695-1709. https://doi.org/10.1785/0220210324\n\n  _Reference publication for software._\n\n---\n\n* Münchmeyer, J., Woollam, J., Rietbrock, A., Tilmann, F., Lange, D., Bornstein, T., ... & Soto, H. (2022). Which picker fits my data? A quantitative evaluation of deep learning based seismic pickers. Journal of Geophysical Research: Solid Earth, 127(1), e2021JB023499.\nhttps://doi.org/10.1029/2021JB023499\n\n  _Example of in-depth bencharking study of deep learning-based picking routines using the SeisBench framework._\n\n---\n\n## Acknowledgement\n\nThe initial version of SeisBench has been developed at [GFZ Potsdam](https://www.gfz-potsdam.de/) and [KIT](https://www.gpi.kit.edu/) with funding from [Helmholtz AI](https://www.helmholtz.ai/). The SeisBench repository is hosted by [HIFIS - Helmholtz Federated IT Services](https://www.hifis.net/).",
        "url": "#",
        "get_started_url": "https://github.com/seisbench/seisbench",
        "repoDoi": "10.5281/zenodo.5568812",
        "amountPublications": {},
        "createdAt": "2025-12-05T12:10:43.602Z",
        "mainPaper": {
            "doi": "10.5281/zenodo.5568812",
            "title": "seisbench/seisbench: SeisBench v0.10 - SkyNet, SeisDAE and a more powerful model API",
            "journal": "Zenodo",
            "dateReleased": "2025-08-11T00:00:00.000Z",
            "abstract": "",
            "citationsArray": []
        },
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.5568812",
            "openAlex": "10.5281/zenodo.5568812",
            "openCitations": "10.5281/zenodo.5568812",
            "dataCite": "10.5281/zenodo.5568812",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "obspy/obspy",
        "url": "https://github.com/obspy/obspy",
        "description": "ObsPy: A Python Toolbox for seismology/seismological observatories.",
        "stars": 1272,
        "forks": 557,
        "readme": "<img alt=\"ObsPy: A Python Toolbox for seismology/seismological observatories.\" class=\"right\" style=\"width: 60%\" src=\"https://raw.github.com/obspy/website/master/logo/obspy_logo_full_highres.png\" />\n\n[![NumFOCUS affiliated project](https://numfocus.org/wp-content/uploads/2018/01/optNumFocus_LRG.png)](https://numfocus.org/sponsored-projects/affiliated-projects)\n\n[![Github Action Status](https://github.com/obspy/obspy/workflows/tests/badge.svg?event=push)](https://github.com/obspy/obspy/actions)\n[![Coverage Status](https://codecov.io/gh/obspy/obspy/branch/master/graph/badge.svg)](https://codecov.io/gh/obspy/obspy)\n[![Supported Python versions](https://img.shields.io/pypi/pyversions/obspy.svg)](https://pypi.python.org/pypi/obspy/)\n\n[![License](https://img.shields.io/pypi/l/obspy.svg)](https://pypi.python.org/pypi/obspy/)\n[![LGPLv3](https://www.gnu.org/graphics/lgplv3-88x31.png)](https://www.gnu.org/licenses/lgpl.html)\n\n[![PyPI Version](https://img.shields.io/pypi/v/obspy.svg)](https://pypi.python.org/pypi/obspy)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.15309143.svg)](https://doi.org/10.5281/zenodo.15309143)\n[![Conda](https://img.shields.io/conda/dn/conda-forge/obspy?label=conda%20downloads)](https://anaconda.org/conda-forge/obspy)\n\n[![Discourse status](https://img.shields.io/discourse/status?server=https%3A%2F%2Fdiscourse.obspy.org)](https://discourse.obspy.org)\n[![Gitter](https://badges.gitter.im/JoinChat.svg)](https://gitter.im/obspy/obspy?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n[![Announcements Mailing List](https://img.shields.io/badge/mailing%20list-announcements-blue)](https://mail.python.org/mailman3/lists/obspy.python.org/)\n\n<a rel=\"me\" href=\"https://fosstodon.org/@obspy\"><img src=\"https://img.shields.io/mastodon/follow/109309284431089274?domain=https%3A%2F%2Ffosstodon.org&style=social\" alt=\"Mastodon Follow\" /></a>\n\nObsPy is an open-source project dedicated to provide a **Python framework for processing seismological** data. It provides parsers for common file formats, clients to access data centers and seismological signal processing routines which allow the manipulation of seismological time series (see [Beyreuther et al. 2010](http://www.seismosoc.org/publications/SRL/SRL_81/srl_81-3_es/), [Megies et al. 2011](http://www.annalsofgeophysics.eu/index.php/annals/article/view/4838), [Krischer et al. 2015](http://iopscience.iop.org/article/10.1088/1749-4699/8/1/014003)).\n\nThe goal of the ObsPy project is to facilitate **rapid application development for seismology**.\n\nObsPy is licensed under the GNU Lesser General Public License (LGPL) v3.0.\n\nA one-hour introduction to ObsPy is [available at YouTube](https://www.youtube.com/watch?v=kFwdjfiK4gk).\n\n#### [Read more in our GitHub wiki](https://github.com/obspy/obspy/wiki)\n\n### Installation\n\nInstallation instructions can be found in the [wiki](https://github.com/obspy/obspy/wiki#installation).\n\n### Getting started\n\nRead about how to get started in the [wiki](https://github.com/obspy/obspy/wiki#getting-started) and in our [**Tutorial** section in the documentation](http://docs.obspy.org/tutorial/).\n\nObsPy Tutorial notebooks -- and much more on specific seismology topics -- can also be found on [**Seismo-Live**](http://seismo-live.org/), both as a static preview and as interactively runnable version.\n\n[![Link to Seismo-Live](https://user-images.githubusercontent.com/1842780/75337134-b4310a80-588c-11ea-8ed2-dbabdedaedfc.png)](http://seismo-live.org/)\n\n```python\nfrom obspy import read\nst = read()  # load example seismogram\nst.filter(type='highpass', freq=3.0)\nst = st.select(component='Z')\nst.plot()\n```\n\n![Example waveform Plot](https://user-images.githubusercontent.com/1842780/75334711-9d88b480-5888-11ea-8bc8-0bfe7021d79e.png)\n\n### Documentation and Changelog\n\nThe **detailed changelog** is available [here](CHANGELOG.txt), our docs can be found at [docs.obspy.org](http://docs.obspy.org/).\n\n### Contributing\n\nPlease see details on how to contribute to the project [here](CONTRIBUTING.md).\n\n### References\n\n  * Moritz Beyreuther, Robert Barsch, Lion Krischer, Tobias Megies, Yannik Behr and Joachim Wassermann (2010), [ObsPy: A Python Toolbox for Seismology](http://www.seismosoc.org/publications/SRL/SRL_81/srl_81-3_es/), _SRL_, 81(3), 530-533,  doi:`10.1785/gssrl.81.3.530`.\n  * Tobias Megies, Moritz Beyreuther, Robert Barsch, Lion Krischer, Joachim Wassermann (2011), [ObsPy – What can it do for data centers and observatories?](http://www.annalsofgeophysics.eu/index.php/annals/article/view/4838) _Annals Of Geophysics_, 54(1), 47-58, doi:`10.4401/ag-4838`.\n  * Lion Krischer, Tobias Megies, Robert Barsch, Moritz Beyreuther, Thomas Lecocq, Corentin Caudron and Joachim Wassermann (2015), [ObsPy: a bridge for seismology into the scientific Python ecosystem](http://iopscience.iop.org/1749-4699/8/1/014003/), _Computational Science & Discovery_, 8(1), 014003, doi:`10.1088/1749-4699/8/1/014003`\n  * [concept DOI, that always resolves to latest ObsPy versions: `10.5281/zenodo.1040769`](https://doi.org/10.5281/zenodo.1040769) (see [Zenodo FAQ](http://help.zenodo.org/#versioning))\n  * [specific version DOIs for all released ObsPy versions](https://zenodo.org/search?ln=en&p=obspy&action_search=)\n\n### Impact\n\n<img src='https://github.com/user-attachments/assets/bde02dde-8644-4bcf-8f66-481edf108293'>\n",
        "createdAt": "2012-09-08T19:17:46.000Z",
        "updatedAt": "2025-12-03T08:58:41.000Z",
        "language": "Python",
        "homepage": "https://www.obspy.org",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.15309143",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.15309143",
            "dataCite": "10.5281/zenodo.15309143",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/obspy/obspy/master/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.15309143",
            "title": "ObsPy 1.4.2",
            "journal": "Zenodo",
            "dateReleased": "2025-04-30T00:00:00.000Z",
            "abstract": "ObsPy: A Python Toolbox for seismology/seismological observatories.\n\n\nObsPy is an open-source project dedicated to provide a Python framework for processing seismological data. It provides parsers for common file formats, clients to access data centers and seismological signal processing routines which allow the manipulation of seismological time series (see Beyreuther et al. 2010, doi: 10.1785/gssrl.81.3.530 ; Megies et al. 2011, doi: 10.1785/gssrl.81.3.530; Krischer et al. 2015, doi: 10.1088/1749-4699/8/1/014003).\n\n\nThe goal of the ObsPy project is to facilitate rapid application development for seismology.",
            "citationsArray": [
                "10.1785/gssrl.81.3.530",
                "10.4401/ag-4838",
                "10.1088/1749-4699/8/1/014003"
            ]
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "DIG-Kaust/Seismology",
        "url": "https://github.com/DIG-Kaust/Seismology",
        "description": "Teaching material for ErSE 210 Seismology course",
        "stars": 127,
        "forks": 39,
        "readme": "![Seismology](https://github.com/DIG-Kaust/Seismology/blob/main/logo.png)\n\nTeaching material for ErSE 210 Seismology course to be held at KAUST during the Fall semester.\n\n## Material\n\nThe repository is organized as follows:\n\n- **Slides**: deck of slides summarizing the key concept introduced in each class. Some of the figures in these slides are taken from the reference textbook (Shearer, P., Introduction to Seismology). \n- **Data**: input data used in the practical sessions:\n- All of the other folders in this repository contains Python codes and Jupyter Notebooks used in the practical sessions:\n\n   - [**PlaneWave**](https://github.com/DIG-Kaust/Seismology/blob/main/PlaneWave/PlaneWave.ipynb): create and display plane waves in time-space and wavenumber domain.\n   - [**GassmannFluidSub**](https://github.com/DIG-Kaust/Seismology/blob/main/GassmannFluidSub/Gassmann.ipynb): implement basic rock physics equations and Gassmann substitution and apply it to the Smehaia well log.\n   - [**SeismicModelling**](https://github.com/DIG-Kaust/Seismology/blob/main/SeismicModelling/SeismicModellingInversion.ipynb): perform convolutional and AVO modelling, and apply pre-stack inversion.\n   - [**RayTrace**](https://github.com/DIG-Kaust/Seismology/blob/main/RayTrace/RayTrace.ipynb): implement 2D raytracing by solving the associated ODE.\n   - [**SeismicTomography**](https://github.com/DIG-Kaust/Seismology/blob/main/SeismicTomography/SeismicTomography.ipynb): create the 2D tomographic matrix and solve the associated inverse problem.\n   - [**ReflectionSeismic**](https://github.com/DIG-Kaust/Seismology/blob/main/ReflectionSeismic): implement basic NMO processing and learn how to work with SEGY files using *segyio* and the Volve dataset.\n   - [**TimeMigration**](https://github.com/DIG-Kaust/Seismology/blob/main/TimeMigration/TimeMigration.ipynb): implement basic time-domain post-stack Kirchhoff demigration-migration on simple synthetic dataset.\n   - [**Dispersion**](https://github.com/DIG-Kaust/Seismology/blob/main/Dispersion/Dispersion.ipynb): create a surface-wave only seismic dataset, compute dispersion panel and perform surface wave dispersion curve inversion.\n   - [**Obspy**](https://github.com/DIG-Kaust/Seismology/blob/main/Obspy/ObspyIntro.ipynb): a short introduction to Obspy and its usage for epicenter localization of earthquakes\n\n\n## Environment\n\nTo ensure reproducibility of the results, we have provided an `environment.yml` file. Ensure to have installed Anaconda or Miniconda on your computer. If you are not familiar with it, we suggest using the \n[KAUST Miniconda Install recipe](https://github.com/kaust-rccl/ibex-miniconda-install). This has been tested both on macOS and Unix operative systems.\n\nAfter that simply run:\n```\n./install_env.sh\n```\nIt will take some time, if at the end you see the work `Done!` on your terminal you are ready to go!\n\n## Binder\n\nAlternatively, you can work directly on Binder. Simply click this button and access\nthe material from your web browser without the need for any local installation\n\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/DIG-Kaust/Seismology/HEAD)",
        "createdAt": "2021-04-30T18:29:11.000Z",
        "updatedAt": "2025-12-03T08:46:39.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/DIG-Kaust/Seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "TonprasertW/CSPY",
        "url": "https://github.com/TonprasertW/CSPY",
        "description": "Computational Seismology Python (CSPY)",
        "stars": 0,
        "forks": 0,
        "readme": "# CSPY\nComputational Seismology Python (CSPY)\n",
        "createdAt": "2024-12-15T16:39:11.000Z",
        "updatedAt": "2024-12-15T16:39:15.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/TonprasertW/CSPY/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "noisepy/NoisePy",
        "url": "https://github.com/noisepy/NoisePy",
        "description": "Ambient-Noise Seismology Package",
        "stars": 205,
        "forks": 83,
        "readme": "# About NoisePy\nNoisePy is a Python package designed for fast and easy computation of ambient noise cross-correlation functions. It provides additional functionality for noise monitoring and surface wave dispersion analysis.\n\n[![Documentation Status](https://github.com/noisepy/NoisePy/actions/workflows/notebooks.yml/badge.svg)](https://noisepy.github.io/NoisePy/)\n[![Build Status](https://github.com/noisepy/NoisePy/actions/workflows/test.yaml/badge.svg)](https://github.com/noisepy/NoisePy/actions/workflows/test.yaml)\n[![Codecov](https://codecov.io/gh/noisepy/NoisePy/branch/main/graph/badge.svg)](https://codecov.io/gh/noisepy/NoisePy)\n[![DOI](https://zenodo.org/badge/157871462.svg)](https://zenodo.org/badge/latestdoi/157871462)\n\n<img src=\"https://raw.githubusercontent.com/noisepy/NoisePy/main/docs_old/figures/logo.png\" width=\"800\" height=\"400\">\n\n## Major updates coming\nNoisePy is going through a major refactoring to make this package easier to develop and deploy. Submit an issue, fork the repository and create pull requests to [contribute](CONTRIBUTING.md).\n\n## Installation\nThe nature of NoisePy being composed of python scripts allows flexible package installation, which is essentially to build dependent libraries the scripts and related functions live upon. We recommend using [conda](https://docs.conda.io/en/latest/) or [pip](https://pypi.org/project/pip/) to install.\n\n**Note the order of the command lines below matters**\n\n### With Conda and pip\n```bash\nconda create -n noisepy -y python=3.10 pip\nconda activate noisepy\npip install noisepy-seis\n```\n\nTo add jupyter dependencies, install them\n```\npip install ipykernel notebook\npython -m ipykernel install --user --name noisepy\n```\n\n### With Conda and pip and MPI support\n```bash\nconda create -n noisepy -y python=3.10 pip mpi4py\nconda activate noisepy\npip install noisepy-seis[mpi]\n```\n\n### With virtual environment\n```bash\npython -m venv noisepy\nsource noisepy/bin/activate\npip install noisepy-seis\n```\n\n### With virtual environment and MPI support\nAn MPI installation is required. E.g. for macOS using [brew](https://brew.sh/) :\n```bash\nbrew install open-mpi\n```\n\n```bash\npython -m venv noisepy\nsource noisepy/bin/activate\npip install noisepy-seis[mpi]\n```\n\n## Functionality\nHere is a list of features of the package:\n* download continuous noise data based:\n   + on webservices using obspy's core functions of [get_station](https://docs.obspy.org/packages/autogen/obspy.clients.fdsn.client.Client.get_stations.html) and [get_waveforms](https://docs.obspy.org/packages/autogen/obspy.clients.fdsn.client.Client.get_waveforms.html)\n   + on AWS S3 bucket calls, with a test on the SCEDC AWS Open Dataset.\n* save seismic data in [ASDF](https://asdf-definition.readthedocs.io/en/latest/) format, which conveniently assembles meta, wavefrom and auxililary data into one single file ([Tutorials](https://github.com/SeismicData/pyasdf/blob/master/doc/tutorial.rst) on reading/writing ASDF files)\n* offers scripts to precondition data sets before cross correlations. This involves working with gappy data from various formats (SAC/miniSEED) and storing it on local in ASDF.\n* performs fast and easy cross-correlation with functionality to run in parallel through [MPI](https://en.wikipedia.org/wiki/Message_Passing_Interface)\n* **Applications module**:\n   + *Ambient noise monitoring*: measure dv/v using a wide variety of techniques in time, fourier, and wavelet domain (Yuan et al., 2021)\n   + *Surface wave dispersion*: construct dispersion images using conventional techniques.\n\n## Usage\n\nTo run the code on a single core, open the terminal and activate the noisepy environment before run following commands. To run on institutional clusters, see installation notes for individual packages on the module list of the cluster.\n\n### Deploy using Docker\nWe use I/O on disk, so users need root access to the file system. To install rootless docker, see instructions [here](https://docs.docker.com/engine/security/rootless/#install).\n```bash\ndocker pull  ghcr.io/noisepy/noisepy:latest\ndocker run -v ~/tmp:/tmp ghcr.io/noisepy/noisepy:latest cross_correlate --path /tmp\n```\n\n## Tutorials\nShort tutorials on how to use NoisePy can be is available [here](https://noisepy.github.io/NoisePy/) and can be run directly in Colab. These tutorials present simple examples of how NoisePy might work. We strongly encourage you to download the NoisePy package and play it on your own! If you have any comments and/or suggestions during running the codes, please do not hesitate to contact us through email or open an issue in this github page!\n\nChengxin Jiang (chengxinjiang@gmail.com)\nMarine Denolle (mdenolle@uw.edu)\nYiyu Ni (niyiyu@uw.edu)\n\n### Taxonomy\nTaxonomy of the NoisePy variables.\n\n* ``station`` refers to the site that has the seismic instruments that records ground shaking.\n* ``channel`` refers to the direction of ground motion investigated for 3 component seismometers. For DAS project, it may refers to the single channel sensors.\n* ``ista`` is the index name for looping over stations\n* ``cc_len`` correlation length, basic window length in seconds\n* ``step`` is the window that get skipped when sliding windows in seconds\n* ``smooth_N`` number of points for smoothing the  time or frequency domain discrete arrays.\n* ``maxlag`` maximum length in seconds saved in files in each side of the correlation (save on storage)\n* ``substack, substack_windows`` boolean, number of window over which to substack the correlation (to save storage or do monitoring).\n* ``time_chunk, nchunk`` refers to the time unit that defined a single job. for instance, ``cc_len`` is the correlation length (e.g., 1 hour, 30 min), the overall duration of the experiment is the total length (1 month, 1 year, ...). The time chunk could be 1 day: the code would loop through each cc_len window in a for loop. But each day will be sent as a thread.\n\n## Acknowledgements\nThanks to our contributors so far!\n\n[![Contributors](https://contrib.rocks/image?repo=noisepy/NoisePy)](https://github.com/noisepy/NoisePy/graphs/contributors)\n\n### Use this reference when publishing on your work with noisepy\n\nMain code:\n\n* Zenodo DOI: [noisepy/NoisePy](https://zenodo.org/badge/latestdoi/157871462)\n* Jiang, C. and Denolle, M. [NoisePy: a new high-performance python tool for seismic ambient noise seismology.](https://doi.org/10.1785/0220190364) _Seismological Research Letter_ 91, no. 3 (2020): 1853–1866. https://doi.org/10.1785/0220190364\n\nAlgorithms used:\n* (data pre-processing) Seats, K. J., Jesse F. L., and German A. P. [Improved ambient noise correlation functions using Welch′ s method.](https://doi.org/10.1111/j.1365-246X.2011.05263.x) _Geophysical Journal International_ 188, no. 2 (2012): 513-523. https://doi.org/10.1111/j.1365-246X.2011.05263.x\n\n* (dv/v in wavelet domain) Yuan, C., Bryan, J. T., and Denolle, M. [Numerical comparison of time-, frequency- and wavelet-domain methods for coda wave interferometry.](https://doi.org/10.1093/gji/ggab140) _Geophysical Journal International_ 226, no. 2 (2021): 828-846. https://doi.org/10.1093/gji/ggab140\n\n* (optimal stacking) Yang X, Bryan J, Okubo K, Jiang C, Clements T, Denolle MA. [Optimal stacking of noise cross-correlation functions/](https://doi.org/10.1093/gji/ggac410) _Geophysical Journal International_. 2023 Mar;232(3):1600-18. https://doi.org/10.1093/gji/ggac410\n\n* (cloud-native support) Ni, Y., Denolle, M. A., Münchmeyer, J., Wang, Y., Feng, K. F., Garcia Jurado Suarez, C., ... & Mencin, D. (2025). A review of cloud computing and storage in seismology. Geophysical Journal International, 243(1), ggaf322. https://doi.org/10.1093/gji/ggaf322\n\nThis research received software engineering support from the University of Washington’s Scientific Software Engineering Center ([SSEC](https://escience.washington.edu/software-engineering/ssec/)) supported by Schmidt Futures, as part of the Virtual Institute for Scientific Software (VISS). We would like to acknowledge [Carlos Garcia Jurado Suarez](https://github.com/carlosgjs) and [Nicholas Rich](https://github.com/nrich20) for their collaboration and contributions to the software.\n",
        "createdAt": "2018-11-16T13:30:48.000Z",
        "updatedAt": "2025-12-02T21:17:37.000Z",
        "language": "Python",
        "homepage": "https://noisepy.github.io/NoisePy/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/noisepy/NoisePy/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "bscully-geophysics/geophysics-portfolio",
        "url": "https://github.com/bscully-geophysics/geophysics-portfolio",
        "description": "Projects in seismology, crustal modeling, and geophysical data analysis",
        "stars": 0,
        "forks": 0,
        "readme": "# geophysics-portfolio\nProjects in seismology, crustal modeling, and geophysical data analysis\n",
        "createdAt": "2025-08-05T11:58:41.000Z",
        "updatedAt": "2025-08-05T12:01:17.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/bscully-geophysics/geophysics-portfolio/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "xumi1993/SurfATT-iso",
        "url": "https://github.com/xumi1993/SurfATT-iso",
        "description": "Surface wave Adjoint Travel-time Tomography",
        "stars": 27,
        "forks": 4,
        "readme": "# SurfATT\n\n[![Language](https://img.shields.io/badge/-Fortran-734f96?logo=fortran&logoColor=white)](https://github.com/topics/fortran)\n[![License](https://img.shields.io/github/license/xumi1993/seispy)]()\n[![Build SurfATT](https://github.com/xumi1993/SurfATT-iso/actions/workflows/build.yml/badge.svg)](https://github.com/xumi1993/SurfATT-iso/actions/workflows/build.yml)\n\nThis is an innovative package for **Surf**ace wave **A**djoint **T**ravel-time **T**omography driven by modern fortran with highlights:\n\n- Calculation of surface wave travel time based on **Eikonal equation** with fast sweeping method ([Tong, 2021a](https://doi.org/10.1029/2021JB021818))\n- Computation of sensitivity kernels through **adjoint method** ([Tong, 2021b](https://doi.org/10.1029/2021JB022365))\n- **Multi-grid model parametrization** utilization in optimization ([Tong et al., 2019](https://doi.org/10.1093/gji/ggz151))\n- Consideration of **surface topography** ([Hao et al., 2024a](https://doi.org/10.1029/2023JB027454))\n\n## Gallery\n\n### Travel time field and sensitivity kernel on curved surface ([Hao et al., 2024a](https://doi.org/10.1029/2023JB027454))\n![jgrb56585-fig-0001-m](https://github.com/xumi1993/SurfATT-iso/assets/7437523/49e205a3-7529-4079-a8c2-471c6e7075fc)\n-------\n\n### Tomographic results of S-wave velocity beneath Hawaii Island\n![Fig2](https://github.com/xumi1993/SurfATT-iso/assets/7437523/f9a0155b-7b83-4970-914d-f13dc42b11e5)\n\n## Installation\n\nPlease refer to the [installation guide](https://surfatt.xumijian.me/installation/dependence.html) for detailed instructions.\n\n## How to use SurfATT\nThe executable file `bin/surfatt_tomo` for inverting surface dispersion data for S-wave velocity can be run with `mpirun` as:\n```\nmpirun -np 4 bin/surfatt_tomo -i input_params.yml\n```\n\n### A quick example\nA case named `test/00_checkerboard_iso` presents an example of inversion for 2x3x2 checkers using ambient noise surface wave data from 25 stations. execute `run_this_example.sh` to run this example under 8 processors.\n",
        "createdAt": "2024-04-02T09:02:52.000Z",
        "updatedAt": "2025-12-03T07:33:17.000Z",
        "language": "Fortran",
        "homepage": "https://surfatt.xumijian.me",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/xumi1993/SurfATT-iso/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "cvergara3/seismology_thesis_vmm_nsb",
        "url": "https://github.com/cvergara3/seismology_thesis_vmm_nsb",
        "description": "Este repositorio sintetiza e integra los códigos utilizados para el desarrollo del trabajo de grado titulado: SISMICIDAD E IMPLICACIONES SISMOTECTÓNICAS DE UNA ZONA DEL CENTRO – ORIENTE DE LA CUENCA DEL VALLE MEDIO DEL MAGDALENA (VMM) Y NIDO SÍSMICO DE BUCARAMANGA con énfasis en detección automática de fases sísmicas y relocalización de sismos.",
        "stars": 0,
        "forks": 0,
        "readme": "# seismology_thesis_vmm_nsb\nEl presente repositorio integra los códigos de Python utilizados para el desarrollo del trabajo de grado titulado \"SISMICIDAD E IMPLICACIONES SISMOTECTÓNICAS DE UNA ZONA DEL CENTRO – ORIENTE DE LA CUENCA DEL VALLE MEDIO DEL MAGDALENA (VMM) Y NIDO SÍSMICO DE BUCARAMANGA\", así como los archivos de salida y resultados del proyecto de investigación con enfoque en la relocalización relativa de sismos. A continuación se explica el contenido de cada uno de los algoritmos, incluyendo PhaseNet y la geración de archivos de entrada para ejecutar GrowClust.\n\n#### 01_procesamiento_catalogos.py\nEste código se implementa para la unificación y eliminación de eventos duplicados entre catálogos del SGC y el ISC para la zona de estudio. Asimismo, se grafica un histograma del catálogo final por año y tipo de catálogo. La fuente de donde se descargó la información sismológica a nivel nacional es: http://bdrsnc.sgc.gov.co/paginas1/catalogo/index.php; y el catálogo internacional se descargó de: http://www.isc.ac.uk/iscbulletin/.\n\n#### 02_descarga_unificacion.py\nSe utilizó este código para la descarga masiva de formas de onda de todas las estaciones en la zona de estudio. Además, se agrupan los sismogramas en archivos independientes por estación y fecha. Los archivos finales son tipo .mseed. La fuente de información de donde se obtienen las formas de onda es: http://sismo.sgc.gov.co:8080/fdsnws/dataselect/1/builder. \n\n#### 03_comando_auto_phasenet.py\nEste código corresponde a una modificación realizada al código original de PhaseNet, con el fin de poder ejecutar el algoritmo masivamente para cada uno de los archivos descargados y unificados de la fase 2. El algoritmo PhaseNet se tomó del siguiente repositorio de GitHub: https://github.com/AI4EPS/PhaseNet/blob/master/docs/README.md.\n\n#### 04_unificacion_cat_fdo.py\nEl código permite la integración del catálogo final de sismicidad (Catalogo Final Tesis.csv en el presente repositorio) con los tiempos de arribo detectados automáticamente con PhaseNet. Se asignan ventanas de tiempo para poder amarrar ambas bases de datos y generar el catálogo de sismicidad input a GrowClust. El archivo final después de correr GrowClust corresponde a catalogo_reloc_total.csv en el presente repositorio).\n\n#### 05_inputs_growclust.py\nEl código se subdivide en tres secciones: Procesamiento Origins, Procesamiento Picks y Procesamiento Stations. Se encarga de procesar los catálogos, encontrando pares de eventos cercanos detectados en estaciones en común, generar correlaciones cruzadas y sintetizar las estaciones sismológicas en formato .pkl. Estos archivos son el input para la ejecución final de GrowClust en Fortran.\n\nPor último, se adjuntan el catálogo unificado de sismicidad con información de sismos entre 1980 y 2023 en la zona de estudio (Valle Medio del Magdalena y Nido Sísmico de Bucaramanga) y el catálogo relocalizado (salida de GrowClust) con datos entre 2016 y 2021.\n",
        "createdAt": "2024-05-07T16:54:03.000Z",
        "updatedAt": "2024-05-07T19:02:36.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/cvergara3/seismology_thesis_vmm_nsb/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "seismo-learn/seismology101",
        "url": "https://github.com/seismo-learn/seismology101",
        "description": "Tutorials for absolute beginners in Seismology",
        "stars": 89,
        "forks": 20,
        "readme": "# 地震“学”科研入门教程\n\n[![Jupyter Book Badge](https://raw.githubusercontent.com/jupyter-book/jupyter-book/next/docs/media/images/badge.svg)](https://seismo-learn.org/seismology101/)\n[![Deploy](https://github.com/seismo-learn/seismology101/actions/workflows/deploy.yml/badge.svg)](https://github.com/seismo-learn/seismology101/actions/workflows/deploy.yml)\n[![License: CC BY-NC 4.0](https://img.shields.io/badge/License-CC%20BY--NC%204.0-blue.svg)](https://creativecommons.org/licenses/by-nc/4.0/deed.zh-hans)\n\n本教程主要面向地震学新手，包括地震学专业的高年级本科生、低年级研究生以及\n其他刚接触地震学的科研人员。\n\n本教程的主要目的是帮助地震学新手快速入门，以尽快开展实际的科研工作。其既可以\n作为地震学新手的入门自学材料，也可以作为地震学研究组的入门培训材料。\n\n- 主页：https://seismo-learn.org/seismology101/\n- 源码：https://github.com/seismo-learn/seismology101\n\n## 文档维护\n\n本文档尚有很多不完善之处，欢迎读者参与到文档的维护与更新中。\n详情见[贡献指南](https://seismo-learn.org/contributing/)。\n\n## 许可协议\n\n本作品采用 [知识共享署名-非商业性使用 4.0 国际许可协议 (CC BY-NC 4.0)](https://creativecommons.org/licenses/by-nc/4.0/deed.zh-hans) 。\n任何人都可以自由地分享、修改本作品，但必须遵循如下条件：\n\n- 署名：必须提到原作者，提供指向此许可协议的链接，表明是否有做修改\n- 非商业性使用：不能对本作品进行任何形式的商业性使用\n",
        "createdAt": "2020-12-28T08:15:00.000Z",
        "updatedAt": "2025-12-04T06:29:16.000Z",
        "language": "Shell",
        "homepage": "https://seismo-learn.org/seismology101/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/seismo-learn/seismology101/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "kfp10/seismology",
        "url": "https://github.com/kfp10/seismology",
        "description": "seismology files",
        "stars": 0,
        "forks": 0,
        "readme": "# seismology\nseismology files\n",
        "createdAt": "2015-01-21T13:48:24.000Z",
        "updatedAt": "2015-01-21T13:48:24.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/kfp10/seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Specufex/specufex",
        "url": "https://github.com/Specufex/specufex",
        "description": "Probabilistic unsupervised feature extraction from seismic spectrograms for machine learning.",
        "stars": 18,
        "forks": 6,
        "readme": "# SpecUFEx\n\n![SCOPED](https://img.shields.io/endpoint?url=https://runkit.io/wangyinz/scoped/branches/master/Specufex)\n[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n![Build](https://github.com/ngroebner/specufex/actions/workflows/python-app.yml/badge.svg?branch=main)\n\nSpecUFEx stands for \"Unsupervised Spectral Feature Extraction\", an unsupervised machine learning algorithm to characterize time variations in spectral content of waveform data. We apply the method to earthquake seismograms. SpecUFEx combines probabilistic non-negative matrix factorization (NMF) and hidden Markov modeling (HMM) of spectrograms (short time Fourier transforms of waveform data) to generate \"fingerprints\", low dimensional representations of spectral variation through time. Both the NMF and HMM models are fit using stochastic variational inference; the method is therefore scalable to tens or hundreds of thousands of spectrograms. The resulting fingerprints can be used as features for either unsupervised (e.g. clustering) or supervised (e.g. classification) machine learning. The method is described in\n\n[Holtzman, B.K., Paté, A., Paisley, J., Waldhauser, F., Repetto, D.: Machine learning reveals cyclic changes in seismic source spectra in geysers geothermal field. Science advances 4(5) (2018)](https://advances.sciencemag.org/content/4/5/eaao2929)\n\nPlease cite this article if you use the package for research purposes.\n\nThis repository is a python port of Matlab code originally written by John Paisley at Columbia University. All python code included is written by Ben Holtzman, Theresa Sawi and Nate Groebner.\n\n## Installation\n\nClone this repository to your computer, cd to the directory, and use `pip` to install.\n\n``` shell\ngit clone https://github.com/specufex/specufex.git\ncd specufex\npip install .\n```\nIf you intend to use the example files with the tutorials, you may have to install [git LFS](https://git-lfs.com) (Large File System) and pull the files. Note: you need to be in te base directory of the repository for the ```git lfs pull``` command to work.\n\n``` shell\ngit lfs install\ngit lfs pull\n```\n\nAlternatively, a Dockerfile is included that builds a container running Jupyterlab with an environment setup for SpecUFEx. A prebuilt container is available through the [SCOPED](https://github.com/SeisSCOPED) project [here](https://github.com/SeisSCOPED/specufex/pkgs/container/specufex). Or you can directly pull the container if you have Docker:\n\n```bash\ndocker pull ghcr.io/seisscoped/specufex:latest\n```\n\n## Usage\n\n### Fitting models and transforming data\n\nSpecUFEx fits a group of $D x M$ spectrograms, where D is the number of rows (frequency bands) and M is the number of columns (timesteps) in each spectrogram. The spectrograms must be in a numpy-compatible matrix of dimension $N x D x M$, N being the number of spectrograms in the dataset. Each spectrogram must consist of all nonnegative (>=0) entries. (Note, this is not yet checked for.)\n\nThe two main classes in this package are `BayesianNonparametricNMF` and `BayesianHMM`. Each has fit, transform, and fit_transform methods to be consistent with the Scikit-learn API style.\n\nThe first step is to calculate the nonnegative matrix factorization of your data. This is done by creating a new `BayesianNonParametricNMF` object and calling its `fit` method. This function estimates the model parameters based on all of the data in X.  In the future, we hope to create a convergence criterion based on the ELBO. We iteratively fit the model, one spectrogram at a time, selecting a random spectrogram from our data set. In the example below, `X` is the numpy matrix of spectrograms. Please note that your data must be nonnegative; i.e., all elements must be >= 0.\n\n```python\nnmf = BayesianNonparametricNMF(X.shape)\n\nbatches = 10000\nbatch_size = 1\n\nfor i in range(batches):\n    idx = np.random.randint(X.shape[0], size-batch_size)\n    nmf.fit(X[idx])\n```\n\nThis finds the left matrix of the NMF of the data. Transform the data to the reduced representation, Hs, (the right matrix) via\n\n```python\nVs = nmf.transform(X)\n```\n\nPro tip: a step can be saved by the convenience method `fit_transform, which does the fitting and transformation in one command.  Note, however, that this can take a long time, so you may want to do this in pieces so you can save the resulting NMF left matrix in case something goes wrong (like a power outage).\n\nNext, fit the HMM model with the BayesianHMM class. Currently, in order to setup the object correctly the number of NMF patterns (`num_pat`) and the gain calculated by `BayesianNonparametricNMF` are passed to the constructor.\n\n```python\nhmm = BayesianHMM(nmf.num_pat, nmf.gain)\n\nbatches = 10000\nbatch_size = 1\n\nfor i in range(batches):\n    idx = np.random.randint(Vs.shape[0], size-batch_size)\n    hmm.fit(Vs[idx])\n```\n\nSimilar to the NMF calculation, the data are transformed to fingerprints with the `transform` function.\n\n```python\nfingerprints, As, gams = hmm.transform(Vs)\n```\n\nOr, if you want to save a step, use `fit_transform` like above.\n\n```python\nfingerprints, As, gams = hmm.fit_transform(Vs, nmf.EW)\n```\n\nThe variable `fingerprints` has the calculated fingerprints (the ultimate matrices of interest), `As` has the state transition matrices of each spectrogram, and `gams` are the state sequence probability matrices.\n\n### Saving and loading models\n\nOnce you have fit either the NMF or HMM model (or both!) you can save the parameters for the model using built in functions. From the above examples where `nmf` and `hmm` are objects that contain trained models, simply use\n\n```shell\nnmf.save(filename)\nhmm.save(filename)\n```\n\nto save the parameters. Likewise, to load an already saved model and instatiate a new model object use\n\n```shell\nnmf = BayesianNonparametricNMF.load(filename)\nhmm = BayesianHMM.load(filename)\n```\n\nand now you have NMF and HMM models that are ready to transform your data.\n\n### Development\n\nIf you are interested in contributing to SpecUFEx development, please fork this repository and create a new branch for your new code. Create a developemnt environment with conda or virtualenv and install the dev dependencies with `pip install -r requirements-dev.txt`.  Code formatting is done with [ruff](https://docs.astral.sh/ruff/) and is performed on every commit with [pre-commit](https://pre-commit.com). Please write tests for the code you develop. We use [pytest](https://docs.pytest.org/en/7.1.x/) and [nox](https://nox.thea.codes/en/stable/) for writing and running tests. When your code is done, submit a pull request.\n",
        "createdAt": "2022-06-09T21:57:30.000Z",
        "updatedAt": "2025-10-23T00:44:19.000Z",
        "language": "Jupyter Notebook",
        "homepage": "https://Specufex.github.io/specufex/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Specufex/specufex/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "bright0516/IntelligentConsultationForSeismology",
        "url": "https://github.com/bright0516/IntelligentConsultationForSeismology",
        "description": "Automaticly generate seismological conference PPT file.",
        "stars": 0,
        "forks": 0,
        "readme": "# IntelligentConsultationForSeismology\nAutomaticly generate seismological conference PPT file.\n",
        "createdAt": "2022-05-08T08:06:28.000Z",
        "updatedAt": "2022-05-20T02:32:21.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/bright0516/IntelligentConsultationForSeismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "seismo-live/seismo_live",
        "url": "https://github.com/seismo-live/seismo_live",
        "description": "Live Jupyter Notebooks for Seismology",
        "stars": 87,
        "forks": 75,
        "readme": "## Live Jupyter Notebooks for Seismology\n\n### Building the Website\n\nIt is currently a two step procedure:\n\nFirst build all notebooks. This takes the jupytext files, creates an exercise\nand solution version of each if necessary, converts both to ipynb, runs them,\nand renders to the outputs to HTML.\n\nAll the output will be store in in `built_notebooks`.\n\n```bash\n$ python conf/convert_to_ipynb.py notebooks built_notebooks\n```\n\nThe second step takes these outputs and create the final website which is\nstored in `final_website`:\n\n```bash\n$ python conf/build_website.py built_notebooks final_website\n```\n\n### Contributing\n\nWe intend seismo-live to be a place to collect all kinds of tutorial and notebooks related to seismology so contributions are gladly accepted and actually crucial for the success of the whole project. To contribute make sure you have the same installation, especially **Python version 3.5**, as documented below. If you require additional packages please mention it in your pull request. Once your environment is setup, create your new notebooks and send us a pull request. Tutorials on how to do that can be found [here](https://yangsu.github.io/pull-request-tutorial/) and [here](https://www.thinkful.com/learn/github-pull-request-tutorial/) and lots of other places online. If you need help, don't hesitate to contact us.\n\n**New contributors, please sign this:** https://www.clahub.com/agreements/krischer/seismo_live\n\n### Server Installation\n\nThis explains how to install seismo-live on a server. For a local installation see below.\n\nBased on: https://github.com/jupyter/tmpnb\n\n#### Installation\n\nInstall Docker for you platform: http://docs.docker.com/installation\n\nDon't use the repository version as that might be very old.\n\n```bash\n# Add the current user to the docker group\nsudo usermod -a -G docker USERNAME\n\n# git is also required, install if not available.\nsudo apt-get install git\n\n# Furthermore `make` must be available.\nsudo apt-get install build-essential\n\n# Checkout the repository (a shallow clone is enough)\ngit clone --depth=1 https://github.com/krischer/seismo_live.git\n\ncd seismo_live\n# Can take quite a while!\nmake build\n```\n\n#### Running\n\nTo start it, edit the Makefile to set the desired number of Docker workers and available containers and start it with\n\n```bash\nmake fresh_start\n```\n\n#### Stop it\n\n```bash\nmake nuke\n```\n\n#### Help\n\n```bash\nmake help\n```\n\n### Local Installation\n\nYou might be interested in running the notebooks locally on your own computer. A big advantage is that any changes you make will no longer be deleted. You can also contribute changes you made (or entirely new notebooks) back to the seismo-live project!\n\nThe notebooks as of now require:\n\n- Python 3.5\n- The scientific Python stack (NumPy, SciPy, matplotlib)\n- The Jupyter notebooks\n- ObsPy >= 1.0.1\n- Instaseis\n\nWe recommend to install ObsPy via Anaconda as written [in its installation instructions](https://github.com/obspy/obspy/wiki/Installation-via-Anaconda). Then install Instaseis (does not work on Windows) according to [its documentation](http://instaseis.net/#installation). Finally install the Jupyter project with\n\n```bash\n$ conda install jupyter\n```\n\nNow just clone the project from Github, cd to the correct folder and launch the notebook server.\n\n```bash\n$ git clone --depth=1 https://github.com/seismo-live/seismo_live_build.git\n$ cd seismo_live_build/notebooks\n$ jupyter-notebook\n```\n\nPlease note that the Instaseis notebooks require a local database symlinked to `seismo_live/notebooks/Instaseis/data/database`. \n\n[//]: You could get one for example with:\n[//]: ```bash\n[//]: $ wget -qO- \"http://www.geophysik.uni-muenchen.de/~krischer/instaseis/20s_PREM_ANI_FORCES.tar.gz\" | tar xvz -C 20s_PREM_ANI_INSTASEIS_DB\n[//]: ```\n",
        "createdAt": "2015-10-26T08:00:42.000Z",
        "updatedAt": "2025-10-21T14:47:25.000Z",
        "language": "Python",
        "homepage": "http://seismo-live.org",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/seismo-live/seismo_live/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "srivastavaresearchgroup/DL-HRGNSS",
        "url": "https://github.com/srivastavaresearchgroup/DL-HRGNSS",
        "description": null,
        "stars": 4,
        "forks": 2,
        "readme": "# DL-HRGNSS\nDL-HRGNSS\n\n### GNSS_Magnitude_V1\n\nThis is our preliminary DL model based on a convolutional neural network \nfor magnitude estimation from HR-GNSS data (1 Hz sampling rate).\n\nWe have trained the model for three cases:\n\n- Magnitude estimation from 3 stations, 181 seconds, 3 components.\n- Magnitude estimation from 7 stations, 181 seconds, 3 components.\n- Magnitude estimation from 7 stations, 501 seconds, 3 components.\n\n\n## Related work:\n\nClaudia Quinteros-Cartaya, Jonas Köhler, Wei Li, Johannes Faber,\nNishtha Srivastava, Exploring a CNN model for earthquake magnitude\nestimation using HR-GNSS data, Journal of South American\nEarth Sciences, 2024, 104815, ISSN 0895-9811,\nhttps://doi.org/10.1016/j.jsames.2024.104815.\n\n\n## Getting Started\n\n# Clone the repository\n\n```\ngit clone https://github.com/srivastavaresearchgroup/DL-HRGNSS\n```\n\n# Install dependencies (with python 3.8)\n\n(virtualenv is recommended)\n\n```\npip install -r requirements.txt\n```\n\n# Data\n\nThe database for each configuration/case is in `./data`.\nFor example the data folder: `./data/GNSS_M3S_181` contains the data for \n3 stations, 181 seconds.\n\nThe data is in numpy format, previously selected from the open-access \ndatabase published by Lin et al., 2020 \n(https://doi.org/10.5281/zenodo.4008690).\n\nYou can find the information related to the data (ID, Hypocenter, \nMagnitude) in the dataframes info_data.csv located in the same folder as \nthe respective data.\n\nYou can use Data_plot.ipynb to plot the waveforms.\n\nRefer to Quinteros et al., 2024 (https://doi.org/10.1016/j.jsames.2024.104815) for \nmore details about the data configuration.\n\n# Pretrained models and results\n\nThe pre-trained  models are located in `./trained_models`.\nThese models are described in Quinteros et al., 2024 \n(https://doi.org/10.1016/j.jsames.2024.104815).\nYou can find the respective results in `./predictions`.\n\n# Processing and output\n\nIf you want to change the default configuration, you can edit the \nvariables in `main.py`.\n\nConfiguration parameters you should change, depending on your choice:\n- Number of stations (nst)\n- Time window length (nt)\n- Paths/folder names\n\nTo split, train and test the data, run:\n\n```\npython main.py\n```\n\nThe outputs will be located in the path that you set.\n\nBy default the outputs will be saved in `./tests`, and in the subfolders:\n\n- `./data_inf`: you can find the numpy files with the data index for \ntrain, validation, and test dataset, associated with the initial \nxdata.npy file.\n- `./models`: trained models are saved in this folder\n- `./predictions`: the predicted magnitude and error values are saved in \ntext files in this folder.\n- `./out_log`: the output from logging is saved in this folder.\n\n",
        "createdAt": "2023-03-27T10:10:57.000Z",
        "updatedAt": "2025-05-12T02:03:43.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.4008690",
            "dataCite": "10.5281/zenodo.4008690",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/srivastavaresearchgroup/DL-HRGNSS/main/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.4008690",
            "title": "Chilean Subduction Zone rupture scenarios and waveform data",
            "journal": "Zenodo",
            "dateReleased": "2020-08-28T00:00:00.000Z",
            "abstract": "Chilean Subduction Zone kinematic rupture scenarios and waveform data for the submitted work <strong>Early warning for great earthquakes from characterization of crustal deformation patterns with deep learning (2020)</strong> by Lin et al.",
            "citationsArray": []
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "arbCoding/sac-format",
        "url": "https://github.com/arbCoding/sac-format",
        "description": "Single-header SAC (Seismic Analaysis Code) I/O library written in C++20.",
        "stars": 6,
        "forks": 1,
        "readme": "",
        "createdAt": "2023-04-07T22:11:56.000Z",
        "updatedAt": "2024-12-31T23:55:16.000Z",
        "language": "C++",
        "homepage": "https://arbcoding.github.io/sac-format/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "torstendahm/seismology_classroom",
        "url": "https://github.com/torstendahm/seismology_classroom",
        "description": null,
        "stars": 7,
        "forks": 0,
        "readme": "# Preface and background to *seismology_classroom* by the Seismology Group at the University of Potsdam (UPo) and the German Research Center for Geosciences (GFZ). \n\nThe Git project and GitHub repository *seismology_classroom* was developed in summer 2022 at UPo and GFZ by three doctoral students (Malte Metz, Angela Carrillo Ponce, Lukas Lehmann) with feedback from professors of geophysics at the University of Potsdam. The goal of the student project was to provide examples of Python scripts and examples to introduce and support the practicals and lectures in seismology in the Master's program (MSc) in Geophysics / Seismology at the University of Potsdam. \n\nThe lecture notes and Jupyter notebook examples were developed for a five-day pre-course attended before the start of the MSc for students coming from other universities or having a different background - to bring everyone to a similar level. In addition, the notebooks were intended to serve as online materials for all MSc students to follow through the master's phase or to begin their own assignments and internships.\n\nThe Jupyter notebook examples are not intended to replace specific lectures, and they are not necessarily correct in all aspects. They serve as code examples to get started in the topic, but do not explain the theoretical background and context. For this, students must attend the lectures and courses offered as part of the MSc program at the University of Potsdam. For each notebook, there are links to lectures where the theoretical background is explained or where the examples are applied. \n\nThe student project is being developed under GitHub. We hope that the *seismology_classroom* at the University of Potsdam will become a project where other doctoral students, Master students and professors contribute and develop the project further. \n\nPotsdam, July 16, 2022\n\nThe [Authors](AUTHORS.md)\n\n# Modules\n1. Introduction to Python, Command Line and GitHub\n2. Signal Processing\n3. Introduction to seismological toolboxes\n4. Retrieving online data\n5. Picking/Phases\n6. Introduction to synthetic traveltime calculation\n7. Synthetic seismograms\n8. Map creation (planned)\n\n# Schedule idea\nDay plan\nTime | What\n--- | ---\n9:00 AM | Start\n12:30 PM | Lunch Break\n1:30 PM | Start of afternoon session\n5:00 PM | End (latest)\n\nModule | Time required | When | Priority\n---|---|---|---\n1 Introduction | 1 day | Mo | High\n2 Signal Processing | 1 day | Tue | High\n3 Seismological Toolboxes | 1 day | Wed | High\n4 Retrieving online data | 0.25 day | Thu| Medium\n5 Picking | 0.5-1 day | Thu | Medium\n6 Cake - Traveltimes | 0.25 day | Thu/Fr | Low\n7 Synthetics | 1 day | Fr | Low\n8 Map generation | X day | All week| Medium\n\n\n## [Authors](AUTHORS.md)\n",
        "createdAt": "2022-04-29T15:05:23.000Z",
        "updatedAt": "2024-09-23T14:54:11.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/torstendahm/seismology_classroom/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "JuliaEarth/SeisKit.jl",
        "url": "https://github.com/JuliaEarth/SeisKit.jl",
        "description": "Fast IO operations with SEG-Y files and other basic utilities for working with seismic data",
        "stars": 3,
        "forks": 0,
        "readme": "<p align=\"center\">\n  <img src=\"docs/logo.webp\" height=\"200\"><br>\n  <a href=\"https://github.com/JuliaEarth/SeisKit.jl/actions\">\n    <img src=\"https://img.shields.io/github/actions/workflow/status/JuliaEarth/SeisKit.jl/CI.yml?branch=main&style=flat-square\">\n  </a>\n  <a href=\"https://codecov.io/gh/JuliaEarth/SeisKit.jl\">\n    <img src=\"https://img.shields.io/codecov/c/github/JuliaEarth/SeisKit.jl?style=flat-square\">\n  </a>\n  <a href=\"LICENSE\">\n    <img src=\"https://img.shields.io/badge/license-MIT-blue.svg?style=flat-square\">\n  </a>\n</p>\n\nFast IO operations with SEG-Y files and other basic utilities for\nworking with seismic data.\n\nSupports all revisions of the SEG-Y standard from 2002 (rev 1) to\n2023 (rev 2.1).\n\nThe [SEG technical standards](https://seg.org/publications/seg-technical-standards)\nused in the development of this package are:\n\n- [SEG-Y rev 1.0 Data Exchange format, May 2002](https://seg.org/wp-content/uploads/2025/11/seg_y_rev1.pdf)\n\n- [SEG-Y rev 2.0 Data Exchange format, January 2017](https://seg.org/wp-content/uploads/2025/11/seg_y_rev2_0_mar2017.pdf)\n\n- [SEG-Y rev 2.1 Data Exchange format, October 2023](https://seg.org/wp-content/uploads/2025/11/seg_y_rev2_1-oct2023.pdf)\n\nFree registration may be required to access these documents.\n\n## Installation\n\nGet the latest stable release with Julia's package manager:\n\n```\n] add SeisKit\n```\n\n## Usage\n\nThe SEG-Y file format is the most widely used format for storing\nseismic data in the industry. It consists of a textual header,\na binary header, optional extended headers, and trace data with\nindividual trace headers.\n\nThe `SeisKit` module exports the `Segy` submodule to work with SEG-Y files:\n\n```julia\njulia> using SeisKit\n```\n\n### Retrieving SEG-Y headers\n\nAll SEG-Y headers can be retrieved with the `Segy.headers` function:\n\n```julia\nth, bh, eh, trh = Segy.headers(\"test/data/stacked2Drev1.sgy\")\n```\n\nbut they can also be retrieved separately:\n\n#### Textual header\n\n```julia\njulia> th = Segy.textualheader(\"test/data/stacked2Drev1.sgy\")\n```\n<details>\n<summary>Click to expand output</summary>>\n<pre>\nC 1 CLIENT                        COMPANY                       CREW NO         C 2 LINE            AREA                        MAP ID                          C 3 REEL NO           DAY-START OF REEL     YEAR      OBSERVER                  C 4 INSTRUMENT: MFG            MODEL            SERIAL NO                       C 5 DATA TRACES/RECORD        AUXILIARY TRACES/RECORD         CDF FOLD          C 6 SAMPLE INTERVAL         SAMPLES/TRACE       BITS/IN     BYTES/SAMPPLE       C 7 RECORDING FORMAT        FORMAT THIS REEL        MEASUREMENT SYSTEM          C 8 SAMPLE CODE: FLOATING PT     FIXED PT     FIXED PT-GAIN     CORRELATED      C 9 GAIN  TYPE: FIXED     BINSRY     FLOATING POINT     OTHER                   \nC10 FILTERS: ALIAS     HZ  NOTCH     HZ BAND      -     HZ  SLOPE    -    DB/OCT\nC11 SOURCE: TYPE            NUMBER/POINT        POINT INTERVAL                  \nC12     PATTERN:                           LENGTH        WIDTH                  \nC13 SWEEP: START     HZ END      HZ  LENGTH      MS  CHANNEL NO     TYPE        \nC14 TAPER: START LENGTH       MS END LENGTH        MS TYPE                      \nC15 SPREAD: OFFSET        MAX DISTANCE        GROUP INTERVAL                    \nC16 GEOPHONES: PER GROUP     SPACING     FREQUENCY     MFG          MODEL       \nC17     PATTERN:                           LENGTH        WIDTH                  \nC18 TRACESSORTED BY: RECORD      CDP     OTHER                                  \nC19 AMPLITUDE RECOVRY: NONE       SPHERICAL DIV       AGC    OTHER              \nC20 MAP PROJECTION                      ZONE ID       COORDINATE UNITS          \nC21 PROCESSING:                                                                 \nC22 PROCESSING:                                                                 \nC23                                                                             \nC24                                                                             \nC25                                                                             \nC26                                                                             \nC27                                                                             \nC28                                                                             \nC29                                                                             \nC30                                                                             \nC31                                                                             \nC32                                                                             \nC33                                                                             \nC34                                                                             \nC35                                                                             \nC36                                                                             \nC37                                                                             \nC38                                                                             \nC39 SEG Y REV1                                                                  \nC40 END TEXTUAL HEADER\n</pre>\n</details>\n\nThe textual header has a `th.content` field with the decoded text,\ncontaining all lines C1 to C40. The SEG-Y standard allows both EBCDIC\nand ASCII encodings, and SeisKit.jl automatically detects and decodes\nthe correct format.\n\n#### Binary header\n\n```julia\njulia> bh = Segy.binaryheader(\"test/data/stacked2Drev1.sgy\")\n```\n<details>\n<summary>Click to expand output</summary>\n<pre>\n              SEG-Y Binary Header\n┌─────────────────────────────────────┬───────┐\n│ field                               │ value │\n│ Symbol                              │ Real  │\n├─────────────────────────────────────┼───────┤\n│ JOB_NUMBER                          │ 0     │\n│ LINE_NUMBER                         │ 0     │\n│ REEL_NUMBER                         │ 0     │\n│ TRACES_PER_ENSEMBLE                 │ 1     │\n│ AUX_TRACES_PER_ENSEMBLE             │ 1     │\n│ SAMPLE_INTERVAL                     │ 10000 │\n│ ORIGINAL_SAMPLE_INTERVAL            │ 0     │\n│ SAMPLES_PER_TRACE                   │ 351   │\n│ ORIGINAL_SAMPLES_PER_TRACE          │ 351   │\n│ SAMPLE_FORMAT_CODE                  │ 1     │\n│ ENSEMBLE_FOLD                       │ 1     │\n│ TRACE_SORTING_CODE                  │ 4     │\n│ VERTICAL_SUM_CODE                   │ 0     │\n│ SWEEP_FREQ_START                    │ 0     │\n│ SWEEP_FREQ_END                      │ 0     │\n│ SWEEP_LENGTH                        │ 0     │\n│ SWEEP_TYPE                          │ 4     │\n│ TRACE_NUMBER_OF_SWEEP_CHANNEL       │ 0     │\n│ SWEEP_TRACE_TAPER_LENGTH_START      │ 0     │\n│ SWEEP_TRACE_TAPER_LENGTH_END        │ 0     │\n│ TAPER_TYPE                          │ 3     │\n│ CORRELATED_TRACES                   │ 1     │\n│ BINARY_GAIN_RECOVERED               │ 2     │\n│ AMPLITUDE_RECOVERY_METHOD           │ 4     │\n│ MEASUREMENT_SYSTEM                  │ 1     │\n│ IMPULSE_SIGNAL_POLARITY             │ 0     │\n│ VIBRATORY_POLARITY_CODE             │ 0     │\n│ EXTENDED_TRACES_PER_ENSEMBLE        │ 0     │\n│ EXTENDED_AUX_TRACES_PER_ENSEMBLE    │ 0     │\n│ EXTENDED_SAMPLES_PER_TRACE          │ 0     │\n│ EXTENDED_SAMPLE_INTERVAL            │ 0.0   │\n│ EXTENDED_ORIGINAL_SAMPLE_INTERVAL   │ 0.0   │\n│ EXTENDED_ORIGINAL_SAMPLES_PER_TRACE │ 0     │\n│ EXTENDED_ENSEMBLE_FOLD              │ 0     │\n│ ENDIAN_CONSTANT                     │ 0     │\n│ MAJOR_REVISION_NUMBER               │ 1     │\n│ MINOR_REVISION_NUMBER               │ 0     │\n│ FIXED_LENGTH_TRACE_FLAG             │ 256   │\n│ EXTENDED_TEXT_HEADER_COUNT          │ 0     │\n│ MAX_EXTENDED_TRACE_HEADERS          │ 0     │\n│ SURVEY_TYPE                         │ 0     │\n│ TIME_BASIS_CODE                     │ 0     │\n│ TRACES_IN_FILE                      │ 0     │\n│ FIRST_TRACE_OFFSET                  │ 0     │\n│ TRAILER_RECORDS                     │ 0     │\n└─────────────────────────────────────┴───────┘\n</pre>\n</details>\n\nThe binary header fields can be accessed directly:\n\n```julia\njulia> bh.SAMPLE_INTERVAL # sample interval in microseconds\n```\n```\n0x2710\n```\n\n#### Extended headers\n\n```julia\njulia> eh = Segy.extendedheaders(\"test/data/stacked2Drev1.sgy\")\n```\n```\nSeisKit.Segy.ExtendedHeader[]\n```\n\nThe extended headers are rarely used in practice.\nThey consist of textual headers similar to the main\ntextual header, but with well-defined formats\n(a.k.a. stanzas).\n\n#### Trace headers\n\n```julia\njulia> trh = Segy.traceheaders(\"test/data/stacked2Drev1.sgy\")\n```\n<details>\n<summary>Click to expand output</summary>\n<pre>\n                    SEG-Y Trace Header\n┌─────────────────────────────────────────────┬───────────┐\n│ field                                       │ value     │\n│ Symbol                                      │ Signed    │\n├─────────────────────────────────────────────┼───────────┤\n│ TRACE_NUMBER_IN_LINE                        │ 1         │\n│ TRACE_NUMBER_IN_FILE                        │ 0         │\n│ ORIGINAL_FIELD_RECORD_NUMBER                │ 0         │\n│ TRACE_NUMBER_IN_ORIGINAL_FIELD_RECORD       │ 0         │\n│ ENERGY_SOURCE_POINT_NUMBER                  │ 0         │\n│ ENSEMBLE_NUMBER                             │ 0         │\n│ TRACE_NUMBER_IN_ENSEMBLE                    │ 0         │\n│ TRACE_ID_CODE                               │ 1         │\n│ VERTICALLY_SUMMED_TRACES                    │ 0         │\n│ HORIZONTALLY_STACKED_TRACES                 │ 0         │\n│ DATA_USE                                    │ 1         │\n│ DISTANCE_SOURCE_RECEIVER                    │ 0         │\n│ RECEIVER_ELEVATION                          │ 0         │\n│ SOURCE_SURFACE_ELEVATION                    │ 0         │\n│ SOURCE_DEPTH_BELOW_SURFACE                  │ 0         │\n│ RECEIVER_DATUM                              │ 0         │\n│ SOURCE_DATUM                                │ 0         │\n│ SOURCE_WATER_DEPTH                          │ 0         │\n│ RECEIVER_WATER_DEPTH                        │ 0         │\n│ ELEVATION_SCALAR                            │ 1         │\n│ COORDINATE_SCALAR                           │ -100      │\n│ SOURCE_X                                    │ 0         │\n│ SOURCE_Y                                    │ 0         │\n│ RECEIVER_X                                  │ 0         │\n│ RECEIVER_Y                                  │ 0         │\n│ COORDINATE_UNIT                             │ 1         │\n│ WEATHERING_VELOCITY                         │ 0         │\n│ SUBWEATHERING_VELOCITY                      │ 0         │\n│ SOURCE_UPHOLE_TIME                          │ 0         │\n│ RECEIVER_UPHOLE_TIME                        │ 0         │\n│ SOURCE_STATIC_CORRECTION                    │ 0         │\n│ RECEIVER_STATIC_CORRECTION                  │ 0         │\n│ TOTAL_STATIC_CORRECTION                     │ 0         │\n│ LAG_TIME_A                                  │ 0         │\n│ LAG_TIME_B                                  │ 0         │\n│ DELAY_RECORDING_TIME                        │ 0         │\n│ MUTE_TIME_START                             │ 0         │\n│ MUTE_TIME_END                               │ 0         │\n│ SAMPLES_IN_TRACE                            │ 351       │\n│ SAMPLE_INTERVAL                             │ 10000     │\n│ GAIN_TYPE                                   │ 0         │\n│ INSTRUMENT_GAIN_CONSTANT                    │ 0         │\n│ INSTRUMENT_INITIAL_GAIN                     │ 0         │\n│ CORRELATED_TRACES                           │ 0         │\n│ SWEEP_FREQ_START                            │ 0         │\n│ SWEEP_FREQ_END                              │ 0         │\n│ SWEEP_LENGTH                                │ 0         │\n│ SWEEP_TYPE                                  │ 0         │\n│ SWEEP_TRACE_TAPER_LENGTH_START              │ 0         │\n│ SWEEP_TRACE_TAPER_LENGTH_END                │ 0         │\n│ TAPER_TYPE                                  │ 0         │\n│ ALIAS_FILTER_FREQ                           │ 0         │\n│ ALIAS_FILTER_SLOPE                          │ 0         │\n│ NOTCH_FILTER_FREQ                           │ 0         │\n│ NOTCH_FILTER_SLOPE                          │ 0         │\n│ LOWCUT_FREQ                                 │ 0         │\n│ HIGHCUT_FREQ                                │ 0         │\n│ LOWCUT_SLOPE                                │ 0         │\n│ HIGHCUT_SLOPE                               │ 0         │\n│ YEAR_DATA_RECORDED                          │ 0         │\n│ DAY_OF_YEAR                                 │ 0         │\n│ HOUR_OF_DAY                                 │ 0         │\n│ MINUTE_OF_HOUR                              │ 0         │\n│ SECOND_OF_MINUTE                            │ 0         │\n│ TIME_BASIS_CODE                             │ 0         │\n│ TRACE_WEIGHTING_FACTOR                      │ 0         │\n│ GROUP_NUMBER_ROW_SWITCH_POS_ONE             │ 0         │\n│ GROUP_NUMBER_FIRST_TRACE_IN_ORIGINAL_RECORD │ 0         │\n│ GROUP_NUMBER_LAST_TRACE_IN_ORIGINAL_RECORD  │ 0         │\n│ GAP_SIZE                                    │ 0         │\n│ TAPER_OVER_TRAVEL                           │ 0         │\n│ ENSEMBLE_X                                  │ 47320112  │\n│ ENSEMBLE_Y                                  │ 719430976 │\n│ INLINE_NUMBER                               │ 0         │\n│ CROSSLINE_NUMBER                            │ 7200      │\n│ SHOTPOINT_NUMBER                            │ 2400      │\n│ SHOTPOINT_SCALAR                            │ 0         │\n│ TRACE_MEASUREMENT_UNIT                      │ 0         │\n│ TRANSDUCTION_CONSTANT                       │ 0         │\n│ TRANSDUCTION_CONSTANT_EXPONENT              │ 0         │\n│ TRANSDUCTION_CONSTANT_UNIT                  │ 0         │\n│ DEVICE_NUMBER                               │ 0         │\n│ TIME_SCALAR                                 │ 0         │\n│ SOURCE_TYPE_ORIENTATION                     │ 0         │\n│ SOURCE_DIRECTION_VERTICAL                   │ 0         │\n│ SOURCE_DIRECTION_CROSSLINE                  │ 0         │\n│ SOURCE_DIRECTION_INLINE                     │ 0         │\n│ SOURCE_CONSTANT                             │ 0         │\n│ SOURCE_CONSTANT_EXPONENT                    │ 0         │\n│ SOURCE_CONSTANT_UNIT                        │ 0         │\n└─────────────────────────────────────────────┴───────────┘\n.\n.\n.\n                    SEG-Y Trace Header\n┌─────────────────────────────────────────────┬───────────┐\n│ field                                       │ value     │\n│ Symbol                                      │ Signed    │\n├─────────────────────────────────────────────┼───────────┤\n│ TRACE_NUMBER_IN_LINE                        │ 7701      │\n│ TRACE_NUMBER_IN_FILE                        │ 0         │\n│ ORIGINAL_FIELD_RECORD_NUMBER                │ 0         │\n│ TRACE_NUMBER_IN_ORIGINAL_FIELD_RECORD       │ 0         │\n│ ENERGY_SOURCE_POINT_NUMBER                  │ 0         │\n│ ENSEMBLE_NUMBER                             │ 0         │\n│ TRACE_NUMBER_IN_ENSEMBLE                    │ 0         │\n│ TRACE_ID_CODE                               │ 1         │\n│ VERTICALLY_SUMMED_TRACES                    │ 0         │\n│ HORIZONTALLY_STACKED_TRACES                 │ 0         │\n│ DATA_USE                                    │ 1         │\n│ DISTANCE_SOURCE_RECEIVER                    │ 0         │\n│ RECEIVER_ELEVATION                          │ 0         │\n│ SOURCE_SURFACE_ELEVATION                    │ 0         │\n│ SOURCE_DEPTH_BELOW_SURFACE                  │ 0         │\n│ RECEIVER_DATUM                              │ 0         │\n│ SOURCE_DATUM                                │ 0         │\n│ SOURCE_WATER_DEPTH                          │ 0         │\n│ RECEIVER_WATER_DEPTH                        │ 0         │\n│ ELEVATION_SCALAR                            │ 1         │\n│ COORDINATE_SCALAR                           │ -100      │\n│ SOURCE_X                                    │ 0         │\n│ SOURCE_Y                                    │ 0         │\n│ RECEIVER_X                                  │ 0         │\n│ RECEIVER_Y                                  │ 0         │\n│ COORDINATE_UNIT                             │ 1         │\n│ WEATHERING_VELOCITY                         │ 0         │\n│ SUBWEATHERING_VELOCITY                      │ 0         │\n│ SOURCE_UPHOLE_TIME                          │ 0         │\n│ RECEIVER_UPHOLE_TIME                        │ 0         │\n│ SOURCE_STATIC_CORRECTION                    │ 0         │\n│ RECEIVER_STATIC_CORRECTION                  │ 0         │\n│ TOTAL_STATIC_CORRECTION                     │ 0         │\n│ LAG_TIME_A                                  │ 0         │\n│ LAG_TIME_B                                  │ 0         │\n│ DELAY_RECORDING_TIME                        │ 0         │\n│ MUTE_TIME_START                             │ 0         │\n│ MUTE_TIME_END                               │ 0         │\n│ SAMPLES_IN_TRACE                            │ 351       │\n│ SAMPLE_INTERVAL                             │ 10000     │\n│ GAIN_TYPE                                   │ 0         │\n│ INSTRUMENT_GAIN_CONSTANT                    │ 0         │\n│ INSTRUMENT_INITIAL_GAIN                     │ 0         │\n│ CORRELATED_TRACES                           │ 0         │\n│ SWEEP_FREQ_START                            │ 0         │\n│ SWEEP_FREQ_END                              │ 0         │\n│ SWEEP_LENGTH                                │ 0         │\n│ SWEEP_TYPE                                  │ 0         │\n│ SWEEP_TRACE_TAPER_LENGTH_START              │ 0         │\n│ SWEEP_TRACE_TAPER_LENGTH_END                │ 0         │\n│ TAPER_TYPE                                  │ 0         │\n│ ALIAS_FILTER_FREQ                           │ 0         │\n│ ALIAS_FILTER_SLOPE                          │ 0         │\n│ NOTCH_FILTER_FREQ                           │ 0         │\n│ NOTCH_FILTER_SLOPE                          │ 0         │\n│ LOWCUT_FREQ                                 │ 0         │\n│ HIGHCUT_FREQ                                │ 0         │\n│ LOWCUT_SLOPE                                │ 0         │\n│ HIGHCUT_SLOPE                               │ 0         │\n│ YEAR_DATA_RECORDED                          │ 0         │\n│ DAY_OF_YEAR                                 │ 0         │\n│ HOUR_OF_DAY                                 │ 0         │\n│ MINUTE_OF_HOUR                              │ 0         │\n│ SECOND_OF_MINUTE                            │ 0         │\n│ TIME_BASIS_CODE                             │ 0         │\n│ TRACE_WEIGHTING_FACTOR                      │ 0         │\n│ GROUP_NUMBER_ROW_SWITCH_POS_ONE             │ 0         │\n│ GROUP_NUMBER_FIRST_TRACE_IN_ORIGINAL_RECORD │ 0         │\n│ GROUP_NUMBER_LAST_TRACE_IN_ORIGINAL_RECORD  │ 0         │\n│ GAP_SIZE                                    │ 0         │\n│ TAPER_OVER_TRAVEL                           │ 0         │\n│ ENSEMBLE_X                                  │ 46642304  │\n│ ENSEMBLE_Y                                  │ 719775424 │\n│ INLINE_NUMBER                               │ 0         │\n│ CROSSLINE_NUMBER                            │ 7500      │\n│ SHOTPOINT_NUMBER                            │ 2500      │\n│ SHOTPOINT_SCALAR                            │ 0         │\n│ TRACE_MEASUREMENT_UNIT                      │ 0         │\n│ TRANSDUCTION_CONSTANT                       │ 0         │\n│ TRANSDUCTION_CONSTANT_EXPONENT              │ 0         │\n│ TRANSDUCTION_CONSTANT_UNIT                  │ 0         │\n│ DEVICE_NUMBER                               │ 0         │\n│ TIME_SCALAR                                 │ 0         │\n│ SOURCE_TYPE_ORIENTATION                     │ 0         │\n│ SOURCE_DIRECTION_VERTICAL                   │ 0         │\n│ SOURCE_DIRECTION_CROSSLINE                  │ 0         │\n│ SOURCE_DIRECTION_INLINE                     │ 0         │\n│ SOURCE_CONSTANT                             │ 0         │\n│ SOURCE_CONSTANT_EXPONENT                    │ 0         │\n│ SOURCE_CONSTANT_UNIT                        │ 0         │\n└─────────────────────────────────────────────┴───────────┘\n</pre>\n</details>\n\nThe trace headers are stored in a vector-like data structure\nthat allows easy access to individual fields without unnecessary\nmemory copies:\n\n```julia\njulia> trh.ENSEMBLE_X # vector of ensemble x coordinates\n```\n```\n7701-element FieldViews.FieldView{:ENSEMBLE_X, Int32, 1, SeisKit.Segy.TraceHeader, Vector{SeisKit.Segy.TraceHeader}}:\n 47320112\n 47320864\n 47321616\n 47322364\n 47323116\n 47323868\n 47324620\n        ⋮\n 46637800\n 46638552\n 46639300\n 46640052\n 46640804\n 46641552\n 46642304\n```\n\n```julia\njulia> trh[1].CROSSLINE_NUMBER # crossline number of the first trace\n```\n```\n7200\n```\n\n### Retrieving SEG-Y traces\n\nThe actual seismic data can be retrieved with the `Segy.load`\nfunction. It calls `Segy.headers` and then `Segy.traces`\nto read the data efficiently into Julia arrays:\n\n```julia\njulia> seismic = Segy.load(\"test/data/stacked2Drev1.sgy\")\n```\n```\nSEG-Y Dataset (rev 1.0)\n├─ Nᵒ traces: 7701\n├─ Nᵒ samples: 351 (fixed)\n├─ Inlines: 0 (fixed)\n└─ X-lines: 7200 ─ 7500\n```\n\nThe arrays are stored in the `seismic.traces` field. The `Segy.save`\nfunction can be used to write the data back to a file that is compliant\nwith SEG-Y rev 2.1:\n\n```julia\njulia> Segy.save(\"path/to/newfile.sgy\", seismic)\n```\n\nWe do not support saving in older revisions because:\n\n> The SEG Technical Standards Committee strongly\n> encourages producers and users of SEG-Y data sets\n> to move to the revised (2.1) standard in an\n> expeditious fashion.\n\n### Retrieving coordinates\n\nThe trace coordinates can be retrieved with the `Segy.coords` function.\nThe package automatically detects the coordinate reference system (CRS)\nusing various heuristics. If no CRS is found, the function returns a generic\nCartesian system with units in meters.\n\n```juliajulia\njulia> Segy.coords(seismic)\n```\n<details>\n<summary>Click to expand output</summary>\n<pre>\n7701-element Vector{CoordRefSystems.Cartesian2D{CoordRefSystems.WGS84Latest, Unitful.Quantity{Float64, 𝐋, Unitful.FreeUnits{(m,), 𝐋, nothing}}}}:\n Cartesian{WGS84Latest}(x: 4.73201e5 m, y: 7.19431e6 m)\n Cartesian{WGS84Latest}(x: 4.73209e5 m, y: 7.19433e6 m)\n Cartesian{WGS84Latest}(x: 4.73216e5 m, y: 7.19436e6 m)\n Cartesian{WGS84Latest}(x: 4.73224e5 m, y: 7.19438e6 m)\n Cartesian{WGS84Latest}(x: 4.73231e5 m, y: 7.1944e6 m)\n Cartesian{WGS84Latest}(x: 4.73239e5 m, y: 7.19443e6 m)\n Cartesian{WGS84Latest}(x: 4.73246e5 m, y: 7.19445e6 m)\n Cartesian{WGS84Latest}(x: 4.73254e5 m, y: 7.19448e6 m)\n ⋮\n Cartesian{WGS84Latest}(x: 4.6637e5 m, y: 7.19759e6 m)\n Cartesian{WGS84Latest}(x: 466378.0 m, y: 7.19761e6 m)\n Cartesian{WGS84Latest}(x: 4.66386e5 m, y: 7.19764e6 m)\n Cartesian{WGS84Latest}(x: 466393.0 m, y: 7.19766e6 m)\n Cartesian{WGS84Latest}(x: 4.66401e5 m, y: 7.19768e6 m)\n Cartesian{WGS84Latest}(x: 466408.0 m, y: 7.19771e6 m)\n Cartesian{WGS84Latest}(x: 4.66416e5 m, y: 7.19773e6 m)\n Cartesian{WGS84Latest}(x: 466423.0 m, y: 7.19775e6 m)\n</pre>\n</details>\n\n### Converting to an image\n\nIn the case of 2D seismic with fixed-length traces (e.g., 2D post-stack),\nit is often useful to place the traces side by side to form an image.\nFor that, we provide the `Segy.image` function:\n\n```julia\njulia> Segy.image(seismic)\n```\n<details>\n<summary>Click to expand output</summary>\n<pre>\n351×7701 Matrix{Float64}:\n 1496.38  1496.4   1496.4   1496.39  1496.37  1496.36  1496.37  1496.39  1496.42  …  1497.82  1497.72  1497.51  1497.05  1496.64  1496.29  1495.98  1495.73\n 1496.38  1496.4   1496.4   1496.39  1496.37  1496.36  1496.37  1496.39  1496.42     1497.82  1497.72  1497.51  1497.05  1496.64  1496.29  1495.98  1495.73\n 1493.5   1493.51  1493.5   1493.49  1493.46  1493.44  1493.44  1493.45  1493.47     1494.6   1494.51  1494.33  1493.94  1493.59  1493.29  1493.03  1492.81\n 1490.57  1490.57  1490.55  1490.53  1490.5   1490.47  1490.46  1490.46  1490.46     1491.32  1491.24  1491.09  1490.77  1490.49  1490.24  1490.03  1489.85\n 1487.21  1487.19  1487.16  1487.13  1487.1   1487.06  1487.04  1487.02  1487.01     1487.55  1487.48  1487.36  1487.12  1486.91  1486.73  1486.56  1486.42\n 1484.38  1484.35  1484.31  1484.27  1484.23  1484.19  1484.16  1484.13  1484.1   …  1484.84  1484.78  1484.68  1484.5   1484.35  1484.21  1484.09  1483.99\n 1483.24  1483.19  1483.15  1483.11  1483.08  1483.05  1483.01  1482.96  1482.91     1484.61  1484.56  1484.47  1484.36  1484.26  1484.18  1484.11  1484.06\n 1485.9   1485.84  1485.8   1485.77  1485.76  1485.74  1485.69  1485.64  1485.57     1487.23  1487.18  1487.12  1487.09  1487.07  1487.05  1487.05  1487.05\n    ⋮                                            ⋮                                ⋱                       ⋮                                            ⋮\n 4306.74  4310.65  4315.24  4319.14  4324.03  4326.27  4321.0   4318.07  4317.0      4122.27  4122.17  4122.27  4122.27  4122.27  4122.27  4122.17  4122.07\n 4140.14  4140.73  4141.51  4142.19  4142.88  4143.07  4142.19  4141.61  4141.7      4122.27  4122.46  4122.56  4122.56  4122.66  4122.56  4122.56  4122.37\n 4135.45  4135.84  4136.33  4136.62  4137.11  4137.41  4137.41  4137.5   4137.6   …  4118.46  4118.66  4118.75  4118.75  4118.56  4118.56  4118.27  4118.17\n 4134.87  4135.65  4136.23  4137.11  4137.8   4138.29  4138.19  4138.19  4138.29     4121.88  4121.78  4121.78  4121.88  4122.17  4122.27  4122.46  4122.46\n 4137.6   4137.99  4138.48  4138.97  4139.75  4140.04  4140.14  4140.14  4140.14     4134.67  4133.99  4133.3   4133.3   4133.4   4134.18  4134.87  4135.65\n 4143.17  4142.29  4141.51  4140.63  4139.75  4139.26  4139.26  4139.36  4139.46     4278.62  4274.52  4272.37  4277.05  4281.84  4286.14  4290.73  4294.83\n 4147.07  4145.41  4143.66  4141.9   4140.04  4139.16  4139.16  4139.26  4139.36     4382.23  4381.84  4382.33  4384.18  4386.14  4387.11  4388.48  4389.85\n 4146.59  4145.22  4143.95  4142.58  4141.12  4140.04  4140.04  4139.95  4139.95  …  4579.3   4575.69  4570.8   4573.44  4576.57  4579.01  4582.43  4583.4\n</pre>\n</details>\n\nThe function sorts the traces based on their inline and crossline\nnumbers, and returns a simple 2D array (i.e., matrix) for image\nprocessing and visualization.\n\n### Troubleshooting\n\nWe provide the `Segy.report` function to report header information\nand highlight issues with SEG-Y files. It can be useful to spot files\nthat are not compliant with the standard, and to anticipate potential\nproblems when loading the data:\n\n```julia\njulia> Segy.report(\"test/data/stacked2Drev1.sgy\")\n```\n<details>\n<summary>Click to expand output</summary>\n<pre>\nC 1 CLIENT                        COMPANY                       CREW NO         C 2 LINE            AREA                        MAP ID                          C 3 REEL NO           DAY-START OF REEL     YEAR      OBSERVER                  C 4 INSTRUMENT: MFG            MODEL            SERIAL NO                       C 5 DATA TRACES/RECORD        AUXILIARY TRACES/RECORD         CDF FOLD          C 6 SAMPLE INTERVAL         SAMPLES/TRACE       BITS/IN     BYTES/SAMPPLE       C 7 RECORDING FORMAT        FORMAT THIS REEL        MEASUREMENT SYSTEM          C 8 SAMPLE CODE: FLOATING PT     FIXED PT     FIXED PT-GAIN     CORRELATED      C 9 GAIN  TYPE: FIXED     BINSRY     FLOATING POINT     OTHER                   \nC10 FILTERS: ALIAS     HZ  NOTCH     HZ BAND      -     HZ  SLOPE    -    DB/OCT\nC11 SOURCE: TYPE            NUMBER/POINT        POINT INTERVAL                  \nC12     PATTERN:                           LENGTH        WIDTH                  \nC13 SWEEP: START     HZ END      HZ  LENGTH      MS  CHANNEL NO     TYPE        \nC14 TAPER: START LENGTH       MS END LENGTH        MS TYPE                      \nC15 SPREAD: OFFSET        MAX DISTANCE        GROUP INTERVAL                    \nC16 GEOPHONES: PER GROUP     SPACING     FREQUENCY     MFG          MODEL       \nC17     PATTERN:                           LENGTH        WIDTH                  \nC18 TRACESSORTED BY: RECORD      CDP     OTHER                                  \nC19 AMPLITUDE RECOVRY: NONE       SPHERICAL DIV       AGC    OTHER              \nC20 MAP PROJECTION                      ZONE ID       COORDINATE UNITS          \nC21 PROCESSING:                                                                 \nC22 PROCESSING:                                                                 \nC23                                                                             \nC24                                                                             \nC25                                                                             \nC26                                                                             \nC27                                                                             \nC28                                                                             \nC29                                                                             \nC30                                                                             \nC31                                                                             \nC32                                                                             \nC33                                                                             \nC34                                                                             \nC35                                                                             \nC36                                                                             \nC37                                                                             \nC38                                                                             \nC39 SEG Y REV1                                                                  \nC40 END TEXTUAL HEADER                                                          \n\n              SEG-Y Binary Header\n┌─────────────────────────────────────┬───────┐\n│ field                               │ value │\n│ Symbol                              │ Real  │\n├─────────────────────────────────────┼───────┤\n│ JOB_NUMBER                          │ 0     │\n│ LINE_NUMBER                         │ 0     │\n│ REEL_NUMBER                         │ 0     │\n│ TRACES_PER_ENSEMBLE                 │ 1     │\n│ AUX_TRACES_PER_ENSEMBLE             │ 1     │\n│ SAMPLE_INTERVAL                     │ 10000 │\n│ ORIGINAL_SAMPLE_INTERVAL            │ 0     │\n│ SAMPLES_PER_TRACE                   │ 351   │\n│ ORIGINAL_SAMPLES_PER_TRACE          │ 351   │\n│ SAMPLE_FORMAT_CODE                  │ 1     │\n│ ENSEMBLE_FOLD                       │ 1     │\n│ TRACE_SORTING_CODE                  │ 4     │\n│ VERTICAL_SUM_CODE                   │ 0     │\n│ SWEEP_FREQ_START                    │ 0     │\n│ SWEEP_FREQ_END                      │ 0     │\n│ SWEEP_LENGTH                        │ 0     │\n│ SWEEP_TYPE                          │ 4     │\n│ TRACE_NUMBER_OF_SWEEP_CHANNEL       │ 0     │\n│ SWEEP_TRACE_TAPER_LENGTH_START      │ 0     │\n│ SWEEP_TRACE_TAPER_LENGTH_END        │ 0     │\n│ TAPER_TYPE                          │ 3     │\n│ CORRELATED_TRACES                   │ 1     │\n│ BINARY_GAIN_RECOVERED               │ 2     │\n│ AMPLITUDE_RECOVERY_METHOD           │ 4     │\n│ MEASUREMENT_SYSTEM                  │ 1     │\n│ IMPULSE_SIGNAL_POLARITY             │ 0     │\n│ VIBRATORY_POLARITY_CODE             │ 0     │\n│ EXTENDED_TRACES_PER_ENSEMBLE        │ 0     │\n│ EXTENDED_AUX_TRACES_PER_ENSEMBLE    │ 0     │\n│ EXTENDED_SAMPLES_PER_TRACE          │ 0     │\n│ EXTENDED_SAMPLE_INTERVAL            │ 0.0   │\n│ EXTENDED_ORIGINAL_SAMPLE_INTERVAL   │ 0.0   │\n│ EXTENDED_ORIGINAL_SAMPLES_PER_TRACE │ 0     │\n│ EXTENDED_ENSEMBLE_FOLD              │ 0     │\n│ ENDIAN_CONSTANT                     │ 0     │\n│ MAJOR_REVISION_NUMBER               │ 1     │\n│ MINOR_REVISION_NUMBER               │ 0     │\n│ FIXED_LENGTH_TRACE_FLAG             │ 256   │\n│ EXTENDED_TEXT_HEADER_COUNT          │ 0     │\n│ MAX_EXTENDED_TRACE_HEADERS          │ 0     │\n│ SURVEY_TYPE                         │ 0     │\n│ TIME_BASIS_CODE                     │ 0     │\n│ TRACES_IN_FILE                      │ 0     │\n│ FIRST_TRACE_OFFSET                  │ 0     │\n│ TRAILER_RECORDS                     │ 0     │\n└─────────────────────────────────────┴───────┘\n\nSEG-Y Trace Header Summary (non-zero fields only)\n┌──────────────────────┬───────────┬───────────┐\n│ field                │ minimum   │ maximum   │\n│ Symbol               │ Int64     │ Int64     │\n├──────────────────────┼───────────┼───────────┤\n│ TRACE_NUMBER_IN_LINE │ 1         │ 7701      │\n│ TRACE_ID_CODE        │ 1         │ 1         │\n│ DATA_USE             │ 1         │ 1         │\n│ ELEVATION_SCALAR     │ 1         │ 1         │\n│ COORDINATE_SCALAR    │ -100      │ -100      │\n│ COORDINATE_UNIT      │ 1         │ 1         │\n│ SAMPLES_IN_TRACE     │ 351       │ 351       │\n│ SAMPLE_INTERVAL      │ 10000     │ 10000     │\n│ ENSEMBLE_X           │ 46604756  │ 47357660  │\n│ ENSEMBLE_Y           │ 719430976 │ 719775424 │\n│ CROSSLINE_NUMBER     │ 7200      │ 7500      │\n│ SHOTPOINT_NUMBER     │ 2400      │ 2500      │\n└──────────────────────┴───────────┴───────────┘\n\nSEG-Y issues report:\n\n- Detected FIXED_LENGTH_TRACE_FLAG = 256 in binary header.\n  FIXED_LENGTH_TRACE_FLAG should be 0 or 1.\n\nYou can fix most of these issues with `Segy.save(...)`\nafter loading the data with `Segy.load(...)`.\n</pre>\n</details>\n\nIf `Segy.report` shows issues that you want to fix manually,\nyou can modify the `Segy.headers`, create a `Segy.Dataset`\nwith the modified headers and traces, and then save it with\n`Segy.save`. The `Segy.save` function fixes most issues found\nin the headers using the `Segy.fixissues` function before\nwriting the new file in SEG-Y rev 2.1.\n\nPlease consult the docstrings of all these functions for more details.\n\n## Contributing\n\nContributions are very welcome. Please [open an issue](https://github.com/JuliaEarth/SeisKit.jl/issues) if you have questions.\n\n## Previous attempts\n\nPackages with similar functionality were written\nfor older versions of the language:\n\n- [SegyIO.jl](https://github.com/slimgroup/SegyIO.jl)\n  provides read/write functions for SEG-Y rev 1 that\n  are ~1.5x slower than SeisKit.jl on average. That\n  is because SeisKit.jl adopted more idiomatic Julia\n  coding styles and optimized IO operations more\n  aggressively.\n\n- [SeisIO.jl](https://github.com/jpjones76/SeisIO.jl)\n  provides tools for SEG-Y rev 0 and rev 1 files, and\n  other seismic data formats. However, it is not\n  actively maintained anymore, and has too many\n  unnecessary dependencies for an IO package.\n",
        "createdAt": "2025-11-14T23:01:16.000Z",
        "updatedAt": "2025-12-04T18:57:10.000Z",
        "language": "Julia",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/JuliaEarth/SeisKit.jl/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "MrMisc/Inverse-Theory-in-Seismology",
        "url": "https://github.com/MrMisc/Inverse-Theory-in-Seismology",
        "description": "Attempted project question in MATLAB",
        "stars": 0,
        "forks": 0,
        "readme": "# Inverse Theory in Seismology\n Attempted project question in MATLAB\n",
        "createdAt": "2022-12-30T17:31:50.000Z",
        "updatedAt": "2022-12-30T17:31:57.000Z",
        "language": "MATLAB",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/MrMisc/Inverse-Theory-in-Seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "dpgraham4401/libseis",
        "url": "https://github.com/dpgraham4401/libseis",
        "description": "A seismic data processing library with Python bindings - a just-for-fun side project",
        "stars": 2,
        "forks": 2,
        "readme": "# Libseis\n\n### Seismic library for geophysical processing\n\nThis is a learn-by-doing project\nto reacquaint myself with the C/C++, geophysics, and binding with Python.\n\n## Getting Started\n\n### Prerequisites\n\n- [Just](https://just.systems/)\n- CMake\n- C++ compiler (e.g., g++, clang++)\n- Python > 3.12\n- Python development headers\n    - On Debian/Ubuntu: `sudo apt-get install python3-dev`\n    - On Fedora: `sudo dnf install python3-devel`\n    - On macOS: `brew install python`\n- [uv](https://docs.astral.sh/uv/) for python project management\n\n### Building the Library\n\n1. Clone the repo\n2. Navigate to the project directory\n3. Create a virtual environment (optional but recommended)\n   ```shell\n   uv venv\n   source .venv/bin/activate\n   ```\n4. Build and install the project\n   ```shell\n    uv sync\n    ```\n5. Run the Python tests\n\n   With the virtual environment activated, libseis should be installed and available.\n\n    ```shell\n    pytest\n    ```\n6. Run the C++ tests\n    ```shell\n    cd build\n    ctest\n    ```\n",
        "createdAt": "2018-10-31T16:16:56.000Z",
        "updatedAt": "2025-06-23T10:19:53.000Z",
        "language": "C++",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/dpgraham4401/libseis/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "NeilWilkins/VesPy",
        "url": "https://github.com/NeilWilkins/VesPy",
        "description": "A Python toolkit for array seismology",
        "stars": 19,
        "forks": 3,
        "readme": "# VesPy\nA Python toolkit for seismic array analysis\n-----\nVersion 1.1\n14th March 2018\n-------\nNeil Wilkins\nUniversity of Bristol\nneil.wilkins (at) bristol (dot) ac (dot) uk\n\n\n\nInstall using Python (distutils):\n\npython setup.py install\n",
        "createdAt": "2017-12-08T14:29:42.000Z",
        "updatedAt": "2025-05-12T08:36:17.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/NeilWilkins/VesPy/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "seisbench/seisbench",
        "url": "https://github.com/seisbench/seisbench",
        "description": "SeisBench - A toolbox for machine learning in seismology",
        "stars": 363,
        "forks": 110,
        "readme": "<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/seisbench/seisbench/main/docs/_static/seisbench_logo_subtitle_outlined.svg\" />\n</p>\n\n---\n\n[![PyPI - License](https://img.shields.io/pypi/l/seisbench)](https://github.com/seisbench/seisbench/blob/main/LICENSE)\n[![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/seisbench/seisbench/main_push.yml?branch=main)](https://github.com/seisbench/seisbench)\n[![Read the Docs](https://img.shields.io/readthedocs/seisbench)](https://seisbench.readthedocs.io/en/latest/)\n[![PyPI](https://img.shields.io/pypi/v/seisbench)](https://pypi.org/project/seisbench/)\n[![Python 3.10](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/release/python-3100/)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5568813.svg)](https://doi.org/10.5281/zenodo.5568813)\n\nThe Seismology Benchmark collection (*SeisBench*) is an open-source python toolbox for\nmachine learning in seismology.\nIt provides a unified API for accessing seismic datasets and both training and applying machine learning algorithms to seismic data.\nSeisBench has been built to reduce the overhead when applying or developing machine learning techniques for seismological tasks.\n\n## Getting started\n\nSeisBench offers three core modules, `data`, `models`, and `generate`.\n`data` provides access to benchmark datasets and offers functionality for loading datasets.\n`models` offers a collection of machine learning models for seismology.\nYou can easily create models, load pretrained models or train models on any dataset.\n`generate` contains tools for building data generation pipelines.\nThey bridge the gap between `data` and `models`.\n\nThe easiest way of getting started is through our colab notebooks.\n\n| Examples                                         |                                                                                                                                                                                                         |\n|--------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Dataset basics                                   | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/01a_dataset_basics.ipynb)                  |\n| Model API                                        | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/01b_model_api.ipynb)                       |\n| Generator Pipelines                              | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/01c_generator_pipelines.ipynb)             |\n| Applied picking                                  | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/02a_deploy_model_on_streams_example.ipynb) |\n| Using DeepDenoiser                               | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/02b_deep_denoiser.ipynb)                   |\n| Depth phases and earthquake depth                | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/02c_depth_phases.ipynb)                    |\n| Training PhaseNet (advanced)                     | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/03a_training_phasenet.ipynb)               |\n| Creating a dataset (advanced)                    | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/03b_creating_a_dataset.ipynb)              |\n| Training Denoiser (advanced)                     | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/03e_training_denoiser.ipynb)               |\n| Building an event catalog with GaMMA (advanced)  | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/03c_catalog_seisbench_gamma.ipynb)         |\n| Building an event catalog with PyOcto (advanced) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/03d_catalog_seisbench_pyocto.ipynb)        |\n\nAlternatively, you can clone the repository and run the same [examples](https://github.com/seisbench/seisbench/tree/main/examples) locally.\n\nFor more detailed information on Seisbench check out the [SeisBench documentation](https://seisbench.readthedocs.io/).\n\n## Installation\n\nSeisBench can be installed in two ways.\nIn both cases, you might consider installing SeisBench in a virtual environment, for example using [conda](https://docs.conda.io/en/latest/).\n\nThe recommended way is installation through pip.\nSimply run:\n```\npip install seisbench\n```\n\nAlternatively, you can install the latest version from source.\nFor this approach, clone the repository, switch to the repository root and run:\n```\npip install .\n```\nwhich will install SeisBench in your current python environment.\n\n### CPU only installation\n\nSeisBench is built on pytorch, which in turn runs on CUDA for GPU acceleration.\nSometimes, it might be preferable to install pytorch without CUDA, for example, because CUDA will not be used and the CUDA binaries are rather large.\nTo install such a pure CPU version, the easiest way is to follow a two-step installation.\nFirst, install pytorch in a pure CPU version [as explained here](https://pytorch.org/).\nSecond, install SeisBench the regular way through pip.\nExample instructions would be:\n```\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\npip install seisbench\n```\n\n## Contributing\nThere are many ways to contribute to SeisBench and we are always looking forward to your contributions.\nCheck out the [contribution guidelines](https://github.com/seisbench/seisbench/blob/main/CONTRIBUTING.md) for details on how to contribute.\n\n## Known issues\n\n- Some institutions and internet providers are blocking access to our data and model repository, as it is running on a non-standard port (2880).\n  This usually manifests in timeouts when trying to download data or model weights.\n  To verify the issue, try accessing [https://hifis-storage.desy.de:2880/](https://hifis-storage.desy.de:2880/) directly from the same machine.\n  As a mitigation, you can use our backup repository. Just run `seisbench.use_backup_repository()`.\n  Please note that the backup repository will usually show lower download speeds.\n  We recommend contacting your network administrator to allow outgoing access to TCP port 2880 on our server as a higher performance solution.\n- We've recently changed the URL of the SeisBench repository. To use the new URL update to SeisBench 0.4.1.\n  It this is not possible, you can use the following commands within your runtime to update the URL manually:\n  ```python\n  import seisbench\n  from urllib.parse import urljoin\n\n  seisbench.remote_root = \"https://hifis-storage.desy.de:2880/Helmholtz/HelmholtzAI/SeisBench/\"\n  seisbench.remote_data_root = urljoin(seisbench.remote_root, \"datasets/\")\n  seisbench.remote_model_root = urljoin(seisbench.remote_root, \"models/v3/\")\n  ```\n- On the Apple M1 and M2 chips, pytorch seems to not always work when installed directly within `pip install seisbench`.\n  As a workaround, follow the instructions at (https://pytorch.org/) to install pytorch and then install SeisBench as usual through pip.\n- EQTransformer model weights \"original\" in version 1 and 2 are incompatible with SeisBench >=0.2.3. Simply use `from_pretrained(\"original\", version=\"3\")` or `from_pretrained(\"original\", update=True)`. The weights will not differ in their predictions.\n\n## References\nReference publications for SeisBench:\n\n---\n\n* [SeisBench - A Toolbox for Machine Learning in Seismology](https://doi.org/10.1785/0220210324)\n\n  _Reference publication for software._\n\n---\n\n* [Which picker fits my data? A quantitative evaluation of deep learning based seismic pickers](https://doi.org/10.1029/2021JB023499)\n\n  _Example of in-depth bencharking study of deep learning-based picking routines using the SeisBench framework._\n\n---\n\n## Acknowledgement\n\nThe initial version of SeisBench has been developed at [GFZ Potsdam](https://www.gfz-potsdam.de/) and [KIT](https://www.gpi.kit.edu/) with funding from [Helmholtz AI](https://www.helmholtz.ai/).\nThe SeisBench repository is hosted by [HIFIS - Helmholtz Federated IT Services](https://www.hifis.net/).\n",
        "createdAt": "2020-11-12T16:25:21.000Z",
        "updatedAt": "2025-12-03T12:25:27.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.5568813",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.5568813",
            "dataCite": "10.5281/zenodo.5568813",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/seisbench/seisbench/main/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.5568813",
            "title": "SeisBench - A Toolbox for Machine Learning in Seismology",
            "journal": "Zenodo",
            "dateReleased": "2021-10-14T00:00:00.000Z",
            "abstract": "The Seismology Benchmark collection (<em>SeisBench</em>) is an open-source python toolbox for machine learning in seismology. It provides a unified API for accessing seismic datasets and both training and applying machine learning algorithms to seismic data. SeisBench has been built to reduce the overhead when applying or developing machine learning techniques for seismological tasks. SeisBench offers three core modules, <code>data</code>, <code>models</code>, and <code>generate</code>. <code>data</code> provides access to benchmark datasets and offers functionality for loading datasets. <code>models</code> offers a collection of machine learning models for seismology. You can easily create models, load pretrained models or train models on any dataset. <code>generate</code> contains tools for building data generation pipelines. They bridge the gap between <code>data</code> and <code>models</code>.",
            "citationsArray": []
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "hemmelig/QuakeMigrate_LiveDemoSSW2021",
        "url": "https://github.com/hemmelig/QuakeMigrate_LiveDemoSSW2021",
        "description": "A live demonstration of QuakeMigrate for the Student Seismology Workshop, 2021",
        "stars": 3,
        "forks": 1,
        "readme": "# QuakeMigrate Live Demonstration - Student Seismology Workshop 2021\nThis repository contains the source code and notebooks for the QuakeMigrate live demo given at the Student Seismology Workshop 2021. This workshop demo is distributed and given using Docker, which allows us to provide a ready-to-use QuakeMigrate environment. No compiler hassle, no dependency faff, and future-proof. To get started, download and install a copy of [Docker](https://www.docker.com).\n\nThe image itself is hosted on DockerHub - once you have downloaded and installed Docker, open a new terminal (MacOS/Linux) or PowerShell (Windows), navigate to the demo directory and run:\n\n```\ndocker pull hemmelig/quakemigrate-ssw\n```\n\nto download the image, followed by:\n\n```\ndocker run -p 8888:8888 quakemigrate-ssw\n```\n\nThen copy the URL output to terminal into any browser to load the Jupyter Lab session. The key example is in the `volcano-tectonic_example` directory.\n\nLicense\n-------\nThis live demo is written and maintained by the QuakeMigrate developers, Copyright QuakeMigrate developers 2020-2021. It is distributed under the GPLv3 License. Please see the [LICENSE](LICENSE) file for a complete description of the rights and freedoms that this provides the user.\n\nWhat is Docker?\n---------------\nDocker is a set of platform as a service products that use OS-level virtualisation to deliver software in packages called containers. Containers are isolated from one another and bundle their own software, libraries and configuration files; they can communicate with each other through well-defined channels. (From Wikipedia)\n\nContact\n-------\nAny additional comments/questions can be directed to:\n* **Conor Bacon** - conor.bacon@esc.cam.ac.uk\n* **Tom Winder** - tom.winder@esc.cam.ac.uk\n",
        "createdAt": "2021-03-23T01:02:02.000Z",
        "updatedAt": "2022-11-04T10:44:53.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/hemmelig/QuakeMigrate_LiveDemoSSW2021/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "llmlks/QuakeWatch",
        "url": "https://github.com/llmlks/QuakeWatch",
        "description": "A tool for visualising and analysing seismological catalogs",
        "stars": 1,
        "forks": 2,
        "readme": "# QuakeWatch\n\n![PEP8-lint](https://github.com/llmlks/QuakeWatch/workflows/PEP8-lint/badge.svg?branch=master)\n\n## Installation\n\nFirst clone the reporitory:\n```\ngit clone https://github.com/llmlks/QuakeWatch.git\ncd QuakeWatch\n```\nInstall the requirements:\n\n```\npip install -r requirements.txt\n```\n\nCreate a new file called ```config.py``` in the app root folder that contains the line ```THUNDERFOREST_API_KEY = '???'``` where ```???``` is replaced by a valid Thunderforest API key.\n\nRun the development server:\n\n```\npython index.py\n```\nYou can access the app on your browser at http://127.0.0.1:8050\n\n## Deployment\n\nIn order to deploy the application, you should use a [WSGI](https://en.wikipedia.org/wiki/Web_Server_Gateway_Interface) of your choice, such as [uWSGI](https://uwsgi-docs.readthedocs.io/en/latest/) or [Gunicorn](https://gunicorn.org/). You can follow any instructions for setting up a WSGI framework for Flask (since Dash uses Flask under the hood). The WSGI module can be accessed with `index:server`.\n\n## Linting\n\nEveryone should use a [PEP8](https://www.python.org/dev/peps/pep-0008/) linter to ensure the code is clean. \n\nIf you're using Visual Studio Code, you can enable PEP8 linting as follows:\n\n1) Install the Python extension from the VSCode extensions marketplace\n2) Open the Command Palette (```Ctrl+Shift+P```) and select the __Python: Select Linter__ command\n3) Select __pycodestyle__ and install the linter if asked to\n4) Now the code should be linted everytime you save a file\n\nAutomatic PEP8 linting is done after every push by using Github Actions.",
        "createdAt": "2020-02-11T19:25:36.000Z",
        "updatedAt": "2023-11-16T08:03:13.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/llmlks/QuakeWatch/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "fgittins/RealisticSeismology",
        "url": "https://github.com/fgittins/RealisticSeismology",
        "description": "An implementation of realistic, finite-temperature nuclear-matter models in neutron-star seismology.",
        "stars": 7,
        "forks": 1,
        "readme": "# RealisticSeismology\n\nThis project presents an implementation of realistic, finite-temperature nuclear-matter models in neutron-star seismology. It was developed to support [Gittins and Andersson (Phys. Rev. D **111**, 083024, 2025)](https://doi.org/10.1103/PhysRevD.111.083024) and [Gittins _et al._ (Phys. Rev. D **111**, 023049, 2025)](https://doi.org/10.1103/PhysRevD.111.023049).\n\n## Installation\n\nThe software is developed using the Julia programming language. To use it:\n\n1. Install [Julia](https://julialang.org/downloads/)\n\n2. Download this repository\n\n3. Run Julia in the repository directory\n\n4. Type `]` to enter Julia's package manager (Pkg.jl) REPL,\n\n```julia-repl\n(@v1.10) pkg>\n```\n\n5. `activate` the project environment with\n\n```julia-repl\n(@v1.10) pkg> activate .\n```\n\n6. `instantiate` the project,\n\n```julia-repl\n(RealisticSeismology) pkg> instantiate\n```\n\nFor more information on Julia packages and environments, see the [Pkg.jl documentation](https://pkgdocs.julialang.org/v1/).\n\n## Getting started\n\nGeneral use of this software is demonstrated in the `scripts` and `notebooks` directories. The notebooks are written in Julia Markdown and may be compiled using [Weave.jl](https://weavejl.mpastell.com/stable/). For example,\n\n```julia-repl\njulia> using Weave\n\njulia> weave(\"mode_demo.jmd\")\n```\n\n## Citation\n\nIf you found this project to be useful in academic work, please cite it using the following references:\n\n```bibtex\n@article{gittins2024neutronstar,\n        title=\"{Neutron-star seismology with realistic, finite-temperature nuclear matter}\", \n       author={{Gittins}, F. and {Andersson}, N.},\n      journal={Phys.\\ Rev.\\ D},\n         year={2025},\n        month=apr,\n       volume={111},\n        issue={8},\n        pages={083024},\n          doi={10.1103/PhysRevD.111.083024},\n       eprint={2406.05177},\narchivePrefix={arXiv},\n primaryClass={gr-qc}\n}\n\n@article{gittins2025problematicsystematics,\n        title=\"{Problematic systematics in neutron-star merger simulations}\", \n       author={{Gittins}, F. and {Matur}, R. and {Andersson}, N. and {Hawke}, I.},\n      journal={Phys.\\ Rev.\\ D},\n         year={2025},\n        month=jan,\n       volume={111},\n        issue={2},\n        pages={023049},\n          doi={10.1103/PhysRevD.111.023049},\n       eprint={2409.13468},\narchivePrefix={arXiv},\n primaryClass={gr-qc}\n}\n```\n",
        "createdAt": "2024-06-07T08:26:17.000Z",
        "updatedAt": "2025-09-05T16:47:00.000Z",
        "language": "Julia",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/fgittins/RealisticSeismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "SupravoCoder/Bhukamp",
        "url": "https://github.com/SupravoCoder/Bhukamp",
        "description": "Bhukamp (भूकंप - meaning \"earthquake\" in Hindi) is an intelligent seismic forecasting platform designed specifically for the Indian subcontinent. Using cutting-edge machine learning and real-time geological data, we help communities prepare for and respond to seismic events.",
        "stars": 3,
        "forks": 0,
        "readme": "# 🌍 Bhukamp - भूकंप\n## Predicting the Unpredictable – Real-Time ML Seismic Forecasting for India\n\n[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://your-app-url.streamlit.app)\n[![Python](https://img.shields.io/badge/python-v3.8+-blue.svg)](https://www.python.org/downloads/)\n[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)\n\n**Bhukamp** (भूकंप - meaning \"earthquake\" in Hindi) is an intelligent seismic forecasting platform designed specifically for the Indian subcontinent. Using cutting-edge machine learning and real-time geological data, we help communities prepare for and respond to seismic events.\n\n## 🎯 Mission\nOur mission is to increase earthquake awareness and promote disaster preparedness across India through advanced AI-powered predictions and comprehensive risk analysis.\n\n## 🔍 Key Features\n\n### 🌐 Real-Time Monitoring\n- **Live Indian Subcontinent Earthquake Tracking**: Real-time data from USGS Earthquake API\n- **Instant Alert System**: Customizable magnitude-based alerts with sound notifications\n- **Multi-Language Support**: Available in English and Hindi with more languages coming soon\n\n### 🤖 Machine Learning Predictions\n- **Advanced ML Models**: Random Forest, LSTM, and Physics-Informed Neural Networks (PINN)\n- **Regional Data Training**: Models specifically trained on Indian subcontinent seismic patterns\n- **Multiple Prediction Types**: \n  - Real-time earthquake prediction\n  - Future seismic activity forecasting\n  - Susceptibility mapping\n\n### 📊 Comprehensive Analytics\n- **Historical Data Analysis**: Extensive visualization of past earthquake patterns\n- **Risk Assessment**: Detailed magnitude and depth-based risk evaluation\n- **Statistical Insights**: Magnitude distribution, depth analysis, and regional activity tracking\n\n### 🎨 Modern User Interface\n- **Responsive Design**: Works seamlessly on desktop and mobile devices\n- **Dark/Light Themes**: Customizable themes for better user experience\n- **Glass-morphism Design**: Modern, beautiful UI with smooth animations\n- **Interactive Dashboards**: Real-time data visualization with dynamic charts\n\n## 🚀 Quick Start\n\n### Prerequisites\n- Python 3.8 or higher\n- pip package manager\n\n### Installation\n\n1. **Clone the repository**\n   ```bash\n   git clone https://github.com/SupravoCoder/Bhukamp.git\n   cd Bhukamp\n   ```\n\n2. **Install dependencies**\n   ```bash\n   cd myproject\n   pip install -r requirements.txt\n   ```\n\n3. **Run the application**\n   ```bash\n   streamlit run Bhukamp_app.py\n   ```\n\n4. **Open your browser**\n   Navigate to `http://localhost:8501` to access the application.\n\n## 📁 Project Structure\n\n```\nStreamlitPython/\n├── myproject/\n│   ├── Bhukamp_app.py              # Main application file\n│   ├── requirements.txt            # Python dependencies\n│   ├── pages/\n│   │   ├── Historical_Analysis.py  # Historical earthquake data analysis\n│   │   ├── Predictor_Earthquake.py # ML prediction interface\n│   │   ├── India_Live_Feed.py      # Real-time earthquake feed\n│   │   └── Susceptibility Predictor.py # Susceptibility mapping\n│   ├── data/                       # Earthquake datasets and predictions\n│   ├── static/                     # Static assets (images, sounds)\n│   └── animations/                 # Animation files\n├── models/                         # Pre-trained ML models\n├── Susceptability_pred_ML/         # ML model development notebooks\n└── Internship_Project_Updated.ipynb # Main research notebook\n```\n\n## 🧠 Machine Learning Models\n\n### 1. Random Forest Classifier\n- **Purpose**: Real-time earthquake prediction and susceptibility mapping\n- **Features**: Latitude, longitude, depth, historical patterns\n- **Accuracy**: 85%+ on test data\n\n### 2. LSTM Neural Network\n- **Purpose**: Time-series earthquake forecasting\n- **Features**: Sequential seismic data, temporal patterns\n- **Use Case**: Future earthquake prediction (25-100 years)\n\n### 3. Physics-Informed Neural Networks (PINN)\n- **Purpose**: Physics-based seismic modeling\n- **Features**: Geological constraints, physical laws\n- **Advantage**: Incorporates domain knowledge\n\n## 📈 Data Sources\n\n- **Real-time Data**: USGS Earthquake API\n- **Historical Data**: Comprehensive earthquake catalogs for Indian subcontinent\n- **Features**: Magnitude, location, depth, time, geological characteristics\n- **Coverage**: 1900-2024 earthquake records\n\n## 👥 Team Bhukamp\n\n| Member | Role | Expertise |\n|--------|------|-----------|\n| **Supravo Biswas** | 🔬 Full Pipeline Developer & ML Contributor | Streamlit, Full Stack, ML, Data Analysis |\n| **Suvanjan Das** | 🧠 Project Coordinator |  Data Science, Machine Learning, Scientific Writing, Seismology |\n| **Abir Saha** | 🌍 ML Model Validator & Feature Engineer | Seismology, ML Validation, Web Development |\n| **Arja Banerjee** | 📚 Scientific Machine Learning | Machine Learning, Deep Learning, Scientific Modelling and Writing, Seismology |\n| **Sayan Rana** | 📊 Data Scientist & ML Contributor | Statistics, Feature Engineering |\n| **Iqbal Shaikh** | 🎨 ML Contributor | Machine Learning, Python |\n\n## 🔧 Technical Stack\n\n- **Frontend**: Streamlit, HTML, CSS, JavaScript\n- **Backend**: Python, Pandas, NumPy\n- **Machine Learning**: Scikit-learn, TensorFlow, Keras\n- **Data Visualization**: Plotly, Matplotlib, Seaborn\n- **APIs**: USGS Earthquake API, Custom ML inference\n- **Database**: SQLite for notifications, CSV for data storage\n\n## 📱 Features in Detail\n\n### Alert System\n- Customizable magnitude thresholds\n- Sound alerts and visual notifications\n- Multi-channel notification support\n- Emergency contact integration\n\n### Prediction Interface\n- Interactive map visualization\n- Adjustable prediction parameters\n- Model comparison tools\n- Export functionality for predictions\n\n### Historical Analysis\n- Time-series visualization\n- Magnitude-frequency relationships\n- Regional seismic patterns\n- Statistical trend analysis\n\n## 🌟 Future Enhancements\n\n- [ ] Mobile app development\n- [ ] WhatsApp Business API integration\n- [ ] Government alert system integration\n- [ ] Advanced geological feature integration\n- [ ] Community reporting system\n- [ ] Multi-platform deployment\n\n## 📄 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## 🤝 Contributing\n\nWe welcome contributions! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.\n\n## 📞 Contact\n\n- **Email**: supravo.biswas@gmail.com\n- **GitHub**: [@SupravoCoder](https://github.com/SupravoCoder)\n- **Project Link**: [https://github.com/SupravoCoder/Bhukamp](https://github.com/SupravoCoder/Bhukamp)\n\n## 🙏 Acknowledgments\n\n- USGS for providing real-time earthquake data\n- Indian Meteorological Department for historical seismic data\n- Open-source community for amazing tools and libraries\n- Our mentors and guides for continuous support\n\n---\n\n**Built with ❤️ by Team Bhukamp for a safer India**\n\n*\"In the face of nature's unpredictability, knowledge and preparation are our strongest defenses.\"*\n",
        "createdAt": "2025-07-11T17:33:54.000Z",
        "updatedAt": "2025-07-21T16:27:32.000Z",
        "language": "Jupyter Notebook",
        "homepage": "https://bhukamp.streamlit.app/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/SupravoCoder/Bhukamp/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "iLUVE69/assignment5_statistical_seismology",
        "url": "https://github.com/iLUVE69/assignment5_statistical_seismology",
        "description": null,
        "stars": 1,
        "forks": 0,
        "readme": "# assignment5_statistical_seismology\n\n## Assignment Write-up: Zaliapin–Zion Earthquake Declustering\n\nThe **Zaliapin–Zion declustering algorithm** is a physics-based method designed to distinguish between background seismicity and clustered aftershocks in earthquake catalogs. The method assumes that aftershocks tend to be closer in space and time to their triggering mainshock, whereas background events are more randomly distributed. This distinction is quantified using a distance metric **η (eta)**, which incorporates spatial, temporal, and magnitude-dependent factors.\n\nIn this assignment, we implement the Zaliapin–Zion declustering algorithm using a dataset of seismic events in Greece. Each seismic event is described by its timestamp, geographic coordinates (latitude and longitude), and magnitude. The η value for each event is computed with respect to all prior events using the formula:\n\n$$\n\\eta = \\frac{r}{10^{-bM}} \\cdot t^q\n$$\n\nWhere:\n- \\( r \\) is the spatial distance between events (computed using the Haversine formula),\n- \\( M \\) is the magnitude of the event,\n- \\( t \\) is the time difference between events (in days),\n- \\( b \\) and \\( q \\) are empirical constants, commonly set as \\( b = 1.0 \\) and \\( q = 0.5 \\).\n\nThe **minimum η** value is extracted for each event. By taking the logarithm of η, we separate the seismic events into two populations: events with **log₁₀(η) < 4.0** are categorized as **aftershocks**, while those with **log₁₀(η) ≥ 4.0** are considered **background events**.\n\nThe results include a histogram showing the distribution of log₁₀(η) values and a clear distinction between clustered and background seismicity. Additionally, the aftershocks are plotted on an interactive map of Greece to visually analyze their spatial distribution. This implementation validates the effectiveness of the Zaliapin–Zion method in earthquake declustering and provides valuable insights into regional seismic patterns.\n\n",
        "createdAt": "2025-04-04T14:32:43.000Z",
        "updatedAt": "2025-04-16T07:03:53.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/iLUVE69/assignment5_statistical_seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ghfbsd/sacbook",
        "url": "https://github.com/ghfbsd/sacbook",
        "description": "Up-to-date online analysis methods provided in \"The Seismic Analysis Code A Primer and User's Guide\" by George Helffrich, James Wookey, Ian Bastow",
        "stars": 8,
        "forks": 9,
        "readme": "sacbook\n=======\n\nUp-to-date online analysis methods provided in \"The Seismic Analysis Code A Primer and User's Guide\" by George Helffrich, James Wookey, Ian Bastow\n",
        "createdAt": "2013-08-13T08:49:03.000Z",
        "updatedAt": "2025-01-14T16:02:56.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ghfbsd/sacbook/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "trichter/rf",
        "url": "https://github.com/trichter/rf",
        "description": "Receiver function calculation in seismology",
        "stars": 120,
        "forks": 63,
        "readme": "# rf\n## Receiver function calculation in seismology\n[![build status](https://github.com/trichter/rf/workflows/tests/badge.svg)](https://github.com/trichter/rf/actions)\n[![docs status](https://readthedocs.org/projects/rf/badge/?version=latest)](https://rf.readthedocs.io)\n[![codecov](https://codecov.io/gh/trichter/rf/branch/master/graph/badge.svg)](https://codecov.io/gh/trichter/rf)\n[![pypi version](https://img.shields.io/pypi/v/rf.svg)](https://pypi.python.org/pypi/rf)\n[![python version](https://img.shields.io/pypi/pyversions/rf.svg)](https://python.org)\n[![JOSS](http://joss.theoj.org/papers/10.21105/joss.01808/status.svg)](https://doi.org/10.21105/joss.01808)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4455036.svg)](https://doi.org/10.5281/zenodo.4455036)\n\n##### Documentation: https://rf.readthedocs.io/\n##### Tutorials:\n  1. Calculate receiver functions - minimal example ([notebook][nb1])\n  2. Calculate receiver functions and stack them by common conversion points to create a profile ([notebook][nb2])\n  3. Compare different deconvolution methods ([notebook][nb3])\n  4. Harmonic deconvolution with synthetics - minimal example ([notebook][nb4])\n\n[nb1]: https://nbviewer.jupyter.org/github/trichter/notebooks/blob/master/receiver_function_minimal_example.ipynb\n[nb2]: https://nbviewer.jupyter.org/github/trichter/notebooks/blob/master/receiver_function_profile_chile.ipynb\n[nb3]: https://nbviewer.jupyter.org/github/hfmark/notebooks/blob/main/rf_comparison.ipynb\n[nb4]: https://nbviewer.jupyter.org/github/hfmark/notebooks/blob/main/rf_harmonics.ipynb\n\n##### Get help and discuss: [ObsPy Related Projects category](https://discourse.obspy.org/c/obspy-related-projects/rf/14) in the ObsPy forum\n\n##### Contribute:\n\nAll contributions are welcome ... e.g. report bugs, discuss or add new features.\n\n##### Citation:\n\nIf you found this package useful, please consider citing it.\n\nTom Eulenfeld (2020), rf: Receiver function calculation in seismology, *Journal of Open Source Software*, 5(48), 1808, doi: [10.21105/joss.01808](https://doi.org/10.21105/joss.01808) [[pdf]](https://www.theoj.org/joss-papers/joss.01808/10.21105.joss.01808.pdf)\n\n##### Related receiver function projects\n\n* [seispy](https://github.com/xumi1993/seispy) including hk-stacking\n* [RFPy](https://github.com/paudetseis/RfPy) including hk-stacking, harmonic decomposition\n* [BayHunter](https://github.com/jenndrei/BayHunter) inversion of receiver functions and surface wave dispersion\n* [telewavesim](https://github.com/paudetseis/Telewavesim) synthetics\n",
        "createdAt": "2013-02-18T17:38:22.000Z",
        "updatedAt": "2025-11-18T08:28:35.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.4455036",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.4455036",
            "dataCite": "10.5281/zenodo.4455036",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/trichter/rf/master/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.4455036",
            "title": "trichter/rf: v1.1.1",
            "journal": "Zenodo",
            "dateReleased": "2025-02-21T00:00:00.000Z",
            "abstract": "Receiver function calculation in seismology",
            "citationsArray": []
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Strafred/Geo-vkr-react",
        "url": "https://github.com/Strafred/Geo-vkr-react",
        "description": "web app for seismological networks monitoring",
        "stars": 0,
        "forks": 0,
        "readme": "Deployed on http://84.237.89.72:88/ (currently dreg version)\n\n### In the project directory, you can run:\n\n`npm install`\n\nTo install all the dependencies required for the project to run.\n\n`npm start`\n\nRuns the app in the development mode.\nOpen http://localhost:3000 to view it in your browser.\n\nThe page will reload when you make changes.\nYou may also see any lint errors in the console.\n\n`npm run build`\n\nBuilds the app for production to the build folder.\nIt correctly bundles React in production mode and optimizes the build for the best performance.\n\nThe build is minified and the filenames include the hashes.\nYour app is ready to be deployed!\n",
        "createdAt": "2022-11-01T14:20:54.000Z",
        "updatedAt": "2023-05-15T11:34:43.000Z",
        "language": "JavaScript",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Strafred/Geo-vkr-react/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "issamchong/process_automation",
        "url": "https://github.com/issamchong/process_automation",
        "description": "seismology",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2019-10-02T00:18:49.000Z",
        "updatedAt": "2019-10-02T00:50:05.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Ryzagi/Seismology-net",
        "url": "https://github.com/Ryzagi/Seismology-net",
        "description": "Research notebook for paper",
        "stars": 0,
        "forks": 0,
        "readme": "# Seismology-net\n# Research notebook for paper\nКомпьютерные системы анализа временных рядов помогают сейсмологам \nпроводить обработку потоков информации сейсмического мониторинга, решая задачу \nпрогнозирования вероятности появления разрушительного события с большой \nмагнитудой. Рекуррентные нейронные сети (RNN-Recurrent neural network)  являются \nодной из самых мощных моделей для обработки последовательных данных, например, \nтаких как временные ряды. Нейросетевая архитектура LSTM-Long short-term memory\n– является успешной разновидностью архитектуры RNN. Целью данной работы является \nразработка, настройка и обучение нейронной сети типа LSTM для прогнозирования \nмагнитуды на основе базы данных «Землетрясения России» от федерального \nисследовательского центра Единая геофизическая служба Российской академии наук \n(ЕГС РАН) \n",
        "createdAt": "2021-03-19T14:21:14.000Z",
        "updatedAt": "2022-06-01T21:33:29.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Ryzagi/Seismology-net/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "RDokht/Seismology",
        "url": "https://github.com/RDokht/Seismology",
        "description": "Tools for research in Seismology",
        "stars": 5,
        "forks": 1,
        "readme": "# Seismology\nJulia Tools for research in Seismology\n\nThe current repository includes the following scripts and modules:\n\n**1. get_response_SACPZs.jl**\\\nThis function extracts the instrument response information from a SAC Poles and Zeros response file for a given DateTime.\n\n**2. removeIR.jl**\\\nThis function deconvolve the instrument response from the input trace(s) using the SAC Poles and Zeros of instrument response.\n\n**3. SubModules.jl**\\\nThis module includes the following functions:\n- freqcnt:\\\nIt calculates the frequency content of input trace(s) using the DSP package.\n- uniqueidx:\\\n  It returns the indices of unique elements of an input 1D array.\n- snrFcn:\\\n  It calculates the SNR function(s) of input trace(s).\n- fftshift:\\\n  It applies time-shift(s) to input trace(s) using the fast fourier transform and the DSP package.\n- bwbp:\\\n  It applies the Butteworth bandpass filter to input trace(s) using the DSP package.\n- resample:\\\n  Ressampling the input trace(s) using the Interpolations package.\n- smooth1d:\\\n  Smoothing the input trace(s) using a moving window.\n- smooth2d:\\\n  Smoothing 2D array using 2D convolution of input array with two moving windows.\n- localminima:\\\n  It returns the indicies of local minima of an input trace.\n- localmaxima:\\\n  It returns the indicies of local maxima of an input trace.\n- stalta:\\\n  It calculates the STA/LTA functions of input traces using the original traces and/or their envelopes.\n- kurtosisFcn:\\\n  Calculating the kurtosis functions of input traces.\n- scale_kurtosisFcn:\\\n  Calculating the modified/scaled kurtosis functions from the original kurtosis functions.\n- pol_covmat:\\\n  It calculates the polarization parameters (such as: rectilinearity, degree of polarization, and etc) for input three-component waveform data using the eigen values of the covariance matrix.\n  \n\n\n",
        "createdAt": "2019-05-16T17:07:53.000Z",
        "updatedAt": "2022-01-06T01:22:06.000Z",
        "language": "Julia",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/RDokht/Seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ppizarror/normalized-spectrogram",
        "url": "https://github.com/ppizarror/normalized-spectrogram",
        "description": "Normalized spectrogram of a seismic acceleration ",
        "stars": 10,
        "forks": 2,
        "readme": "<h1 align=\"center\">\r\n  <img alt=\"Normalized spectrogram\" src=\"https://res.ppizarror.com/other/matlab.png\" width=\"200px\" height=\"200px\" />\r\n  <br /><br />\r\n  Normalized spectrogram</h1>\r\n<p align=\"center\">Normalized spectrogram of a seismic acceleration</p>\r\n<div align=\"center\"><a href=\"https://ppizarror.com\"><img alt=\"@ppizarror\" src=\"https://res.ppizarror.com/badges/author.svg\" /></a>\r\n<a href=\"https://www.gnu.org/licenses/old-licenses/gpl-2.0.html\"><img alt=\"GPL V2.0\" src=\"https://res.ppizarror.com/badges/licensegpl2.svg\" /></a>\r\n</div><br />\r\n\r\nNormalized spectrogram to seismic acceleration written in *Matlab*. The algorithm used is the following:\r\n\r\n```txt\r\n1. Baseline correction\r\n2. Tuckey window is applied with r=5%.\r\n3. FFT on window signal.\r\n4. Spectrum is smoothed by 5 points halfwidth moving average.\r\n5. Each element of spectrum is normalized by maximum spectral amplitude.\r\n```\r\n\r\n## The function\r\n\r\nThe normalized spectrogram function is defined by:\r\n\r\n```matlab\r\n[matrix, matrix_t, matrix_f] = norm_spectrogram(t, acc)\r\n```\r\n\r\nWhere:\r\n\r\n| Variable | Description |\r\n| :-: | :--|\r\n| t | Time of the seismic accelerogram |\r\n| acc | Acceleration (g) of the seismic accelerogram |\r\n\r\nThis function returns a **matrix** in where columns refer to mean window time (**matrix_t**), rows are the frequency of the spectrogram (**matrix_f**) and the value of the matrix are the amplitude of the spectrogram from each time-window on each frequency.\r\n\r\nTo print matrix you should use:\r\n\r\n```matlab\r\nplot_norm_matrix(m, mt, mf, t, acc, regname)\r\n```\r\n\r\nWhere:\r\n\r\n| Variable | Description |\r\n| :-: | :-- |\r\n| m | Normalized matrix |\r\n| mt | Time array from matrix |\r\n| mf | Frequency array from matrix |\r\n| t | Time of the seismic data |\r\n| acc | Acceleration of seismic data |\r\n| regname | Name of the seismic data (plot title) |\r\n\r\n## Example\r\n\r\nLets suppose that a seismic registry is stored on *data/CNV_APED_201604162359_N_100.txt*, the file structure is like:\r\n\r\n```txt\r\n0.000000 -6.329500\r\n0.010000 2.539600\r\n0.020000 12.822900\r\n0.030000 9.435300\r\n0.040000 -5.397100\r\n0.050000 -14.233900\r\n...\r\n```\r\n\r\nThen:\r\n\r\n```matlab\r\n% Load the data\r\ndata = load('data/CNV_APED_201604162359_N_100.txt');\r\n\r\n% Set time and acceleration array\r\nt = data(:, 1);\r\nacc = data(:, 2) ./ 980; % Convert from cm/s2 to g\r\n```\r\n\r\nAfter that we will use ```norm_spectrogram``` function\r\n\r\n```matlab\r\n[m, mt, mf] = norm_spectrogram(t, acc);\r\n```\r\n\r\nThen plot:\r\n\r\n```matlab\r\nplot_norm_matrix(m, mt, mf, t, acc, 'APED 2016/04/16 23:59 N-S');\r\n```\r\n\r\nObtaining:\r\n\r\n<p align=\"center\">\r\n<img src=\"https://res.ppizarror.com/images/normalized-spectrogram/figure.png\" width=\"70%\" >\r\n</p>\r\n\r\n## License\r\n\r\nThis project is licensed under GPLv2 [https://www.gnu.org/licenses/gpl-2.0.html]\r\n\r\n## Author\r\n\r\n[Pablo Pizarro R.](https://ppizarror.com) | 2017 - 2019\r\n",
        "createdAt": "2017-04-02T02:17:51.000Z",
        "updatedAt": "2025-06-07T01:11:21.000Z",
        "language": "MATLAB",
        "homepage": "https://mathworks.com/matlabcentral/fileexchange/108359-normalized-spectrogram",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ppizarror/normalized-spectrogram/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "philippeKowalski/Outils_Glorieuse",
        "url": "https://github.com/philippeKowalski/Outils_Glorieuse",
        "description": "Precise seismic data download from Glorieuse",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2022-10-31T12:29:10.000Z",
        "updatedAt": "2022-10-31T12:43:19.000Z",
        "language": "Shell",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "niyiyu/CloudPyASDF",
        "url": "https://github.com/niyiyu/CloudPyASDF",
        "description": "ASDF Python reader for cloud environment",
        "stars": 1,
        "forks": 1,
        "readme": "# CloudPyASDF\n\nThis is a python module that aims at improving the efficiency reading ASDF data from cloud environment.\n\n## PyASDF\n[PyASDF](https://github.com/SeismicData/pyasdf) is the python ASDF API that is already well developed. You could read the documents [here](https://seismicdata.github.io/pyasdf/index.html) to learn how to use it. We want to make our cloud API similar to the local API so that the advantage of cloud environment could be maximized.\n\n## H5coro\nSee [sliderule project](https://github.com/ICESat2-SlideRule/sliderule) to learn more about H5coro.\n\n\n## Docker\nDocker provides a \"clean\" environment that minimize the computation environment issue when users with different machines could test the module in an almost identical way.\n\nHere we provides a `Dockerfile` that can be used to generate the same Ubuntu intance that was used for our developing and testing. Get the Docker, and run build in the directory that contains the `Dockerfile`.\n```shell\n$ docker build .\n```\n",
        "createdAt": "2021-08-26T07:11:54.000Z",
        "updatedAt": "2022-11-30T23:13:42.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/niyiyu/CloudPyASDF/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "NadimaDwihusna/GPGN_552_Seismology_Course",
        "url": "https://github.com/NadimaDwihusna/GPGN_552_Seismology_Course",
        "description": "Intro to Seismology from Colorado School of Mines",
        "stars": 0,
        "forks": 0,
        "readme": "# GPGN_552_Seismology_Course\nIntro to Seismology from Colorado School of Mines\n",
        "createdAt": "2021-01-14T22:08:15.000Z",
        "updatedAt": "2021-01-14T22:09:05.000Z",
        "language": "MATLAB",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/NadimaDwihusna/GPGN_552_Seismology_Course/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "swiss-seismological-service/SeismoStats",
        "url": "https://github.com/swiss-seismological-service/SeismoStats",
        "description": "A Python package for seismicity analysis.",
        "stars": 32,
        "forks": 4,
        "readme": "<h1 align=\"center\">\n\n<img src=\"./docs/source/_static/seismostats.png\" height=\"175\">\n\n<hr \\>\n\n![pypi](https://img.shields.io/pypi/v/SeismoStats)\n[![PyPI - License](https://img.shields.io/pypi/l/seismostats.svg)](https://pypi.org/project/seismostats/)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/seismostats.svg)](https://pypi.org/project/seismostats/)\n[![test](https://github.com/swiss-seismological-service/SeismoStats/actions/workflows/tests.yml/badge.svg)](https://github.com/swiss-seismological-service/SeismoStats/actions/workflows/tests.yml)\n[![codecov](https://codecov.io/github/swiss-seismological-service/SeismoStats/graph/badge.svg?token=RVJFHYLBKA)](https://codecov.io/github/swiss-seismological-service/SeismoStats)\n[![Documentation Status](https://readthedocs.org/projects/seismostats/badge/?version=latest)](https://seismostats.readthedocs.io/en/latest/?badge=latest)\n\n</h1>\n\nMeasure your seismicity with **SeismoStats**, a Python package for seismicity analysis.\n\n>[!IMPORTANT]  \n> We are happy to receive feedback and suggestions for improvement. Just open an issue here: https://github.com/swiss-seismological-service/SeismoStats/issues, and we will work on this as soon as possible.\n\n## Start using `SeismoStats`:\n\nThis is intended for people interested in using existing functionalities and functions in `SeismoStats`, for example if you want to calculate an Mc, a-value and b-value for your catalogue and plot the frequency magnitude distribution.\n\n```\npip install seismostats\n```\n\nThat's all, you can now use SeismoStats!\n\n## Installation from source\n\nTo get the very latest version of SeismoStats, you can install it directly from the source code. This is especially useful if you want to contribute to the development of SeismoStats.\n\nAfter cloning, you can install SeismoStats by running the following command in the root directory of the repository:\n```\npip install .\n```\nOr in development mode, including the development dependencies.\n```\npip install -e '.[dev]'\n```\nTo run the tests.\n```\ntox\n```\n\n## Citing\nWe are actively working on a publication to submit with the first stable version of `SeismoStats`. If you use the code for scientific work, and until a pre-print is available, please cite `SeismoStats` as:\n\nMirwald, A., Schmid, N., Mizrahi L., Han, M., Rohnacher, A., Ritz, V. A., & Wiemer, S. (2025). SeismoStats: A Python Package for Statistical Seismology. https://github.com/swiss-seismological-service/SeismoStats\n\n```\n@misc{Mirwald2025,\n   author = {Aron Mirwald and Nicolas Schmid and Leila Mizrahi and Marta Han and Alicia Rohnacher and Vanille A. Ritz and Stefan Wiemer},\n   title = {SeismoStats: A Python Package for Statistical Seismology},\n   url = {https://github.com/swiss-seismological-service/SeismoStats},\n   year = {2025}\n}\n```\n",
        "createdAt": "2022-11-03T11:55:48.000Z",
        "updatedAt": "2025-12-01T15:24:34.000Z",
        "language": "Jupyter Notebook",
        "homepage": "https://seismostats.readthedocs.io",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/swiss-seismological-service/SeismoStats/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "mtuqorg/mtuq",
        "url": "https://github.com/mtuqorg/mtuq",
        "description": "moment tensor uncertainty quantification",
        "stars": 79,
        "forks": 28,
        "readme": "# mtuq\n\n[![Build Status](https://github.com/mtuqorg/mtuq/actions/workflows/python-app.yaml/badge.svg)](https://github.com/rmodrak/mtuq/blob/master/.github/workflows/python-app.yaml)\n[![SCOPED](https://img.shields.io/endpoint?url=https://runkit.io/wangyinz/scoped/branches/master/MTUQ)](https://github.com/SeisSCOPED/container/pkgs/container/MTUQ)\n\nMTUQ provides *m*oment *t*ensor estimates and *u*ncertainty *q*uantification from broadband seismic data.  \n\n\n## Getting started\n\n[Installation](https://mtuqorg.github.io/mtuq/install/index.html)\n\n[Quick start](https://mtuqorg.github.io/mtuq/quick_start.html)\n\n\n\n## Documentation\n\n[Acquiring seismic data](https://mtuqorg.github.io/mtuq/user_guide/02.html)\n\n[Acquiring Green's functions](https://mtuqorg.github.io/mtuq/user_guide/03.html)\n\n[Data processing](https://mtuqorg.github.io/mtuq/user_guide/04.html)\n\n[Visualization galleries](https://mtuqorg.github.io/mtuq/user_guide/05.html)\n\n[Library reference](https://mtuqorg.github.io/mtuq/library/index.html)\n\n\n## Highlights\n\nCommon use cases include [double couple moment tensor](https://github.com/mtuqorg/mtuq/blob/master/examples/SerialGridSearch.DoubleCouple.py), [full moment tensor](https://github.com/mtuqorg/mtuq/blob/master/examples/GridSearch.FullMomentTensor.py), [depth](https://github.com/rmodrak/mtuq/blob/master/examples/GridSearch.DoubleCouple%2BMagnitude%2BDepth.py) and [hypocenter](https://github.com/rmodrak/mtuq/blob/master/examples/GridSearch.DoubleCouple%2BMagnitude%2BHypocenter.py) uncertainty analysis.  Applications involving composite sources, force sources, constrained moment tensor sources, source-time functions, and other source parameters are also possible.\n\n\n### Solver interfaces\n\n[I/O functions](https://mtuqorg.github.io/mtuq/library/index.html#data-i-o)\nare included for reading AxiSEM, SPECFEM3D, and FK Green's functions as well as\ndownloading Green's functions from remote [syngine](http://ds.iris.edu/ds/products/syngine/) databases.\n\n\n\n### Misfit evaluation\n\nWaveform difference and cross-correlation time-shift [misfit evaluation](https://mtuqorg.github.io/mtuq/library/index.html#data-processing-and-inversion)\non body-wave and surface-wave windows is implemented in numba-accelerated Python.\n\nThese misfit functions can be used with [mtuq.grid_search](https://mtuqorg.github.io/mtuq/library/generated/mtuq.grid_search.grid_search.html), which automatically partitions the grid over multiple MPI processes if invoked from an MPI environment.  For efficient and unbiased uncertainty quantification, [uniform grids](https://mtuqorg.github.io/mtuq/library/index.html#moment-tensor-and-force-grids) can be used for the grid search, drawing from [Tape2015](https://academic.oup.com/gji/article/202/3/2074/613765).\n\nAlternatively, MTUQ misfit functions can be used as a starting point for Bayesian uncertainty quantification using pymc or other MCMC libraries.\n\n\n### Visualization\n\n[Visualization utilities](https://mtuqorg.github.io/mtuq/user_guide/05/gallery_mt.html) are included for both the [eigenvalue lune](https://onlinelibrary.wiley.com/doi/10.1111/j.1365-246X.2012.05491.x) and [v,w rectangle](https://academic.oup.com/gji/article/202/3/2074/613765), with matplotlib and Generic Mapping Tools graphics backends.\n\n\n### Testing\n\nThe package has been tested against [legacy Perl/C codes](https://github.com/mtuqorg/mtuq/blob/master/tests/benchmark_cap_vs_mtuq.py) as well as [published studies](https://github.com/rmodrak/mtbench).\n\n## Citation\n\nIf you use MTUQ in your research, please cite:\n\nJ Thurin, R Modrak, C Tape, A M McPherson, F R Rodríguez-Cardozo, J Kintner, L Ding, Q Liu, J Braunmiller, MTUQ: a framework for estimating moment tensors, point forces, and their uncertainties, *Geophysical Journal International*, Volume 241, Issue 2, May 2025, Pages 1373–1390, <https://doi.org/10.1093/gji/ggaf080>\n\nScripts for reproducing the grid-search results and figures from Cases 1 to 3 of this publication, along with useful files (best-fitting moment tensor parameters, weight files, first-motion polarity picks, etc.), are available in the Zenodo collection at <https://doi.org/10.5281/zenodo.13868450>.\n\n[Instaseis]: http://instaseis.net/\n\n[obspy]: https://github.com/obspy/obspy/wiki\n\n[ZhaoHelmberger1994]: https://pubs.geoscienceworld.org/ssa/bssa/article-abstract/84/1/91/102552/Source-estimation-from-broadband-regional?redirectedFrom=fulltext\n\n[ZhuHelmberger1996]: https://pubs.geoscienceworld.org/ssa/bssa/article-abstract/86/5/1634/120218/Advancement-in-source-estimation-techniques-using?redirectedFrom=fulltext\n\n",
        "createdAt": "2017-09-25T21:44:19.000Z",
        "updatedAt": "2025-11-19T07:10:45.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.13868450",
            "dataCite": "10.5281/zenodo.13868450",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/mtuqorg/mtuq/master/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.13868450",
            "title": "Supplementary material for \"Moment tensor and point force estimation, with applications to earthquakes, landslides, and the 2017 DPRK nuclear explosion\"",
            "journal": "Zenodo",
            "dateReleased": "2024-10-01T00:00:00.000Z",
            "abstract": "This Zenodo archive contains supplementary data and code for the paper Moment tensor and point force estimation, with applications to earthquakes, landslides, and the 2017 DPRK nuclear explosion (submitted to Geophysical Journal International). The archive provides scripts and data to reproduce some of the figures and results presented in the paper / supplementary material.",
            "citationsArray": []
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "tclements/SeisML.jl",
        "url": "https://github.com/tclements/SeisML.jl",
        "description": "Machine learning for Seismology in Julia",
        "stars": 1,
        "forks": 0,
        "readme": "# SeisML\n\n[![Stable](https://img.shields.io/badge/docs-stable-blue.svg)](https://tclements.github.io/SeisML.jl/stable)\n[![Dev](https://img.shields.io/badge/docs-dev-blue.svg)](https://tclements.github.io/SeisML.jl/dev)\n[![Build Status](https://travis-ci.com/tclements/SeisML.jl.svg?branch=master)](https://travis-ci.com/tclements/SeisML.jl)\n[![Coveralls](https://coveralls.io/repos/github/tclements/SeisML.jl/badge.svg?branch=master)](https://coveralls.io/github/tclements/SeisML.jl?branch=master)\n",
        "createdAt": "2019-07-01T16:20:51.000Z",
        "updatedAt": "2020-10-07T01:07:48.000Z",
        "language": "Julia",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/tclements/SeisML.jl/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "jkmathilda/planetary-seismology",
        "url": "https://github.com/jkmathilda/planetary-seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# Seismic Detection\nWe developed algorithmic and machine learning models to identify seismic events in real data from the Apollo missions and the Mars InSight mission, minimizing the transmission of unnecessary information. For the Moon, we applied traditional signal processing techniques like Fast Fourier Transform and spectrogram analysis to detect seismic events, achieving reasonable accuracy. For Mars, where the signal-to-noise ratio is lower, we used advanced machine learning, specifically a long short term memory model, to handle the time-series nature of seismic data. This model captures subtle temporal patterns and dependencies, helping to predict the timing of seismic events. Though the accuracy of the machine learning model was lower due to potential noise sensitivity, it offers a scalable solution that can be improved with further refinement. Our models help improve seismic event detection and enhance data efficiency and advance our understanding of planetary activity. \n\n## Additional data\nThis Google Drive contains data that we used for our project. \n[https://docs.google.com/document/d/11yPrLIqcA9FDIywRGnuaOL1iYACTb3Usb5KmRUoA8ms/edit?usp=sharing](https://drive.google.com/drive/folders/1UMmU471yz6-G3Kaj_hpo8AOv1dRT_c5i?usp=sharing)\n\n## Project Details\nPlanetary seismology missions face significant challenges with power limitations, particularly when transmitting continuous seismic data from distant planets back to Earth. Given that only a fraction of the data is scientifically useful, optimizing data transmission is critical. This project aims to address this issue by developing a computer program capable of distinguishing between noise and scientifically valuable seismic signals. By analyzing real data from the Apollo missions and the Mars Interior Exploration using Seismic Investigations, the goal is to identify and extract seismic events, minimizing the need to send back unnecessary information. \n\nIn developing our project, we used Google Colab as the primary development environment and Python as the primary programming language. We decided to use different algorithms for Moon and Mars predictions, because of the differences in signal-to-noise ratio and the amount of data that was available for training. \nWe created a webpage to present our results, and to display our results more efficiently, we created interactive plots for visualization. For creating interactive plots, we used Holoviews, a high-level data visualization library that simplifies the creation of interactive plots, and Panel, a powerful tool for building interactive web applications and dashboards in Python. The interactive plot displays continuous seismic records as curves over time with event markers. By hovering on the curve, users can retrieve information about the relative time and amplitude at that point. The user can also zoom in and out on specific sections of the data using the zoom box, time sliders and amplitude sliders. After doing some visual manipulations, the user can also save the plot as an image.\n\nWe approached this challenge by the following methods:\n### Algorithmic Predictive Models\n  <ul>\n    <li>\n      These models apply an algorithm on the raw data to get the predicted value. These models differ from machine learning due to the fact that there is no need to train or test them, the model cannot gain more “knowledge”. One advantage is that it is simplistic and easy to implement, and there is no need to worry about overfitting. However, the model cannot learn from the data as well as machine learning or artificial intelligence models can.\n    </li>\n    <li>\n      <strong>Fast Fourier Transform on Raw Data:</strong> Fast Fourier transform is a type of wave transform that extracts the amplitudes of the wave signal at each individual frequency (amplitude-frequency). This allows for increased signal to noise ratio as instead of the seismic activity occurring on the graph as a large blob, it instead appears as a single signal corresponding to the frequency of the wave that produces the seismic event. The model looks at the graph generated by the FFT, extracts the frequency with the maximum amplitude, and then maps it back to the time that it happened on the normal frequency-time graph, which gives our prediction. Some of the weaknesses that we found with this model was its inability to distinguish between brief high amplitude noise and a seismic event; it would regularly give a wrong prediction when such an event occurred. Further work on this model includes doing some denoising and filtering out outliers in amplitude to avoid false positives\n    </li>\n    <p align=\"center\"><img src=\"./readme_img/data_pipeline.png\" width=\"600px\"></p>\n    <li>\n      <strong>Spectrogram analysis on high-pass filtered data:</strong> High pass filter denoises the data by picking up the higher frequency signals and removes the lower frequency ones, which constitute most of the unwanted noise. The model then detects the highest power signal on the seismograph and returns the time that it occurred, which corresponds to a seismic event. This method has more success than the FFT model, due to the fact that high amplitude signals do not show up after denoising, which removes false positives. However, this model still has drawbacks due to the fact that only detecting the maximum power may not detect other clusters, which was the case when more than one seismic event occurred in one day. \n    </li>\n    <li>\n      <strong>Multi Prediction spectrogram analysis model:</strong> Same as the single prediction model, but to predict multiple seismic events, we check the spectrogram for power exceeding the 99th percentile, and make note of the times that these occur. We then do interval analysis on these time stamps, to find intervals where high energy readings are clustered, and mark it as a seismic event. This allows us to predict more than one seismic event in a single day. However, this causes false positives as even with denoising, noise can cause the model to incorrectly detect intervals of seismic activity. \n    </li>\n    <li>\n      <strong>Spectrogram cluster analysis:</strong> This method was used in the case of the mars data, due to the fact that high-pass filter denoising was not sufficient for the model to act on clear signals. Instead, we denoise and then find clusters of high power readings, and mark clusters instead on the spectrogram. We then take the highest power cluster and mark that as a prediction. A next step is to extend to generate predictions for multiple seismic events for one day. \n    </li>\n  </ul>\n  \n### Machine Learning Model\n  <ul>\n    <li>\n      An LSTM (Long Short-Term Memory) model is a highly suitable approach for the seismic event detection challenge for several reasons. Firstly, seismic data is a time-series signal, where each data point is dependent on the previous points. LSTM models are specifically designed to handle time-series data as they can maintain information across different time steps. In addition, LSTM models excel at capturing these long-term dependencies, unlike traditional RNNs (Recurrent Neural Networks), which suffer from vanishing gradients and struggle to retain information over longer periods. \n    </li>\n    <p align=\"center\"><img src=\"./readme_img/lstm.png\" width=\"600px\"></p>\n    <li>\n      Observing and testing with the provided training dataset, we noticed that the .mseed file contained a sampling rate and learned that there would be a reasonably large number of samples in a short amount of time, so we split each trace into small segments of 30 seconds. After splitting seismic traces into 30-second segments and extracting features (mean, max, skewness, etc.), we normalized the features and the target earthquake occurrence index. Using PyTorch, the LSTM model, with 64 hidden units, 2 layers, and a 0.2 dropout, was trained to predict the segment containing the earthquake. We used the Adam optimizer with MSELoss, training over 10 epochs with each trace processed independently, while allowing the model to learn the relationships between segments within each trace\n    </li>\n    <p align=\"center\"><img src=\"./readme_img/ml_pipeline.png\" width=\"600px\"></p>\n    <li>\n      This approach allows the model to learn temporal dependencies between consecutive 30-second segments, which is crucial for capturing patterns that may indicate an upcoming earthquake. Also, by using an LSTM, the model can recognize subtle temporal patterns within the seismic data that simple feature-based models might miss. Ultimately, the goal is to improve the accuracy of predicting when an earthquake will occur by identifying the specific segment in which it starts. \n    </li>\n  </ul>\n\n## Developer Team\n- [Luna Nguyen](https://github.com/lunanguyen)\n- [Mathilda Lee](https://github.com/jkmathilda)  \n- [Monica Trinh](https://github.com/monmon0)\n- [Steven Gu](https://github.com/br0mabs)\n\n",
        "createdAt": "2025-01-03T04:54:29.000Z",
        "updatedAt": "2025-01-30T18:11:53.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/jkmathilda/planetary-seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "adjtomo/wfdiff",
        "url": "https://github.com/adjtomo/wfdiff",
        "description": "Estimating the minimum resolvable period of synthetic seismograms by waveform (wf) differences (diff)",
        "stars": 3,
        "forks": 3,
        "readme": "# wfdiff\n\nDisclaimer: This utility code was written in Matlab by [Carl Tape](https://github.com/carltape) and [Vipul Silwal](https://github.com/vsilwal), then rewritten in Python by Lion Krischer [Lion Krischer](https://github.com/krischer) (2015) then upgraded by [Julien Thurin](https://github.com/thurinj) (2022). \n\nThe code is functional with Python version 3.6 and up, but the test suite is currently broken.\n\nMap plots are now handled by the [Cartopy](https://scitools.org.uk/cartopy/docs/latest/) module, which replaces the previous Basemap module that has reached its EOS in 2020.\n\nI would like to re-emphasize the original author's warning:\n>***:warning: This package is work in progress and NOT YET READY FOR PRODUCTIVE USE.***:\n\n## Installation\n\n### Installing Conda\n\n`wfdiff` has a couple of dependencies (please check [env_wfdiff.yml](https://github.com/uafgeotools/wfdiff/blob/master/setup.py) if you prefer to manually install dependencies). We recommand that you install wfdiff using `conda`. If Anaconda or (Miniconda) is not available on you system, please download and install [Anaconda](https://www.anaconda.com/products/individual) for your system.\n\nIf you have Anaconda already install and you need to update it, you can do so with\n\n```bash\nconda update conda\n```\n\n### Create the conda environment and install wfdiff\n\nThe following will download the latest version of `wfdiff`, create a conda environment (named wfdiff) and install `wfdiff` and all of its dependencies.\n\n```bash\ngit clone https://github.com/krischer/wfdiff.git\ncd wfdiff\nconda env create -f env_wfdiff.yml\n```\n\nActivate the newly create environment and install wfdiff\n\n ```bash\n conda activate wfdiff\n ```\n\n (On windows just with `$ activate wfdiff`). Remember to activate it everytime you want to use `wfdiff`. You can quit the `wfdiff` environment with `conda deactivate`.\n\nYou can also update an existing wfdiff environment with\n\n```bash\nconda env update -n wfdiff --file env_wfdiff.yml\n```\n\n ---\n\n**Note:** Depending on your cluster, the `mpi4py` shipping with Anaconda might not work with the MPI on your machine. It is best to uninstall the `mpi4py` shipping with Anaconda:\n\n```bash\nconda remove mpi4py\n```\n\nNow make sure the correct mpi is active (e.g. `mpicc` and consorts point to the correct executables) and install `mpi4py` with\n\n```bash\nconda install pip\npip install mpi4py\n```\n\nThis will cause `mpi4py` to be compiled with the MPI compiler on your system which should resolve any issues.\n\n## Running wfdiff\n\nTo run the code, run the `run_wfdiff_test.py` with\n\n```bash\npython run_wfdiff_test.py\n```\n\nAs these calculations can potentially take a long time, you can also run it with MPI:\n\n```bash\nmpirun -n 2 python run_wfdiff.py\n```\n\nNote that the number of available ressources may differ on your machine.\n",
        "createdAt": "2015-03-30T14:19:46.000Z",
        "updatedAt": "2025-07-22T00:18:18.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/adjtomo/wfdiff/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "chenyk1990/pyseisdl",
        "url": "https://github.com/chenyk1990/pyseisdl",
        "description": "A python package for dictionary learning applications in seismology ",
        "stars": 6,
        "forks": 0,
        "readme": "**pyseisdl**\n======\n\n## Description\n\n**pyseisdl** is a python package for different dictionary learning methods and their applications in seismology. DL has a variety of applications in seismology, including but not limited to seismic denoising, seismic reconstruction, seismic diffraction separation, constrained LSRTM, constrained FWI, etc.\n\n## Reference\n    Chen, Y. (2020). Fast dictionary learning for noise attenuation of multidimensional seismic data. Geophysical Journal International, 222(3), 1717-1727.\n    \n    Wang, H., Chen, W., Zhang, Q., Liu, X., Zu, S., & Chen, Y. (2020). Fast dictionary learning for high-dimensional seismic reconstruction. IEEE Transactions on Geoscience and Remote Sensing, 59(8), 7098-7108.\n    \nBibTeX:\n\n\t@article{chen2020sgk,\n\t  title={Fast dictionary learning for noise attenuation of multidimensional seismic data},\n\t  author={Yangkang Chen},\n\t  journal={Geophysical Journal International},\n\t  volume={222},\n\t  number={3},\n\t  issue={3},\n\t  pages={1717-1727},\n\t  year={2020}\n\t}\n\n\t@article{wang2021sgk,\n\t  title={Fast dictionary learning for high-dimensional seismic reconstruction},\n\t  author={Hang Wang and Wei Chen and Quan Zhang and Xingye Liu and Shaohuan Zu and Yangkang Chen},\n\t  journal={IEEE Transactions on Geoscience and Remote Sensing},\n\t  volume={59},\n\t  number={8},\n\t  issue={8},\n\t  pages={7098-7108},\n\t  doi={10.1109/TGRS.2020.3030740},\n\t  year={2021}\n\t}\n\n-----------\n## Copyright\n\tThe pyseisdl developing team, 2021-present\n-----------\n\n## License\n    MIT License \n\n-----------\n\n## Install\nUsing the latest version\n\n    git clone https://github.com/chenyk1990/pyseisdl\n    cd pyseisdl\n    pip install -v -e .\nor using Pypi\n\n    pip install pyseisdl\n\n-----------\n## Examples\n    The \"demo\" directory contains all runable scripts to demonstrate different applications of pyseisdl. \n\n-----------\n## Gallery\nThe gallery figures of the pydrr package can be found at\n    https://github.com/chenyk1990/gallery/tree/main/pyseisdl\nEach figure in the gallery directory corresponds to a DEMO script in the \"demo\" directory with the exactly the same file name.\n\n-----------\n## Dependence Packages\n* scipy \n* numpy \n* matplotlib\n\n-----------\n## Development\n    The development team welcomes voluntary contributions from any open-source enthusiast. \n    If you want to make contribution to this project, feel free to contact the development team. \n\n-----------\n## Contact\n    Regarding any questions, bugs, developments, collaborations, please contact  \n    Yangkang Chen\n    chenyk2016@gmail.com\n\n-----------\n\n\n",
        "createdAt": "2022-04-21T05:03:22.000Z",
        "updatedAt": "2025-11-11T14:22:29.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/chenyk1990/pyseisdl/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ghfbsd/seedstuff",
        "url": "https://github.com/ghfbsd/seedstuff",
        "description": "SEED volume writing and mseed QC tools",
        "stars": 0,
        "forks": 0,
        "readme": "# GEOFON SeedStuff SEED writing tools\n\nThese routines heavily modified by G. Helffrich/U. Bristol since obtained\nin 1995 from W. Hanka of GEOFON.  They form a useful set of utilities to handle\nmseed data and package it into SEED volumes given time windows.  Main\nprograms are:\n\n* check_seed - Verifies that data is continuous and that data counts in mseed\n   blockettes agree with the number of samples in the blockettes.\n\n* make_qseed - Reblocks mseed data into 4096 byte blocks and names output\n   files in a way consistent with copy_seed use.  Will also change\n   site information in blockettes (station name, network code, channel code).\n\n* copy_seed - Extracts a seed volume from a pool of mseed blockettes and a\n   network description.  This is the main program of the lot.\n\n* make_dlsv - Make a dataless seed volume to describe network parameters.\n\n* cfg - A directory of prototype network configuration files.  These files\n   describe a network's characteristics and controls data retrieval from\n   the data archive (kept separately).  The directory contains configuration\n   files from previous networks that can serve as starting points for new\n   network descriptions, and contains useful instrument response information\n   (FIR coefficient files) for some typical instruments used in temporary\n   deployments.\n\n* contrib/utils - A directory of utility programs for querying raw mseed files\n   and creating file names acceptable for use with the GEOFON SEED writing\n   programs.\n\n## Documentation\n\nSketchy.  Run each program with no command line parameters to get a fairly\nhelpful summary of program options.\n\n## History\n\nThe basic differences between the GFZ versions and the present ones are in\nhandling\n1) Y2K problems (yes, there were some);\n2) endian problems (yes, there were rather more of those, too);\n3) making the programs work with mseed data of any endianness (not part of the\n   original design, which assumed all big-endian data);\n4) cleaning up strange programming practices (e.g.\n   using functions rather than subroutines, even though no useful function\n   result was returned ... eh?);\n5) the configuration file format for response descriptions was extended to\n   allow for a range of FIR coefficients rather than a list (e.g. 1-5 for FIR\n   coeffs. 1 to 5 rather than 1/2/3/4/5);\n6) include the analog response of the digitizer in the response information;\n7) add capability of describing station SOH channels in the network database\n   file and generating response information for them;\n8) and creating an automatic configuration procedure.\n\nFinally, these programs were changed to compile and run properly with `gfortran`.\nThis is a bigger headache that you think due to the way that `g77` and\n`gfortran` handle fixed-format input:  `g77` is lenient, while `gfortran` is strict.\n(Long lines that extend beyond column 71 but that are valid Fortran are accepted\nby `g77` but truncated by `gfortran`.)  Usually this leads to syntax errors, but\noccasionally it leads to a subtly malfunctioning program.  There may\nstill be (rare) bugs of this sort.\n\nOther `gfortran` compatibility problems that were fixed are:\n\n1) the `g77` runtime library allowed the same file to be attached to multiple\ni/o units; `gfortran`'s does not.\n\n2) `g77` runtime returns -1 for EOF on read, used by these programs to tell the\ndifference between an i/o error and EOF.  The only reliably-defined codes are\n0 = OK and nonzero = not ok.  So, interpret all nonzero values as indicating\nEOF.\n\n## Compiling\nThis version of the package has an automatic configuration script.  For\nbasic configuration, use:\n\n`./configure ; make`\n\nIf on a FreeBSD system, use gmake rather than make due to different make file\nsemantics:\n\n`./configure ; gmake`\n\nYou can get a summary of configure options by issuing\n\n`./configure --help`\n\nThe options `--prefix=` and `BINDIR=` control where the programs are installed when\n`make install` is used.\n\nIf you don't have a Fortran compiler on your machine but you do have a C\ncompiler, you will find the `f2c` package in contrib to convert to C.  There\nis no automatic configuration for this procedure; you are on your own to develop\na command that uses f2c to compile Fortran using C to object code files.\n`fort77` is such a program that is available from online source repositories.\n\nSome gfortran compiler builds default to 32 bit output; others default to 64.\nIf the defaults differ between the C and Fortran compilers, explicitly force\nthe C compiler to the Fortran default.  E.g., if `gfortran` is 32 bit by\ndefault, add the `-m32` option to your CFLAGS, for example by\n\n`./configure CFLAGS=-m32`\n\n## Compile history\n```\nSeedStuff - Original version by Winfried Hanka, GFZ Potsdam for GEOFON network.\n\nPast versions of SeedStuff were compiled and tested with the following sytems.\n\nSolaris 5.5    - gcc version 2.7.2\n               - f77\n\nSunOS 4.1.3    - gcc version 2.5.8\n               - f77\n\nLinux 2.0      - gcc version 2.7.2\n               - f2c\n\nHP-UX A.09.05  - cc\n               - f77 \n\nThe present version of SeedStuff was compiled and tested with the following\nsystems.\n\nLinux 2.2      - gcc version 2.7.2\n               - g77 version 2.95.2\n\nLinux 2.2      - gcc egcs-2.91.66\n               - g77 egcs-2.91.66\n\nFreeBSD 6.3    - gcc version 3.4.6\n (sparc)       - f77 version 3.4.6 and gfortran\n\nDarwin 6,7     - gcc\n               - g77\n\nDarwin 8       - gcc version 4.0.1 (Apple Inc. build 5367)\n               - g77 version 3.4.0 and gfortran\n\nDarwin 10      - gcc version 4.2.1 (Apple Inc. build 5664)\n\t       - g77 version 3.4.3 and gfortran\n\nDarwin 18      - cc Apple LLVM 10.0.1\n               - gfortran 11.2.0\n```\n",
        "createdAt": "2023-01-31T13:11:41.000Z",
        "updatedAt": "2024-12-27T18:43:10.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ghfbsd/seedstuff/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "HuizheDi/Seismology-software-installation-guide",
        "url": "https://github.com/HuizheDi/Seismology-software-installation-guide",
        "description": "Some tips on common seismology software installation",
        "stars": 0,
        "forks": 0,
        "readme": "# Seismology-software-installation-guide\nSome tips on common seismology software installation\n\n1. Seismic Unix\n",
        "createdAt": "2023-11-08T00:44:51.000Z",
        "updatedAt": "2023-11-08T00:44:52.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/HuizheDi/Seismology-software-installation-guide/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "seismo-learn/software",
        "url": "https://github.com/seismo-learn/software",
        "description": "Tutorials for popular seismological software",
        "stars": 12,
        "forks": 3,
        "readme": "# 地震“学”软件\n\n[![Jupyter Book Badge](https://jupyterbook.org/badge.svg)](https://seismo-learn.org/software/)\n[![Deploy](https://github.com/seismo-learn/software/actions/workflows/deploy.yml/badge.svg)](https://github.com/seismo-learn/software/actions/workflows/deploy.yml)\n[![License: CC BY-NC 4.0](https://img.shields.io/badge/License-CC%20BY--NC%204.0-blue.svg)](https://creativecommons.org/licenses/by-nc/4.0/deed.zh-hans)\n\n\n本文档主要介绍地震学常用软件，包括简介、安装、用法等。\n\n- 主页：https://seismo-learn.org/software/\n- 源码：https://github.com/seismo-learn/software\n\n## 文档维护\n\n本文档尚有很多不完善之处，欢迎读者参与到文档的维护与更新中。\n详情见[贡献指南](https://seismo-learn.org/contributing/)。\n\n## 许可协议\n\n本作品采用 [知识共享署名-非商业性使用 4.0 国际许可协议 (CC BY-NC 4.0)](https://creativecommons.org/licenses/by-nc/4.0/deed.zh-hans) 。\n任何人都可以自由地分享、修改本作品，但必须遵循如下条件：\n\n- 署名：必须提到原作者，提供指向此许可协议的链接，表明是否有做修改\n- 非商业性使用：不能对本作品进行任何形式的商业性使用\n",
        "createdAt": "2020-12-08T18:47:21.000Z",
        "updatedAt": "2025-11-25T06:10:36.000Z",
        "language": "Makefile",
        "homepage": "https://seismo-learn.org/software",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/seismo-learn/software/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "xumi1993/seispy",
        "url": "https://github.com/xumi1993/seispy",
        "description": "Python module of seismology and  receiver functions",
        "stars": 108,
        "forks": 35,
        "readme": "# <img src=\"https://user-images.githubusercontent.com/7437523/128596331-dc5c5e40-93e1-4d9e-b92d-9c53fe51145a.png\" width=\"500\"/> \n\n\n[![License](https://img.shields.io/github/license/xumi1993/seispy)]()\n[![](https://img.shields.io/github/last-commit/xumi1993/seispy)]()\n[![GitHub code size in bytes](https://img.shields.io/github/languages/code-size/xumi1993/seispy)]()\n[![GitHub repo size](https://img.shields.io/github/repo-size/xumi1993/seispy)]()\n[![Static Badge](https://img.shields.io/badge/DOI-10.1785%2F0220220288-pink)](https://doi.org/10.1785/0220220288)\n\n\n[![CRV test](https://github.com/xumi1993/seispy/actions/workflows/test.yml/badge.svg?branch=dev)](https://github.com/xumi1993/seispy/actions/workflows/test.yml)\n[![codecov](https://codecov.io/gh/xumi1993/seispy/branch/dev/graph/badge.svg?token=XN3E3N6S3V)](https://codecov.io/gh/xumi1993/seispy)\n[![Upload Python Package](https://github.com/xumi1993/seispy/actions/workflows/python-publish.yml/badge.svg)](https://github.com/xumi1993/seispy/actions/workflows/python-publish.yml)\n[![Seispy docs](https://github.com/xumi1993/seispy/actions/workflows/doc_build.yml/badge.svg)](https://github.com/xumi1993/seispy/actions/workflows/doc_build.yml)\n<a href=\"https://dev.azure.com/conda-forge/feedstock-builds/_build/latest?definitionId=13623&branchName=master\">\n  <img src=\"https://dev.azure.com/conda-forge/feedstock-builds/_apis/build/status/seispy-feedstock?branchName=master\">\n</a> \n\n[![Anaconda-Server Badge](https://anaconda.org/conda-forge/seispy/badges/version.svg)](https://anaconda.org/conda-forge/seispy)\n[![Conda Version](https://img.shields.io/conda/vn/conda-forge/seispy.svg)](https://anaconda.org/conda-forge/seispy)\n[![Anaconda-Server Badge](https://anaconda.org/conda-forge/seispy/badges/downloads.svg)](https://anaconda.org/conda-forge/seispy)\n\n[![PyPI](https://img.shields.io/pypi/v/python-seispy)](https://pypi.org/project/python-seispy/)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/python-seispy)](https://pypi.org/project/python-seispy/)\n\n[![GitHub stars](https://img.shields.io/github/stars/xumi1993/seispy?style=social)]()\n[![](https://img.shields.io/github/forks/xumi1993/seispy?style=social)]()\n\n\nSeispy is a Python module for processing seismological data and calculating Receiver Functions. The advanced functions are available to improve the Obspy.\n\n## Acknowledgements\n\nFor the use of the Seispy package, please cite as:\n\n- Xu, M. & He, J. (2023). Seispy: Python Module for Batch Calculation and Postprocessing of Receiver Functions. Seismological Research Letters, 94 (2A): 935–943. [![Static Badge](https://img.shields.io/badge/DOI-10.1785%2F0220220288-pink)](https://doi.org/10.1785/0220220288)\n\nFor 3D time-difference correction, please also consider citing:\n\n- Xu, M., Huang, H., Huang, Z., Wang, P., Wang, L., Xu, M., ... & Yuan, X. (2018). Insight into the subducted Indian slab and origin of the Tengchong volcano in SE Tibet from receiver function analysis. Earth and Planetary Science Letters, 482, 567-579.  [![Static Badge](https://img.shields.io/badge/DOI-10.1016%2Fj.epsl.2017.11.048-pink)\n](https://doi.org/10.1785/0220220288)\n\n- Xu, M., Huang, Z., Wang, L., Xu, M., Mi, N., & Yu, D. (2020). Lateral variation of the mantle transition zone beneath the Tibetan plateau: Insight into thermal processes during Indian–Asian collision. Physics of the Earth and Planetary Interiors, 301, 106452. [![Static Badge](https://img.shields.io/badge/DOI-10.1016%2Fj.pepi.2020.106452-pink)](https://doi.org/10.1016/j.pepi.2020.106452)\n\nFor 2D and 3D CCP stacking, please also consider citing:\n\n- Xu, M., Huang, Z., Wang, L., Xu, M., Zhang, Y., Mi, N., ... & Yuan, X. (2020). Sharp lateral Moho variations across the SE Tibetan margin and their implications for plateau growth. Journal of Geophysical Research: Solid Earth, 125(5), e2019JB018117. [![Static Badge](https://img.shields.io/badge/DOI-10.1029%2F2019JB018117-pink)](https://doi.org/10.1029/2019JB018117)\n\n## Installation\n\nSee [Seispy documentation](https://seispy.xumijian.me/installation.html) in detail.\n \n## Libraries\n- `seispy.distaz`: Calculate distance and azimuth credited by the [lithospheric seismology program at USC](http://www.seis.sc.edu/software/distaz/), but `numpy.ndarray` operations are supported.\n- `seispy.geo`: Tiny codes of geophysics.\n- `seispy.decon`: Functions of deconvolution transferred from [iwbailey/processRFmatlab](https://github.com/iwbailey/processRFmatlab) including\n  - Iterative time domain deconvolution method (Ligorría and Ammon 1999 BSSA). \n  - Water level frequency domain deconvolution method (CJ. Ammon 1991 BSSA)\n- `seispy.rf`: Procedure for RF calculation. The functions of `match_eq`, `search_eq` invoked `obspy.core.UTCDateTime` and `obspy.clients` from the [Obspy](https://docs.obspy.org/).\n- `seispy.eq`: RF processing for each event, which invoked `obspy.io.sac`, `obspy.signal`, `obspy.taup` and `obspy.core.Stream` from the [Obspy](https://docs.obspy.org/).\n- `seispy.hk`: H-k stacking for single station (Zhu and Kanamori 2000 JGR).\n- `seispy.rfani`: A joint method for crustal anisotropic calculation (Liu and Niu 2011 GJI).\n- `seispy.slantstack`: Slant stacking for single station (Tauzin et al., 2008)\n- `seispy.rfcorrect`: Subsequent process of RFs including moveout correction and time to depth conversion (1D and 3D) (see [Xu et al., 2018 EPSL](https://www.sciencedirect.com/science/article/pii/S0012821X17306921?via%3Dihub))\n- `seispy.ccpprofile`: CCP stacking along a profile.\n- `seispy.ccp3d`: 3-D CCP stacking with extracting depth D410 and D660.\n\n  [lithospheric seismology program at USC]: http://www.seis.sc.edu/software/distaz/\n  [scikits-bootstrap]: https://github.com/cgevans/scikits-bootstrap\n  [iwbailey/processRFmatlab]: https://github.com/iwbailey/processRFmatlab\n  [Obspy]: https://docs.obspy.org/\n  [Xu et al., 2018 EPSL]: https://www.sciencedirect.com/science/article/pii/S0012821X17306921?via%3Dihub\n\n\n## Commands\n### Receiver Functions\n * `prf`: Calculate PRFs for a station.\n * `pickrf`: Pick PRFs with virtual quality control after the calculation.\n * `plotrt`: Plot PRFs with R and T components order by back-azimuth.\n * `plotr`: Plot PRFs with R component order by back-azimuth.\n * `hk`: H-Kappa stacking for estimating Moho depth and crustal Vp/Vs.\n * `rf2depth`: Convert PRFs to depth axis.\n * `ccp_profile`: Stack PRFs along a profile with a CCP stacking method.\n * `ccp3d`: Stack PRFs with spaced bins.\n * `rfani`: Estimating crustal anisotropy with a joint method.\n * `rfharmo`: Harmonic decomposition to extract constant component of RF and plot dip/anisotropic components.\n * `pickdepth`: Pick depth of stacked PRFs following `ccp3d`\n\n### Others\n * `veltxt2mod`: Create 3D velocity model with `numpy.lib.npyio.NpzFile` format from a ASCII table file.\n * `downlod_catalog`: Download catalogs from FDSN web-service.\n * `gen_rayp_lib`: Generate a rap-parameter library with depth of source and epicentral distance.\n * `setpar`: Set up the values in configure files.\n\n",
        "createdAt": "2015-08-19T01:25:53.000Z",
        "updatedAt": "2025-11-24T08:23:41.000Z",
        "language": "Python",
        "homepage": "https://seispy.xumijian.me",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/xumi1993/seispy/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "EmraanAbbasi/Seismology",
        "url": "https://github.com/EmraanAbbasi/Seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2019-04-11T18:28:09.000Z",
        "updatedAt": "2019-04-11T18:28:12.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "FernandoL12/IC-Seismology",
        "url": "https://github.com/FernandoL12/IC-Seismology",
        "description": "Data and code produced as part of my scientific initiation project in seismology.",
        "stars": 0,
        "forks": 0,
        "readme": "# IC-Seismology-2025\nData and codes developed during my scientific initiation in seismology at Center of Seismology (IAG-USP).\n\n## Objective\nDevelop a code for automatic P pick correction for aftershocks based on the maximum correlation with the main event.\n\n## Method\nGets and cut data from a given FDSN Client, interval (in seconds) and event(s) ID(s). After that, the code makes data cross-correlation, generates and saves as figure: 1. Correlation matrix; 2. Time correction times matrix; 3. A post P pick time correction seismogram with superposed waveforms.\n",
        "createdAt": "2025-05-08T21:53:28.000Z",
        "updatedAt": "2025-11-29T13:06:07.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/FernandoL12/IC-Seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "shipengcheng1230/Quaycle.jl",
        "url": "https://github.com/shipengcheng1230/Quaycle.jl",
        "description": "Earthquake Sequences Simulator",
        "stars": 4,
        "forks": 0,
        "readme": "# Quaycle.jl (Julia Earthquake Machine)\n\n<p align=\"center\">\n  <img src=\"/assets/logo.svg\" alt=\"Quaycle.jl\" width=\"200\"/>\n</p>\n\n| Build Status | Coverage | Documentation  | Social | License |\n|:---:|:---:|:---:|:---:|:---:|\n| [![BuildStatus](https://travis-ci.com/shipengcheng1230/Quaycle.jl.svg?token=zsZu59CsqQTTp7wzi7zP&branch=master)](https://travis-ci.com/shipengcheng1230/Quaycle.jl) [![Build Status](https://dev.azure.com/jsjyspc/Julia%20Earthquake%20Machine/_apis/build/status/shipengcheng1230.Quaycle.jl?branchName=master)](https://dev.azure.com/jsjyspc/Julia%20Earthquake%20Machine/_build/latest?definitionId=1&branchName=master) | [![codecov.io](https://codecov.io/gh/shipengcheng1230/Quaycle.jl/coverage.svg?token=ag6kv61zOW&branch=master)](https://codecov.io/gh/shipengcheng1230/Quaycle.jl?branch=master) | [![](https://img.shields.io/badge/docs-dev-blue.svg)](https://shipengcheng1230.github.io/Quaycle.jl/dev) ![](https://github.com/shipengcheng1230/Quaycle.jl/workflows/Documentation/badge.svg) | [![Gitter](https://img.shields.io/badge/chat-on%20gitter-ff69b4.svg)](https://gitter.im/Quaycle-jl/Lobby) | [![License:GPLv3](https://img.shields.io/badge/license-GPLv3-brightgreen)](https://www.gnu.org/licenses/quick-guide-gplv3.en.html) |\n\nThis is a suite for numerically simulating earthquake sequences in [Julia](https://julialang.org/). The purpose of this package is to provide efficient Julia implementations for simulations in the field of earthquake physics.\n\n## Developing\n**Quaycle.jl** is still in alpha-stage. Breaking changes are imminent.\n\n## Supporting\nThis software in this ecosystem is developed as part of academic research in\n[Earthquake Physics Lab](http://weilab.uri.edu/) at\n[Graduate School of Oceanography](https://web.uri.edu/gso/), University of Rhode Island.\n",
        "createdAt": "2018-05-29T19:44:21.000Z",
        "updatedAt": "2023-05-05T04:01:20.000Z",
        "language": "Julia",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/shipengcheng1230/Quaycle.jl/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ghfbsd/nmxtools",
        "url": "https://github.com/ghfbsd/nmxtools",
        "description": "Tools for extracting and managing data recorded on Nanometrics Taurus dataloggers",
        "stars": 0,
        "forks": 1,
        "readme": "",
        "createdAt": "2023-02-05T16:04:48.000Z",
        "updatedAt": "2023-02-18T12:08:57.000Z",
        "language": "C",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "DrGMA/TESLA",
        "url": "https://github.com/DrGMA/TESLA",
        "description": "TESLA, a Tool for Automatic Earthquake Low-Frequency Spectral Level Estimation",
        "stars": 5,
        "forks": 2,
        "readme": "<div align=\"center\">\n<img src=\"logo.png\" alt=\"Alt text\" title=\"Optional title\" width=\"400\"/>\n</div>\n\n---\n[![Tesla Version](https://img.shields.io/badge/tesla-v0.0.1-green)](https://github.com/DrGMA/tesla/tree/main?tab=readme-ov-file#documentation-and-references-)\n[![Read the Docs](https://img.shields.io/badge/read_the_manual-tesla_v0.0.1-blue)](https://tesla.readthedocs.io/en/latest/)\n[![Cite the paper](https://img.shields.io/badge/cite_the_paper-DOI-red)](https://doi.org/10.1785/0220230033)\n[![Need Help? Ask Info!](https://img.shields.io/badge/need_help%3F-ask_info!-yellow)](mailto:guidomaria.adinolfi@unito.it)\n\n\n## Overview 📜\nTESLA (Tool for Automatic Earthquake Low-Frequency Spectral Level Estimation) is a Python-based tool designed to optimize the spectral analysis of earthquake waveforms. It focuses on low-frequency spectral level estimation by adapting signal window selection based on the signal complexity. By inverting the P- and S-displacement spectra, TESLA identifies the optimal signal window for spectral analysis, moving beyond traditional fixed-duration signal windows to offer more accurate analysis, especially for microseismicity.\n\n## Features 🚀\n- **Automatic Signal Window Selection** : TESLA automates the selection process of signal windows, ensuring the spectral analysis is as precise as possible by considering the effect of earthquake's magnitude and the  characteristics of microseismic waveforms.\n- **Systematic Exploration** : The tool systematically explores various signal windows, performing a comprehensive analysis to compute the displacement spectra for both P and S phases.\n- **Comprehensive Spectral Analysis** : Evaluates all selected signal windows to conduct in-depth spectral analyses and calculate displacement spectra.\n- **Quantitative Evaluation** : Utilizes quantitative criteria to select the best displacement spectra, offering accurate estimates of low-frequency spectral levels.\n\n## Purpose 🎯\nTESLA enhances low-frequency spectral level estimation by systematically selecting signal windows, overcoming the limitations of traditional fixed-duration window methods, especially in the analysis of microseismicity. TESLA enhances seismic event analysis, especially for low magnitude events, by providing additional observables such as P and S low-frequency spectral levels. These insights can refine focal mechanism constraints together with P-wave polarities and facilitate seismic moment and moment magnitude estimations.\n\n## Advantages :trophy:\n\n- **Open Source** : Written in Python, TESLA is freely available for seismic research.\n- **Modular Architecture** : Designed for easy modifications to meet specific research needs.\n- **Highly Configurable** : Fine-tune TESLA's settings for varied processing needs, making it a versatile tool for seismic data analysis.\n\n## Getting Started 🏁\nClone the TESLA repository and follow the setup indications provided in the documentation. Detailed instructions will guide users through data preparation and the estimation of low-frequency spectral levels.\n\n## Documentation and References 📚\nFor complete documentation, updates, and examples, visit the [TESLA Documentation](https://tesla.readthedocs.io/en/latest/index.html). Find relevant research and papers in the documentation's references section.\n\n### Key Reference 📖\nWhen using TESLA for your projects and research, please ensure to reference the following publication:\n\n- Adinolfi, Guido Maria, Vincenzo Convertito, and Raffaella De Matteis. \"TESLA, A Tool for Automatic Earthquake Low‐Frequency Spectral Level Estimation: The Study of 2013 St. Gallen Earthquake Fault‐Plane Solutions.\" Seismological Research Letters 94.5 (2023): 2441-2455. [DOI](https://doi.org/10.1785/0220230033)\n\n## Version Information 📅\n- **Version:** 0.0.1\n- **Last Updated:** March 10, 2024\n- **Author:** Guido Maria Adinolfi\n\n## Copyright 📄\nCopyright (C) 2023 Guido Maria Adinolfi, University of Turin, Turin, Italy.\n\nTESLA is developed for research purposes and its use should adhere to ethical standards.\n\n## Contact and Collaboration 📧\nFor any questions, technical support, or to report bugs related to the code, and for any scientific inquiries, please do not hesitate to contact me at guidomaria.adinolfi@unito.it. I am always open to discussing research interests and exploring potential scientific collaborations.\n\n## Future Updates and Upcoming Events :hourglass_flowing_sand:\nKeep an eye on this section for the latest on TESLA's development and upcoming events related to our project. We are committed to continuous improvement and making TESLA more efficient.\n\nStay tuned for more information and specific updates!\n",
        "createdAt": "2023-07-05T11:30:28.000Z",
        "updatedAt": "2025-09-06T12:01:43.000Z",
        "language": "Python",
        "homepage": "https://tesla.readthedocs.io/en/latest/index.html",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/DrGMA/TESLA/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "miili/FDSN-rush",
        "url": "https://github.com/miili/FDSN-rush",
        "description": "Fast and modern FDSNWS Downloader",
        "stars": 1,
        "forks": 0,
        "readme": "# FDSN Rush\n\n*Fast and modern FDSN Download*\n\n[![uv](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/uv/main/assets/badge/v0.json)](https://github.com/astral-sh/uv)\n[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)\n[![Pre-commit](https://github.com/miili/FDSN-rush/actions/workflows/pre-commit.yaml/badge.svg)](https://github.com/miili/FDSN-rush/actions/workflows/pre-commit.yaml)\n\n*FDSN Rush* allows to download seismic waveform data from [FDSN](https://www.fdsn.org/services/) servers in a fast, reproducible and reliable way.\n\n## Installation\n\nInstallation using Python's pip\n\n```sh\npip install git+https://github.com/miili/fdsn-rush\n```\n\nThe user CLI is exposed as `fdsn-rush` command.\n\n```sh\n$> fdsn-rush\n\n\n Usage: fdsn-rush [OPTIONS] COMMAND [ARGS]...\n\n FDSN Download to SDS Archive\n\n╭─ Options ────────────────────────────────────────────────────────────────────────────╮\n│ --help          Show this message and exit.                                          │\n╰──────────────────────────────────────────────────────────────────────────────────────╯\n╭─ Commands ───────────────────────────────────────────────────────────────────────────╮\n│ init       Print the configuration.                                                  │\n│ download   Download data from FDSN to local SDS archive.                             │\n│ convert    Convert existing MiniSEED files to SDS archive.                           │\n╰──────────────────────────────────────────────────────────────────────────────────────╯\n```\n\n## Download data\n\nCreate a new config file and write it out to `config.json` with the following command:\n\n```sh\nfdsn-rush init > config.json\n```\n\nIn this config file configure:\n\n1. FDSN servers to fetch data from, in this example <https://geofon.gfz.de>.\n2. Timerange to download, here substitutes `today` and `yesterday` are allowed.\n3. Stations NSL codes (SEED convention `<network>.<station>.<location>`) to download.\n4. The channel priority. In the default config `HH` channels would have the highest priority.\n\nIf you have an [EIDA](https://www.orfeus-eu.org/data/eida/) key to access restricted waveform data, put the path into `Client.eida_key`.\n\nThe MiniSeed data will be writen out into an [SDS](https://www.seiscomp.de/seiscomp3/doc/applications/slarchive/SDS.html) directory structure located at `data/`. All metadata will be saved in a `metadata/` folder.\n\n```json{\n  \"writer\": {\n    \"sds_archive\": \"data/\",\n    \"steim_compression\": 1,\n    \"record_length\": 4096,\n    \"min_length_seconds\": \"PT1M\",\n    \"squirrel_environment\": null\n  },\n  \"clients\": [\n    {\n      \"url\": \"https://geofon.gfz.de/\",\n      \"timeout\": 30.0,\n      \"n_workers\": 8,\n      \"n_connections\": 24,\n      \"chunk_size\": \"4.0MiB\",\n      \"rate_limit\": 20,\n      \"eida_key\": null\n    }\n  ],\n  \"metadata_path\": \"metadata\",\n  \"time_range\": [\n    \"2025-11-03\",\n    \"today\"\n  ],\n  \"station_selection\": [\n    \"2D..\"\n  ],\n  \"channel_priority\": [\n    \"HH[ZNE12]\",\n    \"EH[ZNE12]\",\n    \"HN[ZNE12]\"\n  ],\n  \"station_blacklist\": [],\n  \"min_channels_per_station\": 3,\n  \"min_sampling_rate\": 100.0,\n  \"max_sampling_rate\": 200.0\n}\n```\n\n### Start the Download\n\nStart the asynchronous download with:\n\n```sh\nfdsn-rush download config.json\n```\n\n### Converting existing MiniSeed data to SDS Archive\n\nThis can be useful to convert unstructured MiniSeed data to an [SDS archive structure](https://www.seiscomp.de/seiscomp3/doc/applications/slarchive/SDS.html).\n\n```sh\nfdsn-rush convert in-folder/ out-sds-folder/\n```\n",
        "createdAt": "2025-07-27T17:02:25.000Z",
        "updatedAt": "2025-11-10T16:51:12.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/miili/FDSN-rush/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "danxhuber/very-metal-poor-seismology",
        "url": "https://github.com/danxhuber/very-metal-poor-seismology",
        "description": "Code to reproduce plots for \"Stellar Models are Reliable at Low Metallicity: An Asteroseismic Age for the Ancient Very Metal-Poor Star KIC 8144907\", Huber et al. 2024",
        "stars": 0,
        "forks": 0,
        "readme": "This repository containts code and data to reproduce plots for the paper \"Stellar Models are Reliable at Low Metallicity: An Asteroseismic Age for the Ancient Very Metal-Poor Star KIC 8144907\", Huber et al. 2024 (https://arxiv.org/abs/2407.17566).\n\nData from Xiang & Rix (2022) to reproduce Fig 4 can be found here:\nhttps://vizier.cds.unistra.fr/viz-bin/VizieR?-source=J/other/Nat/603.599\nhttps://keeper.mpdl.mpg.de/d/019ec71212934847bfed/\n",
        "createdAt": "2024-06-14T18:02:50.000Z",
        "updatedAt": "2024-08-13T10:48:56.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/danxhuber/very-metal-poor-seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "geophydog/Seismology_Useful_Tools",
        "url": "https://github.com/geophydog/Seismology_Useful_Tools",
        "description": null,
        "stars": 41,
        "forks": 16,
        "readme": "# Useful Seismic Tools\n\n***\n:pencil2: :pencil2: :pencil2:\n\n##  :arrow_down:  Seismic Data Processing\n- :zero: :one: [SAC](http://ds.iris.edu/ds/nodes/dmc/forms/sac/)\n    `\n    Seismic data pre-processing and plot.\n    `\n\n- :zero: :two: [ObsPy](http://docs.obspy.org/tutorial/)\n    `\n    Read data (SEED, MiniSEED, SAC, SEDY and so on.)\n    `\n- :zero: :three: [PyWEED](https://iris-edu.github.io/pyweed/)\n    `\n    Retrieve earthquake-based seismic data with interactive steps (Python version of JWEED).\n    `\n\n***\n:pencil2: :pencil2: :pencil2:\n\n## :arrow_down:  Convert Seismic Data\n- :zero: :one: [rdseed](http://ds.iris.edu/ds/nodes/dmc/forms/rdseed/)\n    `\n    SEED -> SAC.\n    `\n\n- :zero: :two: [ObsPy](http://docs.obspy.org/tutorial/)\n    `\n    SEED -> SAC  \n    MiniSEED -> SAC  \n    SEDY -> SAC  \n    `\n\n- :zero: :three: [mseed2sac](https://github.com/iris-edu/mseed2sac)\n    `\n    MiniSEED -> SAC\n    `\n\n***\n:pencil2: :pencil2: :pencil2:\n\n## :arrow_down:  Travel time, ray parameter and ray path calculation\n- :zero: :one: [TauP](http://www.seis.sc.edu/taup/)\n    `\n    Travel time, incident, takeoff, ray parameter and ray path calculation and show.\n    `\n\n- :zero: :two: [ObsPy TauP](http://docs.obspy.org/tutorial/code_snippets/travel_time.html#cartesian-ray-paths)\n    `\n    TauP implemented by ObsPy.\n    `\n\n- :zero: :three: [Cake](https://pyrocko.org/docs/current/apps/cake/manual.html#python-script-examples)\n    `\n    Cake is a tool which can be used to solve classical seismic ray theory problems for layered earth models (layer cake models).\n    `\n\n- :zero: :four: [FM3D](http://rses.anu.edu.au/seismology/soft/fmmcode/)\n    `\n    3D FAST MARCHING CODE.\n    `\n\n- :zero: :five: [pySeismicFMM](https://github.com/gozwei/pySeismicFMM)\n    `\n    Calculating 3-D travel times by FAST MARCHING.\n    `\n\n- :zero: :six: [ANISOtime](https://github.com/kensuke1984/Kibrary/wiki/ANISOtime)\n    `\n    Calculating travel times in homogeneous layers, transversely isotropic (TI), spherically symmetric medium.\n    `\n\n- :zero: :seven: [fteikpy](https://github.com/keurfonluu/fteikpy)\n  `\n  A Python library that computes accurate first arrival traveltimes in 2D and 3D heterogeneous isotropic velocity models. \n  `\n\n***\n:pencil2: :pencil2: :pencil2:\n\n## :arrow_down:  Visualization\n- :zero: :one: [GMT](http://gmt.soest.hawaii.edu/)  \n    `\n    Almost the most usual mapping software in geophysical field.\n    `\n\n- :zero: :two: [matplotlib](https://matplotlib.org/)  \n    `\n    Not only in seismology, it's useful to plot beautiful figures.\n    `\n\n- :zero: :three: [SAC](http://ds.iris.edu/ds/nodes/dmc/forms/sac/)  \n    `\n    Plot simple figures and seismic data processing.\n    `\n\n- :zero: :four: [PSSAC](http://www.eas.slu.edu/People/LZhu/home.html)  \n    `\n    Show seismic waveforms calling GMT coomand.\n    `\n\n- :zero: :five: [POV-Ray](http://www.povray.org/)  \n    `\n    Apply colors to drawing and it's beautiful.\n    `\n\n- :zero: :six: [ECharts](https://www.echartsjs.com/examples/zh/index.html)  \n`\nAwesome figures, tables building created by BAIDU Inc and implemented by JavaScript.\n`\n\n- :zero: :seven: [PyECharts](https://pyecharts.org/#/zh-cn/intro)  \n    `\n    A python library implementing ECHarts.\n    `\n\n- :zero: :eight:[ObsPy](http://docs.obspy.org/tutorial/)    \n    `\n    Spectragram, beachball or seismic data processing.\n    `\n\n- :zero: :nine: [Snuffler](https://pyrocko.org/docs/current/apps/snuffler/tutorial.html)  \n    `\n    View seismic waveforms.\n    `\n    \n- :one: :zero: [Desmos](https://www.desmos.com/)    \n    `\n    Demo of graphs of functions.\n    `\n    \n- :one: :one: [GeoGebra](https://www.geogebra.org/)     \n    `\n    Geometry Algebra, an amazing tool.\n    `\n    \n- :one: :two: [Mathematica](https://www.wolfram.com/mathematica/)\n    \n    `\n    Data processing, visualization, numerical and symbolic computation.\n    `\n    \n- :one: :three: [Obtaining colors](http://www.jiniannet.com/Page/allcolor)\n\n    `\n    You can get these colors on line when you upload your picture\n    `\n\n- :one: :four: [Choosing colors](https://www.sioe.cn/yingyong/yanse-rgb-16/)\n\n    `\n    There are many colors and you can choose them if necessary.\n    `\n    \n- :one: :five: [PyVista](https://docs.pyvista.org/)\n\n    `\n    An awesome visualization tool for 3D shapes.\n    `\n- :one: :six: [Basemap](https://matplotlib.org/basemap/stable/)       \n  `\n    A Python library of plotting 2D data on geomaps.\n  `\n\n    \n\n***\n:pencil2: :pencil2: :pencil2:\n\n## :arrow_down:  Synthetic Seismograms\n- :zero: :one: [Fomosto](https://pyrocko.org/docs/current/apps/fomosto/)\n    `\n    Computing synthetic and store seismograms.\n    `\n  \n- :zero: :two: [fk](http://www.eas.slu.edu/People/LZhu/home.html)\n    `\n    Computing synthetic seismograms in horizontal layered medium.\n    `\n    \n- :zero: :three: [DSM](http://www-solid.eps.s.u-tokyo.ac.jp/~dsm/software/software.htm)\n    `\n    Synthesizing seismograms in homogeneous layers, transversely isotropic (TI), \n    spherically symmetric medium with Direct Solution Method (DSM).\n    `\n- :zero: :four: [yaseis](https://seiscode.iris.washington.edu/projects/yaseis) \n    `\n    Computing synthetic seismograms inhomogeneous layers, transversely isotropic (TI), \n    spherically symmetric medium.\n    `\n    \n- :zero: :five: [Mineos](https://github.com/geodynamics/mineos)\n    `\n    Computing synthetic seismograms in a spherically symmetric non-rotating Earth by summing normal modes.\n    `\n\n- :zero: :six: [SOFI2D](https://git.scc.kit.edu/GPIAG-Software/SOFI2D/)\n        `\n        Computing 2-D PSV wavefield in viscoelastic medium with finite difference method.\n        `\n    \n- :zero: :seven: [SOFI2D-sh](https://git.scc.kit.edu/GPIAG-Software/SOFI2D_sh)\n        `\n        Computing 2-D SH wavefield in viscoelastic medium with finite difference method.\n        `\n- :zero: :eight: [SOFI3D](https://git.scc.kit.edu/GPIAG-Software/SOFI3D)\n        `\n        Computing 3-D wave field in viscoelastic medium with finite difference method.\n\n- :zero: :nine: [sw4](https://github.com/geodynamics/sw4)\n        `\n         3-D seismic modeling with finite difference method.\n        `\n    \n- :one: :zero: [OpenSWPC](https://github.com/tktmyd/OpenSWPC)\n        `\n        2/3-D  seismic wave propagation modeling with finite difference method.\n        `\n    \n- :one: :one: [AxiSEM](https://github.com/geodynamics/axisem)\n        `\n        AxiSEM is a parallel spectral-element method to solve 3D wave propagation \n        in a sphere with axisymmetric or spherically symmetric visco-elastic, acoustic, anisotropic structures.\n        `\n\n- :one: :two: [NEXD](http://www.gmg.ruhr-uni-bochum.de/geophysik/seismology/nexd.html)\n        `\n        NEXD: A SOFTWARE PACKAGE FOR HIGH ORDER SIMULATION OF SEISMIC WAVES \n        USING THE NODAL DISCONTINUOUS GALERKIN METHOD.\n        `\n- :one: :three: [PRINCETON GEODYNAMICS SOFTWARES](https://geodynamics.org/cig/software/)\n\n***\n:pencil2: :pencil2: :pencil2:\n## :arrow_down: Focal mechanism\n- :zero: :one: [gCAP](http://www.eas.slu.edu/People/LZhu/home.html)\n        `\n        Inversion of seismic source parameters with generalized Cut And Paste (gCAP) method.\n        `\n\n- :zero: :two: [pydmt](https://github.com/fabriziobernardi/pydmt)\n        `\n        Time Domain Moment Tensor Inversion,\n        `\n    \n- :zero: :three: [HASH](https://earthquake.usgs.gov/research/software/#HASH)\n        `\n        First motion polarity.\n        `\n    \n- :zero: :four: [focmec](https://seiscode.iris.washington.edu/projects/focmec)\n        `\n        Determine and display focal mechanism.\n        `\n    \n- :zero: :five: [WPhase](http://eost.u-strasbg.fr/wphase/)\n        `\n        W Phase source inversion.\n        Fast and reliable moment tensor estimation.\n        `\n        \n***\n:pencil2: :pencil2: :pencil2:\n\n## :arrow_down: Earthquake location\n- :zero: :one: [HypoDD](https://www.ldeo.columbia.edu/~felixw/hypoDD.html)\n        `\n        HypoDD is a Fortran computer program package for relocating earthquakes with the double-difference (DD) algorithm.\n        `\n\n***\n:pencil2: :pencil2: :pencil2:\n## :arrow_down: Ray tracing\n- :zero: :one: [scikit-fmm](https://github.com/scikit-fmm/scikit-fmm)\n          `\n          scikit-fmm is a Python extension module which implements the fast marching method.\n          `\n\n***\n:pencil2: :pencil2: :pencil2:\n## :arrow_down: Numerical computation\n- :zero: :one: [GSL](https://www.gnu.org/software/gsl/)\n    `\n    The GNU Scientific Library (GSL) is a numerical library implemented by C and C++.\n    `\n        \n\n***\n:pencil2: :pencil2: :pencil2:\n## :arrow_down: Blog notes\n### SeisMan\n\n[SeisMan's blogs](https://blog.seisman.info/)\n\n### Geophydog\n[Geophydog's blog](https://geophydog.cool)\n\n\n***\n:pencil2:  :pencil2: :pencil2:\n## :arrow_down:  Forward modeling and inversion\n### Surface wave topics\n- :zero: :one: [disba](https://github.com/keurfonluu/disba)\n  `\n     A python library of computing synthetic dispersion curve from Computer Programs in Seismology (CPS) with numba.\n  `\n\n- :zero: :two: [pysurf96](https://github.com/miili/pysurf96)\n  `\n    A python library of surf96 from Computer programs in seismology for computing dispersion curves.\n  `\n\n- :zero: :three: [CC-FJpy](https://github.com/ColinLii/CC-FJpy)\n  `\n    A python library of extracting multimodal dispersion curves from ambient noise or event data.\n  `\n\n- :zero: :four: [DisbaTomo](https://github.com/pan3rock/DisbaTomo)\n  `\n    Dispersion curve inversion with the BFGS optimization algorithm.\n  `\n### General forward and inversion problems\n- :zero: :one: [PyGimli](https://www.pygimli.org/#)\n  `\n  Multi-method modelling and inversion in geophysics.\n  `\n  \n  \n\n\n\n # to be continued ...\n",
        "createdAt": "2019-09-04T11:36:53.000Z",
        "updatedAt": "2025-11-12T06:47:12.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/geophydog/Seismology_Useful_Tools/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "SED-EEW/SED-EEW-SeisComP-contributions",
        "url": "https://github.com/SED-EEW/SED-EEW-SeisComP-contributions",
        "description": "Libraries and modules for Earthquake Early Warning (EEW) contributed by the Swiss Seismological Service (SED) at ETH Zurich, base for the ETHZ-SED SeisComP EEW system (so-called ESE). ",
        "stars": 9,
        "forks": 6,
        "readme": "[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5948948.svg)](https://doi.org/10.5281/zenodo.5948948)\n[![Docker](https://github.com/SED-EEW/SED-EEW-SeisComP-contributions/actions/workflows/docker-publish.yml/badge.svg)](https://github.com/SED-EEW/SED-EEW-SeisComP-contributions/actions/workflows/docker-publish.yml)\n[![Documentation Status](https://readthedocs.org/projects/sed-eew-seiscomp-contributions/badge/?version=latest)](https://sed-eew-seiscomp-contributions.readthedocs.io/en/latest/?badge=latest)\n\n# Description\n\nThe *SED-EEW-SeisComP-contributions* package contains SeisComP-based software developed by the ETHZ-SED seismic network team that has not been integrated into the main SeisComP distribution. Software includes the modules\n\n  - **scfinder** which matches emerging patterns of strong motion from the seismic network with most likely seismic sources to predict\n    location and size of finite faults, and from this, infers event magnitude and expected ground motion patterns.\n  - **sceewenv** which produces ground motion envelope values for EEW modules such as **scfinder** and **scvsmag**.\n\n# Requirements\n\nTo run the *SED-EEW-SeisComP-contributions* you need\n\n  - [SeisComP](https://www.seiscomp.de/) (version 3 or higher).\n\nAdditionally, if you also intend to run the **scfinder** module, you need:\n\n  - The [FinDer](https://github.com/SED-EEW/FinDer) library\n  - The [SED SeisComP contributions](https://github.com/swiss-seismological-service/sed-SeisComP-contributions) data model extension\n\nPlease see the SeisComP [README](https://github.com/SeisComP/seiscomp/blob/master/README.md) and FinDer\n[README](https://github.com/SED-EEW/FinDer/blob/master/README.md) for additional dependencies.\n\n# Installation\nFor installation, the *SED-EEW-SeisComP-contributions* sources have to be added as a SeisComP submodule and compiled together with SeisComP.\n\n## Using the latest SeisComP version\n\nFollowing SeisComP's [README](https://github.com/SeisComP/seiscomp/blob/master/README.md) to compile the *SED-EEW-SeisComP-contributions* modules with the latest version of SeisComP, checkout all\nrequired repositories and add *SED-EEW-SeisComP-contributions*:\n\n```bash\n# This is experimental\nTAG=master\n# But that is more safe:\nTAG=5.4.0 \ngit clone --branch $TAG https://github.com/SeisComP/seiscomp seiscomp\ngit clone --branch $TAG https://github.com/SeisComP/common seiscomp/src/base/common\ngit clone --branch $TAG https://github.com/SeisComP/main seiscomp/src/base/main\n# You might add more repo...\ngit clone --branch $TAG https://github.com/swiss-seismological-service/sed-SeisComP-contributions seiscomp/src/base/sed-contrib\ngit clone --branch latest https://github.com/SED-EEW/SED-EEW-SeisComP-contributions seiscomp/src/extras/sed-addons\n```\n\nIf you have already performed the previous steps and just want to update to latest version, run\n\n```bash\ncd seiscomp\ngit pull\ncd src/extras/sed-addons\ngit pull\n```\n\n## Using SeisComP3\nIf you have not installed SeisComP3 before, checkout the SeisComP3 source code using\n\n```bash\ngit clone --branch release/jakarta https://github.com/SeisComP3/seiscomp3 seiscomp3\ngit clone --branch v3 https://github.com/SED-EEW/SED-EEW-SeisComP-contributions seiscomp3/src/sed-addons\n```\n\nIf you have already performed the previous steps and just want to update to latest version, run\n\n```bash\ncd seiscomp3\ngit pull\ncd src/sed-addons\ngit pull\ngit checkout jakarta\n```\n\n## Adding scfinder\n\nIf you intend to use the scfinder SeisComP module you need to indicate **where FinDer was installed** by\nsetting the `FinDer_INCLUDE_DIR` and `FinDer_LIBRARY` environment variables. [By default](https://github.com/SED-EEW/FinDer)\nthese should be\n\n      - `FinDer_INCLUDE_DIR           /usr/local/include/finder`\n      - `FinDer_LIBRARY               /usr/local/lib/libFinder.a`\n\nIf you are using bash you can add these to your `~/.bashrc` using\n\n```bash\necho 'export FinDer_INCLUDE_DIR=/usr/local/include/finder' >> ~/.bashrc\necho 'export FinDer_LIBRARY=/usr/local/lib/libFinder.a' >> ~/.bashrc\necho 'export GMT_INCLUDE_DIR=/usr/local/include/gmt' >> ~/.bashrc\n```\n\n## Compilation\n\nSeisComP uses cmake to to manage the build process. In the case of SeisComP (v4+) run\n\n```bash\ncd seiscomp\nmake\n```\n\nand in case of SeisComP3, use\n\n```bash\ncd seiscomp3\nmake -f Makefile.cvs\n```\n\nPress *c* to start configuring the build and *g* to save the configuration.\nIf you need to run the configuration in headless mode you can just run cmake\ndirectly\n\n``` bash\ncd seiscomp\nmkdir build\ncd build\ncmake .. -DFinDer_INCLUDE_DIR=$FinDer_INCLUDE_DIR -DFinDer_LIBRARY=$FinDer_LIBRARY -DGMT_INCLUDE_DIR=$GMT_INCLUDE_DIR\n```\nFor other cmake option see the SeisComP [README] (https://github.com/SeisComP/seiscomp/blob/master/README.md).\nTo start the compilation run\n\n```bash\ncd build\nmake\n```\n\nIf your machine has multiple CPU cores available, you can speed up the compilation with `make -j 4`.\nIf the build succeeds you can install the new executables with\n\n```bash\nsudo make install\n```\nNote that this will overwrite your previous installation of SeisComP\n\n# Usage\n\nTo test the functionality of a particular module (e.g. scfinder) use the following command\n\n```bash\nseiscomp exec [scfinder] -u testuser --debug\n```\nand stop it with `CRT+c`\n\n# Cheatsheet\n - For scfinder to include GMT: `LD_LIBRARY_PATH=/usr/<gmt path>:$LD_LIBRARY_PATH` (.profile)\n - For scfinder to include libgmt and libpostscriptlight: `c++ ... -lgmt -lpostscriptlight` (seiscomp' make)\n - For FinDer to include libgmt: `g++  ... -lgmt` (libfinder' make)\n",
        "createdAt": "2018-12-05T15:52:10.000Z",
        "updatedAt": "2025-11-20T09:56:58.000Z",
        "language": "C++",
        "homepage": "https://docs.gempa.de/sed-eew/current",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.5948948",
            "openAlex": "10.5281/zenodo.5948948",
            "openCitations": "10.5281/zenodo.5948948",
            "dataCite": "10.5281/zenodo.5948948",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/SED-EEW/SED-EEW-SeisComP-contributions/master/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.5948948",
            "title": "SED-EEW-SeisComP-contributions",
            "journal": "Zenodo",
            "dateReleased": "2024-05-03T00:00:00.000Z",
            "abstract": "",
            "citationsArray": []
        },
        "repoDoi": "10.5281/zenodo.5948948",
        "publications": [
            {
                "doi": "10.5281/zenodo.5948948",
                "name": "Status of Earthquake Early Warning in Switzerland",
                "source": "",
                "authorNames": [],
                "abstract": "",
                "publicationDate": "2024-12-05T12:11:07.540Z"
            }
        ]
    },
    {
        "source": "GitHub",
        "name": "HNossrati/Seismology",
        "url": "https://github.com/HNossrati/Seismology",
        "description": "Useful codes in exploration seismology",
        "stars": 0,
        "forks": 0,
        "readme": "# Seismology\nUseful codes in exploration seismology\n",
        "createdAt": "2020-06-16T12:39:42.000Z",
        "updatedAt": "2020-07-11T21:33:13.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/HNossrati/Seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "AnmolHarshana/GLEE_Simulations_Seismology",
        "url": "https://github.com/AnmolHarshana/GLEE_Simulations_Seismology",
        "description": "This repository contains simulations for the seismology payload of GLEE.",
        "stars": 0,
        "forks": 0,
        "readme": "# GLEE_Simulations_Seismology\nThis repository contains simulations for the seismology payload of GLEE system of IITBSSP. The software used here is devito.\n\nThe simple simulation uses the inbuilt method model directly to get a quick understanding.This is an acoustic model however, our final aim is for an elastic model.\n\nThe simulation with boundary conditions is actually the final model required for answering the questions for feasibility. It implements rigorious boundary conditions using subdomains. This too is an acoustic model.\n\nThe elastic models have been constructed using three different ways:\n\n1. Using the model object of devito and setting parameters required.\n2. Using subdomains\n3. Using the demo_model of devito and modifying the parameters according to our requirements\n\nThe 2D models have been extrapolated to 3D.\n\nThe 3D models with vertical force have  also been created using subdomains for manual implementation of boundary conditions.\n\nOutput.csv contains the output generated by using a model with randomly placed receivers. vert_foce_3D_energy_plot.ipynb contains the code for the generation and plotting of the same data. \n",
        "createdAt": "2020-07-25T15:31:21.000Z",
        "updatedAt": "2021-08-11T19:09:45.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/AnmolHarshana/GLEE_Simulations_Seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "blazing216/seisflow",
        "url": "https://github.com/blazing216/seisflow",
        "description": "A workflow based package for seismology",
        "stars": 0,
        "forks": 0,
        "readme": "# seisflow\nA workflow based package for seismology\n",
        "createdAt": "2023-07-20T08:52:51.000Z",
        "updatedAt": "2023-07-20T08:52:51.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/blazing216/seisflow/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "lorenzomie/Seismic-Angle-Recognition",
        "url": "https://github.com/lorenzomie/Seismic-Angle-Recognition",
        "description": "Deep learning-based approach for predicting seismic signal arrival angles using a VAE for feature extraction and a neural network trained with triplet & contrastive loss. Achieves robust predictions with a 0.15° test error.",
        "stars": 1,
        "forks": 0,
        "readme": "# Seismic-Angle-Recognition\n\n<a target=\"_blank\" href=\"https://cookiecutter-data-science.drivendata.org/\">\n    <img src=\"https://img.shields.io/badge/CCDS-Project%20template-328F97?logo=cookiecutter\" />\n</a>\n\n## Abstract\n\nThis project presents a novel approach to predicting the arrival angle of seismic signals relative to a reference station using a neural network. The methodology involves encoding the input signals into a compact latent space representation using a Variational Autoencoder (VAE) and subsequently mapping these embeddings to the target angles using a neural network trained with triplet loss and contrastive loss. This approach leverages the power of convolutional neural networks (CNNs) for feature extraction and the effectiveness of metric learning techniques for robust angle prediction.\n\n## Introduction\n\nSeismic signal analysis is crucial for understanding the Earth's subsurface structures and for monitoring seismic activities. Traditional methods often rely on manual feature extraction and heuristic-based models, which can be time-consuming and less accurate. In this project, we propose an automated and data-driven approach using deep learning techniques to predict the arrival angle of seismic signals.\n\n## Methodology\n\n### Variational Autoencoder (VAE)\n\nThe VAE is employed to encode the 3D seismic signals into a lower-dimensional latent space. The VAE consists of an encoder and a decoder network. The encoder compresses the input signals into a latent space, capturing the essential features of the data. The decoder reconstructs the signals from the latent space representation. The VAE is trained to minimize the reconstruction loss and the Kullback-Leibler (KL) divergence loss.\n\n#### Encoder\n\nThe encoder network is a convolutional neural network (CNN) that processes the input signals and outputs the mean and log variance of the latent space distribution. The encoder layers are designed to capture hierarchical features from the input signals.\n\n#### Decoder\n\nThe decoder network is a transposed convolutional neural network that reconstructs the input signals from the latent space representation. The decoder layers are designed to upsample the latent space representation back to the original signal dimensions.\n\n#### Reparameterization Trick\n\nTo enable backpropagation through the stochastic latent variables, the reparameterization trick is used. This involves sampling from a standard normal distribution and scaling it by the mean and log variance obtained from the encoder.\n\n### Triplet Loss\n\nTo ensure that the latent space representation is meaningful and discriminative, we employ triplet loss during training. Triplet loss encourages the embeddings of similar signals to be closer together and the embeddings of dissimilar signals to be farther apart. This is achieved by forming triplets of anchor, positive, and negative samples and minimizing the distance between the anchor and positive samples while maximizing the distance between the anchor and negative samples.\n\n### Contrastive Loss\n\nIn addition to triplet loss, we use contrastive loss to further refine the embeddings. Contrastive loss penalizes large differences in predictions for similar targets and enforces a minimum difference in predictions for dissimilar targets. This helps in creating a more robust and discriminative latent space representation.\n\n## Implementation\n\nThe implementation is organized into several modules:\n\n- [`vae_model.py`](seismic_angle_recognition/vae_model.py): Defines the VAE architecture, including the encoder, decoder, and reparameterization trick.\n- [`train_model.py`](seismic_angle_recognition/train_model.py): Contains the training loop for the VAE and the embedding-to-label model, including the implementation of triplet loss and contrastive loss.\n- [`data_module.py`](seismic_angle_recognition/data_module.py): Handles data loading, preprocessing, and splitting into training, validation, and test sets.\n- [`config.yaml`](seismic_angle_recognition/config/config.yaml): Configuration file containing hyperparameters for the models and training process.\n\n## Results\n\nThe proposed approach demonstrates significant improvements in predicting the arrival angle of seismic signals. The use of VAE for encoding the signals into a compact latent space, combined with triplet loss and contrastive loss for training the embedding-to-label model, results in accurate and robust angle predictions.\n\nHere is a ordered latent space of the Variational AutoEncoder.\n\n![Latent Space](reports/figures/latent_space.png)\n\nSome metrics are available here\n\n### VAE TRAINING\n\n![Training Loss](reports/figures/train_VAE_loss.PNG)\n\n![Validation Loss](reports/figures/val_Loss_VAE.PNG)\n\n### MAPPER TRAINING\n\n![Training Loss](reports/figures/train_val_mapper.PNG)\n\n![Validation Loss](reports/figures/train_vs_contrastive.PNG)\n\nThe mean of test dataset is:\n\nMean of error: 0.15 [deg]\n\n## Conclusion\n\nThis project presents a novel and effective approach to seismic signal analysis using deep learning techniques. The combination of VAE, triplet loss, and contrastive loss provides a powerful framework for predicting the arrival angle of seismic signals. Future work can explore the extension of this approach to 3D angle reconstruction.\n\n## Project Organization\n\n```\n├── LICENSE            <- Open-source license\n├── Makefile           <- Makefile with convenience commands like `make data` or `make train`\n├── README.md          <- The top-level README for developers using this project.\n├── data\n│   └── signals_name   <- Signals data\n│\n├── docs               <- A default mkdocs project; see www.mkdocs.org for details\n│\n├── models             <- Trained and serialized models, model predictions, or model summaries\n│\n├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),\n│                         the creator's initials, and a short `-` delimited description, e.g.\n│                         `1.0-jqp-initial-data-exploration`.\n│\n├── pyproject.toml     <- Project configuration file with package metadata for \n│                         seismic_angle_recognition and configuration for tools like black\n│\n├── references         <- Data dictionaries, manuals, and all other explanatory materials.\n│\n├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.\n│   └── figures        <- Generated graphics and figures to be used in reporting\n│\n├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.\n│                         generated with `pip freeze > requirements.txt`\n│\n├── setup.cfg          <- Configuration file for flake8\n│\n└── seismic_angle_recognition   <- Source code for use in this project.\n    │\n    ├── __init__.py             <- Makes seismic_angle_recognition a Python module\n    │\n    ├── config.py               <- Store useful variables and configuration\n    │\n    ├── dataset.py              <- Scripts to download or generate data\n    │\n    ├── features.py             <- Code to create features for modeling\n    │\n    ├── modeling                \n    │   ├── __init__.py \n    │   ├── predict.py          <- Code to run model inference with trained models          \n    │   └── train.py            <- Code to train models\n    │\n    └── plots.py                <- Code to create visualizations\n```\n\n--------\n\n",
        "createdAt": "2025-02-09T12:36:30.000Z",
        "updatedAt": "2025-09-26T08:46:43.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/lorenzomie/Seismic-Angle-Recognition/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Seis-yanglei/UrbanDenoiser",
        "url": "https://github.com/Seis-yanglei/UrbanDenoiser",
        "description": "Denoise for urban seismological noise",
        "stars": 3,
        "forks": 1,
        "readme": "# UrbanDenoiser - Denoise for urban seismological noise\n\nWe develop a deep-learning-based denoising algorithm, UrbanDenoiser, to filter out urban seismological noise. \n\nThe trained deep-learning model of UrbanDenoiser is in 210906-023102.zip, which is a transfer learning from DeepDenoiser (https://github.com/wayneweiqiang/DeepDenoiser), using waveform datasets containing rich noise sources from the urban Long Beach dense array and high signal-to-noise ratio (SNR) earthquake signals from the rural San Jacinto dense array.\n\nWith the denoised Long Beach dense array data by UrbanDenoiser, we relocate the seismisity beneath Long Beach, and store the earthquake catalog in LB Catalog_2012_061-067.txt. \nThe first column is the date. The second column is the number of the three-second window during which the earthquake happened referenced to the starting time of each day (UTC). The third to fifth columns represent the earthquake location in our 3D imaging volume, with the grid spacing of 200 m in each direction (Depth, E-W, N-S). California State Plane Coordinate System Zone 7 is applied here. The origin of the 3D imaging volume represents (1293400 m, 1226000 m, 0 m) in Zone 7. The last column represents the number of median absolute deviation (MAD) the back-projected amplitudes exceeding the detection threshold.\n\nThis work is under the Creative Commons (CC) license 3.0: Attribution-NonCommercial-NoDerivs 3.0 https://creativecommons.org/licenses/by-nc-nd/3.0/ \n\nIf you use any part of this program in your research, please cite:\nLei Yang, Xin Liu, Weiqiang Zhu, Liang Zhao, Gregory C. Beroza. 2022. Toward Improved Urban Earthquake Monitoring through Deep-Learning-Based Noise Suppression. Science Advances, DOI: 10.1126/sciadv.abl3564.\n",
        "createdAt": "2022-02-21T04:10:53.000Z",
        "updatedAt": "2025-06-17T06:51:38.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Seis-yanglei/UrbanDenoiser/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ikahbasi/DigiTest",
        "url": "https://github.com/ikahbasi/DigiTest",
        "description": "Python code for testing the functionality of Digitizers that are being used in seismology.",
        "stars": 0,
        "forks": 0,
        "readme": "# DigiTest (Digitizer Test)\r\n## A python package of some routine functions for testing digitizers of seismology.\r\n## by: Iman Kahbasi\r\n\r\n\r\nThis project started from 2019 by Iman Kahbasi at IIEES.\r\n\r\nFirst publication in GitHub was at 2021-06-21.\r\n",
        "createdAt": "2021-06-21T18:00:39.000Z",
        "updatedAt": "2022-03-24T08:55:46.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ikahbasi/DigiTest/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "boland1992/SeisSuite",
        "url": "https://github.com/boland1992/SeisSuite",
        "description": "Python toolkit for ambient noise seismology methods. (combination of existing and newly written codes)",
        "stars": 19,
        "forks": 9,
        "readme": "",
        "createdAt": "2015-08-31T05:01:12.000Z",
        "updatedAt": "2024-04-23T08:13:35.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "OpenSeismology/ossr.github.io",
        "url": "https://github.com/OpenSeismology/ossr.github.io",
        "description": "Open source seismology repository",
        "stars": 0,
        "forks": 0,
        "readme": "# ossr.github.io\nOpen source seismology repository\n",
        "createdAt": "2018-10-17T12:39:04.000Z",
        "updatedAt": "2018-10-19T02:01:35.000Z",
        "language": null,
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/OpenSeismology/ossr.github.io/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "SpaceShiftN/seismology",
        "url": "https://github.com/SpaceShiftN/seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# Mars Seismic Event Detection & Analysis\n\nThis project focuses on detecting and analyzing significant seismic events from Mars using signal processing techniques and machine learning. The code reads seismic data, applies filtering, detects events using the STA/LTA algorithm, and leverages machine learning to classify and log significant events.\n\n# Table of Contents\n- Project Structure\n- Requirements\n- Setup Instructions\n- Running the Project\n- Workflow\n- Event Detection\n- Machine Learning\n- Logging & Visualization\n- Resources\n- Troubleshooting\n- Acknowledgements\n\n# Project Structure\n```\n├── data                          # Directory for Mars seismic data (.mseed files)\n│   └── XB.ELYSE.02.BHV.2022-01-02HR04_evid0006.mseed\n│\n├── results                       # Output directory for detected events and anomalies\n│   ├── detected_events_catalog.csv\n│   └── historical_anomalies.csv\n│\n├── src                           # Source code files\n│   ├── main.py                   # Main script for data processing and event detection\n│   ├── filtering.py              # Functions for signal filtering (bandpass, Butterworth)\n│   ├── event_detection.py        # STA/LTA algorithm and event detection functions\n│   ├── feature_extraction.py     # Extracts features for machine learning\n│   ├── anomaly_detection.py      # One-Class SVM for identifying significant events\n│   ├── visualization.py          # Plotting and visualizing the seismic data\n│   ├── data_logging.py           # Functions to log and compare events with historical data\n│   └── utils.py                  # Utility functions for data handling and processing\n│\n└── README.md                     # This file (documentation)\n```\n\n# Requirements\nEnsure you have the following dependencies installed:\n\n- Python 3.9+\n- ObsPy\n- Pandas\n- NumPy\n- Scikit-learn\n- SciPy\n- Matplotlib\n\nYou can install the necessary packages using:\n\n```bash\npip install -r requirements.txt\n```\n\n# Setup Instructions\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/username/Mars-Seismic-Detection.git\ncd Mars-Seismic-Detection\n```\n\n2. Place your seismic data files (`.mseed`) in the `data` directory.\n\n# Running the Project\nTo run the project, you can use the following command:\n\n```bash\npython src/main.py --mode <mode> --data <data_path> [other_arguments]\n```\n\n# Command-Line Arguments:\n- `--mode`: Specify the mode of operation. Choose from `train`, `retrain`, or `infer`. (required)\n- `--data`: Path to the data file or directory containing seismic data. (required)\n- `--model`: Path to the model file (default is `seismic_model.pth`).\n- `--epochs`: Number of training epochs (default is `20`).\n- `--batch_size`: Size of the batch for training (default is `64`).\n- `--window_size`: Size of the window for processing (default is `512`).\n- `--minfreq`: Minimum filter frequency (default is `0.5` Hz).\n- `--maxfreq`: Maximum filter frequency (default is `20.0` Hz).\n- `--output`: Path to the output CSV file for detected events (default is `detected_events.csv`).\n- `--threshold`: Detection threshold for events (default is `0.5`).\n- `--target_sampling_rate`: Target sampling rate for resampling the data (default is `20.0` Hz).\n- `--save_plots`: Optional flag to save plots as PNG files.\n- `--plots_dir`: Directory to save plots (default is `plots`).\n- `--max_files`: Maximum number of files to process for training (optional).\n- `--step`: Step size for generating samples in the dataset (default is `256`).\n- `--learning_rate`: Learning rate for training (default is `0.001`).\n\n# Workflow\nThe workflow includes several key steps:\n1. **Data Preprocessing**: Raw seismic data is read from `.mseed` files and passed through bandpass and Butterworth filters to enhance the signal quality.\n2. **Event Detection**: The STA/LTA algorithm identifies potential seismic events based on amplitude changes over time.\n3. **Feature Extraction**: Key features (duration, amplitude, energy) are extracted from the detected events for machine learning.\n4. **Machine Learning**: A One-Class SVM model is trained to classify events as significant or insignificant.\n5. **Logging & Visualization**: Detected events are logged into a CSV file, and visualizations (waveform plots, spectrograms) are generated.\n\n# Event Detection\n- **STA/LTA Algorithm**: Detects seismic events by comparing the short-term average (STA) and long-term average (LTA) of the signal’s amplitude.\n- **Thresholding**: Events are triggered when the STA/LTA ratio exceeds the defined thresholds.\n\n# Machine Learning\n- **Feature Scaling**: Extracted features are scaled using `StandardScaler` to prepare them for machine learning.\n- **Anomaly Detection**: One-Class SVM is trained on extracted features to identify significant events.\n- **Historical Comparison**: Detected events are compared with historical anomalies to find similar patterns using pairwise distances.\n\n# Logging & Visualization\n- **Event Catalog**: Detected significant events are saved in `detected_events_catalog.csv`.\n- **Plotting**: The original seismic signal, filtered signal, STA/LTA ratio, and spectrogram are plotted to visualize the event detection process.\n\n# Resources\n- [ObsPy Documentation](https://docs.obspy.org)\n- [SciPy Documentation](https://docs.scipy.org)\n\n# Troubleshooting\n1. **FileNotFoundError**: Ensure the seismic data file paths are correct and located in the `data` directory.\n2. **No Events Detected**: Check the thresholds for the STA/LTA algorithm. Adjust the `threshold_on` and `threshold_off` parameters to improve detection sensitivity.\n3. **Model Accuracy**: Experiment with different SVM parameters (`nu`, `kernel`, etc.) for better classification results.\n4. **Performance Issues**: If the code is slow, reduce the size of the window for the STA/LTA algorithm or optimize your filtering steps.\n\nHere's the updated section for running the project, including examples for inference, training, and fine-tuning:\n\n---\n\n# Running the Project\nTo run the project, you can use the following command:\n\n```bash\npython src/main.py --mode <mode> --data <data_path> [other_arguments]\n```\n\n# Command-Line Arguments:\n- `--mode`: Specify the mode of operation. Choose from `train`, `retrain`, or `infer`. (required)\n- `--data`: Path to the data file or directory containing seismic data. (required)\n- `--model`: Path to the model file (default is `seismic_model.pth`).\n- `--epochs`: Number of training epochs (default is `20`).\n- `--batch_size`: Size of the batch for training (default is `64`).\n- `--window_size`: Size of the window for processing (default is `512`).\n- `--minfreq`: Minimum filter frequency (default is `0.5` Hz).\n- `--maxfreq`: Maximum filter frequency (default is `20.0` Hz).\n- `--output`: Path to the output CSV file for detected events (default is `detected_events.csv`).\n- `--threshold`: Detection threshold for events (default is `0.5`).\n- `--target_sampling_rate`: Target sampling rate for resampling the data (default is `20.0` Hz).\n- `--save_plots`: Optional flag to save plots as PNG files.\n- `--plots_dir`: Directory to save plots (default is `plots`).\n- `--max_files`: Maximum number of files to process for training (optional).\n- `--step`: Step size for generating samples in the dataset (default is `256`).\n- `--learning_rate`: Learning rate for training (default is `0.001`).\n\n# Examples\n\n1. **Training a New Model**:\n   To train a new model using a dataset located in `data/training_data`, use the following command:\n\n   ```bash\n   python src/main.py --mode train --data data/training_data --epochs 50 --batch_size 32 --output trained_model.pth\n   ```\n\n   In this example:\n   - The model will be trained for 50 epochs.\n   - The batch size is set to 32.\n   - The trained model will be saved as `trained_model.pth`.\n\n2. **Fine-Tuning an Existing Model**:\n   To fine-tune an existing model (e.g., `pretrained_model.pth`) using new training data, use:\n\n   ```bash\n   python src/main.py --mode retrain --data data/new_training_data --model pretrained_model.pth --epochs 20 --batch_size 16 --output fine_tuned_model.pth\n   ```\n\n   In this example:\n   - The existing model will be fine-tuned for 20 epochs.\n   - The batch size is set to 16.\n   - The updated model will be saved as `fine_tuned_model.pth`.\n\n3. **Running Inference**:\n   To perform inference on a new dataset located in `data/inference_data` using a trained model, run:\n\n   ```bash\n   python src/main.py --mode infer --data data/inference_data --model trained_model.pth --output detected_events.csv --threshold 0.6 --save_plots\n   ```\n\n   In this example:\n   - The trained model (`trained_model.pth`) will be used for inference.\n   - Detected events will be saved to `detected_events.csv`.\n   - The threshold for detection is set to 0.6.\n   - Plots will be saved as PNG files in the default directory.\n\n# Workflow\nThe workflow includes several key steps:\n1. **Data Preprocessing**: Raw seismic data is read from `.mseed` files and passed through bandpass and Butterworth filters to enhance the signal quality.\n2. **Event Detection**: The STA/LTA algorithm identifies potential seismic events based on amplitude changes over time.\n3. **Feature Extraction**: Key features (duration, amplitude, energy) are extracted from the detected events for machine learning.\n4. **Machine Learning**: A One-Class SVM model is trained to classify events as significant or insignificant.\n5. **Logging & Visualization**: Detected events are logged into a CSV file, and visualizations (waveform plots, spectrograms) are generated.\n\n# Acknowledgements\nThis project is a part of ongoing research in Mars seismic event detection.\n",
        "createdAt": "2024-10-05T03:47:11.000Z",
        "updatedAt": "2024-10-06T20:07:52.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/SpaceShiftN/seismology/nweaver-ai-2/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "claudiodsf/earthquake_seismology_software",
        "url": "https://github.com/claudiodsf/earthquake_seismology_software",
        "description": "A curated list of earthquake seismology software",
        "stars": 42,
        "forks": 15,
        "readme": "# Earthquake Seismology Software\nA curated list of earthquake seismology software, mostly on GitHub.\n\nFeel free to contribute through a pull request!\n\n## Earthquake Detection\nGitHub topic: https://github.com/topics/earthquake-detection (consider adding the topic `earthquake-detection` to your repository).\n\n- [BackTrackBB](https://github.com/BackTrackBB/backtrackbb):\n  Multi-band array detection and location of seismic sources.\n  ![GitHub last commit](https://img.shields.io/github/last-commit/BackTrackBB/backtrackbb)\n- [CDRP_TF](https://github.com/YijianZhou/CDRP_TF):\n  CNN Event Detection & RNN Phase Picking (in Tensorflow)\n  ![GitHub last commit](https://img.shields.io/github/last-commit/YijianZhou/CDRP_TF)\n- [Detex](https://github.com/d-chambers/Detex):\n  A Python package for subspace detection and waveform similarity clustering\n  ![GitHub last commit](https://img.shields.io/github/last-commit/d-chambers/Detex)\n- [easyQuake](https://github.com/jakewalter/easyQuake):\n  Simplified machine-learning driven earthquake detection, location, and analysis in one easy-to-implement python package.\n  ![GitHub last commit](https://img.shields.io/github/last-commit/jakewalter/easyQuake)\n- [EQcorrscan](https://github.com/eqcorrscan/EQcorrscan):\n  A python package for the detection and analysis of repeating and near-repeating earthquakes.\n  ![GitHub last commit](https://img.shields.io/github/last-commit/eqcorrscan/EQcorrscan)\n- [EQTransformer](https://github.com/smousavi05/EQTransformer):\n  EQTransformer, a python package for earthquake signal detection and phase picking using AI.\n  ![GitHub last commit](https://img.shields.io/github/last-commit/smousavi05/EQTransformer)\n- [FAST](https://github.com/stanford-futuredata/FAST):\n  End-to-end earthquake detection pipeline via efficient time series similarity search.\n  ![GitHub last commit](https://img.shields.io/github/last-commit/stanford-futuredata/FAST)\n- [Fast Matched Filter](https://github.com/beridel/fast_matched_filter):\n  An efficient seismic matched-filter search for both CPU and GPU architectures.\n  ![GitHub last commit](https://img.shields.io/github/last-commit/beridel/fast_matched_filter)\n- [GPD](https://github.com/interseismic/generalized-phase-detection):\n  Generalized Seismic Phase Detection with Deep Learning.\n  ![GitHub last commit](https://img.shields.io/github/last-commit/interseismic/generalized-phase-detection)\n- [LOKI](https://github.com/wulwife/LOKI):\n  earthquake LOcation by waveform staKIng.\n  ![GitHub last commit](https://img.shields.io/github/last-commit/wulwife/LOKI)  \n- [MSMS](https://github.com/YijianZhou/MSMS):\n  Catalog augmentation (event detection) based on matched filter technique.\n  ![GitHub last commit](https://img.shields.io/github/last-commit/YijianZhou/MSMS)\n- [neic-glass3](https://github.com/usgs/neic-glass3):\n  Next generation seismic event detection and association algorithm.\n  ![GitHub last commit](https://img.shields.io/github/last-commit/usgs/neic-glass3)\n- [PAD](https://github.com/YijianZhou/PAD):\n  Earthquake detection from raw continuous waveform.\n  ![GitHub last commit](https://img.shields.io/github/last-commit/YijianZhou/PAD)\n- [PhasePApy](https://github.com/austinholland/PhasePApy):\n  Python Seismic Phase Picker and Associator.\n  ![GitHub last commit](https://img.shields.io/github/last-commit/austinholland/PhasePApy)\n- [PhaseLink](https://github.com/interseismic/PhaseLink):\n  A deep learning approach to seismic phase association.\n  ![GitHub last commit](https://img.shields.io/github/last-commit/interseismic/PhaseLink)\n- [PhaseNet](https://github.com/wayneweiqiang/PhaseNet):\n  A Deep-Neural-Network-Based Seismic Arrival Time Picking Method.\n  ![GitHub last commit](https://img.shields.io/github/last-commit/wayneweiqiang/PhaseNet)\n- [Pinky](https://github.com/HerrMuellerluedenscheid/pinky):\n  Deep learning toolbox for earthquake localization and detection.\n  ![GitHub last commit](https://img.shields.io/github/last-commit/HerrMuellerluedenscheid/pinky)\n- [QuakeMigrate](https://github.com/QuakeMigrate/QuakeMigrate):\n  A Python package for automatic earthquake detection and location using waveform migration and stacking.\n  ![GitHub last commit](https://img.shields.io/github/last-commit/QuakeMigrate/QuakeMigrate)\n- [REDPy](https://github.com/ahotovec/REDPy):\n  Repeating Earthquake Detector in Python.\n  ![GitHub last commit](https://img.shields.io/github/last-commit/ahotovec/REDPy)\n- [S-SNAP](https://github.com/tanfengzhou/S-SNAP1.1):\n  Seismicity-Scanning based on Navigated Automatic Phase-picking.\n  ![GitHub last commit](https://img.shields.io/github/last-commit/tanfengzhou/S-SNAP1.1)\n- [SeisBench](https://github.com/seisbench/seisbench):\n  A toolbox for machine learning in seismology.\n  ![GitHub last commit](https://img.shields.io/github/last-commit/seisbench/seisbench)\n\n\n## Earthquake Location\nGitHub topic: https://github.com/topics/earthquake-location (consider adding the topic `earthquake-location` to your repository).\n\n\n## Earthquake processing systems\n- [EarthWorm](http://www.earthwormcentral.org):\n  Automatic earthquake process system\n- [SeisComP](https://www.seiscomp.de):\n  Seismological software for data acquisition, processing, distribution and interactive analysis.\n  \n",
        "createdAt": "2020-08-19T07:49:28.000Z",
        "updatedAt": "2025-11-10T08:59:46.000Z",
        "language": null,
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/claudiodsf/earthquake_seismology_software/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "paudetseis/PyRaysum",
        "url": "https://github.com/paudetseis/PyRaysum",
        "description": "Teleseismic body wave modeling through stacks of (dipping/anisotropic) layers",
        "stars": 52,
        "forks": 15,
        "readme": "![](./pyraysum/examples/picture/PyRaysum_logo.png)\n## Software for modeling ray-theoretical body-wave propagation\n\nThis program generates sets of ray-theoretical seismograms for incident plane waves \n(teleseismic approximation) for seismic velocity models consisting of a stack of layers \nwith planar but non-parallel (dipping) interfaces, allowing the possibility of anisotropy \nin the layers. Incident P and S waves are supported.\n\n`PyRaysum` is a Python wrapper around the Fortran software `Raysum`, originally developed by \n[Andrew Frederiksen](https://umanitoba.ca/faculties/environment/departments/geo_sciences/research_facilities/AndrewFrederiksen.html). \nA trimmed down version of the Fortran code is supplied with `PyRaysum`. You can find the \noriginal version [here](https://home.cc.umanitoba.ca/~frederik/Software/).\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7468301.svg)](https://doi.org/10.5281/zenodo.7468301)\n[![build](https://github.com/paudetseis/PyRaysum/workflows/Build/badge.svg)](https://github.com/paudetseis/PyRaysum/actions)\n[![codecov](https://codecov.io/gh/paudetseis/PyRaysum/branch/main/graph/badge.svg?token=59F1SWLM9Q)](https://codecov.io/gh/paudetseis/PyRaysum)\n![GitHub](https://img.shields.io/github/license/paudetseis/pyraysum)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n\nAuthors: [Wasja Bloch](https://www.eoas.ubc.ca/people/wasjabloch), [Pascal Audet](https://www.uogeophysics.com/authors/admin/) (Developers and Maintainers of `PyRaysum`) & [Andrew Frederiksen](https://umanitoba.ca/faculties/environment/departments/geo_sciences/research_facilities/AndrewFrederiksen.html) & (Developer of original `Fortran` version)\n\n\n#### Installation\n\n*PyRaysum* can be installed from PyPI or from source.\n\nTo avoid conflicts with other programs, it is recommended to install *PyRaysum* inside a designated `conda` environment (called here `prs`) alongside its dependecies.\n```\nconda create -n prs \"python<3.12\" \"numpy<1.23\" setuptools fortran-compiler obspy -c conda-forge\nconda activate prs\n```\nIf you are using an alternative *Python* interpreter (e.g., *IPython* or a *Jupyter*\nnotebook), include it into your installation as well. This ensures that the interpreter\nuses the correct *Python* version.\n```\n# IPython\nconda install ipython\n\n# Jupyter notebooks\nconda install jupyter\n```\n\n##### Installing from PyPI\n\n```\npip install pyraysum\n```\n\n##### Installing from source\n\nThe source code of *PyRaysum* can also be downloaded from *GitHub* and installed via `pip`.\n\n```\ngit clone https://github.com/paudetseis/PyRaysum.git\ncd PyRaysum\npip install .\n```\n\n#### Getting Started\n\nTo compute receiver functions for a range of back-azimuths considering a simple 1-layer \nover a half-space subsurface seismic velocity model, in a *Python* or *iPython* console, execute:\n```\nfrom pyraysum import Model, Geometry, Control, run\n\n# Build a 1-layer-over-half-space subsurface model\nmodel = Model(\n    thickn=[32000, 0],  # m; half-space thickness is irrelevant\n    rho=[2800, 3600],  # kg/m^3\n    vp=[6400, 8100],  # m/s\n    vs=[3600, 4650],  # m/s\n)\nmodel.plot()\n\n# Define back-azimuth range and single horizontal slowness value\ngeometry = Geometry(baz=range(0, 360, 30), slow=0.07)\ngeometry.plot()\n\n# Set sampling interval, number of points, alignment and ray-coordinate rotation\ncontrol = Control(dt=1./20., npts=800, align=\"P\", rot=\"PVH\")\n\n# Run the simulation\nresult = run(model, geometry, control, rf=True)\n\n# Filter below 2s period\nresult.filter(\"rfs\", \"lowpass\", freq=0.5)\n\n# Plot the results\nresult.plot(\"rfs\")\n```\n\n#### Documentation\nThe complete API documentation, scripts and tutorials are described at https://paudetseis.github.io/PyRaysum/\n\n#### Citing\n\nIf you use `PyRaysum` in your work, please cite the following references:\n\n- Frederiksen, A.W., and Bostock, M.G. (2000) Modelling teleseismic waves in dipping anisotropic structures. Geophysical Journal International 141: 401-412. https://doi.org/10.1046/j.1365-246x.2000.00090.x\n\n- Bloch, W., and Audet, P. (2023). PyRaysum: Software for modeling ray-theoretical plane body-wave propagation in dipping anisotropic media. Seismica. https://doi.org/10.26443/seismica.v2i1.220\n\n- Audet, P., and Bloch, W. (2022). PyRaysum: Software for modeling ray-theoretical body-wave propagation. Zenodo. https://doi.org/10.5281/zenodo.7468301\n\n#### Contributing\n\nAll constructive contributions are welcome, e.g. bug reports, discussions or suggestions for new features. You can either [open an issue on GitHub](https://github.com/paudetseis/PyRaysum/issues) or make a pull request with your proposed changes. Before making a pull request, check if there is a corresponding issue opened and reference it in the pull request. If there isn't one, it is recommended to open one with your rationale for the change. New functionality or significant changes to the code that alter its behavior should come with corresponding tests and documentation. If you are new to contributing, you can open a work-in-progress pull request and have it iteratively reviewed. \n\nOther examples of contributions include notebooks that describe published examples of `PyRaysum` usage and processing. Suggestions for improvements (speed, accuracy, plotting, etc.) are also welcome.\n\n",
        "createdAt": "2020-11-27T22:28:51.000Z",
        "updatedAt": "2025-11-14T12:14:57.000Z",
        "language": "Jupyter Notebook",
        "homepage": "https://paudetseis.github.io/PyRaysum/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.7468301",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.7468301",
            "dataCite": "10.5281/zenodo.7468301",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/paudetseis/PyRaysum/main/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.7468301",
            "title": "PyRaysum: Software for modeling ray-theoretical body-wave propagation",
            "journal": "Zenodo",
            "dateReleased": "2022-12-21T00:00:00.000Z",
            "abstract": "This is the first official release of <em>PyRaysum</em>. This program generates sets of ray-theoretical seismograms for incident plane waves (teleseismic approximation) for seismic velocity models consisting of a stack of layers with planar but non-parallel (dipping) interfaces, allowing the possibility of anisotropy in the layers. Incident P and S waves are supported. <em>PyRaysum</em> is a Python wrapper around the (revamped and corrected) Fortran software <em>Raysum</em>, originally developed by Andrew Frederiksen. A trimmed down version of the Fortran code is supplied with <em>PyRaysum</em>. You can find the original version here. Authors: @wsja and @paudetseis If you use <em>PyRaysum</em> in your work, please cite the following references: Frederiksen, A.W., and Bostock, M.G. (1999) Modelling teleseismic waves in dipping anisotropic structures. Geophysical Journal International 141: 401-412. https://doi.org/10.1046/j.1365-246x.2000.00090.x Bloch, W., and Audet, P. (2023). PyRaysum: Software for modeling ray-theoretical plane body-wave propagation in dipping anisotropic media. Seismica, <em>2</em>(1). https://doi.org/10.26443/seismica.v2i1.220 Audet, P., and Bloch, W. (2022). PyRaysum: Software for modeling ray-theoretical body-wave propagation. Zenodo. https://doi.org/10.5281/zenodo.6095749",
            "citationsArray": []
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ekrarwinata/PSEPICK",
        "url": "https://github.com/ekrarwinata/PSEPICK",
        "description": "Just share code for seismology processing",
        "stars": 0,
        "forks": 0,
        "readme": "# PSEPICK\nJust share code for seismology processing\n\nPSEPICKPy is an open-source project dedicated to provide a Python framework for processing seismological data.\n",
        "createdAt": "2016-11-23T02:51:16.000Z",
        "updatedAt": "2016-11-23T02:51:16.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ekrarwinata/PSEPICK/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "anowacki/SeisTau.jl",
        "url": "https://github.com/anowacki/SeisTau.jl",
        "description": "Add seismic travel times to your traces with Seis.jl",
        "stars": 2,
        "forks": 1,
        "readme": "# SeisTau.jl\n\n## Build status\n[![Build Status](https://travis-ci.org/anowacki/SeisTau.jl.svg?branch=master)](https://travis-ci.org/anowacki/SeisTau.jl)\n[![Build status](https://ci.appveyor.com/api/projects/status/n2ggp9fsxx59jnoh?svg=true)](https://ci.appveyor.com/project/AndyNowacki/seistau-jl)\n[![Coverage Status](https://coveralls.io/repos/github/anowacki/SeisTau.jl/badge.svg?branch=master)](https://coveralls.io/github/anowacki/SeisTau.jl?branch=master)\n\n## Using\n\nSeisTau integrates the [TauPy](https://github.com/anowacki/TauPy.jl) and\n[Seis](ttps://github.com/anowacki/Seis.jl) packages, allowing one to easily\nuse seismic travel time predictions for 1D Earth models with `Seis.Trace`s.\n\nSeisTau extends `Seis.add_picks!` so that you can add picks to your `Trace`s\nlike so:\n\n```julia\njulia> using Seis, SeisTau\n\njulia> t = sample_data(:array);\n\njulia> add_picks!.(t, \"PKIKP\")\n60-element Array{Seis.SeisDict{Union{Int64, Symbol},NamedTuple{(:time, :name),Tuple{Float32,Union{Missing, String}}}},1}:\n Seis.SeisDict(:A=>Seis.Pick{Float32,String}((time=1134.08, name=\"A\")),:PKIKP=>Seis.Pick{Float32,String}((time=1126.4486, name=\"PKIKP\")),1=>Seis.Pick{Float32,String}((time=137.35, name=missing)))\n...\n```\n\nHere we have added a pick for the\n[PKIKP](https://www.ldeo.columbia.edu/res/pi/Monitoring/Doc/Srr_2006/GUIDE.PDF)\nphase to all of the traces at once using Julia's `.`-notation, broadcasting the call to\n`add_picks!` across each of the `Trace`s in `t`.\n\nYou can now plot up the data aligned on this pick:\n\n```julia\njulia> using Plots, Seis.Plot\n\njulia> section(t, align=\"PKIKP\", xlim=(-10,20), zoom=2)\n```\n\n![Example record section aligned on pick](docs/images/aligned_PKIKP_section.jpg)\n\nSee `Seis.picks` for more information on access picks in Seis.\n\n\n## Installation\n\nFirst, [install Seis.jl](https://github.com/anowacki/Seis.jl#installation).\n\nThen, having chosen [one of the options below](#note-on-python-installation):\n\n```julia\njulia> ] # Pressing ']' enters pkg mode\n\n(v1.1) pkg> add https://github.com/anowacki/TauPy.jl https://github.com/anowacki/SeisTau.jl\n```\n\n### Note on Python installation\n\nThe TauPy package on which SeisTau relies uses the Obspy module `obspy.taup` to\ncalculate travel times.  Therefore, one needs Obspy to be installed.  There are\ntwo options:\n\n1. Install Obspy locally if it is not already installed on your machine, then [build\n   PyCall pointing at that\n   installation](https://github.com/JuliaPy/PyCall.jl#specifying-the-python-version)\n   by setting the `$PYTHON` environment variable to the path to your python executable:\n   - `ENV[\"PYTHON\"] = \"python\"; import Pkg; Pkg.build(\"PyCall\")`\n2. Allow PyCall to install its own private Miniconda distribution.  In this case,\n   Obspy will be automatically installed when `add`ing TauPy above.  **N.B. There\n   are intermittent problems with automatic installation via this option, so option\n   1 is recommended at present.**\n",
        "createdAt": "2019-02-27T09:47:49.000Z",
        "updatedAt": "2022-12-09T11:02:19.000Z",
        "language": "Julia",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/anowacki/SeisTau.jl/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "SeisPider/CPS-Demos",
        "url": "https://github.com/SeisPider/CPS-Demos",
        "description": "Demos of usage of Computer Programs in Seismology",
        "stars": 1,
        "forks": 2,
        "readme": "## CPS-Demos\n\n- 记载一些利用 [Computer Programs in Seismology](http://www.eas.slu.edu/eqc/eqc_cps/CPS/CPS330.html)\n软件包处理地震学数据及理论模拟的脚本及用法笔记\n\n\n",
        "createdAt": "2017-09-24T13:15:19.000Z",
        "updatedAt": "2020-10-07T00:49:41.000Z",
        "language": "Shell",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/SeisPider/CPS-Demos/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "seismo-learn/seismology",
        "url": "https://github.com/seismo-learn/seismology",
        "description": "A reference book for seismology",
        "stars": 7,
        "forks": 2,
        "readme": "# 地震“学”参考书\n\n[![Jupyter Book Badge](https://jupyterbook.org/badge.svg)](https://seismo-learn.org/seismology/)\n[![Deploy](https://github.com/seismo-learn/seismology/actions/workflows/deploy.yml/badge.svg)](https://github.com/seismo-learn/seismology/actions/workflows/deploy.yml)\n[![License: CC BY-NC 4.0](https://img.shields.io/badge/License-CC%20BY--NC%204.0-blue.svg)](https://creativecommons.org/licenses/by-nc/4.0/deed.zh-hans)\n\n\n本文档主要介绍地震学基础知识。\n\n- 主页：https://seismo-learn.org/seismology/\n- 源码：https://github.com/seismo-learn/seismology\n\n## 文档维护\n\n本文档尚有很多不完善之处，欢迎读者参与到文档的维护与更新中。\n详情见[贡献指南](https://seismo-learn.org/contributing/)。\n\n## 许可协议\n\n本作品采用 [知识共享署名-非商业性使用 4.0 国际许可协议 (CC BY-NC 4.0)](https://creativecommons.org/licenses/by-nc/4.0/deed.zh-hans) 。\n任何人都可以自由地分享、修改本作品，但必须遵循如下条件：\n\n- 署名：必须提到原作者，提供指向此许可协议的链接，表明是否有做修改\n- 非商业性使用：不能对本作品进行任何形式的商业性使用\n",
        "createdAt": "2020-12-29T14:10:41.000Z",
        "updatedAt": "2025-09-23T12:17:52.000Z",
        "language": "Makefile",
        "homepage": "https://seismo-learn.org/seismology/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/seismo-learn/seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "hagabbar/Terramon",
        "url": "https://github.com/hagabbar/Terramon",
        "description": "Pipeline to predict earthquake arrival times and amplitudes at the LIGO observatories",
        "stars": 0,
        "forks": 1,
        "readme": "# Terramon\n",
        "createdAt": "2016-11-29T15:45:10.000Z",
        "updatedAt": "2017-12-30T18:23:33.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/hagabbar/Terramon/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "utkarshp023/Seismology",
        "url": "https://github.com/utkarshp023/Seismology",
        "description": "Programme that enables and makes it easy to compute the necessary data that helps in the study of Attenuation trends of coda waves",
        "stars": 1,
        "forks": 1,
        "readme": "# Seismology\nThis programme is supposed to calculate all the necessary data required to calculate the Quality factor i.e. Qc which is \ninversaly propotional to the attenuation of seismic waves. \nThe math in this programme is derived from the research done by Aki(1980) and others.\nThe code is java based, and an external java code for determining the slopes is used.\n\n",
        "createdAt": "2015-06-24T06:00:10.000Z",
        "updatedAt": "2016-04-04T19:51:10.000Z",
        "language": "Java",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/utkarshp023/Seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ammcpherson/General_seismology",
        "url": "https://github.com/ammcpherson/General_seismology",
        "description": "Scripts for generally useful things in seismology as I write them",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2025-06-30T19:18:13.000Z",
        "updatedAt": "2025-07-01T15:11:36.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "iod-ine/philoseismos",
        "url": "https://github.com/iod-ine/philoseismos",
        "description": "A python package for engineering seismology every day tasks",
        "stars": 6,
        "forks": 2,
        "readme": "# philoseismos: engineering seismologist's toolbox.\n\n\n<p align=\"center\">\n  <i>\n    The Classical Greeks had a love for wisdom —  <br>\n    It came down to us as <b>philo·sophia</b>.  <br>\n    And I have a passion for the seismic method —  <br>\n    Let this be an ode to <b>philo·seismos</b>.  <br>\n    O how sweet it is —  <br>\n    Listening to the echos from the earth. <br>\n  </i>\n</p>\n<p align=\"right\">\n  Öz Yilmaz <br>\n  Seismic Data Analysis\n </p>\n \n ## Features\n * Working with SEG-Y files\n * Dispersion image calculation from seismograms (phase-shift method)\n * Dispersion curve calculation for Rayleigh waves for horizontally layered media\n",
        "createdAt": "2020-03-25T13:11:03.000Z",
        "updatedAt": "2024-12-04T04:26:20.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/iod-ine/philoseismos/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "samhaug/Doornbos_Raytracing",
        "url": "https://github.com/samhaug/Doornbos_Raytracing",
        "description": "Doornbos seismological algorithm for calculating traveltimes",
        "stars": 1,
        "forks": 0,
        "readme": "# Doornbos_Raytracing\n\nThe command is ./ray1d\n\nYou will have to change line 31 of modps.f. Set this line to the directory in which rmod.dat lives. \nThe code needsto find this file. \n\n     rm *.o and make\n\nThe input file is ray1d.in. Run the executable like this:\n\n     ./ray1d < ray1d.in\n\nSample entry explanation for SAMPLE.in\n\nSAMPLE.in has many ‘blocks’, but the code will only run on the first block in the input file. \nYou may specify the number of phases by the number in the first line.\n\n1 is number of phases computed, 0.5 is a stabilization number that is less than 1. \nSatoshi has never changed it.\n     1   0.5 \n5.2433 is ballpark guess for slowness, 2 is for lines that follow                                                                  \n     5.2433          2\nA simple phase like P has two branches, the downgoing part and the upgoing part. Hence two lines:\n5871.0 is radius for source, 3479.5 is radius for turning point. \nFor P wave, make this the CMB. The middle 1 means P.                                                             \n5871.0  3479.500000  1 1  1\nThis is the upgoing part, modeled as a source at the earth’s surface and a receiver at the CMB, so it goes backwards, hence the -1                                                    \n6371.0  3479.500000  1 1 -1                                                    \n     -- P for h=500 D=80  AK135\n\nThis is a similar block of text for PcP. \nEverything is the same, except the slowness guess on the second line. \nThis dictates what raypath the code searches for.\n     1   0.5                                                                        \n     4.37606         2                                                              \n     5871.0  3479.500000  1 1  1                                                    \n     6371.0  3479.500000  1 1 -1                                                    \n      -- PcP for h=500 D=80  AK135\n\n\n\n\n\nThis block is for P4KP. There are three branches to specify. The in-going P, the 4K, and the outgoing P.\n      1   0.5                                                                        \n      4.42815  3                 \nThe in-going P is the same as usual, it has one branch                                                 \n      6371.0  3479.500000  1 1 1\nThe core bouncing branch has 4 legs, each leg has a downward and upward going section, making 8 total. Put the receiver at the center of the earth.                                                                                         \n      3479.5  0.000000  8 1 1     \n      The out-going P is the same as usual, it has one branch going backwards                                                \n      6371.0  3479.500000  1 1 -1                                                    \n      -------- P4KP ----------  \n\nScS PREM block. Top two lines are the same as previous parts.\n      1   0.5                                                                        \n      6.93724966      2\nFirst branch, going backwards. Starting at surface and going to CMB. The middle number is 2 to indicate an S phase.                                                                     \n      6371.0  3480.000000  1 2 -1      \nSecond branch going forward.                                \n      6171.0  3480.000000  1 2  1                                                          \n      ------ ScS for PREM   ----\n\nP to S conversion. The P branch only goes downward, where it converts to a S wave. The S wave has a downgoing and upgoing part. So three lines to describe the entire phase.\n      1   0.5                                                                        \n      9.80711124       3                                                             \n      5731.0 5711.0  1  1  1                                                        \n      5711.0 3482.0  1  2  1                                                        \n      6371.0 3482.0  1  2 -1                                                        \n      ----- P660S-h640km 84 deg-----\n\nDoing three\n      3   0.5                                                                                                            \n      3.5                2                                                             \n      5741.0  3482.000000  1 1  1                                                    \n      6371.0  3482.000000  1 1 -1                                                    \n      3.331            3                                                             \n      6371.0  5741.000000  1 1 -1                                                    \n      6371.0  3482.000000  1 1  1                                                    \n      6371.0  3482.000000  1 1 -1                                                    \n      3.372            3                                                             \n      6371.0  5741.000000  1 2 -1                                                    \n      6371.0  3482.000000  1 1  1                                                    \n      6371.0  3482.000000  1 1 -1                                                    \n      ---PcP, pPcP, sPcP for IASP91 ----\n",
        "createdAt": "2017-02-16T17:13:11.000Z",
        "updatedAt": "2020-10-07T01:06:50.000Z",
        "language": "Fortran",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/samhaug/Doornbos_Raytracing/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "tso1257771/RockNet",
        "url": "https://github.com/tso1257771/RockNet",
        "description": "Rockfall and earthquake detection and association via multitask learning and transfer learning",
        "stars": 14,
        "forks": 0,
        "readme": "[![DOI](https://zenodo.org/badge/565642324.svg)](https://zenodo.org/badge/latestdoi/565642324)\n# RockNet\nRockfall and earthquake detection and association via multitask learning and transfer learning.<br />\nOur preprint article can be found [here](https://essopenarchive.org/doi/full/10.22541/essoar.167160646.63337688/v1).\n\n![2020-03-28T13:41:20 00](https://user-images.githubusercontent.com/30610646/203888301-ba149105-6701-43b7-a2fe-8c7be1852894.png)\n\n## Complete dataset\nPlease also download the complete data hosted on Dryad (https://doi.org/10.5061/dryad.tx95x6b2f),\nfollow the instructions and place the files to specified directories in this repository.\n\n## Summary\n\n* [Installation](#installation)\n* [Make prediction on hourly SAC files](#Make-prediction-on-hourly-SAC-files)\n\n### Installation\nTo run this repository, we suggest Anaconda and pip for environment managements.\n\nClone this repository:\n\n```bash\ngit clone https://github.com/tso1257771/RockNet.git\ncd RockNet\n```\n\nCreate a new environment \n\n```bash\nconda create -n rocknet python==3.7.3 anaconda\nconda activate rocknet\npip install --upgrade pip\npip install -r ./requirements.txt --ignore-installed\n```\n\n### Make prediction on hourly SAC files\nIn this repository, we provide two hourly three-component seismograms as examples for making predictions on continuous data.<br />\nThe data seismograms were collected in the Luhu tribe, Miaoli county, Taiwan.<br />\n\nEnter the directory  ```./Luhu_pred_ex```<br />\n```bash\ncd ./Luhu_pred_ex\n```\n1. Run script ```Luhu_pred_ex/P01_net_STMF.py``` to generate the output functions (also in SAC format) in ```Luhu_pred_ex/net_pred``` from the provided SAC files ```Luhu_pred_ex/sac```<br />\n```\npython P01_net_STMF.py\n```\n2. Plot some prediction results<br />\n```\npython P02_plot.py\n```\n\n\n\n\n\n",
        "createdAt": "2022-11-14T02:07:33.000Z",
        "updatedAt": "2025-10-28T10:59:21.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.7458411",
            "openAlex": "10.5281/zenodo.7458411",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "10.5281/zenodo.7458411",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/tso1257771/RockNet/main/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.7458411",
            "title": "tso1257771/RockNet: RockNet_v1.0.0",
            "journal": "Zenodo",
            "dateReleased": "2022-12-19T00:00:00.000Z",
            "abstract": "",
            "citationsArray": []
        },
        "repoDoi": "10.5281/zenodo.7458411",
        "publications": [
            {
                "doi": "10.5281/zenodo.7458411",
                "name": "RockNet: Rockfall and earthquake detection and association via multitask learning and transfer learning",
                "source": "",
                "authorNames": [],
                "abstract": "",
                "publicationDate": "2022-12-20T00:00:00.000Z"
            }
        ]
    },
    {
        "source": "GitHub",
        "name": "jwellik/vdapseisutils",
        "url": "https://github.com/jwellik/vdapseisutils",
        "description": "Python utilities for VDAP Seismology",
        "stars": 3,
        "forks": 0,
        "readme": "<img src=\"https://github.com/jwellik/vdapseisutils/blob/main/img/vseis-logo.png\" width=1510 alt=\"VDAP\" />\n\n## Overview\nVDAPSEISUTILS is a set of (mostly) Python code that provides easy methods for common tasks in operational volcano seismology.\n\nAt the moment, core tasks include:\n- VolcanoMap: Plot a basic map and cross section of earthquakes around a volcano.\n- ObsPy Catalog & Inventory IO: Import/Export ObsPy Catalog & Inventory formats from Swarm, Earthworm, and NonLinLoc.\n\nSandbox tasks:\n(These routines are available, but I may change them significantly before they are stored in core. I am still working on them.)\n- Velocity: Load, save, and plot velocity models.\n- SwarmMPL: MatPlotLib routines for [Swarm](https://volcanoes.usgs.gov/software/swarm/index.shtml)-like plots (Helicorders, waveform traces, spectrograms, spectra).\n\nPending tasks:\n- CCMatrix: Create, save, load, and plot cross correlation matrices.\n- Waveform statistics: E.g., compute Frequency Index for a list of Stream objects and compare results across events.\n- DataSource: A wrapper for ObsPy Clients with a more universal usage (automatically determines Client type).\n\n## Installation\nThis package is not quite ready yet for installation with pip. Instead, download the repository and add it to your path before running other Python code.\n\nDownload the [zip file](https://github.com/jwellik/vdapseisutils/archive/main.zip) or use `git` to clone the entire repository to a working directory (e.g., mine is `/home/jwellik/PYTHON/vdapseisutils`).\n\nVDAPSEISUTILS runs on Python 3.12. The suite of codes in this repository is comprehensive. Thus, many dependencies are required to run all of them. Common packages include: \n\n[numpy](http://www.numpy.org/) | [scipy](http://www.scipy.org/) | [matplotlib](http://www.matplotlib.org/) | [obspy](http://www.obspy.org/) | [pytables](http://www.pytables.org/) | [pandas](http://pandas.pydata.org/) | [bokeh](http://bokeh.pydata.org/) | [cartopy](http://scitools.org.uk/cartopy/) | [timezonefinder](https://pypi.org/project/timezonefinder/)\n\nOther git repositories are also installed as dependencies:\n- Claudio Satriano's [nllgrid](https://github.com/claudiodsf/nllgrid)\n\nAll of these dependencies can be easily installed via [Anaconda](https://www.continuum.io/) on the command line. I *highly* recommend using a virtual environment so that your environment does not conflict with any other Python packages you may be using. This can be done with the following commands:\n```\n$ conda config --add channels conda-forge\n$ conda create -n seismology312 python=3.12 obspy pandas cartopy pygmt bokeh\n$ conda activate seismology312\n$ conda install -c conda-forge timezonefinder unidecode geopy\n$ conda install -c conda-forge nllgrid\n$ cd <path to vdapseisutils>\n$ pip install .\n```\n\n## Usage\nThis package is still in development. If you have trouble with these codes, let me know.\n```\n$ cd /home/jwellik/PYTHON\n$ mv vdapseisutils-main vdapseisutils\n$ cd /home/jwellik/PYTHON/vdapseisutils/gallery\n$ conda activate seismology312\n$ python\n>>> import sys\n>>> sys.path.append(\"/home/jwellik/PYTHON\")  # Add all codes in the repository to your path\n>>> from vdapseisutils.gallery import Mapping_tutorial.py\n>>> Mapping_tutorial.main()  # Make sure your terminal has graphics forwarding\n```\nThis will run a script that reads .arc files from Wy'East/Mt Hood, Oregon and plots them on a map and cross sections. Look at the [Gallery](https://github.com/jwellik/vdapseisutils/tree/main/gallery) for more examlpes and detailed usage.\n\n",
        "createdAt": "2023-10-09T23:36:42.000Z",
        "updatedAt": "2025-10-07T15:28:15.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/jwellik/vdapseisutils/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "uafgeoteach/GEOS626_seis",
        "url": "https://github.com/uafgeoteach/GEOS626_seis",
        "description": "GEOS 626 Applied Seismology (U. Alaska Fairbanks)",
        "stars": 11,
        "forks": 4,
        "readme": "# GEOS626_seis\n\nA public GitHub repository within the organization\n[uafgeoteach](https://github.com/uafgeoteach). Contains materials for GEOS 626 Applied Seismology, a class at the University of Alaska Fairbanks by [Carl Tape](https://sites.google.com/alaska.edu/carltape/) ([ctape@alaska.edu](mailto:ctape@alaska.edu))\n\nCourse webpage: [GEOS 626](https://sites.google.com/alaska.edu/carltape/home/teaching/aseis)  \n\nThe repository can be obtained from GitHub with this command:\n```\ngit clone --depth=1 https://github.com/uafgeoteach/GEOS626_seis.git\n```\n\nSeveral students and seismologists have contributed toward improving these materials; please see history.txt for details.\n\n### Setup\n---\nA `.yml` file (see setup/ folder) lists dependencies. This file, executed within conda or docker, enables a user to establish the software tools needed to execute the iPython notebooks. (A dockerfile is also provided in setup/)\n\n### How to run using Conda\n---\n\n- install conda (miniconda or anaconda, former recommended) if not done already\n- navigate to the setup folder\n  ```bash\n  cd GEOS626_seis/setup\n  ```\n- setup the conda environment\n  ```bash\n  conda env create -f seismo.yml\n  ```\n- activate the conda environment once the setup is complete\n  ```bash\n  conda activate seismo\n  ```\n- navigate back to the root of repository and launch jupyter\n  ```bash\n  cd ..\n  jupyter notebook\n  ```\n- browse and run notebooks as desired\n\n\n### How to run using Docker\n---\n\nFirst, go into setup directory by using following command:\n\n```bash\n# move into setup directory\ncd setup\n\n# (optional): check Makefile exists \nls | grep Makefile\n```\n\n\nOnce you are in the setup directory, run following command to build and run docker container.\n\n``` bash\nmake\n```\n\nFor more details, see `README.md` located in setup directory.",
        "createdAt": "2020-10-27T22:30:20.000Z",
        "updatedAt": "2025-10-29T23:10:18.000Z",
        "language": "TeX",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/uafgeoteach/GEOS626_seis/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "senli1073/LaNCoR",
        "url": "https://github.com/senli1073/LaNCoR",
        "description": "Label Noise-Contrastive Robust Learning for Seismic Signal Processing",
        "stars": 2,
        "forks": 0,
        "readme": "\n![License](https://img.shields.io/github/license/senli1073/LNRL)\n![LastCommit](https://img.shields.io/github/last-commit/senli1073/LNRL)\n------------------\n\n- [Architecture](#architecture)\n- [Introduction](#introduction)\n- [Usage](#usage)\n  - [Data preparation](#data-preparation)\n  - [Training](#training)\n  - [Testing](#testing)\n- [License](#license)\n\n## Architecture\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/senli1073/LNRL/main/images/Architecture.png\">\n</p>\n\n## Introduction\nLabel Noise-Robust Learning (LNRL) approach was designed for handling label noise in microseismic tasks with small-scale datasets. LNRL aligns feature representation and label representation distribution in multiple feature spaces, learns the correlation between instances and label noise, and mitigates the impact of label noise.\n\nThe code of this project is developed based on [SeisT](https://github.com/senli1073/SeisT). \n\n## Usage\n\n### Data Preparation\n\n- **For training and evaluation**\n  \n  Create a new file named `mydata.py` in the directory `dataset/` to read the metadata and seismograms of the dataset. And the `@register_dataset` decorator needs to be used to register the custom dataset. \n\n  (Please refer to the code example `datasets/sos.py`)\n\n### Training\n\n- **Model**<br/>\n  Before starting training, please make sure that the model code is in the directory `models/` and register it using the `@register_model` decorator. All available models in the project can be inspected by using the following method: \n  ```Python\n  >>> from models import get_model_list\n  >>> get_model_list()\n  ['lnrl','seist']\n  ```\n\n- **Model Configuration**<br/>\n  The configurations of the loss functions, labels, and the corresponding models are in `config.py` which also provides a detailed explanation of all the fields.\n\n\n- **Start training**<br/>\n  To start training with a CPU or a single GPU, please use the following command to start training:\n  ```Shell\n  python main.py \\\n    --seed 0 \\\n    --mode \"train_test\" \\\n    --model-name \"lnrl\" \\\n    --log-base \"./logs\" \\\n    --device \"cuda:0\" \\\n    --data \"/root/data/Datasets/SOS\" \\\n    --dataset-name \"sos\" \\\n    --sigma 600 \\\n    --data-split true \\\n    --train-size 0.8 \\\n    --val-size 0.1 \\\n    --shuffle true \\\n    --workers 8 \\\n    --in-samples 6000 \\\n    --augmentation true \\\n    --epochs 200 \\\n    --patience 30 \\\n    --batch-size 300\n  ```\n  \n  Use `torchrun` if training with multiple GPUs.\n\n  There are also a variety of other custom arguments which are not mentioned above. Use the command `python main.py --help` to see more details.\n\n\n### Testing\n  Use the following command to start testing:\n\n  ```Shell\n  python main.py \\\n    --seed 0 \\\n    --mode \"test\" \\\n    --model-name \"lnrl\" \\\n    --log-base \"./logs\" \\\n    --device \"cuda:0\" \\\n    --data \"/root/data/Datasets/SOS\" \\\n    --dataset-name \"sos\" \\\n    --data-split true \\\n    --train-size 0.8 \\\n    --val-size 0.1 \\\n    --workers 8 \\\n    --in-samples 6000 \\\n    --batch-size 300\n  ```\n\n  It should be noted that the `train_size`, `val_size`, and `seed` in the test phase must be consistent with that training phase. Otherwise, the test results may be distorted.\n\n\n## Acknowledgement\nThis project refers to some excellent open source projects: [PhaseNet](https://github.com/AI4EPS/PhaseNet), [EQTransformer](https://github.com/smousavi05/EQTransformer)\n\n\n## License\nCopyright S.Li et al. 2024. Licensed under an MIT license.\n\n\n\n\n\n\n\n",
        "createdAt": "2024-04-19T14:12:11.000Z",
        "updatedAt": "2025-11-09T04:35:42.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/senli1073/LaNCoR/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "g2e/seizmo",
        "url": "https://github.com/g2e/seizmo",
        "description": "passive(ly maintained) seismology toolbox for Matlab & GNU Octave",
        "stars": 74,
        "forks": 56,
        "readme": "",
        "createdAt": "2011-06-15T19:22:45.000Z",
        "updatedAt": "2025-10-13T01:55:48.000Z",
        "language": "Matlab",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "RongjiangWang/SMBLC_2023",
        "url": "https://github.com/RongjiangWang/SMBLC_2023",
        "description": "Empirical baseline correction for strong-motion records",
        "stars": 1,
        "forks": 0,
        "readme": "For Windows user, the executable file is provided under folder \"WindowsEXE\". Linux user may compile the source codes with \"gfortran\" via a single command like, e.g.,\n\n~>cd .../SourceCode\n\n~>gfortran -o smblc *.f -O3\n\nto get the excutable code smblc.\n\nAfter start the executable code, the program ask for an input file in the ASCII format. An example input file is provided under folder \"InputFile\". You may change the input data included in this file for your own applications.\n\nFor more details, please see the enclosed Update_Method.pdf.\n",
        "createdAt": "2025-04-10T02:56:43.000Z",
        "updatedAt": "2025-06-30T06:52:29.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/RongjiangWang/SMBLC_2023/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Sergey-Anfinogentov/seismo-corona",
        "url": "https://github.com/Sergey-Anfinogentov/seismo-corona",
        "description": "A GUI tool for MHD-seismology of solar active regions using kink oscillation",
        "stars": 2,
        "forks": 0,
        "readme": "# Seismo-Corona\r\nSeismo-Corona is a GUI tool for MHD-seismology of solar active regions using kink oscillation.\r\n\r\n## How to run\r\nLaunch run seismo_corona in IDL command promt within the Solar Soft environment.\r\nAlso, this tool depends on the [Solar Bayesian Analysis toolkit](https://github.com/Sergey-Anfinogentov/SoBAT) library that is used to infer oscillations parameters.\r\n\r\n## Acknowledgements\r\nThe development of this project is supported by the Russian Scientific Foundation under research grant No 18-72-00144.\r\n",
        "createdAt": "2020-05-12T02:57:37.000Z",
        "updatedAt": "2025-04-07T16:43:45.000Z",
        "language": "IDL",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Sergey-Anfinogentov/seismo-corona/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "0Quake/Seismometer",
        "url": "https://github.com/0Quake/Seismometer",
        "description": "スマホで簡単地震観測！スマホ1台あれば、インストール不要で、地震の波形を見たり、震度の目安を確認したりできます。加速度・速度・変位といった、マニアにうれしい詳しい情報も表示します。地震観測配信などにもおすすめ。",
        "stars": 6,
        "forks": 0,
        "readme": "# ZeroQake 簡易地震計 on the Web\n\n![ss2](https://github.com/0Quake/Seismometer/assets/93989835/91d51b63-8a93-447b-9baa-b02c63f7b54e)\n\n### スマホで簡単地震観測\n正確な震度相当値に加え、加速度、速度、変位などの代表値や、迅速に揺れが反映される震度概算値、加速度波形を表示できます。\n\n加速度センサーを搭載したスマートフォンがあれば、インストール等不要で、すぐに観測を始められます。地震観測配信などにおすすめ！\n\n[➤ご利用はこちらから](https://0quake.github.io/Seismometer/)\n\n<details>\n                    <summary style=\"color:#CCC;font-size:14px\">参考文献など </summary>\n                    <ul>\n                        <li>藤本一雄・翠川三郎 (2005) : 近年の強震記録に基づく地震動強さ指標による計測震度推定法</li>\n                        <li>【ネット記事】<a href=\"https://qiita.com/soshi1822/items/6ae0c2d14a72478c79a0\">加速度から計測震度を計算してみる</li>\n                        <li>【ネット記事】<a href=\"https://qiita.com/bellbind/items/ba7aa07f6c915d400000#7-javascript%E3%81%AB%E3%82%88%E3%82%8B%E3%83%AB%E3%83%BC%E3%83%97%E7%89%88fft%E5%AE%9F%E8%A3%85%E3%82%B3%E3%83%BC%E3%83%89\">高速フーリエ変換FFTを理解する</a></li>\n                    </ul>\n</details>\n",
        "createdAt": "2024-04-04T18:15:11.000Z",
        "updatedAt": "2025-11-22T04:46:25.000Z",
        "language": "HTML",
        "homepage": "https://0quake.github.io/Seismometer/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/0Quake/Seismometer/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "WayneCrawford/pspicker",
        "url": "https://github.com/WayneCrawford/pspicker",
        "description": "Seismological P and S wave picker using Baillard et al. Kurtosis implemenation",
        "stars": 12,
        "forks": 2,
        "readme": "ps_picker\n===========\n\nSeismological P- and S- wave picker using the modified Kurtosis method\n\nPython port of the picker described in Baillard et al., 2014\n\ndebugging information is saved to the local file ``run_{datetime}.log``\n\nMethodology\n------------\n\nThe picker is based around the Kurtosis, but also uses energy levels, polarity,\nclustering and phase association in a 3-step process:\n\n### Step 1: define a global pick window\n\n\nThe *Kurtosis* is calculated for all stations.  The global window\nsurrounds the most densely clustered region of triggers.\n\n### Step 2: pick P and S arrivals on each station individually\n\nFor each station:\n    - calculate the *Kurtosis* over coarse to fine scales.\n    - Identify candidates on the coarse scale and refine their times using\n      the finier scales\n    - Choose P- and S- candidates based on the *signal-to-noise level* of\n      each pick\n    - Verify the candidates using the waveform *polarity*, if possible\n       - polarity is only used if one of the picks has a dip of > 30 degrees\n\n### Step 3: associate picks\n\n- Calculate origin times for each trace, based on the P-S delay and\n  a simple velocity model (could I use a single Vp/Vs value?)\n- If at least 3 origin times are clustered, use their average origin time\n  to validate all candidates, possibly dipping into the pool of unused\n  candidates for replacemene P and S picks\n- If less than 3 origin times are clustered, reject bad P- and S- picks\n  based on clustering of P-pick times, S-pick times and P-S delays\n\n\n\nDatabase and waveform files\n---------------------------\n\nAre assumed to be in SEISAN structure:\n  - Database files: NORDIC format, in ``database_path_in``/``YEAR``/``MONTH``/\n    (except run_one, for which the file may be local)\n  - Waveform files: one miniseed file per event.  Filename is read from the\n    database file and assumed to start with ``YEAR``-``MONTH``.  File is read\n    from ``waveform_path_in``/``YEAR``/``MONTH``/\n  \n \nExample workflow\n----------------\n\n### Start by autopicking a few events, with all bells and whistles on:\n\nTo pick one event from a database in ``/SEISAN/MAYOBS``:\n\n```python\nfrom pspicker import PSPicker\npicker = PSPicker('parameters_C.yaml', '/SEISAN/MAYOBS/WAV/MAYOB',  '/SEISAN/MAYOBS/REA/MAYOB')\npicker.run_one('19-0607-59L.S201905', plot_global=True, plot_stations=True, log_level='verbose')\n```\n\nLook at all of the plots and verify that the picks and association are as\nyou expect.  If not, change the paramters and run again.\n\n### Next, pick several events with only the global plots on\n\nThe bells and whistles text will be saved to a log file named\nrun_{DATETIME}.log\n\nTo pick events from May 5th to 25th in the same database:\n\n```python\nfrom pspicker import PSPicker\npicker = PSPicker('parameters_C.yaml', '/SEISAN/MAYOBS/WAV/MAYOB',  '/SEISAN/MAYOBS/REA/MAYOB')\npicker.run_many('20190505', '20190525', plot_global=True)\n```\n\n### Finally, run the whole database without plots\n\n*(run_{DATETIME}.log is always created)*\n\nTo pick events from May 26th 2019 May 1st 2020:\n\n```python\nfrom pspicker import PSPicker\npicker = PSPicker('parameters_C.yaml', '/SEISAN/MAYOBS/WAV/MAYOB', '/SEISAN/MAYOBS/REA/MAYOB')\npicker.run_many('20190526', '20200501')\n```\n\nThe three main methods:\n-----------------------\n\n```python\ndef __init__(self, parm_file, wav_base_path, database_path_in,\n             database_path_out='Sfile_directory', database_format='NORDIC'):\n    \"\"\"\n    :param parm_file: path/name of the parameter file\n    :param wav_base_path: absolute basepath to the waveform files (just before\n                          the YEAR/MONTH subdirectories)\n    :param database_path_in: absolute basepath to the database/catalog file(s)\n                             (just before the YEAR/MONTH subdirectories)\n    :param database_path_out: path to output database files\n    :param database_format: 'NORDIC' is the only choice for now\n        'NORDIC': Use SEISAN conventions for waveform  and database files\n                  (naming, and location in YEAR/MONTH subdirectories)\n    \"\"\"\n```\n```python\ndef run_one(self, database_filename, plot_global=True, plot_stations=False,\n            assoc=None, log_level=\"verbose\", plot_debug=None):\n    \"\"\"\n    Picks P and S arrivals on one waveform, using the Kurtosis\n\n    Information in the database file will be appended with the picks.\n    :param database_filename: database file to read\n    :param plot_global: show global and overall pick plots\n    :param plot_stations: show individual station plots\n    :param assoc: Associator object (used by run_many())\n    :param log_level: console log level (choices = 'debug', 'verbose',\n        'info', 'warning', 'error', 'critical'), default='info'\n    :param plot_debug: show some debugging plots\n    \"\"\"\n```\n```python\ndef run_many(self, start_date, end_date, plot_global=False,\n    plot_stations=False, ignore_fails=False, log_level='info'):\n    \"\"\"\n    Loops over events in a date range\n\n    :param start_date: \"YYYYMMDD\" or \"YYYYMMDDHHMM\" of first data to process\n    :param end_date: \"YYYYMMDD\" of last data to process\n    :param plot_global: show global and overall pick plots\n    :param plot_stations: show individual station plots\n    :param ignore_fails: keep going if one run fails\n    :param log_level: console log level (choices = 'debug', 'verbose',\n                      'info', 'warning', 'error', 'critical'), default='info'        \n    \"\"\"\n```\n\nParameter and response files \n-----------------------------\n\n[Are documented here](file_examples.md)\n\nTo get the same results as with the old Matlab program, set the following\nvalues:\n\n- set ``association:method`` to **\"arrival_time\"**\n- set ``station_parameters:{type}:max_candidates`` to **2**\n- set ``SNR:threshold_parameter`` to **0.2**\n- set ``SNR:max_threshold_crossings`` to **5**\n- set ``global_window:max_candidates`` to **2**\n\nEvent amplitudes \n-----------------\n\nEvent amplitudes calculations need accurate instrument responses.  The\ninstrument response filename(s) are input in the parameter file.  If you have\nas stationxml file, you can make a pspicker_compatible json_pz file like this:\n\n```python\npaz = PAZ.read_stationxml(filename, channel=xxx[, station=xxxx])\npaz.write_json_pz (ps_filename)\n```\n\nIf you have a response in another format that you can read in using obspy,\nyou can output it to a pspicker-compatible json_pz file like this:\n\n```python\npaz = PAZ.from_obspy_response(resp)\npaz.write_json_pz(pz_filename)\n```\n\nIn both cases, you can look at the response using `paz.plot(min_freq=xxx)`, or\nyou could compare it to the obspy_response using:\n\n```python\nfig = resp.plot(min_freq=xxx, label='obspy', show=False)\npaz = PAZ.from_obspy_response(resp)\npaz.plot(min_freq=xxx, axes=fig.axes, label='PAZ', sym='g.')\n```\n\nTo Do\n-------\n\n- Add event location-based acceptance of solitary P- and S- candidates\n- In P-, S- and P-S clustering stage, allow unused candidates to be\n  substituted for rejected picks\n- Dedicated [To Do file](ToDo.md)\n    \nAlso see the [profiling file](profiling.md)\n",
        "createdAt": "2020-10-01T09:17:40.000Z",
        "updatedAt": "2025-04-28T08:00:23.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/WayneCrawford/pspicker/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "swfrench/QuakeHS",
        "url": "https://github.com/swfrench/QuakeHS",
        "description": "Haskell tools for seismology",
        "stars": 0,
        "forks": 0,
        "readme": "QuakeHS\n=======\n\nMiscellaneous Haskell tools for seismology\n------------------------------------------\n\nLong-term goals:\n* Better FDSN StationXML support\n* Interaction with the IRIS / FDSN Web Service\n* Waveform data file (SAC,mseed) manipulation, likely via FFI\n* Various tools for common calculations in the sphere\n* Local mirroring / query of Global CMT project source info\n",
        "createdAt": "2013-05-19T19:15:36.000Z",
        "updatedAt": "2014-05-05T06:33:51.000Z",
        "language": "Haskell",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/swfrench/QuakeHS/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "VittorioMinio93/MISARA",
        "url": "https://github.com/VittorioMinio93/MISARA",
        "description": "MISARA (Matlab Interface for the Seismo-Acoustic aRary Analysis), is an open-source Matlab GUI that supports visualisation, detection and localization of volcano seismic and acoustic signals, with a focus on array techniques.",
        "stars": 5,
        "forks": 3,
        "readme": "# MISARA:Matlab Interface for Seismo-Acoustic aRray Analysis\nMISARA (Matlab Interface for Seismo-Acoustic aRray Analysis) is a open-source Matlab-based GUI designed to perform analyses of seismic and acoustic waveform data. A suite of well-established algorithms for volcano seismic and acoustic signal processing have been integrated into our GUI interface, with a special focus on array techniques (for more details, see Rost and Thomas, 2000). We note that although MISARA was developed to facilitate the analysis of seismic and acoustic signals in volcanic environments, it can be used for other research purposes. Furthermore, owing to its modular structure, it is possible to easily integrate additional functionalities.\nThe different data analysis modules of MISARA are independent of each other. The modules were designed to easily manage every step of the data processing and to quickly inspect the results. Most of the processes are automated, reducing user’s errors and efforts. One advantage consists of the possibility to reset some parameters directly from the module itself, allowing to repeat the analysis many times. Other fundamental aspects of this modular structure are the possibility to deal with different formats of input traces, the systematic saving of the results and the optional activation of many subroutines.\nThe main structure of the interface consists of:\n\n-\tHome window, the main panel for the management of all utilities of MISARA.\n-\tData preparation window, for the formatting of the Input data.\n-\tData Pre-processing modules, for the data quality control.\n-\tSignal Features modules, for those analytic routines that support the array methods, such as spectral, amplitude, polarization and detection analysis.\n-\tArray analysis modules, for the source localization methods based on the multichannel techniques.\n\n# Requirements\nMISARA can be run on any operation system with Matlab from Release 2021b. In addition, you need to have the following toolboxes installed: \n\n-\tControl System Toolbox, version 10.5.\n-\tFinancial Toolbox, version 5.12.\n-\tMapping Toolbox, version 4.7.\n-\tSignal Processing Toolbox, version 8.1.\n-\tStatistics and Machine Learning Toolbox, version 11.4.\n-\tWavelet Toolbox, version 5.1.\n\n# Documentations\nYou can consult the manual of the software from the \"Help\" menu of the control panel of MISARA or you can open it from \"MISARA/Doc/\" directory.\n\n# Starting MISARA\nAfter you have downloaded the software, you should unzip the source code to a suitable directory. To start MISARA, you should run “MISARA.m” by pressing F5 in the Matlab editor. The software automatically sets the main paths and functions, but it requires the installation of the “GIPPtools” and “irisFetch” libraries. For any information, see the section 2.1 of the User Manual. After the running, the Home Window appears on the screen, allowing you to set the analysis parameters and to use all modules. To test the software, you can refer to “Video Tutorial” from the “Help” menu located in the upper-right part of the Home Window. To use these video tutorials, we suggest you to download the software from this URL: https://doi.org/10.5281/zenodo.7410076. \n\nThe purpose of these video tutorials is to train users to perform different types of analyses of seismic and acoustic waveform data acquired in volcanic environment. In particular they are grouped into three sections. They show you a series of brief tutorials on how to use the software on three real cases studies. In First one, we will perform the analysis of volcanic tremor recorded by a seismic array deployed at Mt. Etna (Italy) in 2011, when the volcano produced intense lava fountain activity from its New South East Crater (NSEC; for more details about volcanic activity, see Bencke et al, 2014). In the second one, we will demonstrate analyses of Long Period (LP) and Very Long Period (VLP) earthquakes recorded by Mt. Etna permanent seismic network in 2010, accompanying explosive activity at the Bocca Nuova crater (BN; for more details about volcanic activity, see Andronico et al, 2010). In the third one, we will show how to analyse the infrasound data acquired by an infrasound array deployed at Mt. Etna in 2019, when the NSEC crater was affected by intense Strombolian activity (for more details about volcanic activity, see De Angelis et al, 2020). The raw data that we will use in these tutorials are in the “MISARA/Data_org/” directory. However, it is possible to use the Matlab data format located in “MISARA/Data_example/” directory. To use this demostration dataset, we suggest you to download the software from this URL: https://doi.org/10.5281/zenodo.7410076.  \n\n# Citation \nThis is a modified version of the GSpecDisp package (Sadeghisorkhani et al., 2017).\nIf you use this code for your work, please cite the following DOI:\n-\thttps://doi.org/10.1785/0220220267\n\n# Contact\nYou can send an email to vittorio.minio@phd.unict.it to report suggestions, comments and bugs.\n\n# References\n-\tAndronico, D., Lo Castro, M. D., Sciotto, M., Spina, L., 2013. The 2010 ash emissions at the summit craters of Mt Etna: Relationship with seismo‐acoustic signals. Journal of Geophysical Research: Solid Earth, 118(1), 51-70.\n-\tBehncke, B., Branca, S., Corsaro, R. A., De Beni, E., Miraglia, L., Proietti, C., 2014. The 2011–2012 summit activity of Mount Etna: Birth, growth and products of the new SE crater. Journal of Volcanology and Geothermal Research, 270, 10-21.\n-\tDe Angelis, S., Haney, M. M., Lyons, J. J., Wech, A., Fee, D., Diaz-Moreno, A., Zuccarello, L., 2020. Uncertainty in detection of volcanic activity using infrasound arrays: examples from Mt. Etna, Italy, Frontiers in Earth Science, 8, 169.\n-\tRost, S., and Thomas, C., 2002. Array seismology: Methods and applications. Reviews of geophysics, 40(3).\n-\tSadeghisorkhani, H., Gudmundsson, O., Tryggvason, A., 2017. GSpecDisp: a Matlab GUI package for phase-velocity dispersion measurements from ambient-noise correlations, Computers and Geosciences.\n",
        "createdAt": "2022-12-07T16:06:12.000Z",
        "updatedAt": "2024-11-12T10:21:29.000Z",
        "language": "MATLAB",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.7410076",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.7410076",
            "dataCite": "10.5281/zenodo.7410076",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/VittorioMinio93/MISARA/main/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.7410076",
            "title": "MISARA: Matlab Interface for Seismo-Acoustic aRray Analysis",
            "journal": "Zenodo",
            "dateReleased": "2022-12-01T00:00:00.000Z",
            "abstract": "<strong>MISARA:Matlab Interface for Seismo-Acoustic aRray Analysis</strong><br> MISARA (Matlab Interface for Seismo-Acoustic aRray Analysis) is a open-source Matlab-based GUI designed to perform analyses of seismic and acoustic waveform data. A suite of well-established algorithms for volcano seismic and acoustic signal processing have been integrated into our GUI interface, with a special focus on array techniques (for more details, see Rost and Thomas, 2000). We note that although MISARA was developed to facilitate the analysis of seismic and acoustic signals in volcanic environments, it can be used for other research purposes. Furthermore, owing to its modular structure, it is possible to easily integrate additional functionalities. The different data analysis modules of MISARA are independent of each other. The modules were designed to easily manage every step of the data processing and to quickly inspect the results. Most of the processes are automated, reducing user’s errors and efforts. One advantage consists of the possibility to reset some parameters directly from the module itself, allowing to repeat the analysis many times. Other fundamental aspects of this modular structure are the possibility to deal with different formats of input traces, the systematic saving of the results and the optional activation of many subroutines. The main structure of the interface consists of: Home window, the main panel for the management of all utilities of MISARA. Data preparation window, for the formatting of the Input data. Data Pre-processing modules, for the data quality control. Signal Features modules, for those analytic routines that support the array methods, such as spectral, amplitude, polarization and detection analysis. Array analysis modules, for the source localization methods based on the multichannel techniques. <br> <strong>Requirements</strong><br> MISARA can be run on any operation system with Matlab from Release 2021b. In addition, you need to have the following toolboxes installed: Control System Toolbox, version 10.5. Financial Toolbox, version 5.12. Mapping Toolbox, version 4.7. Signal Processing Toolbox, version 8.1. Statistics and Machine Learning Toolbox, version 11.4. Wavelet Toolbox, version 5.1. <strong>Documentations</strong><br> You can consult the manual of the software from the \"Help\" menu of the control panel of MISARA or you can open it from \"MISARA/Doc/\" directory. <strong>Starting MISARA</strong><br> After you have downloaded the software, you should unzip the source code to a suitable directory. To start MISARA, you should run “MISARA.m” by pressing F5 in the Matlab editor. The software automatically sets the main paths and functions, but it requires the installation of the “GIPPtools” and “irisFetch” libraries. For any information, see the section 2.1 of the User Manual. After the running, the Home Window appears on the screen, allowing you to set the analysis parameters and to use all modules. To test the software, you can refer to “Video Tutorial” from the “Help” menu located in the upper-right part of the Home Window. The purpose of these video tutorials is to train users to perform different types of analyses of seismic and acoustic waveform data acquired in volcanic environment. In particular they are grouped into three sections. They show you a series of brief tutorials on how to use the software on three real cases studies. In First one, we will perform the analysis of volcanic tremor recorded by a seismic array deployed at Mt. Etna (Italy) in 2011, when the volcano produced intense lava fountain activity from its New South East Crater (NSEC; for more details about volcanic activity, see Bencke et al, 2014). In the second one, we will demonstrate analyses of Long Period (LP) and Very Long Period (VLP) earthquakes recorded by Mt. Etna permanent seismic network in 2010, accompanying explosive activity at the Bocca Nuova crater (BN; for more details about volcanic activity, see Andronico et al, 2010). In the third one, we will show how to analyse the infrasound data acquired by an infrasound array deployed at Mt. Etna in 2019, when the NSEC crater was affected by intense Strombolian activity (for more details about volcanic activity, see De Angelis et al, 2020). The raw data that we will use in these tutorials are in the “MISARA/Data_org/” directory. However, it is possible to use the Matlab data format located in “MISARA/Data_example/” directory. <strong>Citation</strong><br> This is a modified version of the GSpecDisp package (Sadeghisorkhani et al., 2017). If you use this code for your work, please cite the following DOI: https://doi.org/10.1785/0220220267 <strong>Contact</strong><br> You can send an email to vittorio.minio@phd.unict.it to report suggestions, comments and bugs. <strong>References</strong> Andronico, D., Lo Castro, M. D., Sciotto, M., Spina, L., 2013. The 2010 ash emissions at the summit craters of Mt Etna: Relationship with seismo‐acoustic signals. Journal of Geophysical Research: Solid Earth, 118(1), 51-70. Behncke, B., Branca, S., Corsaro, R. A., De Beni, E., Miraglia, L., Proietti, C., 2014. The 2011–2012 summit activity of Mount Etna: Birth, growth and products of the new SE crater. Journal of Volcanology and Geothermal Research, 270, 10-21. De Angelis, S., Haney, M. M., Lyons, J. J., Wech, A., Fee, D., Diaz-Moreno, A., Zuccarello, L., 2020. Uncertainty in detection of volcanic activity using infrasound arrays: examples from Mt. Etna, Italy, Frontiers in Earth Science, 8, 169. Rost, S., and Thomas, C., 2002. Array seismology: Methods and applications. Reviews of geophysics, 40(3). Sadeghisorkhani, H., Gudmundsson, O., Tryggvason, A., 2017. GSpecDisp: a Matlab GUI package for phase-velocity dispersion measurements from ambient-noise correlations, Computers and Geosciences.",
            "citationsArray": []
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ROBelgium/MSNoise",
        "url": "https://github.com/ROBelgium/MSNoise",
        "description": "A Python Package for Monitoring Seismic Velocity Changes using Ambient Seismic Noise | http://www.msnoise.org",
        "stars": 203,
        "forks": 87,
        "readme": "MSNoise\n=======\nA Python Package for Monitoring Seismic Velocity Changes using Ambient Seismic Noise.\n\nCI Builds: [![Github Action Status](https://github.com/ROBelgium/MSNoise/actions/workflows/test_full.yml/badge.svg)](https://github.com/ROBelgium/MSNoise/actions)\n[![codecov](https://codecov.io/gh/ROBelgium/MSNoise/branch/master/graph/badge.svg)](https://codecov.io/gh/ROBelgium/MSNoise)\n\nPyPI: [![PyPI version](https://badge.fury.io/py/msnoise.svg)](https://pypi.org/project/msnoise/) [![PyPI downloads](https://img.shields.io/pypi/dm/msnoise.svg)](https://pypi.org/project/msnoise/)\n\nConda: [![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/msnoise.svg)](https://anaconda.org/conda-forge/msnoise) [![Conda Version](https://anaconda.org/conda-forge/msnoise/badges/version.svg)](https://anaconda.org/conda-forge/msnoise) [![Conda Platforms](https://img.shields.io/conda/pn/conda-forge/msnoise.svg)](https://anaconda.org/conda-forge/msnoise)\n\nMSNoise is the first complete software package for computing and monitoring relative velocity variations using ambient seismic noise. \nMSNoise is a fully-integrated solution that automatically scans data archives and determines which jobs need to be done whenever the scheduled task is executed. \n\nMSNoise is developed by Thomas Lecocq (Royal Observatory of Belgium, ROB). Corentin Caudron used MSNoise during his PhD at ROB and still continuously provides invaluable debug information.\nThe group of active users (providing questions, feedback, snippets of code) is growing and the full list of Contributors is available here: http://msnoise.org/doc/contributors.html. \n\n\nHistory\n-------\n\n* 2010: MSNoise is based on Matlab, c++, csh and fortran codes developped at ISTerre/Univ. Grenoble and IPGP in the framework of the [ERC Whisper project](https://whisper.obs.ujf-grenoble.fr/).\n* 2011/12: MSNoise is tested on Undervolc data, and used by Corentin for his PhD thesis.\n* 2013: First release of MSNoise for the IAVCEI 2013 in Kagoshima ([Release Notes](http://msnoise.org/doc/releasenotes/msnoise-1.0.html)).\n* 2014: Improvements and bugfixes, release 1.2.5. Publication of the [SRL article](http://srl.geoscienceworld.org/content/85/3/715.full) ([Release Notes](http://msnoise.org/doc/releasenotes/msnoise-1.2.5.html)).\n* 2015: MSNoise 1.3: MSNoise is real python package, with a documented API and new plots ([Release Notes](http://msnoise.org/doc/releasenotes/msnoise-1.3.html)).\n* 2016: MSNoise 1.4: new web admin interface, plugin support, instrument response removal and phase weighted stacking ([Release Notes](http://msnoise.org/doc/releasenotes/msnoise-1.4.html)).\n* 2017: MSNoise 1.5: Autocorrelation / Single Station correlation support, rewritten preprocessing, new_jobs and scan_archive for more performance, better instrument response preloading ([Release Notes](http://msnoise.org/doc/releasenotes/msnoise-1.5.html)).\n* 2019: MSNoise 1.6: Optimisation of the workflow (one job type per step), HPC support, faster *compute_cc* step, PSD-whitening, DB optimisations ([Release Notes](http://msnoise.org/doc/releasenotes/msnoise-1.6.html))\n* 2019-2024: MSNoise dev: lots of work, lots of tests, lots of improvements, but no time to release a version ... \n* 2025: MSNoise 2.0 is really coming: Massive optimisation of all steps (xarray-based), sub-daily dv/v computation, PSD QC calculation, ... Already available if you install the \"dev\" version (could be unstable!).\n\n\nDocumentation\n-------------\nThe full documentation can be found on: http://www.msnoise.org/doc.\nThe documentation of the _master_ branch (active development): http://www.msnoise.org/doc/master\n\n\nInstallation\n------------\n\nPlease follow the instructions in the documentation: http://msnoise.org/doc/installation.html\n\nRemember, always consider the current GitHub *master* as not stable!\n\n\nGetting Help\n------------\n\n✨ Please make use of the https://github.com/ROBelgium/MSNoise/discussions the GitHub Discussions for questions !✨\n\n--\n\nYou can also search the mailing list's archive on http://mailman-as.oma.be/pipermail/msnoise/ or https://www.mail-archive.com/msnoise@mailman-as.oma.be/.\n\n\nCiting MSNoise\n--------------\n\nIf you use MSNoise, even a small part of it, for your research and publications, please consider citing it:\n\n**Lecocq, T., C. Caudron, et F. Brenguier (2014)**, MSNoise, a Python Package\nfor Monitoring Seismic Velocity Changes Using Ambient Seismic Noise,\n*Seismological Research Letters*, 85(3), 715‑726, doi:10.1785/0220130073.\n\nThanks to all [who already did so](https://scholar.google.com/scholar?oi=bibs&hl=en&cites=7742894338804325257)! \n\nDisclaimer\n----------\n\nAlthough we have cross-checked the whole code, we cannot warranty it is exempt of bugs. The package is provided as-is, we will not be held responsible for any use you make of it, nor for the results and conclusions you may find using MSNoise.\n\n\n\nLicence\n-------\n\nMSNoise is released under EUPL v1.1\n",
        "createdAt": "2013-08-25T13:52:34.000Z",
        "updatedAt": "2025-11-27T10:38:59.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ROBelgium/MSNoise/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "u11210013/Seismology",
        "url": "https://github.com/u11210013/Seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2025-03-19T07:14:45.000Z",
        "updatedAt": "2025-09-24T07:45:49.000Z",
        "language": "HTML",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "jacksonfellows/research_summer_23",
        "url": "https://github.com/jacksonfellows/research_summer_23",
        "description": "My summer research for the URISE/IRIS seismology internship.",
        "stars": 0,
        "forks": 0,
        "readme": "# Investigating Shallow Forearc Structure Within the Southern 1964 MW 9.2 Great Alaska Earthquake Asperity With a Dense Nodal Seismic Array and Marine Airgun Shots #\n\nThis repository contains code, data, and figures for my [URISE](https://www.iris.edu/hq/internship/) summer internship project.\nMy mentor was Professor Lindsay Worthington at University of New Mexico.\nI am planning on presenting my results at the Fall 2023 AGU session in San Francisco.\n\n## Abstract ##\n\nWe investigate structure of the shallow subduction forearc along a 250 km transect perpendicular to the Aleutian trench offshore of Kodiak Island in southern Alaska. The survey location lies within the southern asperity of the 1964 MW 9.2 Great Alaska Earthquake. The 1964 rupture zone contains along-margin variations in earthquake history, plate structure, sediment input, and plate coupling and is a good location to examine the relationship between structure and seismicity. Specifically, this study aims to characterize the upper plate velocity structure in the accretionary prism and how it compares to structure along-margin. The material properties of the forearc are relevant for understanding earthquake rupture propagation and potential tsunami behavior.\n\nThis analysis uses data collected in 2019 as part of the Alaska Amphibious Community Seismic Experiment. The receivers were a dense array of 398 nodal 5 Hz geophones distributed with ~200–300 m spacing along the road network in northeastern Kodiak Island. The source was a 6600 in3 marine airgun array towed by the R/V Marcus Langseth and shot at ~400 m spacing along the study transect offshore.\n\nTo obtain first arrivals we applied bandpass and STA/LTA filters. To further improve the signal-to-noise ratio we binned traces by source-receiver offset and stacked adjacent shots within 5 km bins by time-shifting within a velocity range to maximize cross-correlation.\n\nAt source-receiver offsets <100 km we observe a first arrival with an apparent velocity of ~6 km/s. Between ~100–180 km, first arrival energy is suppressed and sporadic, likely due to a low velocity zone at depth or a highly attenuating region in the upper plate. At offsets >180 km we observe a first arrival with an apparent velocity of ~8 km/s. At <100 km offsets we also observe a strong reflective phase. The travel times from these seismic phases can be used in ray tracing and inversion to determine velocity structure in the shallow subduction forearc.\n\n## Results ##\n\n<img src=\"figures/tt_curves.png\" width=\"800\">\n<img src=\"figures/model.png\" width=\"800\">\n\n## Python environment ##\n\nTo build the lock file:\n```\nconda-lock -f environment.yml\n```\nTo create a Conda environment from the lock file:\n```\nconda-lock install --name YOUR_ENVIRONMENT_NAME_HERE conda-lock.yml\n```\n\n## Stuff to download ##\n\nCan download USGS quaternary faults here: <https://earthquake.usgs.gov/static/lfs/nshm/qfaults/Qfaults_GIS.zip>.\nCan find volcanoes for a search range here: <https://www.ngdc.noaa.gov/hazel/view/hazards/volcano/loc-search>.\nCan download tectonic plate geometries here: <https://github.com/fraxen/tectonicplates/archive/master.zip>.\n\n## Testing ##\n\nTo run all tests run:\n```\npytest\n```\n\n## Notes ##\n\n1964 rupture traced from Suleimani, E., Nicolsky, D. J., Haeussler, P. J., & Hansen, R. (2011). Combined Effects of Tectonic and Landslide-Generated Tsunami Runup at Seward, Alaska During the M W 9.2 1964 Earthquake. Pure and Applied Geophysics, 168(6–7), 1053–1074. https://doi.org/10.1007/s00024-010-0228-4\n\nAll other ruptures traced (and all rupture magnitudes taken) from Liu, C., Lay, T., & Xiong, X. (2022). The 29 July 2021 MW 8.2 Chignik, Alaska Peninsula Earthquake Rupture Inferred From Seismic and Geodetic Observations: Re-Rupture of the Western 2/3 of the 1938 Rupture Zone. Geophysical Research Letters, 49(4), e2021GL096004. https://doi.org/10.1029/2021GL096004\n\nRetrieved ALEUT lines from <https://www.marine-geo.org/tools/search/Events.php?event_set_uid=1329>.\n\nEDGE lines traced from Moore, J. C., Diebold, J., Fisher, M. A., Sample, J., Brocher, T., Talwani, M., et al. (1991). EDGE deep seismic reflection transect of the eastern Aleutian arc-trench layered lower crust reveals underplating and continental growth. Geology, 19(5), 420–424. <https://doi.org/10.1130/0091-7613(1991)019%3c0420:EDSRTO%3e2.3.CO%3b2>\n\nSlab depth contours downloaded from <https://www.sciencebase.gov/catalog/item/5aa2c535e4b0b1c392ea3ca2>.\n",
        "createdAt": "2023-05-24T14:13:00.000Z",
        "updatedAt": "2024-03-11T19:07:43.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/jacksonfellows/research_summer_23/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "seismo-learn/links",
        "url": "https://github.com/seismo-learn/links",
        "description": "Links for Geophysics & Seismology research",
        "stars": 41,
        "forks": 19,
        "readme": "# Seismo Links\n\n[![Deploy](https://github.com/seismo-learn/links/actions/workflows/deploy.yml/badge.svg)](https://github.com/seismo-learn/links/actions/workflows/deploy.yml)\n[![License: CC BY-NC 4.0](https://img.shields.io/badge/License-CC%20BY--NC%204.0-blue.svg)](https://creativecommons.org/licenses/by-nc/4.0/deed.en)\n\nThis repository contains the source codes for the [Seismo Links](https://seismo-learn.org/links/)\nsite, which collects useful links for Geophysics and Seismology research.\n\n## Build the site locally\n\nThe site is themed by the [Bootstrap](https://getbootstrap.com/) framework and\nbeautiful icons from [Font Awesome](https://fontawesome.com/), and powered by\nthe fastest static site generator [Hugo](https://gohugo.io/).\n\nTo build it locally, you need to follow the steps below:\n\n1.  [Install hugo](https://gohugo.io/getting-started/installing/)\n\n2.  Clone the repository\n\n    ```\n    # Clone the repository and then switch to the directory\n    $ git clone https://github.com/seismo-learn/links.git\n    $ cd links\n    ```\n\n3.  Build the website\n\n\tRun the following command, then you can view the site by visiting\n    http://localhost:1313/links/ in your local web browser.\n\n    ```\n    $ hugo server\n    ```\n\n## Contributing\n\nEveryone is welcome to contribute to this site. Contributions include but not limited to:\n\n- Add more useful links\n- Report/Fix broken links\n- Improve the web design\n- Any brilliant ideas\n\nFor any contributions, please open an [issue](https://github.com/seismo-learn/links/issues)\nor submit a [Pull Request](https://github.com/seismo-learn/links/pulls).\nYou could also refer to the [contributing guides](https://seismo-learn.org/contributing/) (in Chinese)\nif you are unfamilar with git, GitHub, Markdown and so on.\n\n\n## License\n\nThis material uses [Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)](https://creativecommons.org/licenses/by-nc/4.0/deed.en).\nYou are free to share and adapt the material as long as you follow the following\nlicense terms:\n\n- Attribution: You must give appropriate credit, provide a link to the license, and indicate if changes were made.\n- NonCommercial: You may not use the material for commercial purposes.\n",
        "createdAt": "2018-09-14T13:51:03.000Z",
        "updatedAt": "2025-11-25T05:12:14.000Z",
        "language": "HTML",
        "homepage": "https://seismo-learn.org/links/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/seismo-learn/links/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "avillasenorh/Seismology_Notes",
        "url": "https://github.com/avillasenorh/Seismology_Notes",
        "description": "Notes on seismic data processing",
        "stars": 0,
        "forks": 0,
        "readme": "# Seismology_Notes\nNotes on seismic data processing\n",
        "createdAt": "2022-09-07T03:49:37.000Z",
        "updatedAt": "2022-09-07T04:15:36.000Z",
        "language": null,
        "homepage": "https://avillasenorh.github.io/Seismology_Notes/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/avillasenorh/Seismology_Notes/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "INGV/ttime",
        "url": "https://github.com/INGV/ttime",
        "description": "Travel Time Web Service",
        "stars": 0,
        "forks": 0,
        "readme": "<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*\n\n- [ttime - Travel Time](#ttime---travel-time)\n  - [Introduction](#introduction)\n  - [Quickstart](#quickstart)\n    - [Get Docker image](#get-docker-image)\n      - [1) Get built image from DockerHub (*preferred*)](#1-get-built-image-from-dockerhub-preferred)\n      - [2) Build by yourself](#2-build-by-yourself)\n    - [Run as a service](#run-as-a-service)\n  - [Test ttime as a stand alone script](#test-ttime-as-a-stand-alone-script)\n  - [Contribute](#contribute)\n  - [Authors](#authors)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n[![License](https://img.shields.io/github/license/INGV/ttime.svg)](https://github.com/INGV/ttime/blob/main/LICENSE) [![GitHub issues](https://img.shields.io/github/issues/INGV/ttime.svg)](https://github.com/INGV/ttime/issues)\n\n[![Docker build](https://img.shields.io/badge/docker%20build-from%20CI-yellow)](https://hub.docker.com/r/ingv/ttime)![Docker Image Size (latest semver)](https://img.shields.io/docker/image-size/ingv/ttime?sort=semver)![Docker Pulls](https://img.shields.io/docker/pulls/ingv/ttime)\n\n[![CI](https://github.com/INGV/ttime/actions/workflows/docker-image.yml/badge.svg)](https://github.com/INGV/ttime/actions)[![GitHub](https://img.shields.io/static/v1?label=GitHub&message=Link%20to%20repository&color=blueviolet)](https://github.com/INGV/ttime)\n\n# ttime - Travel Time\n\n## Introduction\nThis project implement the web services to calculate travel time\n\n## Quickstart\n### Get Docker image\nTo obtain *ttime* docker image, you have two options:\n\n#### 1) Get built image from DockerHub (*preferred*)\nGet the last built image from DockerHub repository:\n```sh\ndocker pull ingv/ttime:latest\n```\n\n#### 2) Build by yourself\nClone the git repositry:\n```sh\ngit clone https://github.com/INGV/ttime.git\ncd ttime\n```\nbuild the image:\n```sh\ndocker build --tag ingv/ttime . evaluator\n```\n\nin case of errors, try:\n```sh\ndocker build --no-cache --pull --tag ingv/ttime . \n```\n\n### Run as a service\nrun the container in daemon (`-d`) mode:\n```\ndocker run -it --rm --name flask_ttime -p 8383:5000 -d --user $(id -u):$(id -g) ingv/ttime\ndocker exec -i flask_ttime tail -f /opt/log/ttime.log\n```\n\nThen test access to: http://localhost:8383/Contribute\n\nExamples of URL:\n\n- http://localhost:8383/api/get_phase_circle?lat=45.492599&lon=9.19289&depth=50&time=100&phases=P%2CS&azimuth_interval=30\n\n\n\n## Test ttime as a stand alone script\n\nIf you have cloned the project, you can check the evaluator engine from command line.\nTo do this you need *python3.x* installed on your machine and also some python libraries:\n\n```sh\npip install -r requirements.txt\npython main/api/queries.py --help\n```\n\nHere is some examples of launch:\n\n```sh\npython main/api/queries.py --lat 35 --lon 10 --depth 50 --time 100 --phases P,S\npython main/api/queries.py -l 35 -o 10 -d 50 -t 100 -p P,S -a 30\n```\n\n\n\n## Contribute\n\nThanks to your contributions!\n\nHere is a list of users who already contributed to this repository: \\\n<a href=\"https://github.com/ingv/ttime/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=ingv/ttime\" />\n</a>\n\n## Authors\n(c) 2023 Sergio Bruni sergio.bruni[at]ingv.it \\\n(c) 2023 Fabrizio Bernardi fabrizio.bernardi[at]ingv.it \\\n(c) 2023 Valentino Lauciani valentino.lauciani[at]ingv.it\n\nIstituto Nazionale di Geofisica e Vulcanologia, Italia\n",
        "createdAt": "2023-01-24T16:45:20.000Z",
        "updatedAt": "2023-01-24T17:17:06.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/INGV/ttime/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Sarvandani/Source_wavelet_estimation",
        "url": "https://github.com/Sarvandani/Source_wavelet_estimation",
        "description": "Auto-correlation, Source wavelet estimation, Signal processing",
        "stars": 7,
        "forks": 1,
        "readme": "# Source wavelet estimation\nThe autocorrelation of seismic signals can provide some preliminary information about the source. \n\nThe code `SOURCE_WAVELET_ESTIMATION.m` was written to auto-correlate two files of seismic data in Segy format (`synthetic_data1.segy` and `synthetic_data2.segy`). The source wavelet is estimated based on the autocorrelation, and its amplitude and phase spectra can be found in the folder. \nThe estmated source can be found in the folder `SOURCE_OUTPUT_FILE`. \n\nA) Auto-correlation of seismic signals:\n\n<img src=\"Auto-correlation.png\" width=\"600\" height=\"400\">\n\nB)Estimated source wavelet:\n\n<img src=\"Source wavelet (positive-negative time).png\" width=\"600\" height=\"300\">\n\nC)Amplitude spectrum of the source:\n\n<img src=\"amplitude spectrum.png\" width=\"600\" height=\"300\">\n\nC)Phase spectrum of the source:\n\n\n<img src=\"phase spectrum.png\" width=\"600\" height=\"300\">\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`DISCLAIMER`:  I don't warrant this code in any way whatsoever. This code is provided \"as-is\" to be used at your own risk.\n\nThis work was done as part of my PhD, I would be happy if you could cite my PhD thesis:\nSeismic tomography of an amagmatic ultra-slow spreading ridge\nhttps://theses.hal.science/tel-04020124/\n",
        "createdAt": "2023-01-24T17:01:02.000Z",
        "updatedAt": "2024-08-09T07:18:26.000Z",
        "language": "MATLAB",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Sarvandani/Source_wavelet_estimation/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "sixu0/SeisCLIP",
        "url": "https://github.com/sixu0/SeisCLIP",
        "description": "The code of Paper 'SeisCLIP: A seismology foundation model pre-trained by multimodal data for multipurpose seismic feature extraction'",
        "stars": 54,
        "forks": 2,
        "readme": "<p align=\"center\" width=\"100%\">\n<img src=\"assets\\SeisCLIP.png\"  width=\"80%\" height=\"80%\">\n</p>\n\n\n<div>\n<div align=\"center\">\n    <a href='https://sixu0.github.io/' target='_blank'>Xu Si<sup>1</sup></a>&emsp;\n    <a href='http://cig.ustc.edu.cn/people/list.htm' target='_blank'>Xinming  Wu<sup>1,†,‡</sup></a>&emsp;\n    <a href='http://cig.ustc.edu.cn/people/list.htm' target='_blank'>Hanlin Sheng<sup>1</sup></a>&emsp;\n    </br>\n    <a href='https://dams.ustc.edu.cn/main.htm' \n    target='_blank'>Jun Zhu<sup>1</sup></a>&emsp;\n    <a href='https://dams.ustc.edu.cn/main.htm' \n    target='_blank'>Zefeng Li<sup>1</sup></a>&emsp;\n</div>\n<div>\n\n<div align=\"center\">\n    <sup>1</sup>\n    University of Science and Technology of China&emsp;\n    </br>\n    <!-- <sup>*</sup> Equal Contribution&emsp; -->\n    <sup>†</sup> Corresponding Author&emsp;\n    <sup>‡</sup> Project Lead&emsp;\n</div>\n\n-----------------\n\n[![arXiv](https://img.shields.io/badge/arxiv-2309.02320-b31b1b?style=plastic&color=b31b1b&link=https%3A%2F%2Farxiv.org%2Fabs%2F2309.02320)](https://arxiv.org/abs/2309.02320)\n[![TGRS](https://img.shields.io/badge/IEEE_TGRS-2024-3480bc)](https://www.nature.com/articles/s43247-023-01188-4)\n![GitHub followers](https://img.shields.io/github/followers/sixu0?style=social)\n![GitHub stars](https://img.shields.io/github/stars/sixu0/SeisCLIP?style=social)\n\n### 🌟 Spec-based Foundation Model Supports A Wide Range of Seismology\n\n\n As shown in this figure, SeisCLIP can provide services for downstream tasks including event classification 💥 , location 🌍 , mechanism ⛰, etc.\n\nDue to the limitations of hinet data transmission, we have not made the location and focal mechanism analysis datasets publicly available. They can be accessed through Baidu Netdisk.[Links](https://pan.baidu.com/s/1khkeHHX5pf-Nq3fZNH4mKw?pwd=SEIS)(Password:SEIS)\n\n# 🌟 News\n\n\n* **2025.11.20:** Update Social Circle examples.\n* **2024.2.2:**  🌟🌟🌟 Congratulation! The paper has been published on IEEE Transactions on Geoscience and Remote Sensing (IEEE TGRS) [Links](https://ieeexplore.ieee.org/abstract/document/10400506). \n* **2023.9.14:** 🌟🌟🌟 Pretrained weight and a simple usage demo for out SeisCLIP have been released. The implementation of SeisCLIP for event classification also released. Because the location and focal mechanism analysis code need lib 'Pytorch_geometric', it may be challenging for beginners. To provide a more detailed documentation, we will release it later. (Python Version 3.9.0 is recommended)\n* **2023.9.8:** Paper is released at [arxiv](https://arxiv.org/abs/2309.02320), and code will be gradually released.\n* **2023.8.7:** Github Repository Initialization. (copy README template from Meta-Transformer)\n\n\n# 🔓 Model Zoo\n\n<!-- <details> -->\n<summary> Open-source Modality-Agnostic Models </summary>\n<br>\n<div>\n\n|      Model      |   Pretraining   | Spec Size | #Param |                                               Download | 国内下载源                                               |\n| :------------: | :----------: | :----------------------: | :----: | :---------------------------------------------------------------------------------------------------: | :--------: | \n| SeisCLIP  | STEAD-1M |         50 × 120          |  -  |   [ckpt](https://drive.google.com/file/d/1UIeFWl2wENr83GRtdi6Tlj4MLDZ3UctN/view?usp=drive_link)     | [ckpt]\n| SeisCLIP  | STEAD-1M |         50 × 600          |  -  |   [ckpt](https://drive.google.com/file/d/1_YiqeaBlBg-EKJ36Yvluoc50n4Y86aI3/view?usp=drive_link)     | [ckpt]\n\n\n&ensp;\n# Citation\nIf the code and paper help your research, please kindly cite:\n```\n@ARTICLE{\n  author={Si, Xu and Wu, Xinming and Sheng, Hanlin and Zhu, Jun and Li, Zefeng},\n  journal={IEEE Transactions on Geoscience and Remote Sensing}, \n  title={SeisCLIP: A Seismology Foundation Model Pre-Trained by Multimodal Data for Multipurpose Seismic Feature Extraction}, \n  year={2024},\n  volume={62},\n  pages={1-13},\n  doi={10.1109/TGRS.2024.3354456}}\n\n```\n# License\nThis project is released under the [MIT license](LICENSE).\n\n# Acknowledgement\nThis code is developed based on excellent open-sourced projects including [CLIP](https://github.com/openai/CLIP), [OpenCLIP](https://github.com/mlfoundations/open_clip/tree/main), [AST](https://github.com/YuanGongND/ast), [MetaTransformer](https://github.com/invictus717/MetaTransformer/tree/master), [ViT-Adapter](https://github.com/czczup/ViT-Adapter), [Seisbench](https://github.com/seisbench/seisbench), [STEAD](https://github.com/smousavi05/STEAD) and [PNW](https://github.com/niyiyu/PNW-ML).\n",
        "createdAt": "2023-08-07T03:24:24.000Z",
        "updatedAt": "2025-11-26T00:28:18.000Z",
        "language": "Jupyter Notebook",
        "homepage": "https://ieeexplore.ieee.org/document/10400506",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/sixu0/SeisCLIP/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "marcelobianchi/pypqlx",
        "url": "https://github.com/marcelobianchi/pypqlx",
        "description": "A Python Wrapper to a PQLX seismological database. It must help the task to query the PQLX database.",
        "stars": 5,
        "forks": 2,
        "readme": "# PyPQLx Package\n\nThis package offer a class to handle the PQLX database and on the creation of \nthe PSD and PDF plots. The final goal is to implement an WebService to serve\nPSD and PDF data to a set of rich web-pages.\n\n## Instalation\n\nThis code uses 'records' a light database driver.\n\nhttps://github.com/kennethreitz/records\n\nOn Debian the \"better\" way to install is using the pip system on local user account.\n\n```\n% pip install records --no-deps\n```\n\nand later, I fill in the dependencies manually by try-and-error.\n",
        "createdAt": "2019-10-09T13:56:57.000Z",
        "updatedAt": "2025-08-07T06:57:59.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/marcelobianchi/pypqlx/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "sgr/csnd",
        "url": "https://github.com/sgr/csnd",
        "description": "Citizen Seismology Network Daemon",
        "stars": 0,
        "forks": 1,
        "readme": "# csnd\n\nCitizen Seismology Network Daemon\n\n## Prerequisites\n\n* build-essential\n* cmake (>= 2.8.12)\n* following dependencies\n\nなお、USB接続のセンサーから読み取るため、csndを動かすユーザーは dialout グループに属している必要がある。\n\n## Dependencies\n\n### common\n\n|  name   | debian (raspbian) |    centos     |          note          |\n|:-------:|:-----------------:|:-------------:|:----------------------:|\n| spdlog  |  N/A (embedded*)  | spdlog (epel) | *required CMake >= 3.1 |\n\n\n### [Avro C++](https://github.com/apache/avro)\n\n|  name   | debian (raspbian) |    centos     |          note          |\n|:-------:|:-----------------:|:-------------:|:----------------------:|\n|  boost  | libboost-filesystem-dev <br/> libboost-system-dev <br/> libboost-program-options-dev <br/> libboost-iostreams-dev |  <br/> boost-devel  | version >= 1.38        |\n|  zlib   |     liblz-dev     |  zlib-devel   ||\n\n### [Avro C](https://github.com/apache/avro)\n\n|  name   | debian (raspbian) |    centos     |          note          |\n|:-------:|:-----------------:|:-------------:|:----------------------:|\n|  lzma   |    liblzma-dev    |    xz-devel   ||\n|  zlib   |     liblz-dev     |  zlib-devel   ||\n| jansson |  libjansson-dev   | jansson-devel |    version >= 2.3      |\n| snappy  |  libsnappy-dev    | snappy-devel  ||\n\n### [azure-iothub](https://github.com/sgr/azure-iothub) (contains [azure-iot-sdk-c](https://github.com/Azure/azure-iot-sdk-c))\n\n|  name   |   debian (raspbian)  |    centos     |          note          |\n|:-------:|:--------------------:|:-------------:|:----------------------:|\n| OpenSSL |      libssl-dev      | openssl-devel ||\n|  cURL   | libcurl4-openssl-dev | libcurl-devel ||\n|  uuid   |       uuid-dev       | libuuid-devel ||\n\n## Build\n\n* git clone https://github.com/sgr/csnd.git\n* mkdir <BUILD_DIRECTORY>\n* cd <BUILD_DIRECTORY>\n* cmake <SOURCE_DIRECTORY>\n* make\n\n## Install\n\n0. (CentOSのみ) [curl](https://curl.haxx.se/) をソースコードからビルドし、インストールする。\n   - csnd が OS の libcurl より先に見つけられるディレクトリにインストールする。例えば以下の csnd のインストールディレクトリと同一にする。\n1. csnd を配置する場所を決める。\n   - インストールディレクトリ (例えば /opt/csnd)\n   - 設定ファイルを置くディレクトリ (例えば /opt/csnd/etc)\n   - 加速度データおよびイベントデータ出力ディレクトリ (例えば /opt/csnd/out 設定ファイルでは out_dir で指定)\n   - ログファイル (例えば /opt/csnd/log/csnd.log 設定ファイルでは logging.file_settings.filename で指定)\n   - pidファイル (例えば /opt/csnd/run/csnd.pid 設定ファイルでは pid_file で指定)\n2. cmake -DCMAKE_INSTALL_PREFIX=<インストールディレクトリ> <csndのソースディレクトリ>\n3. make install\n4. <インストールディレクトリ>/csnd.yml を作成\n   - ひな形ファイル <インストールディレクトリ>/etc/csnd.yml.example をもとに作ると良い。\n   - iothub.connection_string は CSN 管理者から発行された接続文字列を設定する。接続文字列がない場合は offline_mode を true に設定し、オフラインモードで使用する。\n   - logging.logger を console にする場合は、 -d オプションを用いずフォアグラウンドモードで実行する。\n5. /etc/systemd/system/csnd.service を作成\n   - ひな形ファイル <インストールディレクトリ>/etc/csnd.service.example をもとに上で決めた内容に合わせて作成する。\n6. csnd.service を systemctl で有効化する。\n   ```(sh)\n   # systemctl enable csnd.service\n   ```\n\n## Command line options\n\n| option | description|\n|------|-------------|\n| -c /path/of/csnd.yml | set config file path |\n| -d | run on daemon mode |\n\n## Known Issues\n\n* 今のところ Avro-C シリアライザー (libserializer_avroc.a) は正しく動作しない。 Avro-C++ 版を用いること。\n* azure-iot-sdk-c は libcurl を必要とするが、CentOS 7 標準パッケージは OpenSSL を使っていないため、HTTP接続の場合初期化で Segmentation Fault を起こす。curl のソースコードから --with-ssl つきでビルドし、ライブラリの場所を LD_LIBRARY_PATH に与えて起動する必要がある。\n",
        "createdAt": "2017-11-03T18:50:11.000Z",
        "updatedAt": "2018-04-26T02:27:25.000Z",
        "language": "C++",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/sgr/csnd/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ds-modules/EPS-130-SP22",
        "url": "https://github.com/ds-modules/EPS-130-SP22",
        "description": "UC Berkeley EPS 130 (Strong Motion Seismology) Spring 2022",
        "stars": 0,
        "forks": 0,
        "readme": "# EPS-130-SP22\n\nEPS 130 - Strong Motion Seismology - Doug Dreger - Spring 2022\n\n [![Datahub](https://img.shields.io/badge/Launch-UCB%20Datahub-blue.svg)](http://datahub.berkeley.edu/user-redirect/interact?account=ds-modules&repo=EPS-130-SP22&branch=main&path=)\n\n[![Datahub](https://img.shields.io/badge/Launch-UCB%20Datahub%20PS1-blue.svg)](http://datahub.berkeley.edu/user-redirect/interact?account=ds-modules&repo=EPS-130-SP22&branch=main&path=EPS_Homework1/eps130_hw1_gutenberg_richter_v3.0.ipynb)\n\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/ds-modules/EPS-130-SP22/master)\n",
        "createdAt": "2022-01-05T21:23:22.000Z",
        "updatedAt": "2025-07-07T21:23:32.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ds-modules/EPS-130-SP22/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "anowacki/SACPlot.jl",
        "url": "https://github.com/anowacki/SACPlot.jl",
        "description": "SACPlot.jl plots seismic traces in SAC format using Julia",
        "stars": 6,
        "forks": 1,
        "readme": "# SACPlot.jl is archived\nThis repository has been archived as it is no longer being maintained.\n**No future development of SACPlot.jl will take place.**\nPlease instead use [Seis.jl](https://github.com/anowacki/Seis.jl).\n\n# SACPlot.jl\n\n## What is SACPlot.jl?\nA [Julia](http://julialang.org) package for plotting seismic data in the\n[SAC](http://ds.iris.edu/files/sac-manual/manual/file_format.html) format,\ndesigned to work with the [SAC.jl](https://github.com/anowacki/SAC.jl)\nmodule.\n\nSACPlot.jl is compatitble with Julia v0.7+.\n\n## How to install\nFirst install the unregistered package SAC.jl:\n\n```julia\njulia> import Pkg\n\njulia> Pkg.add(\"https://github.com/anowacki/SAC.jl\")\n```\n\nThen you can install the package itself:\n\n```julia\njulia> Pkg.add(\"https://github.com/anowacki/SACPlot.jl\")\n```\n\nYou then need only do\n\n```julia\nusing SACPlot\n```\n\nand if that works, you're ready to go.\n\n(Older versions of SACPlot compatible with Julia v0.6 can be installed like so:\n`Pkg.clone(\"https://github.com/anowacki/SAC.jl\"); Pkg.clone(\"https://github.com/anowacki/SACPlot.jl\")`.)\n\n\n## How to use\n### SAC.jl\nSACPlot.jl relies on the SAC.jl and [Plots.jl](https://github.com/JuliaPlots/Plots.jl) packages, so make sure to install\nthese first (and their respective dependencies).\n\n### Simple plots\nAs an example, let's plot the sample data that comes with SAC.jl:\n\n```julia\njulia> using SAC, SACPlot\n\njulia> t = SAC.sample();\n\njulia> plot1(t)\n```\n\nAssuming your Plots.jl installation works, and you are in the REPL or another\ninteractive environment, you should see a plot:\n\n![Plot of sample trace](docs/sample_plot.png)\n\n### Plotting multiple traces\nSimply pass an array of traces in to `plot1` (also called `p1`).  The following\nexample creates a set of traces showing the effect of changing the limit of a\nlowpass filter, ranging from 0.33&nbsp;Hz to 10&nbsp;Hz.  We put the corner\nfrequency in header variable `user0`, and pass the name of this variable as a\nsymbol to the `plot1` method (`label=:user0`), which then shows this on the\nright of each plot.\n\n```julia\njulia> using SAC, SACPlot\n\njulia> A = [SAC.sample() for i in 1:5]  |> rtrend! |> taper!\n5-element Array{SAC.SACtr,1}:\n SAC.SACtr(delta=0.01, b=9.459999, npts=1000, kstnm=CDV, gcarc=3.357463, az=88.14708, baz=271.8529)\n SAC.SACtr(delta=0.01, b=9.459999, npts=1000, kstnm=CDV, gcarc=3.357463, az=88.14708, baz=271.8529)\n SAC.SACtr(delta=0.01, b=9.459999, npts=1000, kstnm=CDV, gcarc=3.357463, az=88.14708, baz=271.8529)\n SAC.SACtr(delta=0.01, b=9.459999, npts=1000, kstnm=CDV, gcarc=3.357463, az=88.14708, baz=271.8529)\n SAC.SACtr(delta=0.01, b=9.459999, npts=1000, kstnm=CDV, gcarc=3.357463, az=88.14708, baz=271.8529)\n\njulia> freqs = [1/3, 1, 3, 6, 10];\n\njulia> A[:user0] = freqs;\n\njulia> lowpass!.(A, freqs);\n\njulia> p1(A)\n```\n\n![Plot of multiple traces](docs/multiple_traces.png)\n\n### Record sections\nRecord sections are plotted with `plotrs`:\n\nNote that the y-axis variable is set using the keyword argument `y=<value>`,\nand defaults to `:gcarc`, the epicentral distance, as is usual for record\nsections.\n\nAligning traces on a certain arrival, say, is as simple as passing a second\nargument to `prs`.  It can be a header `Symbol` (e.g., `:a`) or an array of\nnumbers (e.g., `prs(A, :a)` or `prs(A, rand(length(A)))`).  Let's plot some\ndata for the UK network from an event beneath Fiji, which has picks for the\nPKIKP phase in header `:a`:\n\n```julia\njulia> B = SAC.sample(:array); # Load sample data\n\njulia> B = cut(B, :a, -30, :a, 30); # Cut traces\n\njulia> import Pkg; Pkg.add(\"Plots\"); # This allows us to call Plots directly below\n\njulia> import Plots; Plots.default(size=(600,1000), margin=4Plots.mm) # Change the default figure size and margin\n\njulia> plotrs(B, :a, qdp=false, label=:kstnm, xlabel=\"Time rel. PKIKP / s\", ylabel=\"Distance / °\")\n```\n\n![Record section of UK network](docs/record_section.png)\n\nIn this case we used the `label` keyword argument to label the traces by the\nstation name (header `:kstnm`) and add labels to the axes.  We also turned off\n&lsquo;quick-and-dirty-plotting&rsquo; with the `qdp=false` option.\n\n`plotrs` can show traces against any header value or array passed in.  Here we\nplot the earlier example traces against frequency:\n\n```julia\njulia> plotrs(A, y=:user0, xlabel=\"Time / s\", ylabel=\"Lowpass frequency / Hz\")\n```\n\n![Record section plot of traces against frequency](docs/frequency_section.png)\n\n\n\n## Getting help\nFunctions are documented, so at the REPL type `?` to get a `help?>` prompt,\nand type the name of the function:\n\n```julia\nhelp?> plot1\nsearch: plot1 plot2 plotsp plotpm SACPlot PyPlot prevfloat parsefloat PartialQuickSort\n\n  plot1(s::Array{SACtr}; xlim=[NaN, NaN], ylim=[NaN, NaN], label=:default, title=\"\")\n\n  Create a plot of the SAC trace(s) s.\n\n  Define limits in time with xlim\n\n  Define dependent variable axis limits with ylim, which can be a 2-array of values, or\n  \"all\" to set all axes to have the same automatic limits.\n\n  Define the text labels with an array of sumbols, which correspond to the names of SAC\n  headers.\n```\n",
        "createdAt": "2016-06-14T16:51:45.000Z",
        "updatedAt": "2025-01-17T16:34:30.000Z",
        "language": "Julia",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/anowacki/SACPlot.jl/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "sannecottaar/smurfpy",
        "url": "https://github.com/sannecottaar/smurfpy",
        "description": "seismological methods utilizing receiver functions in python",
        "stars": 9,
        "forks": 4,
        "readme": "\n# smurfpy\nseismological methods utilizing receiver functions in python3\n\nContributors: Sanne Cottaar, Jennifer Jenkins, Stephen Pugh, Alistair Boyce, Matthew Kemp, Annemijn van Stiphout, Simon Thomas, Kieran Gilmore, Matt Harding, Sophia Baker and others\n\n\nREADME last updated by: S. Pugh, 18/08/22\n\n-----------------------------------------------------------------------\n--------------------------- OUTLINE -----------------------------------\n-----------------------------------------------------------------------\n\n1. Processing_Scripts\n    --> Data download, pre-processing, RF calculation, post-processing (inc. quality control)\n2. Migration_Scripts\n    --> Calculate RF pierce points, 1D & 3D time-to-depth conversion\n3. Stacking_Scripts\n    --> Epicentral distance, depth, slowness, common-conversion-point stacking\n4. Plotting_Scripts\n    --> Plot Pierce points, CCP stack volumes (weights, Maps, XC etc)\n5. Tools\n    --> Misc tools (inc velocity models)\n6. South_Africa_Data\n    --> Test dataset from XA network in Southern Africa\n    \n-----------------------------------------------------------------------\n----------------------------- HELP ------------------------------------\n-----------------------------------------------------------------------\n    \nFor help with a script, use the command line argument 'help'. E.g. >> python3 1_download_data_per_station.py help\n\nInvalid use of a script will also return the help information\n\n-----------------------------------------------------------------------\n----------------------------- CITATION --------------------------------\n-----------------------------------------------------------------------\n\nIf you use (part of) this code for your research, please cite: [![DOI](https://zenodo.org/badge/110966711.svg)](https://zenodo.org/badge/latestdoi/110966711) \n\nAlso consider citing one of these papers:   \n• Pugh, S., J. Jenkins, A. Boyce, and S. Cottaar (2021) Global receiver function observations of the X-discontinuity reveal recycled basalt beneath hotspots, Earth and Planetary Science Letters  \n• Boyce, A.,  and S. Cottaar (2021) Insights into Deep Mantle Thermochemical Contributions to African Magmatism from Converted Seismic Phases, Geochemistry, Geophysics, Geosystems  \n\nEarlier papers using SMURFPy are:  \n• Kemp., M., Jenkins, J., Maclennan, J. and Cottaar, S., 2019. X-discontinuity and transition zone structure beneath Hawaii suggests a heterogeneous plume. Earth and Planetary Science Letters, 527, p.115781.  \n• Van Stiphout., A.M., Cottaar, S. and Deuss, A., 2019. Receiver function mapping of mantle transition zone discontinuities beneath Alaska using scaled 3-D velocity corrections. Geophysical Journal International, 219(2), pp.1432-1446.  \n• Cottaar, S. and Deuss, A., 2016, Large-scale mantle discontinuity topography beneath Europe: signature of akimotoite in subducting slabs, Journal of Geophysical Research, 121, 279-292  \n\n---------------------------------------------------------------------------------\n--------------------------- Processing SCRIPTS ----------------------------------\n---------------------------------------------------------------------------------\n\n# 1_download_data_per_station.py  \n• Description: select and download appropriate events and stations based on user inputs  \n• Inputs: search area lat/lon, start and end times (in function), epicentral dist of ev/station, event magnitude range, trace length, data filter band, station networks to search, dataclient  \n• Outputs: python stream objects in PICKLE format with a dictionary of header info for each event. Saves to ../Data/NT.STA/Originals/  \n• Usage: >> python3 1_download_data_per_station.py  \n\n# 2_rotate_data_NE_RT.py\t  \t\n• Description: Preprocessing for RF calculation: merges truncations, trims, donsamples, rotates components Z-R-T, renames based on BAZ and EPI-DIST.  \n• Inputs: Data directory (usually ../Data/)  \n• Outputs: python stream objects in PICKLE format with a dictionary of header info for each event in a new folder leaving a copy of the unprocessed original data for future use  \n• Usage: >> python3 2_rotate_data_NE_RT.py  \n\n# 3_add_travel_times.py  \n• Description: compute predicted travel-times for user defined phases based on TauP, predicted times are added to waveform header information (python dictionary)  \n• Inputs: Data directory (usually ../Data/), 1D velocity model, phases to compute TTs for  \n• Outputs: Overwrites files from above with new dictionary (seis[0].stats.traveltimes)  \n• Usage: >> python3 3_add_travel_times.py P S P660s P410s  \n\n# 4_plot_data_preRF_perstation.py        \n• Description: [OPTIONAL] plot Z-R-T seismogram components with predicted arrival times after initial processing, before RF creation  \n• Inputs: Data directory (usually ../Data/), station directory  \n• Outputs: On-screen plotting  \n• Usage: >> python3 4_plot_data_preRF_perstation.py  \n\n# receiver_function.py  \n• Description: Various RF algorithms: water level deconvolution, multitaper, iterative deconvolution  \n• Usage: called below  \n\n# 5_compute_receiver_functions.py      \n• Description:  \n• Inputs: Data directory (usually ../Data/), horizontal component (usually radial), filter band, decon algorithm (usually iterative decon - default)  \n• Outputs: Adds computed RF to pre-existing PICKLE waveform file  \n• Usage: >> python3 5_compute_receiver_functions.py jgf1      \n\n# 6_auto_select_receiver_functions.py   \n• Description: Removes low quality ones based on set criteria:   \n        1. Minimum percentage of radial compoment to be fit (after reconvolving the RF with the vertical component (fitmin)  \n        2. Peak amplitude max threshold before main P-wave arrival (noisebefore)  \n        3. Peak amplitude max threshold after main P-wave arrival (noiseafter)  \n        4. Peak amplitude min threshold after main P-wave arrival (minamp)  \n• Inputs: Data directory (usually ../Data/), horizontal component (usually radial), filter band, SNR calculation type, fitmin, noisebefore, noiseafter, minamp  \n• Outputs: Two \".dat\" files specific to the chosen filter band recording the good RF files and the good RF file SNR ratios (V & R components)  \n• Usage: >> python3 6_auto_select_receiver_functions.py jgf1  \n\n# 7_plot_data_selection.py  \n• Description: [OPTIONAL] Plots the perstation distribution of \"Acceptable - Green\" and \"Removed - red\" events as a funciton of EQ magnitude and epicentral   distance.  \n• Inputs: Data directory (usually ../Data/)  \n• Outputs: On-screen plotting  \n• Usage: >> python3 7_plot_data_selection.py jgf1  \n\n# 8_plot_data_perstation.py  \n• Description: [OPTIONAL] Plots V,R,RF as a function of time and epicentral distance.  \n• Inputs: Data directory (usually ../Data/), horizontal component (usually radial), filter band  \n• Outputs: On-screen plotting  \n• Usage: python3 8_plot_data_perstation.py jgf1  \n\n\n---------------------------------------------------------------------------------\n--------------------------- MIGRATION SCRIPTS -----------------------------------\n---------------------------------------------------------------------------------\n\n# calculate_pierce_points.py  \n• Description: Calculate converted phase pierce points at discontinuity depths  \n• Inputs: Depth of piercepoints, Phase, filter band, 1D velocity model  \n• Outputs: Adds PP for given phase and discont depth to each Pickle file, prints to file PP_'DEPTH'km_'PHASE'_'FILTER'.txt'  \n• Usage: python3 calculate_pierce_points.py 410 P410s jgf1  \n\n\n# convert_to_depth_obspy.py  \n• Description: Convert RF from time to depth using 1D model (coded for Prem)  \n• Inputs: Filter band, 1D velocity model  \n• Outputs: Adds dictionary seis[0].conversions['<nameof1Dmodel>'] to each Pickle file  \n• Usage: python3 convert_to_depth_obspy.py jgf1  \n\n# dep_conv_AFR_AFRP20CR_AK135.py  \n• [OPTIONAL] As above but based on ak135 depths, converts RF from time to depth using 3D model and appropriate crustal model  \n• 3D model example is AFRP20 (Boyce et al., 2021 Gcubed)  \n• Also accounts for 3D crustal model (See Boyce et al., 2020 supplementary material) and station elevations.  \n• Usage: python3 dep_conv_AFR_AFRP20CR_AK135.py jgf1  \n\n\n---------------------------------------------------------------------------------\n---------------------------STACKING SCRIPTS -----------------------------------\n---------------------------------------------------------------------------------\n\n# epicentral_distance_stack.py  \n• Description: Stacks the RFs in bins of epicentral distance to show the most prominent features  \n• Inputs: bin_size, smoothing, lon/lat box, epi_dist_limits, filter band  \n• Outputs: Epicentral distance stack plot  \n• Usage: python3 epicentral_distance_stack.py 5 False -179 179 -89 89 30 90 jgf1  \n\n\n# depth_stack.py  \n• Description: Stacks all the RFs within the bounds for the depth stated producing one trace.  \n• Inputs: conversion, lon/lat box, filter band  \n• Outputs: Depth stack pickle file and pdf/png  \n• Usage: python3 depth_stack.py prem -179 179 -89 89 jgf1  \n\n# slowness_stack.py  \n• Description: Plot of slowness against time, using a specfic epicentral reference distance.  \n• Inputs: lon/lat box, filter band  \n• Outputs: Slowness stack pickle file and pdf/png  \n• Usage: python3 slowness_stack.py -179 179 -89 89 jgf1  \n\n# depth_slowness_stack.py  \n• [OPTIONAL] Description: Combines previously calculated depth and slowness stack in one figure.  \n• Inputs: Depth and slowness stack pickle files  \n• Outputs: Combined depth and slowness stack image pdf/png  \n• Usage: python3 depth_slowness_stack.py prem -179 179 -89 89 jgf1 339  \n\n# common_conversion_point_stack.py  \n• Description: Common conversion point stacking routines (see Cottaar and Deuss, 2016). Calculates weighting factor based on Lekic et al., (2011, Science) based on distance and fresnel zone at given depth   \n• Inputs: see below  \n• Outputs:  Common conversion point stack, weights, errors  \n• Usage: see below  \n\n# stack_CCP.py  \n• Description: wrapper for the function contained within common_conversion_point_stack.py  \n• Inputs: Name, conversion, lon/lat box, filter band, smoothing factor, newstack  \n• Outputs: Common conversion point stack volume (PICKLE)  \n• Usage: python3 stack_CCP.py CCP_Global prem -179.0 179.0 -89.0 89.0 jgf1 2.0 True  \n\n%%%%%%%%%%%% Parallel processing of CCP stack %%%%%%%%%%%%%%%%%%%%%%%\n%%%%%%%%%%%%%%%%%% BETA VERSION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n# stack_CCP_par_beta.py  \n# common_conversion_point_stack_par_beta.py  \n• [OPTIONAL] Scripts as above but a beta version coded in Parallel. Also specify num. cores.  \n• Computes CCP-subvolume for each RF in each station (specify max cores) and sums to master volume.  \n\n---------------------------------------------------------------------------------\n--------------------------- Plotting SCRIPTS ------------------------------------\n---------------------------------------------------------------------------------\n\n# plot_map_pierce_points.py  \n• Description: Plots discontinuity depth pierce points  \n• Inputs: discontinuity depth, converted phase  \n• Outputs: matplotlib plot  \n• Usage: python3 plot_map_pierce_points.py 410 P410s jgf1  \n\n\n# CCP_plottingroutines.py  \n• Description: Routines for various CCP stack plot types (discontinuity depth/sampling maps, cross sections, moveout)  \n• Inputs: see below  \n• Outputs: Various matplotlib plot windows.  \n• Usage: see below  \n\n# plot_CCP.py  \n• Description: Wrapper for the function contained within CCP_plottingroutines.py  \n• Inputs: name, conversion, filter band, smoothing factor, mincoverage, plot_type, plot_params  \n• Outputs: Various matplotlib plot windows.  \n• Usage: python3 plot_CCP.py CCP_Global prem jgf1 2.0 2.0 COV 410  \n\n---------------------------------------------------------------------------------\n----------------------         Tools           ----------------------------------\n---------------------------------------------------------------------------------\n\n# /MODELS/  \n\nVarious models used for 1D and 3D depth stacking examples  \n\n# /PLOTTING/  \n\n# Africa_AFRP20_RF_CR1.py  \n• Description: Script used to plot 3D tomographic model of Boyce et al., 2020 in 3D time to depth conversion example  \n• Inputs: Plot type, various model parameters  \n• Outputs: Pdf tomogrpahic model plot  \n• Usage: python3 Africa_AFRP20_RF_CR1.py  \n\n# /Travel_Times_Slowness/  \n\nMultiple reference travel-time files used in slowness stacking.  \n\n# /Moveout_with_epicentral_distance/  \n\nPhase moveout files used in epicentral distance stacking.  \n\n---------------------------------------------------------------------------------\n------------------------------ 3D Velocity Models -------------------------------\n---------------------------------------------------------------------------------\n\n3D velocity models are recommended for time-to-depth conversion though they aren't provided here due to their non-standard formats and considerations about inversion set-up specific to each model.\nFor examples of currently working models see Boyce et al. (2021) and Pugh et al. (2021), or contact the authors for assistance.\n\n---------------------------------------------------------------------------------\n----------------------  South_Africa_Data      ----------------------------------\n---------------------------------------------------------------------------------\n\nTest data set for 45 stations in the XA network (doi:10.7914/SN/XA_1997)  \nThis data is unprocessed.   \nTo use, copy directory to a new directory called \"Data\"  \nThen proceed from script Processing_Scripts/2_rotate_data_NE_RT.py   \n\n---------------------------------------------------------------------------------\n----------------------  List of Python package versions -------------------------\n---------------------------- Most recently checked ------------------------------\n\nFor a comparison of your current python modules to a checked working verison see:  \n\n/Tools/check_import_versions.py  \n\n--- Checked system package versions ---  \n\npython 3.8.8 (default, Apr 13 2021, 12:59:45)\nClang 10.0.0\n\ngeographiclib 1.52\nmatplotlib 3.3.4\nnumpy 1.20.1\nobspy 1.2.2\nscipy 1.6.2\nshapely 1.7.1\n\n---------------------------------------------------------------------------------\n---------------------------------- References -----------------------------------\n---------------------------------------------------------------------------------\n\nBoyce, A. and Bastow, I. D. and Cottaar, S. and Kounoudis, R. and Guilloud De Courbeville, J. and Caunt, E. and Desai, S. (2020: Manuscript under review at Geochemistry, Geophysics, Geosystems) AFRP20: New P-wavespeed Model for the African Mantle Reveals Two Whole-Mantle Plumes Below East Africa and Neoproterozoic Modification of the Tanzania Craton  \n\nLekic, V., French, S. W., & Fischer, K. M.    (2011).    Lithospheric Thinning Beneath Rifted Regions of Southern California. Science, 334 (6057), 783–787. doi:10.1126/science.1208898  \n\n",
        "createdAt": "2017-11-16T12:05:39.000Z",
        "updatedAt": "2025-10-21T01:21:11.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/sannecottaar/smurfpy/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "SeisComP3/seiscomp3",
        "url": "https://github.com/SeisComP3/seiscomp3",
        "description": "SeisComP is a seismological software for data acquisition, processing, distribution and interactive analysis.",
        "stars": 112,
        "forks": 89,
        "readme": "> **Note**  \n> This project is archived and will not longer be supported. The successor of SeisComP3 is **SeisComP** which\n> is being continued at https://github.com/seiscomp. The current homepage can be found at https://www.seiscomp.de.\n\n# About\n\nProject homepage: http://www.seiscomp3.org\n\nThis software has been developed by the [GEOFON Program](http://geofon.gfz-potsdam.de) at [Helmholtz Centre Potsdam, GFZ German Research Centre for Geosciences](http://www.gfz-potsdam.de) and [gempa GmbH](http://www.gempa.de).\n\nSeisComP3 is distributed under the [SeisComP Public License](COPYING)\n\n> **Note**\n\n> - The purpose of this repository is to test upcoming features and to\n>   integrate community source code and patches\n> - For production systems only use the official releases from http://www.seiscomp3.org or the corresponding tags in this repository.\n> - Commercial modules obtained from [gempa GmbH](http://www.gempa.de) are only\n>   available for official releases. Binary compatibility of intermediate\n>   SeisComP3 versions is not guaranteed.\n\n\n# Compiling\n\nThe easiest way to compile SeisComP3 is to use the provided Makefile.cvs which\ncreates the build directory inside the source tree.\n\n```\n$ make -f Makefile.cvs\n$ cd build\n$ make\n$ make install\n```\n\nBy default all files are installed under $HOME/seiscomp3. This location can be\nchanged with cmake or with its frontend ccmake.\n\nBasically the build directory can live anywhere. The following steps create\na build directory, configure the build and start it:\n\n```\n$ mkdir sc3-build\n$ cd sc3-build\n$ ccmake /path/to/sc3-src\n# Configure with ccmake\n$ make\n$ make install\n```\n\n## Step-by-step instructions\n\n1. Checkout SeisComP3 source code from Github\n\n   ```\n   sysop@host:~$ git clone https://github.com/SeisComP3/seiscomp3.git sc3-src\n   sysop@host:~$ cd sc3-src\n   sysop@host:~/sc3-src$\n   ```\n\n2. Change into the desired branch (if not master) or checkout tag\n   ```\n   sysop@host:~/sc3-src$ git checkout release/jakarta/2017.124.02\n   ```\n\n3. Configure the build\n\n   SeisComP3 is using cmake as build environment. For users that are not experienced\n   with cmake it is recommended to use `ccmake`, an ncurses frontend which is launched\n   by the default `Makefile.cvs`.\n   \n   ```\n   sysop@host:~/sc3-src$ make -f Makefile.cvs\n   ```\n   \n   This will bring up the cmake frontend. Press `c` to configure the build initially.\n   If cmake is being used, the variables can be passed as command line options:\n\n   ```\n   sysop@host:~/sc3-src/build$ cmake -DCMAKE_INSTALL_PREFIX=/path/to/install/dir ..\n   ```\n\n   With ccmake some components can be activated and deactivated such as database\n   backends you want to compile support for. The default just enables MySQL. Once done\n   with options, press `c` again to apply the changes. If everything runs without errors,\n   press `g` to generate the Makefiles. `ccmake` will quit if the Makefiles have been\n   generated:\n   \n   ```\n   *** To build the sources change into the 'build' directory and enter make[ install] ***\n   sysop@host:~/sc3-src$ cd build\n   sysop@host:~/sc3-src/build$ make\n   ```\n   \n   If `make` finished without errors, install SeisComP3 with\n   \n   ```\n   sysop@host:~/sc3-src/build$ make install\n   ```\n   \n   All files are then installed under `~/seiscomp3` or under the directory you have\n   specified with ```CMAKE_INSTALL_PREFIX```.\n  \n\n# Dependencies\n\nTo compile the sources the following development packages are required (Redhat/CentOS package names):\n\n- flex\n- libxml2-devel\n- boost-devel\n- openssl-devel\n- ncurses-devel\n- mysql-devel\n- postgresql-devel (optional)\n- python-devel\n- m2crypto-devel\n- qt4-devel\n",
        "createdAt": "2014-09-19T08:37:55.000Z",
        "updatedAt": "2025-04-30T09:46:22.000Z",
        "language": "C++",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/SeisComP3/seiscomp3/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "victoresende19/Earthquakes",
        "url": "https://github.com/victoresende19/Earthquakes",
        "description": "This repository aims to create a seismological observatory consuming USGS API data using python.",
        "stars": 0,
        "forks": 0,
        "readme": "# earthquakes\nThis repository aims to create a seismological observatory consuming USGS API data using python. All the technical explanation about the process of creating this solution is on the PDF file that is in Portuguese.\n\n<p align=\"center\">\n  <img src=\"https://news.northwestern.edu/assets/Images/2015/yoshimoto638__FitMaxWzk3MCw2NTBd.jpg\" />\n</p>\n\nThe suggested architecture for this solution is simple, as shown on the PDF file. Using python to manipulate the data on ETL (and schedule it with Watson Studio) and insert it into PostgreSQL to make an interactive view with streamlit library.\n\n\n",
        "createdAt": "2022-05-13T23:53:26.000Z",
        "updatedAt": "2025-07-30T02:03:47.000Z",
        "language": "Jupyter Notebook",
        "homepage": "https://sismo-map.streamlit.app/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/victoresende19/Earthquakes/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "eseism/Mizzou_Seism",
        "url": "https://github.com/eseism/Mizzou_Seism",
        "description": "University of Missouri Seismology Code Repository",
        "stars": 0,
        "forks": 0,
        "readme": "# Mizzou_Seism\nUniversity of Missouri Seismology Code Repository\n",
        "createdAt": "2021-11-22T21:01:20.000Z",
        "updatedAt": "2021-11-22T21:01:24.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/eseism/Mizzou_Seism/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "UT-GlobalSeismology/DSMsynTI-mpi",
        "url": "https://github.com/UT-GlobalSeismology/DSMsynTI-mpi",
        "description": "Software for computing synthetic seismograms in spherically symmetric, transversely isotropic (TI) media using the Direct Solution Method (DSM)",
        "stars": 8,
        "forks": 0,
        "readme": "# UT-GlobalSeismology/DSMsynTI-mpi\n\n![version](https://img.shields.io/github/v/release/UT-GlobalSeismology/DSMsynTI-mpi)\n[![DOI](https://zenodo.org/badge/464719338.svg)](https://zenodo.org/badge/latestdoi/464719338)\n![GitHub License](https://img.shields.io/github/license/UT-GlobalSeismology/DSMsynTI-mpi)\n\n**_Welcome to the DSM world._**\n\nThis is a software for computing synthetic seismograms in a spherically symmetric, transversely isotropic (TI) model using the Direct Solution Method (DSM).\n\nThis package is a bundle of 3 separate programs: tish, tipsv, and spcsac.\nYou must build each program separately.\n\n```\n% cd tish-mpi\n% make\n% cd ../tipsv-mpi\n% make\n% cd ../spcsac\n% make\n```\n\n## Important note\n\nIf you compute only the toroidal contribution to the transverse component and the spheroidal contribution to the radial component, this will lead to errors on the order of 1% or more. Therefore you **must** include the toroidal contribution to the radial displacement and the spheroidal contribution to the transverse component.\n\n\n## Papers describing the methods and theory used by this software\n\n-   Kawai, K., N. Takeuchi, and R.J. Geller, Complete synthetic seismograms up to 2 Hz for transversely isotropic spherically symmetric media, Geophys. J. Int., 164, 411-424, 2006.\n-   Takeuchi, N., R.J. Geller, and P.R. Cummins, Highly accurate P-SV complete synthetic seismograms using modified DSM operators, Geophys. Res. Lett., 23, 1175-1178, 1996.\n-   Geller, R.J., and N. Takeuchi, A new method for computing highly accurate DSM synthetic seismograms, Geophys. J. Int., 123, 449-470, 1995.\n-   Cummins, P.R., R.J. Geller, T. Hatori, and N. Takeuchi, DSM complete synthetic seismograms: SH, spherically symmetric, case, Geophys. Res. Lett., 21, 533-536, 1994.\n-   Cummins, P.R., R.J. Geller, and N. Takeuchi, DSM complete synthetic seismograms: P-SV, spherically symmetric, case, Geophys. Res. Lett., 21, 1663-1666, 1994.\n-   Geller, R.J., and T. Ohminato, Computation of synthetic seismograms and their partial derivatives for heterogeneous media with arbitrary natural boundary conditions using the Direct Solution Method, Geophys. J. Int., 116, 421-446, 1994.\n\n\n## Authorship and copyright of software\n\nThis software was written and improved, and is copyrighted &copy;, by the members of the Global Seismology Group of the University of Tokyo from 1994 to present.\n\n\n## License\n\nThis software is made available under the GNU Public License v3.0 https://www.gnu.org/licenses/gpl-3.0.en.html\n\n\n## Cite as\n\nDOI for the latest versioned release (Version DOI):<br>\n[![DOI](https://zenodo.org/badge/464719338.svg)](https://zenodo.org/badge/latestdoi/464719338)\n\nIf you wish to cite the DOI representing all versions of this software (Concept DOI), use https://doi.org/10.5281/zenodo.13384312 instead. This will always redirect to the latest release.\n",
        "createdAt": "2022-03-01T02:41:30.000Z",
        "updatedAt": "2025-10-22T03:13:45.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/UT-GlobalSeismology/DSMsynTI-mpi/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "pyrocko/chimer",
        "url": "https://github.com/pyrocko/chimer",
        "description": "Earthquake Moment Magnitude Calculation from Ground Motions",
        "stars": 8,
        "forks": 2,
        "readme": "# Chimer\n\n*Earthquake Moment Magnitude Estimation from Peak Ground Motion*\n\n[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)\n![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)\n[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://pre-commit.com/)\n[![DOI](https://zenodo.org/badge/852235156.svg)](https://doi.org/10.5281/zenodo.14217498)\n\n\n*Chimer* is a framework for earthquake moment magnitude estimation from observed peak ground motions (PGM). It builds on top of `pyrocko.gf` Green's function data bases for forward modelling seismic sources and waveforms.\n\n![image](https://github.com/user-attachments/assets/196406da-4127-4aef-8b85-125109567c8a)\n\nKey features include:\n\n* Creation of synthetic ground motion databases\n* Caluclation of statistical peak acceleration, velocity and displacement\n* Forward modelling of PGM\n\n![image](https://github.com/user-attachments/assets/f4998004-a0e8-4d76-942c-bf48baa67e81)\n\n\n## Example\n\n```python\nfrom chimer.magnitude_store import PeakAmplitudesBase, PeakAmplitudesStore\n\nfrom pyrocko import gf\n\nKM = 1e3\n\nengine = gf.LocalEngine(use_config=True)\n\npeak_amplitudes = PeakAmplitudesBase(\n        gf_store_id=store_id,\n        quantity=\"displacement\",\n    )\n\nPeakAmplitudesStore.set_engine(engine)\nstore = PeakAmplitudesStore.from_selector(peak_amplitudes)\n\nawait store.compute_site_amplitudes(source_depth=2 * KM, reference_magnitude=1.0)\nawait store.find_moment_magnitude(\n    source_depth=2 * KM,\n    distance=10 * KM,\n    observed_amplitude=0.0001,\n)\n```\n\n## Installation\n\nSimple installation from GitHub.\n\n```sh\npip install git+https://github.com/pyrocko/chimer\n```\n\n## Citation\n\nPlease cite chimer as:\n\n> Torsten Dahm, Daniela Kühn, Simone Cesca, Marius Paul Isken, Sebastian Heimann, Earthquake Moment Magnitudes from Peak Ground Displacements and Synthetic Green's Functions, Seismica, 2024, *submitted*\n\n## License\n\nContribution and merge requests by the community are welcome!\n\nQseek was written by Marius Paul Isken and is licensed under the GNU GENERAL PUBLIC LICENSE v3.\n",
        "createdAt": "2024-09-04T13:12:07.000Z",
        "updatedAt": "2025-10-01T06:07:54.000Z",
        "language": "Python",
        "homepage": "https://pyrocko.org",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.14217498",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.14217498",
            "dataCite": "10.5281/zenodo.14217498",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/pyrocko/chimer/main/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.14217498",
            "title": "pyrocko/chimer: v0.1.1",
            "journal": "Zenodo",
            "dateReleased": "2024-11-25T00:00:00.000Z",
            "abstract": "initial release",
            "citationsArray": []
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "lclgithublcl/homework-for-Theoretical-Seismology",
        "url": "https://github.com/lclgithublcl/homework-for-Theoretical-Seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# homework-for-Theoretical-Seismology\n# homework-for-Theoretical-Seismology\n",
        "createdAt": "2023-04-16T13:19:13.000Z",
        "updatedAt": "2023-04-16T13:34:57.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/lclgithublcl/homework-for-Theoretical-Seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "hottahd/seismology",
        "url": "https://github.com/hottahd/seismology",
        "description": "日震学の勉強",
        "stars": 0,
        "forks": 0,
        "readme": "# seismology\n日震学の勉強\n\n## 目的\n極域ミッションに向けた日震学の勉強メモ",
        "createdAt": "2021-11-21T04:18:25.000Z",
        "updatedAt": "2021-12-10T00:30:46.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/hottahd/seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "SeisPider/ULVZ-Detector",
        "url": "https://github.com/SeisPider/ULVZ-Detector",
        "description": "class project for computational seismology, aiming at detecting ULVZ",
        "stars": 0,
        "forks": 0,
        "readme": "![status: active](https://img.shields.io/badge/status-active-red.svg)\n\nThis project is under developing.\n\n## ULVZ-Detector\n\nComputational seismology class project aiming at developing tools needed in \ndetecting ULVZ from multiple seismic phases\n\n## Before you begin\n\n1.  Download this repository\n\n2.  Configure your local computer environments. This repository requires\n    - [SOD](http://www.seis.sc.edu/sod/)\n\t- python 3.6\n\n## Contributing changes\n\n* We'd love to accept all issues or PRs to complements these tools.\n\n## Licensing\n\n* See [LICENSE](LICENSE)\n",
        "createdAt": "2018-05-20T14:56:45.000Z",
        "updatedAt": "2018-08-22T02:05:47.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/SeisPider/ULVZ-Detector/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "meskalito666/seismology_bot",
        "url": "https://github.com/meskalito666/seismology_bot",
        "description": "seismic activity monitoring",
        "stars": 1,
        "forks": 0,
        "readme": "",
        "createdAt": "2023-02-11T17:26:55.000Z",
        "updatedAt": "2025-04-01T10:42:44.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "eqcorrscan/EQcorrscan",
        "url": "https://github.com/eqcorrscan/EQcorrscan",
        "description": "Earthquake detection and analysis in Python.",
        "stars": 180,
        "forks": 89,
        "readme": "# EQcorrscan\n## A python package for the detection and analysis of repeating and near-repeating earthquakes.\n\n## Citation:\nWe have a manuscript on the development of EQcorrscan, if you make use of EQcorrscan please cite the following paper:\n\nChamberlain, C. J., Hopp, C. J., Boese, C. M., Warren-Smith, E., Chambers, D., Chu, S. X., Michailos, K., Townend, J., [EQcorrscan: Repeating and near-repeating earthquake detection and analysis in Python.](https://pubs.geoscienceworld.org/ssa/srl/article/89/1/173/524875/eqcorrscan-repeating-and-near-repeating-earthquake) Seismological Research Letters *2017*\n\nIf you want to you should also cite the version number:\n[![DOI](https://zenodo.org/badge/35918157.svg)](https://zenodo.org/badge/latestdoi/35918157)\n\n# Installation\n\nThe easiest way to install EQcorrscan is through anaconda:\n[![Anaconda-Server Badge](https://anaconda.org/conda-forge/eqcorrscan/badges/installer/conda.svg)](https://conda.anaconda.org/conda-forge)\n\nInstructions for installing EQcorrscan and the required dependency, fftw\nare linked from the\n[docs](http://eqcorrscan.readthedocs.io/en/latest/intro.html#installation)\n\n\n# Updates\n\nIf you want to be kept informed about releases, bug-tracking and enhancements\nwithout having to keep looking on github, subscribe to our [google group](https://groups.google.com/forum/#!forum/eqcorrscan-users).\n\n# Documentation\n\nThe full documentation for this package can be found here:\n[Docs](http://eqcorrscan.readthedocs.org/en/latest/?badge=latest).\nAny errors including typos and just missing bits can either be fixed by you,\nor flagged in the issues tab here.  We host our docs on readthedocs, which\nuses sphinx to scrape the docstrings in the codes, so it is simple to\nmatch the docs to the codes and change the docstrings.\n\n# Contributing\n\nPlease fork this project and work on it there then create a pull request to\nmerge back to this main repository.  Please create a branch from *develop*.\n\nWhen you make changes please run the tests in the test directory to ensure\neverything merges with minimum effort.  If there is not yet a test to cope\nwith your changes then please write one.\n\nPlease document your functions following the other documentation within the\nfunctions, these doc-scripts will then be built into the main documentation\nusing Sphinx.\n\n# Functionality\n\nThis package contains routines to enable the user to conduct matched-filter earthquake\ndetections using [obspy](https://github.com/obspy/obspy/wiki) bindings when reading\nand writing seismic data, as well as subspace detection, brightness source-scanning,\nrelative moment calculation using singular-value decomposition,\nand correlation pick-adjustment for similar events.\n\nAlso within this package are:\n* Clustering routines for seismic data;\n* Peak finding algorithm (basic, but appropriate for noisy data);\n* Automatic amplitude picker for local magnitude scale;\n* Obspy.core.event integration, which opens up lots of other functions (Seishub, hypoDDpy etc.);\n* Stacking routines including phase-weighted stacking based on Thurber at al. (2014);\n* Brightness based template creation based on the work of Frank et al. (2014);\n* Singular Value Decomposition derived magnitude calculations based on Rubinstein & Ellsworth (2010).\n\nThe code-base has grown to be quite large - it is probably worth\nhaving a look at the docs to check what functions we have.\nWe are writing a series of tutorials included on the EQcorrscan API\nto highlight key functions.\n\n*A note on correlation precision*\n*EQcorrscan* computes normalised cross-correlations in the frequency-domain using the\n[fftw](www.fftw.org) (Fastest Fourier Transform in the West).  Internally\nthe C routines enforce double-precision (64-Bit floating point numbers)\nfor all aspects of the cross-correlations (despite requiring 32-Bit float\ninput and output). Results in testing are accurate to within ~0.0001 of\ntime-domain cross-correlation results.\n\n# Test status\nNote that tests for travis and appveyor are run daily on master as cron jobs, and may reflect time-out issues.\n\n| Service tests | Badge |\n|---------------|-------|\n| CI checks | ![test](https://github.com/eqcorrscan/EQcorrscan/workflows/test/badge.svg)\n| Code coverage | [![codecov](https://codecov.io/gh/eqcorrscan/EQcorrscan/branch/master/graph/badge.svg)](https://codecov.io/gh/eqcorrscan/EQcorrscan) \n\n# Licence\n\nThis package is written  and maintained by the EQcorrscan developers,\nand is distributed under the LGPL GNU License, \nCopyright EQcorrscan developers 2018.\n\n\n# Funding\n\n![RCET](eqcorrscan/doc/RCET_logo_transparent.png)\n\nContinued development of the EQcorrscan package is directly supported by the \n[RCET](https://www.rcet.science/), Rapid Characterisation of Earthquakes and Tsunami\nprogramme funded by the New Zealand Ministry of Business, Innovation and Employment\nEndeavour fund.\n\nDevelopment is indirectly funded by grants from [Toku Tū Ake: EQC](https://www.eqc.govt.nz/)\nand a [Rutherford Discovery Fellowship](https://www.royalsociety.org.nz/what-we-do/funds-and-opportunities/rutherford-discovery-fellowships/rutherford-discovery-fellowship-recipients/calum-chamberlain/).\n",
        "createdAt": "2015-05-20T01:22:44.000Z",
        "updatedAt": "2025-12-05T10:23:51.000Z",
        "language": "Python",
        "homepage": "https://eqcorrscan.readthedocs.io/en/latest/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/eqcorrscan/EQcorrscan/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "cangyeone/seismological-ai-tools",
        "url": "https://github.com/cangyeone/seismological-ai-tools",
        "description": "Seismology AI Tool",
        "stars": 51,
        "forks": 10,
        "readme": "## Seismology AI Tool\n\n### Introduction\nThis is an open-source tool for seismological data processing and analysis, including phase picking, polarization, and dispersion extraction.\n\n- We have open-sourced the 100Hz model for the China region, with some models trained on the CSNCD dataset. \n- In the \"seismic-event-detection\" directory of this project, if the PS pick-up rate within 800 kilometers on your dataset is less than 75%, or if the PS pick-up error is greater than 300ms or 350ms respectively, please submit the BUG here or contact cangye@hotmail.com. \n- Among the currently open-sourced models, the picking accuracy of PgSgPnSn four-phase seismic waves is highest.\n- Currently, picker.py defaults to outputting initial movements, including their quality and requires using the onnxruntime library; it was also trained using a nationwide fixed network.\n\nMy telegram:\n![telegram](qr.jpg)\n\n### Software Architecture\nThe software is entirely built on Python. Each project is independent and relies on:\n- obspy: for data reading\n- PyTorch: for deep learning\n- OpenCV: for image processing\n  \n### Installation Tutorial\n1. It is recommended to install the latest version of Anaconda.\n2. Other libraries can be installed using pip.\n3. For deep learning libraries, it is suggested to use conda for basic environment installation process.\nFor installation guidance, please refer to the article [Python Environment and Usage Issues - Such Articles - Zhihu](https://zhuanlan.zhihu.com/p/414300182).\n\n### Instructions for Use\nPlease refer to the content in each respective folder.\n\n#### Contribution Participation\nContributions can be made by contacting cangye@Hotmail.com.\n\n#### License \nGPLv3\n",
        "createdAt": "2022-07-29T08:49:04.000Z",
        "updatedAt": "2025-11-27T05:49:34.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/cangyeone/seismological-ai-tools/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "AEQC/AEQC-Tools",
        "url": "https://github.com/AEQC/AEQC-Tools",
        "description": "AEQC tools for retrieval and completion of seismological waveforms databases from heterogeneous networks",
        "stars": 2,
        "forks": 0,
        "readme": "# AEQC-Tools\nAEQC tools for retrieval and completion of seismological waveforms databases from heterogeneous networks.\n\n-- Licensing --\n\nThis program is free software; you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation; either version 2 of the License, or (at\nyour option) any later version; please keep the name of original author\nin the sources.\n\nThis program is distributed in the hope that it will be useful, but\nWITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\nGeneral Public License (GNU-GPL) for more details.  The GNU-GPL and\nfurther information can be found here: http://www.gnu.org/\n\n",
        "createdAt": "2015-03-26T10:49:46.000Z",
        "updatedAt": "2015-12-01T19:46:15.000Z",
        "language": "Shell",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/AEQC/AEQC-Tools/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "jhsa26/Seis_pacakge",
        "url": "https://github.com/jhsa26/Seis_pacakge",
        "description": "this repository is used to store source codes which are popular in the seismology",
        "stars": 0,
        "forks": 0,
        "readme": "# Seis_pacakge\nthis repository is used to store source codes which are popular in the seismology\n",
        "createdAt": "2017-01-06T13:05:03.000Z",
        "updatedAt": "2017-01-06T13:05:03.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/jhsa26/Seis_pacakge/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "lystom/WMSAN",
        "url": "https://github.com/lystom/WMSAN",
        "description": "User-friendly Python package to help seismologists model their observations through maps of ambient noise sources from WAVEWATCHIII hindcast outputs",
        "stars": 11,
        "forks": 1,
        "readme": "# WMSAN Python Package\n[![DOI](https://zenodo.org/badge/793568997.svg)](https://zenodo.org/doi/10.5281/zenodo.13374110)\n\n## Description\nThis package is built to help computation of seismic ambient noise source maps and other products based on the WAVEWATCHIII hindcast output.\n\n## Documentation\nA detailed documentation is available [on this page](https://tomasetl.gricad-pages.univ-grenoble-alpes.fr/ww3-source-maps/). \n\n## Contents\n```\nww3-source-maps/\n|-- LICENSE\n|-- pyproject.toml\n|-- README.md\n|-- mkdocs.yml\n|-- docs/\n|-- site/\n|-- src/\n│   └── wmsan/\n│       ├── readWW31.py\n│       ├── read_hs_p2l.py\n│       ├── subfunctions_body_waves.py\n│       ├── subfunctions_rayleigh_waves.py\n│       └── synthetics.py\n│       └── wmsan_to_noisi.py\n│       └── temporal_variation.py\n│       └── synthetic_CCF.ipynb\n│\n|-- notebooks/\n|   └── body_waves/\n│       ├── amplification_coeff.ipynb\n│       └── microseismic_sources.ipynb\n│       └── synthetic_CCF.ipynb\n│       └── temporal_variations.ipynb\n│   └── rayleigh_waves/\n│       ├── amplification_coeff.ipynb\n│       ├── microseismic_sources.ipynb\n│       ├── spectrograms.ipynb\n│       ├── rayleigh_source.ipynb\n│       └── synthetic_CCF.ipynb\n│       └── wmsan_to_noisi.ipynb\n│       └── temporal_variations.ipynb\n|-- data/\n│   ├── C.nc\n│   ├── cP.nc\n│   ├── cS.nc\n│   ├── longuet_higgins.txt\n│   ├── stations_pair.txt\n│   └── ww3.07121700.dpt\n```\n- src/ : contains all Python scripts and subfunctions.\n- notebooks/ : contains Jupyter Notebooks with detailed examples on how to use this package. Rayleigh waves and body waves are separated.\n- data/: contains additional files used in computation.\n\n## Installation\n\n### PyPI\n\nThe package is available on [PyPI](https://pypi.org/).\n\n#### Create an environment and install\n\n- if you use [Conda](https://docs.anaconda.com/free/miniconda/#quick-command-line-install) environments:\n    ```\n    conda create --name wmsan python=3.12\n    conda activate wmsan\n    conda install pip pyproj\n    python3 -m pip install wmsan\n    ```\n\n    to deactivate your environment:\n\n    ```\n    conda deactivate\n    ```\n- otherwise\n    ```\n    python3 -m venv venv\n    source venv/bin/activate\n    python3 -m pip install wmsan\n    ```\n    to deactivate your environment:\n    ```\n    deactivate\n    ```\n\n### From Source\n1. Clone the repository \n\n    ``` \n    cd path_to_your_wmsan_directory/\n    git clone https://gricad-gitlab.univ-grenoble-alpes.fr/tomasetl/ww3-source-maps.git \n    cd ww3-source-maps/\n    ```\n\n2. Create an environment and install \n\n- if you use [Conda](https://docs.anaconda.com/free/miniconda/#quick-command-line-install) environments:\n\n    ```\n    conda create --name wmsan python=3.12\n    conda activate wmsan\n    conda install pip pyproj\n    pip install .\n    ```\n    to deactivate your environment:\n    ```\n    conda deactivate\n    ```\n\n- otherwise\n\n    ```\n    python3 -m venv venv\n    source venv/bin/activate\n    python3 -m pip install .\n    ```\n    to deactivate your environment:\n    ```\n    deactivate\n    ```\n\n### Dependencies\n\n- [numpy](https://numpy.org/doc/stable/)\n- [matplotlib](https://matplotlib.org/stable/)\n- [cartopy](https://scitools.org.uk/cartopy/docs/latest/index.html)\n- [xarray](https://docs.xarray.dev/en/stable/)\n- [netCDF4](https://unidata.github.io/netcdf4-python/)\n- [obspy](https://docs.obspy.org/)\n- [datetime](https://docs.python.org/3/library/datetime.html)\n- [scipy](https://scipy.org/)\n- [pandas](https://pandas.pydata.org/pandas-docs/version/2.1.4/index.html)\n- [dask](https://www.dask.org/)\n- [ipykernel](https://pypi.org/project/ipykernel/)\n- [pyproj](https://pyproj4.github.io/pyproj/stable/)\n- [h5py](https://docs.h5py.org/en/stable/)\n- [tqdm](https://tqdm.github.io/)\n- [notebook](https://jupyter-notebook.readthedocs.io/en/stable/)\n- [dask](https://www.dask.org/) \n\n## Where should I start ?\n\n![Table representing the differrent paths to Jupyter Notebooks examples and where to find what you wish to compute.](https://gricad-gitlab.univ-grenoble-alpes.fr/tomasetl/ww3-source-maps/-/raw/main/sumup.png) \n\n## Architecture of WMSAN Python Package\n\n![Scheme showing the different codes and Notebooks present in this repository and how they connect.](https://gricad-gitlab.univ-grenoble-alpes.fr/tomasetl/ww3-source-maps/-/raw/main/package_archi.png)\n\n## How to Cite WMSAN ?\n\n- [Tomasetto, L., Boué, P., Ardhuin, F., Stutzman, E., Xu, Z., De Plaen, R., & Stehly, L. (2024). WMSAN Python Package: From Oceanic Forcing to Synthetic Cross-correlations of Microseismic Noise. EathArXiv.](https://doi.org/10.26443/seismica.v4i1.1483)",
        "createdAt": "2024-04-29T13:19:15.000Z",
        "updatedAt": "2025-11-21T10:36:09.000Z",
        "language": "HTML",
        "homepage": "https://tomasetl.gricad-pages.univ-grenoble-alpes.fr/ww3-source-maps/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.13374110",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.13374110",
            "dataCite": "10.5281/zenodo.13374110",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/lystom/WMSAN/main/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.13374110",
            "title": "lystom/WMSAN: v2024.1.3",
            "journal": "Zenodo",
            "dateReleased": "2024-12-18T00:00:00.000Z",
            "abstract": "[2024.1.3] - 2024-12-18\n\nAdded\n\nChanged\n\nfix dependency to dask in pyproject.toml and README",
            "citationsArray": []
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "mjy-888/seismology",
        "url": "https://github.com/mjy-888/seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2025-02-01T07:50:46.000Z",
        "updatedAt": "2025-02-01T07:50:46.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "js1019/NormalModes",
        "url": "https://github.com/js1019/NormalModes",
        "description": "Normal Modes at Planetary Scales",
        "stars": 32,
        "forks": 7,
        "readme": "# Normal Modes at Planetary Scales\n<img src=\"https://img.shields.io/github/issues/js1019/NormalModes.svg\"><img src=\"https://img.shields.io/github/languages/code-size/js1019/NormalModes.svg\"> <img src=\"https://img.shields.io/github/forks/js1019/NormalModes.svg\"> <img src=\"https://img.shields.io/github/stars/js1019/NormalModes.svg\"> <img src=\"https://img.shields.io/github/license/js1019/NormalModes.svg\">\n\n\nThis repository applied a combination of several highly parallel algorithms to compute the planetary interior normal modes. \nThe elastic-gravitational system is discretized using the Continuous Galerkin mixed finite element method. \nA Lanczos approach with polynomial filtering is utilized for solving the resulting generalized eigenvalue problems. \n\nSelf-gravitation and rotation are included in the branch \"**rotation**\".  \n\n------\n**News**: This work has been selected to serve as the benchmark for [the student cluster competition](http://www.studentclustercompetition.us/) reproducibility challenge at [SC'19](https://sc19.supercomputing.org). \nPlease see [the official announcement](https://sc19.supercomputing.org/2019/04/19/from-sc-papers-to-student-cluster-competition-benchmarks-joining-forces-to-promote-reproducibility-in-hpc/) \nand [teams named](https://sc19.supercomputing.org/2019/06/10/teams-named-for-sc19-student-cluster-competition/). \nIf you look for the code for the competition, please see SCC19 version in the release. \n\n## Outcome of this work\nMany different modes are expected. Please see below for selected normal modes computed from a standard spherically symmetric Earth model.\n<p align=\"center\">\n<img src=\"figs/PREM2M_J2_0S2.png\" width=\"200\"/> <img src=\"figs/PREM2M_J2_0S3.png\" width=\"200\"/> <img src=\"figs/PREM2M_J2_0S5.png\" width=\"200\"/> <img src=\"figs/PREM2M_J2_0S7.png\" width=\"200\"/> \n</p>\n<p align=\"center\">\n<img src=\"figs/PREM2M_J2_1S3.png\" width=\"200\"/> <img src=\"figs/PREM2M_J2_1S4.png\" width=\"200\"/> <img src=\"figs/PREM2M_J2_1S6.png\" width=\"200\"/> <img src=\"figs/PREM2M_J2_1S9.png\" width=\"200\"/> \n</p>\n<p align=\"center\">\n<img src=\"figs/PREM2M_J2_2S3.png\" width=\"200\"/> <img src=\"figs/PREM2M_J2_3S3.png\" width=\"200\"/> <img src=\"figs/PREM2M_J2_4S3.png\" width=\"200\"/> <img src=\"figs/PREM2M_J2_5S3.png\" width=\"200\"/> \n</p>\n<p align=\"center\">\n<img src=\"figs/PREM2M_J2_0T2.png\" width=\"200\"/> <img src=\"figs/PREM2M_J2_1T1.png\" width=\"200\"/> <img src=\"figs/PREM2M_J2_1T2.png\" width=\"200\"/> <img src=\"figs/PREM2M_J2_2T4.png\" width=\"200\"/> \n</p>\n\n\n## A note on the design of this application \nIt is not straightforward to compute the normal modes of a fully heterogeneous planet. \nWe have to deal with multiphysics at large scales, \nwhich computationally involves many techniques in numerical linear algebra, finite element method, computer graphics, high-performance computing, etc. \nWe do not expect that we can obtain everything via a single click. \nHowever, to solve this complicated problem, we divide the original one into several smaller subproblems. \nIndeed, via solving each subproblem separately, it actually simplifies our work significantly. \nWe develop three repositories to provide a solid solution for this application:\n+ [PlanetaryModels](https://github.com/js1019/PlanetaryModels): It is a planet model builder and provides scripts to generate planetary models on fully unstructured tetrahedral meshes. It provides a simple and flexible way to create a 3D body with almost arbitrary exterior and interior shapes.   \n+ [pEVSL](https://github.com/eigs/pEVSL): It is a parallel eigenvalue slicing library. It provides general algebraic algorithms to solve large-scale generalized Hermitian extreme and interior eigenvalue problems. \nYou may use [the forked pEVSL version](https://github.com/js1019/pEVSL) \nfor this application, since it contains several modifications for your convenience. \n+ [NormalModes](https://github.com/js1019/NormalModes): It builds up the matrices from the elastic-gravitational system using finite element method and utilizes several external libraries, including [pEVSL](https://github.com/eigs/pEVSL), to solve for the normal modes of the planetary model generated by [PlanetaryModels](https://github.com/js1019/PlanetaryModels). \n+ [exafmm](https://github.com/js1019/exafmm-beta): It computes the gravitational potentials and fields using fast multiple method. \n\nThe separated repositories also provide flexibility to extend our work for other purposes. \nPlease let us know if you plan to utilize what we have to your work.  \n\n\n\n## How to run this application? \nPlease follow INSTALL.md to install the application. \nPlease check the demos/global_conf, which shows an **extremely simple** parameter setting. \nSince the problem is deterministic, there are only a few parameters that are needed to compute the normal modes. \nYou can then obtain _all_ the eigenpairs in the prescribed frequency interval. \nThe values of eigenfrequencies will be shown at the end of the computation as well as their relative errors, i.e., ||Ax-&lambda;Bx||/||&lambda;||, which is typically around **10^-13**. \nThe eigenfunctions will be saved in the binary format. Please check the README.md under demos/ for more details. \n\n**Tips**: Please always check the performance and scalability before running large-scale applications. \n\n**Visualization**: You can use scripts in [PlanetaryModels](https://github.com/js1019/PlanetaryModels) and [Paraview](https://www.paraview.org/) to visualize your results. Here, we show animations created by Paraview: [0S2](https://www.youtube.com/watch?v=DDfGHmqCMN0&list=PLUp2thaj3ruEVTLWazoRfqRK53t4hbYel&index=5&t=0s), \n[0T2](https://www.youtube.com/watch?v=hxeDz8ncNH4), \n[3S9](https://www.youtube.com/watch?v=YR6N3AOTwoU&index=7&list=PLUp2thaj3ruEVTLWazoRfqRK53t4hbYel&t=0s) and\n[1T11](https://www.youtube.com/watch?v=XWY_dNAYAjE&index=6&list=PLUp2thaj3ruEVTLWazoRfqRK53t4hbYel&t=0s). 3S9 and 1T11 are illustrated below. \n\n<p align=\"center\">\n<img src=\"figs/PREM3S9.gif\" width=\"400\"/> <img src=\"figs/PREM1T11.gif\" width=\"400\"/> \n</p>\n\nYou can also use the [*NMPostProcess*](https://github.com/harrymd/NMPostProcess) library to visualize modes and automatically identify them.\n\n## Reference\n+ [_**Theory, discretization and validation**_]: Jia Shi, Ruipeng Li, Yuanzhe Xi, Yousef Saad, and Maarten V. de Hoop. \"A non-perturbative approach to computing seismic normal modes in rotating planets.\" Journal of Scientific Computing, 91:67, 2022, [the paper link](https://doi.org/10.1007/s10915-022-01836-5).\n+ [_**Reproducibility, summary and discussion of the student cluster competition results**_]: Jia Shi, Ruipeng Li, Yuanzhe Xi, Yousef Saad, and Maarten V. de Hoop. \"Planetary normal mode computation: Parallel algorithms, performance, and reproducibility.\" IEEE Transactions on Parallel and Distributed Systems, 32, no. 11 (2021): 2609-2622, [the paper link](https://ieeexplore.ieee.org/abstract/document/9319555). \n+ [_**Parallel performace and algorithms**_]: Jia Shi, Ruipeng Li, Yuanzhe Xi, Yousef Saad, and Maarten V. de Hoop. \"Computing planetary interior normal modes with a highly parallel polynomial filtering eigensolver.\" In SC18: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 894-906. IEEE, 2018, [the paper link](https://dl.acm.org/citation.cfm?id=3291751).\n\n\n## Contact \nPlease report issues under this repository. Contributions are welcome. \n",
        "createdAt": "2019-02-14T21:55:02.000Z",
        "updatedAt": "2023-12-26T00:37:26.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/js1019/NormalModes/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "dariagomez/UrbanSeismology_Infrasound_CGEO",
        "url": "https://github.com/dariagomez/UrbanSeismology_Infrasound_CGEO",
        "description": "This GitHub repository presents a comprehensive urban seismology project aimed at detecting and analyzing anthropogenic activities in urban areas. Leveraging the power of spectrometers and ObsPy, a Python library for seismology, this project provides valuable insights into the impact of human-induced vibrations on the urban environment.",
        "stars": 1,
        "forks": 0,
        "readme": "# UrbanSeismology_Infrasound_CGEO\nThis GitHub repository presents a comprehensive urban seismology project aimed at detecting and analyzing anthropogenic activities in urban areas. Leveraging the power of spectrometers and ObsPy, a Python library for seismology, this project provides insights into the impact of human-induced vibrations on the urban environment.\nProject was developed 2023 Summer in Queretaro, Mexico as part of a research programm for undergraduate students.\n\nClock code belongs to Thomas Lecocq SeismoRMS repository, with a number of modifications. https://github.com/ThomasLecocq/SeismoRMS\nSpectrogram code was developped by the owner of this repository, send questions to: daria.gomez5378@alumnos.udg.mx\n\nI only uploaded R95F0 AM station data as an example. This repository is expected to serve as an extra material or guide for future research in urban seismology for students, so different mseed files must work here!\nImages are part of our results. \n",
        "createdAt": "2023-07-04T22:35:50.000Z",
        "updatedAt": "2023-08-30T02:53:47.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/dariagomez/UrbanSeismology_Infrasound_CGEO/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "riikbasu/ETH_TinyQuakes",
        "url": "https://github.com/riikbasu/ETH_TinyQuakes",
        "description": "Repository containing codes for Seismology work in ETH Zurich",
        "stars": 0,
        "forks": 0,
        "readme": "# ETH_Seismology\nRepository containing codes for Seismology work in ETH Zurich\n",
        "createdAt": "2023-01-10T21:45:01.000Z",
        "updatedAt": "2023-01-12T22:15:31.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/riikbasu/ETH_TinyQuakes/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "esemsc-dbk24/Faultless-Seismology",
        "url": "https://github.com/esemsc-dbk24/Faultless-Seismology",
        "description": "Seismic Imaging Revision Assignment - Group Project",
        "stars": 0,
        "forks": 0,
        "readme": "# EDSML group project\n\n## Seismic imaging revision assignment\n\nThe purpose of this exercise is to provide you with a structured way to consolidate your understanding of seismic imaging. We have been through a lot of content quickly in 2.5 days and this exercise offers you an opportunity to think more deeply about these concepts to help you prepare for the assessed quiz next week (on Fri 24th Jan). You could rush through the activity quickly in a couple of hours just copying and pasting from the notes- but I encourage you to use this time to really make sure you understand things. Talk things through with your group as well as the GTAs who will host drop-in sessions. I will provide you with feedback on your submissions by Tuesday 21st Jan, to help you prepare for the assessed quiz. At the bottom of this notebook you will find three example questions to show you the format of the assessed quiz. Please answer these questions and you will receive feedback on these too.\n\n## Materials\n\nYou are provided with the following materials:\n- a question note (EDSML_group_project 1);\n- Thebe_small2.sgy which you can find in the 'data' folder here**- https://imperiallondon-my.sharepoint.com/personal/psalah_ic_ac_uk/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fpsalah%5Fic%5Fac%5Fuk%2FDocuments%2Fdata%2DBecky%2Dexercises&ga=1 \n\n## Deliverables\n written answers to questions in a Jupyter notebook (please also include figures from the lecture notes or other referenced sources), coding to view seismic and calculate attributes\nAssignment set: 10 am 16th Jan\nAssignment to be submitted: 12 noon 17th Jan\n\n\n#### Submission process:\n\nAs part of your group project, it's crucial that we maintain an organized and efficient submission process. To ensure this, please adhere to the following guidelines:\n\n- One Submission Per Group: Each group is required to submit only one answer. It's important that you collaborate and consolidate your work within your group to finalize a single, representative notebook.\n\n- Please include your group name, your answers in th email and send it to:\n\nrebecca.bell@imperial.ac.uk\n\npsalah@imperial.ac.uk\n\n- Feedback Process: After the submission deadline, Becky will review the submitted notebooks and provide feedback. It's essential to submit by the deadline to receive timely feedback.\n\n\n\n#### Ancillary files\n\nIf your code or answers depend on additional files, such as images, photos, or small data files, then make sure to include these in your submission as well. There is no need to include the Thebe_small2 data file\n",
        "createdAt": "2025-01-16T10:20:03.000Z",
        "updatedAt": "2025-06-05T11:19:47.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/esemsc-dbk24/Faultless-Seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "JuliaSeismo/SeisNoise.jl",
        "url": "https://github.com/JuliaSeismo/SeisNoise.jl",
        "description": "Ambient Noise Cross-Correlation in Julia",
        "stars": 58,
        "forks": 19,
        "readme": "# SeisNoise.jl :sound: :earth_americas:\nSeisNoise.jl is designed for fast and easy ambient noise cross-correlation on the CPU and GPU in Julia.\n\n| **Documentation**                       | **Build Status**              | **Coverage** | **Chat**   |\n|:---------------------------------------:|:-----------------------------------------:|:---------------------:|:---------------------:|\n| [![](https://img.shields.io/badge/docs-latest-blue.svg)](https://tclements.github.io/SeisNoise.jl/latest) | [![Build Status](https://github.com/tclements/SeisNoise.jl/actions/workflows/ci.yml/badge.svg)](https://github.com/tclements/SeisNoise.jl/actions/workflows/ci.yml) |  [![Coverage Status](https://codecov.io/gh/tclements/SeisNoise.jl/branch/master/graph/badge.svg?token=MCpg8PlToL)](https://codecov.io/gh/tclements/SeisNoise.jl) | [![](https://img.shields.io/badge/chat-on%20slack-yellow.svg)](https://slackinvite.julialang.org/) |\n\n## Installation\nYou can install the latest version of SeisNoise using the Julia package manager (Press `]` to enter `pkg`).\nFrom the Julia command prompt:\n\n```julia\njulia>]\n(@v1.9) pkg> add SeisNoise\n```\n\nOr, equivalently, via the `Pkg` API:\n\n```julia\njulia> import Pkg; Pkg.add(\"SeisNoise\")\n```\n\nWe recommend using the latest version of SeisNoise by updating with the Julia package manager:\n\n```julia\n(@v1.9) pkg> update SeisNoise\n```\n\n## Package Features\n\n![flow](/docs/src/assets/SeisNoise-DataFlow.jpg)\n\n  - Built upon [SeisBase](https://juliaseismo.github.io/SeisBase.jl/dev/) for easy and fast I/O.\n  - Custom structures for storing Raw Data, Fourier Transforms of data, and cross-correlations\n  - CPU/GPU compatible functions for cross-correlation.\n  - Methods for [*dv/v* measurements](https://github.com/tclements/SeisDvv.jl).\n  - Coming soon: Dispersion analysis.\n\nCheck out the SeisNoise [GPU tutorial on NextJournal](https://nextjournal.com/thclements/seisnoisejl-gpu-computing-tutorial)!\n\n## SeisNoise Cross-Correlation Example\nOnce you have installed the package you can type `using SeisNoise` to start\ncross-correlating. SeisNoise uses a functional syntax to implement cross-correlation. For example\n\n```Julia\nusing SeisNoise, SeisIO, Plots\nfs = 40. # sampling frequency in Hz\nfreqmin,freqmax = 0.1,0.2 # min and max frequencies\ncc_step, cc_len = 450, 1800 # corrleation step and length in S\nmaxlag = 60. # maximum lag time in correlation\ns = \"2019-02-03\"\nt = \"2019-02-04\"\nS1 = get_data(\"FDSN\",\"CI.SDD..BHZ\",src=\"SCEDC\",s=s,t=t)\nS2 = get_data(\"FDSN\",\"CI.PER..BHZ\",src=\"SCEDC\",s=s,t=t)\nprocess_raw!(S1,fs)\nprocess_raw!(S2,fs)\nR = RawData.([S1,S2],cc_len,cc_step)\ndetrend!.(R)\ntaper!.(R)\nbandpass!.(R,freqmin,freqmax,zerophase=true)\nFFT = rfft.(R)\nwhiten!.(FFT,freqmin,freqmax)\nC = correlate(FFT[1],FFT[2],maxlag)\nclean_up!(C,freqmin,freqmax)\nabs_max!(C)\nplot(C)\n```\nwill produce this figure:\n\n![plot1](/docs/src/assets/xcorr-example.png)\n\n## Cross-correlation on the GPU\n\nSeisNoise can process data and compute cross-correlations on the GPU with CUDA. The [JuliaGPU](https://github.com/JuliaGPU) suite provides a high-level interface for CUDA programming through the CUDA.jl package. CUDA.jl provides an the `CuArray` type for storing data on the GPU. Data in SeisNoise structures (`R.x`, `F.fft`, and `C.corr` fields, for `RawData`, `FFTData`, and `CorrData`, respectively) can move between an `Array` on the CPU to a `CuArray` on the GPU using the `gpu` and `cpu` functions, as shown below.   \n\n> :warning: Only **Nvidia** GPUs are suported at the moment. Hold in there for AMD/OpenCL support...\n\n```julia\n# create raw data and send to GPU\nR = RawData(S1, cc_len, cc_step) |> gpu\nR.x\n72000×188 CUDA.CuArray{Float32,2,Nothing}\n\n# send data back to the CPU\nR = R |> cpu\nR.x\n72000×188 Array{Float32,2}\n```\n\nAll basic processing remains the same on the GPU. Here is a complete cross-correlation routine on the GPU:\n\n```julia\n# send data to GPU\nR1 = RawData(S1, cc_len, cc_step) |> gpu\nR2 = RawData(S2, cc_len, cc_step) |> gpu\nR = [R1,R2]\n\n# preprocess on the GPU\ndetrend!.(R)\ntaper!.(R)\nbandpass!.(R,freqmin,freqmax,zerophase=true)\n\n# Real FFT on GPU\nFFT = rfft.(R)\nwhiten!.(FFT,freqmin,freqmax)\n\n# compute correlation and send to cpu\nC = correlate(FFT[1],FFT[2],maxlag) |> cpu\n```\n\n### Routines Implemented on the GPU\n\n![gpu times](/docs/src/assets/Fig2.jpg)\n\nProcessing times for a selection of routines on the GPU with Julia + GPU (white), Julia + CPU (black), and Python (grey). Currently these operations are implemented in SeisNoise on the GPU: \n\n\n- Preprocessing:\n  - `detrend`,`demean`, `taper`, `onebit`, `smooth`\n- Filtering:\n  - `bandpass`, `bandstop`, `lowpass`, `highpass`\n- Fourier Domain:\n  - `whiten`, `rfft`, `irfft`\n- Cross-correlation:\n  - `correlate`, `cross-coherence`, `deconvolution`\n- Post-processing:\n  - `stack`, `filter`s, etc..\n\n## Cite SeisNoise \nIf you use SeisNoise in your work, please star the package and cite our work [DOI: 10.1785/0220200192](https://doi.org/10.1785/0220200192): \n\n```bib\n@article{SeisNoise.jl-2020,\n  author = {Clements, Timothy and Denolle, Marine A.},\n  title = {SeisNoise.jl: Ambient Seismic Noise Cross Correlation on the CPU and GPU in Julia},\n  journal = {Seismological Research Letters},\n  year = {2020},\n  month = {09},\n  issn = {0895-0695},\n  doi = {10.1785/0220200192},\n  url = {https://doi.org/10.1785/0220200192},\n  eprint = {https://pubs.geoscienceworld.org/srl/article-pdf/doi/10.1785/0220200192/5156069/srl-2020192.1.pdf},\n}\n```\n\n## Contributing\nWe welcome folks interested in contributing to SeisNoise. Please [open an issue](https://github.com/JuliaSeismo/SeisNoise.jl/issues/new) to let us know about bug reports, new methods/code, and or feature requests/usage cases. If you would like to submit a pull request (PR), please include accompanying [tests](https://github.com/JuliaSeismo/SeisNoise.jl/tree/master/test).\n",
        "createdAt": "2019-04-05T23:27:02.000Z",
        "updatedAt": "2025-10-24T17:44:45.000Z",
        "language": "Julia",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/JuliaSeismo/SeisNoise.jl/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ghfbsd/cdseis",
        "url": "https://github.com/ghfbsd/cdseis",
        "description": "CDSEIS reads seismic data from SEED volumes and writes SAC (or AH) files for analysis.  Unlike other SEED-reading tools, the files are event-oriented.",
        "stars": 2,
        "forks": 0,
        "readme": "WHAT IT DOES\n============\n\nCDSEIS reads seismic data from SEED volumes and writes SAC (or AH) files for\nanalysis.  Unlike other SEED-reading tools, the files that it outputs are event\noriented, cut around the arrival times of specific (user-tailored) phases, and\nnamed using event-specific names.  The files have event and station information\npopulating the file headers. The result is a collection of traces that is\nimmediately available for analysis.\n\nTo select waveforms for analysis, CDSEIS uses a text index (log file) made by\nMAKELOG for each SEED volume (or collection).  The logs summarize the waveform\nstart and stop times and associates each waveform with an event.  The user\nspecifies various search parameters to identify the desired seismograms.  Once\nthe desired time windows are identified, they can then be read directly from the\nSEED volume.  The use of an index file results in greatly increased speed\ncompared to searching through the entire SEED volume to find the desired time\nseries.\n\nCDSEIS uses a command language (similar to PLOTXY, etc.) to decide which\nwaveforms to extract.  The basic idea is to define a number of search parameters\nthat are used to select which seismograms to read.  The defaults defined for\nmost of the parameters minimize user input.  CDSEIS reads the search commands\nand outputs either a list of seismograms matching the search criteria (`SCAN`\noption) or the seismograms themselves (`READ` option).\n\nHOW TO BUILD IT\n===============\n\n```\n./configure        ## run configuration - to get help, ./configure --help\nmake               ## makes cdseis, makelog, make_tt\nmake test          ## extracts SAC traces from a SEED volume for testing\nmake install       ## install programs in binary directory of choice\n```\n\nThe configure step tries to find the SAC libraries (for cdseis) and the\nBuland & Kennett tau-p routines (for make_tt).  If they are not available, or\nin an unexpected place, compilation will either fail (cdseis) or will produce\nno output (make_tt).  If a failure occurs, use\n```\n    ./configure LDFLAGS='-L<directory-with-libsacio.a>'\n```\nor\n```\n    ./configure SACAUX=<directory-with-SAC-aux-files>\n```\nto help the configure process find the SAC libraries.\n\nAfter installing CDSEIS, print the documentation to the screen by:\n```\n    man cdseis\n```\nor\n```\n    man makelog\n```\nTo get hard copy, use\n```\n    man -t cdseis > cdseis.ps\n```\nor\n```\n    groff -man -t cdseis.man > cdseis.ps\n```\nand view cdseis.ps with your favorite PostScript reader.  Use similar methods\nto get makelog documentation.\n\nHOW TO REMOVE IT\n================\n\nType\n```\nmake uninstall\n```\nto remove the programs and travel time data files.\n\nUSAGE\n=====\n\nThere are three commonly used programs in the src directory: makelog, make_tt\nand cdseis itself.\n\n1) MAKELOG reads a list of SEED volumes and makes an ASCII log file of all\nwaveform start and stop times, associating each waveform with an event.  The\nevent list is either taken from the SEED volume itself (if it contains one),\nor from an external catalog provided to the program.  Possible catalog formats\nare the Global CMT .ndk format or a file with one entry per line (see the\nfile test/events.dat for an example or read the program program for format\ndetails).\n\n2) CDSEIS reads the log file(s) created by MAKELOG, accepts a \nvariety of search parameters in an interactive mode, and either \ncreates a list of available waveforms (without reading data) or \nretrieves requested waveforms from the SEED volume (or a CD).  \n\n3) MAKE_TT makes up a set of approximate phase travel time tables from the\nKennett and Buland tau-p routines.  CDSEIS uses these tables to calculate\nexpected arrival times for extracting data around major phase arrivals.\nThe routine uses the subroutine interface to tau-p; you need to have a\nworking version of it to compile this program.\n\nThe programs can be compiled by typing: `make all`  \nThe programs can be tested by typing: `make test`  \nThe programs can be installed by typing: `make install`  \n\nSome example (approximate) travel time curves are given in files \nbeginning with `tt_`.  They will be installed in a library directory associated\nwith CDSEIS (see cdseis.in.scan for the location; when `make install' is done,\nyou'll get a report of the location too).\n\nYou will need to use MAKELOG to create a log file for new SEED volumes that you\nreceive.  MAKELOG is now fairly robust in dealing with SEED volumes, and is\nunlikely to fail, but it is possible that MAKELOG could crash on a subsequent\nSEED volume because of some peculiarity associated with new data.  After\nrunning MAKELOG always check the log file, makelog.log, for errors and warnings.\n\n> Originally, MAKELOG read an entire cdrom and made an ASCII log file of \nall waveform start and stop times, associating each waveform with \nan event.  On a SUN sparcstation this took about 1-2 hours for each \ncdrom.  This program also read all the station corrections and instrument\nresponse poles and zeros, and reported errors and warnings to standard\noutput when it does not recognize the format.  CONVERTLOG essentially tidied\nup the output from this program.  (This version of MAKELOG is in the src/cd\ndirectory for historical completeness.)\n\n> CONVERTLOG (in src/cd for historical completeness) reads a cdrom log created\nby the old, CD version of MAKELOG, removes trailing blanks to reduce the file\nsize, and assigns a unique event designation to each event.\n\n> Logs created by the old MAKELOG and CONVERTLOG for almost all CDs that were\never distributed by seismological agencies are supplied in the directory\nlogfiles, in files named files log*.  The first CD put out by the NEIC was\ndistributed with integer bytes in the order that is standard for DEC computers\nand has the number 5033 stamped on the front of the disk.  It was also\ndistributed using the UNIX convention and has the number 5054 stamped on the\nfront.  As far as I can tell these are identical except for the order of the\nbytes in each integer.  The log file 'log5033' works for either disk.  All\nsubsequet CD's use the UNIX convention.  CDSEIS reads 5054 correctly if the\nvariable 'byteswap' in the main program is set to '.false.' and will read 5033\nproperly if 'byteswap' is set to '.true.'.\n\n> Though they all may be overridden in the cdseis input file,\nyou may wish to change the default values of some of the parameters to suit\nyour particular needs.  The default values are all set in data statements in\nthe main program (cdseis.f).  In particular, expert programmers\nmight wish to change the default directories for the phase files, and catalog\nfiles, and the cdroms, which are in variables phsdir, logdir, and cddir.\n\nEXAMPLES\n========\n\nExample input for cdseis\n------------------------\n\n```\nCOMM This test file extracts events suitable for P receiver function analysis\nCOMM from the SEED volumes described in CDLV-RF.log.  All events between 30\nCOMM and 95 degrees with magnitudes between 5.8 and 10 are selected.  CDSEIS\nCOMM outputs traces in a time window 2 minutes before the P arrival and 3\nCOMM after it for each station.  Three-component seismograms are produced,\nCOMM labeled with the station name, event name, p, and channel name (*h[enz]),\nCOMM depending on the sample rate (between 1 and 100 sps).\nLDIR ./  \nPDIR ../tt  \nCDIR ./seed  \nODIR /tmp  \nRANG 30, 95  \nQMAG 5.8, 10.0  \nWIND -2 3  \nPHAS tt_p  \nSRAT 1 100  \nCOMP 1  \nFILE st ev p ch  \nLOGF CDLV-RF.log  \nSTAT  \nREAD  \nQUIT  \n```\n\nThe input file above illustrates a typical use of CDSEIS in a research\nproject to get the data from repository format (SEED) into an analysis format\n(SAC).  A list of the available seismograms could be obtained by replacing the\n`READ` command with the `SCAN` command.\n\n\n```\ncomm This extracts three-component seismograms starting before the P wave\ncomm arrival to 20 minutes after it, for an event on 28 Aug. 1985.  It\ncomm reads data from the CDROM labeled 5461.\ndmin 85 8 28 0 0\ndmax 85 8 28 23 59\ncdir /cdrom\nlogf log5461\nsrat 2 16\nphas tt_p\nwind -2 20\nfile ev ch\notyp sac\nwtyp n\nstat\ncomp 1\nread\nquit\n```\n\nThis gives 3 files for vertical, north and east components of intermediate-\nperiod seismograms from a deep-focus earthquake. The vertical\ncomponent seismograms can be displayed using 'read *.ihz' if you have\nSAC running.  Make sure CDROM 5461 is mounted at the directory\n/cdrom. You can get a list of the seismograms by replacing 'read' command by\nby 'scan'.\n\nExample input for makelog\n-------------------------\n\n```\n(cd DDIR ; ls *.seed | \\\n   makelog -cat cmt DDIR /usr/share/data/CMT/cmtdat ) > logDDIR\n```\n\nThis tells makelog to scan all of the SEED volumes in the directory DDIR ending\nwith the name `*.seed` and to find all of the events associated with data\ntraces in the SEED volume with earthquakes in the copy of the CMT catalog in\n`/usr/share/data/CMT/cmtdat`.  The resulting log file is named `logDDIR`.  To\nextract data, you would run cdseis with `cdir DDIR` and `logf logDDIR`.  (See\n[the Global CMT Project](https://www.globalcmt.org/CMTfiles.html) for CMT\ncatalog download information.)\n\nCREDITS\n-------\n\nCDSEIS was originally written to extract data from GDSN CD-ROMs. Subsequently,\nit was expanded to work with SEED-oriented datasets.  CDSEIS was written by\nPeter Shearer (1989) to run on a Macintosh.  Ken Creager added several options\nand modified the code to run in a UNIX environment.  Tom McSweeney wrote the\nroutine write_ah.c.  Doug Wiens wrote an early version of the instrument\nresponse reader.  George Helffrich added SAC output and implemented reading\nof SEED volumes, and is responsible for continued maintenance of CDSEIS.\nThanks to the IRIS DMC (and its' many software developers) for providing\nrdseed code, from which EVALRESP-compatible response writing was derived.\n\nPlease report bugs, suggestions, and comments to G. Helffrich.\n\nCHANGES AND BUG FIXES IN CDSEIS 2.0\n-----------------------------------\n\n1.  The time corrections were given, improperly, as either the first or last\n    time correction in the station log.  It now properly interpolates the \n    given time corrections to the time of the first sample in the seismogram.\n2.  Fix bug where gain is returned incorrectly if 3 component SP data is \n    followed by LP.\n3.  ##Data output in AH format now has gain reported in counts/meter, instead\n    of counts/micrometer.##\n4.  CDSEIS now reads the band (short-period, long-period, etc.) from the \n    station log, and puts this in the filename if requested.  The channel\n    name in the headers is converted to SEED format (eg LHZ for long-period\n    high gain, vertical).  Channel used to be called VERT for vertical.\n5.  SCAN will output a subset of the log file in the log-file format.\n6.  Add COMM, * commands for comments in input files.\n7.  Make STAT output prettier, align.\n8.  Comment out some debugging code that was left active. It printed stuff on\n    range/midpoint searches.\n9.  New station codes.  Made changes to SROSTA2 and stalist.  In the present\n    version, this code isn't used, but one of these may form the basis of the\n    subroutine that Peter Shearer was asking for.  Instead, the station log is\n    parsed to obtain the station names, locations and data format information\n    on the CD being processed.  I'm somewhat chary of the instrument locations\n    in this list simply because a lot of it is typed in by hand and there is\n    a potential of screwing up the locations (thus the ckstainfo program).\n    I checked the ones I added, but not the original ones.\n10. Fix bug on EOF -- causes infinite loop.  Proper command is QUIT not STOP.\n11. Moved nonblank length and file name subroutines into cdsubs.f -- used by\n    write_sac.f\n12. Omit blank first output line in CONVERTLOG.\n13. Change julian date handling to eliminate 1995 restriction.\n14. Obtain station info from the data log when the data is being retrieved.\n    This change was needed because on some CDs the same station has different\n    GDSN ID numbers, and the same GDSN ID number refers to different stations\n    on different CDs.\n15. OTYP=10 added and set to default:  Get any available orientations.\n    If OTYPE=5 during a search, all traces (both\n    horizontals & verticals) are listed.  If doing a READ, only the vertical\n    component is extracted.  Thus, you can't match up a SCAN listing with the\n    result of a READ if OTYPE=5.  Adding OTYPE=10 lets you see and ask for any \n    data during either READ and SCAN, and lets you get *only* vertical component\n    data if OTYPE=5.  In other words, what you see is what you get whether\n    you SCAN or READ.\n16. Fix zero division bug if you extract data that is flatlined at zero.\n17. Decode new data formats:  1) Grafenberg; 2) Echery; 3) GEOSCOPE 24-bit.\n18. Make MAKELIST terminate gracefully if an EOF is given.\n19. Make MAKELIST dump out a list of stations and locations encountered on\n    the CD processed to the output file.\n20. Name the new CDs depending on range of data.\n21. SAC output format.\n22. Poles and zeroes output in SAC format.\n23. Comments are read from cdrom and passed to output files\n24. Able to extract from SEED volumes\n25. Station elevation and event magnitude in SAC output\n26. Make CDSEIS exit gracefully if EOF on input given.\n27. Fix pole-zero response information parsing and output.\n28. Add network and instrument type to station type information.\n29. Improve handling of time-dependent channel descriptions (bug fix).\n30. Handle new seed data block types (M, Q, etc.)\n31. Make g77 & gfortran compatible.\n32. Put network name and LOCID into SAC output files\n33. Added EVALRESP response output.\n34. Made error messages about invalid input more descriptive.\n\n\nWISHLIST\n--------\n- Change log file format to recognize that different LOCIDs with the same\n  sample rate may be present at one station.  At present, if different LOCIDs\n  have the same sample rate, they can't all be listed in the log file.  (The\n  \"seed\" designator should also be changed at the same time to an identifier\n  that takes up less space.)\n- Change SAC output so that freq-amp-phase response information may be written\n  out in SAC's FAP file format.\n\nKeep wishing for no bugs in the code.\n",
        "createdAt": "2015-11-18T06:55:18.000Z",
        "updatedAt": "2024-12-27T18:43:04.000Z",
        "language": "C",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ghfbsd/cdseis/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "krischer/2014_AdvancedSeismologySeminar",
        "url": "https://github.com/krischer/2014_AdvancedSeismologySeminar",
        "description": null,
        "stars": 12,
        "forks": 18,
        "readme": "2014_AdvancedSeismologySeminar\n==============================\n\n[View all Notebooks Online](http://nbviewer.ipython.org/github/krischer/2014_AdvancedSeismologySeminar/tree/master/)\n\n0. [Git and Python Introduction](http://nbviewer.ipython.org/github/krischer/2014_AdvancedSeismologySeminar/blob/master/00_git_and_python_intro/git_and_python_intro.ipynb)\n1. [Introduction to Advanced Methods in Numerical Wave Propagation](https://github.com/krischer/2014_AdvancedSeismologySeminar/raw/master/01_intro_numerical_methods_vandriel.pdf)\n2. [Pseudospectral Method](http://nbviewer.ipython.org/github/krischer/2014_AdvancedSeismologySeminar/blob/master/PS_Fourier_1D.ipynb)\n",
        "createdAt": "2014-10-22T16:23:39.000Z",
        "updatedAt": "2025-03-27T21:00:21.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/krischer/2014_AdvancedSeismologySeminar/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "RongjiangWang/MTD_2020",
        "url": "https://github.com/RongjiangWang/MTD_2020",
        "description": "Moment Tensor Decomposition (MTD) into isotropic, tensile/CLVD and DC parts",
        "stars": 1,
        "forks": 0,
        "readme": "FORTRAN code for Moment Tensor Decomposition (MTD) into isotropic, tensile/CLVD and DC parts\n\nDescription of earthquake source mechanism\n",
        "createdAt": "2025-04-14T02:14:34.000Z",
        "updatedAt": "2025-05-29T12:01:01.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/RongjiangWang/MTD_2020/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "abramsci/seismology",
        "url": "https://github.com/abramsci/seismology",
        "description": "A collection of lectures, tutorials and recipes beneficial for a fresh seismologist.",
        "stars": 1,
        "forks": 1,
        "readme": "# seismology\nA collection of lectures, tutorials and recipes beneficial for a fresh seismologist.\n[Version in russian](https://github.com/abramsci/seismology/tree/ru)\n",
        "createdAt": "2022-05-12T14:41:59.000Z",
        "updatedAt": "2025-11-27T16:25:22.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/abramsci/seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "anowacki/TauPy.jl",
        "url": "https://github.com/anowacki/TauPy.jl",
        "description": "Calculate global seismic travel times and raypaths in 1D Earth models using Julia",
        "stars": 5,
        "forks": 3,
        "readme": "# TauPy\n\nCalculate properties of teleseismic arrivals through a selection of\n1D Earth models, using the [ObsPy](https://github.com/obspy/obspy/wiki) Python software.\n\n[![Build Status](https://github.com/anowacki/TauPy.jl/workflows/CI/badge.svg)](https://github.com/anowacki/TauPy.jl/actions)\n[![codecov](https://codecov.io/gh/anowacki/TauPy.jl/graph/badge.svg?token=BNKAR08NW9)](https://codecov.io/gh/anowacki/TauPy.jl)\n\n## Install\n\nTo install on Julia versions v1.6 and above:\n```julia\njulia> import Pkg; Pkg.add(\"https://github.com/anowacki/TauPy.jl\")\n```\n\nThis package uses [PyCall.jl](https://github.com/JuliaPy/PyCall.jl) package to\naccess ObsPy.  If you have the default PyCall installation, then ObsPy will be\ninstalled automatically via its own Conda environment.  If you use your own\nPython with PyCall, then you may need to install ObsPy for you installation\nvia `conda`, `pip`, or another means.\n\n### Problems importing `geographiclib` or `obspy.taup`\n\nIf you receive and error like `ERROR: InitError: Failed to import required Python\nmodule geographiclib` when you first try `using TauPy`, then it's likely that\nPyCall is set up to use your system `python` command, but the required packages\naren't installed or available.  The easiest way to get things working is:\n\n```julia\njulia> ENV[\"PYTHON\"] = \"\"; Pkg.build(\"PyCall\")\n```\n\nRestart, and then try to do `using TauPy` again.  Note, however, that PyCall\nwill from hereon in always use its internal Conda `python` (which is at\n`PyCall.python`).\n\n## Use\n\nTauPy exports three functions:\n\n- `travel_time`\n- `path`\n- `turning_depth`\n\nThese take either epicentral distance, or source and receiver coordinates, and\nreturn `TauPy.Phase` objects containing information about the phase.  There are\ntwo types exported by TauPy:\n\n- `Phase`, containing information about a seismic phase calculated using only\n  event depth and epicentral distance; and\n- `PhaseGeog`, which is the same but for source and receiver locations specified\n  geographically (with longitude and latitude).\n\nThe interactive help describes the fields contained by `Phase`s and `PhaseGeog`s.\nTo bring this up, type `?Phase` or `?PhaseGeog` and hit return.\n\n### Specifying the seismic phase\n\nThe final positional argument of both `travel_time` and `path` is the name of\nthe seismic phase.  This can be a single string, or an array of names.  E.g.:\n\n```julia\njulia> using TauPy\n\njulia> p = travel_time(0, 10, [\"P\", \"PcP\"])\n2-element Array{Phase{Float64},1}:\n Phase{Float64}(\"ak135\", \"P\", 10.0, 0.0, 144.89570946391675, 13.700630345173362, 45.613198013389635, 45.613198013389635, Float64[], Float64[], Float64[])\n Phase{Float64}(\"ak135\", \"PcP\", 10.0, 0.0, 516.4444277972648, 0.9479529695834205, 2.834193976594543, 2.834193976594543, Float64[], Float64[], Float64[]) \n\n```\n\nBy default, all arrivals from a predetermined list are returned, corresponding\nto the &lsquo;phase&rsquo; `\"ttall\"`.\n\n### Specifying the Earth model\n\nWith both `travel_time` and `path`, specify the Earth model by using the `model`\nkeyword argument like so:\n\n```julia\njulia> arr = travel_time(0, 10, \"P\", model=\"sp6\")\n1-element Array{Phase{Float64},1}:\n Phase{Float64}(\"sp6\", \"P\", 10.0, 0.0, 144.8972605261263, 13.7011317118041, 45.61534012667141, 45.61534012667141, Float64[], Float64[], Float64[])\n\n```\n\nAvailable models are listed by calling `TauPy.available_models()`.\n\n### Examples\n\nUse the `travel_time` function to quickly calculate the arrival times for\nthe triplicated arrivals at around 20&deg; epicentral distance:\n\n```julia\njulia> using TauPy\n\njulia> p = travel_time(110, 20, \"P\")\n5-element Array{TauPy.Phase{Float64},1}:\n TauPy.Phase{Float64}(\"ak135\", \"P\", 20.0, 110.0, 263.806, 10.7956, 34.2707, 52.6707, Float64[], Float64[], Float64[])\n TauPy.Phase{Float64}(\"ak135\", \"P\", 20.0, 110.0, 266.524, 11.5422, 37.0166, 58.2286, Float64[], Float64[], Float64[])\n TauPy.Phase{Float64}(\"ak135\", \"P\", 20.0, 110.0, 266.525, 11.5214, 36.9391, 58.063, Float64[], Float64[], Float64[]) \n TauPy.Phase{Float64}(\"ak135\", \"P\", 20.0, 110.0, 267.698, 9.21572, 28.731, 42.7498, Float64[], Float64[], Float64[]) \n TauPy.Phase{Float64}(\"ak135\", \"P\", 20.0, 110.0, 268.261, 9.5515, 29.8818, 44.7109, Float64[], Float64[], Float64[]) \n\njulia> times = getfield.(p, :time)\n5-element Array{Float64,1}:\n 263.80556674138126\n 266.52428738827155\n 266.5253559803772 \n 267.6979484052481 \n 268.26087550766334\n\n```\n\nGood luck in picking all of those!\n\nYou can also calculate the ray paths between the event and station:\n\n```julia\njulia> p = path(110, 20, \"P\")\n5-element Array{TauPy.Phase{Float64},1}:\n TauPy.Phase{Float64}(\"ak135\", \"P\", 20.0, 110.0, 263.806, 10.7956, 34.2707, 52.6707, Float64[], [0.0, 0.120381, 0.142296, 0.1643, 0.208576, 0.298225, 0.472472, 0.652408, 0.667963, 0.683563  …  19.8216, 19.8493, 19.8632, 19.8701, 19.877, 19.9267, 19.9634, 19.9817, 19.9909, 20.0001], [6261.0, 6251.0, 6249.19, 6247.37, 6243.74, 6236.44, 6222.49, 6208.41, 6207.2, 6206.0  …  6343.5, 6347.25, 6349.13, 6350.06, 6351.0, 6359.05, 6365.02, 6368.01, 6369.51, 6371.0])\n TauPy.Phase{Float64}(\"ak135\", \"P\", 20.0, 110.0, 266.524, 11.5422, 37.0166, 58.2286, Float64[], [0.0, 0.148344, 0.175382, 0.202552, 0.257294, 0.368423, 0.585572, 0.811497, 0.831113, 0.850797  …  19.8016, 19.8328, 19.8484, 19.8562, 19.864, 19.9189, 19.9596, 19.9799, 19.99, 20.0001], [6261.0, 6251.0, 6249.19, 6247.37, 6243.74, 6236.44, 6222.49, 6208.41, 6207.2, 6206.0  …  6343.5, 6347.25, 6349.13, 6350.06, 6351.0, 6359.05, 6365.02, 6368.01, 6369.51, 6371.0])\n TauPy.Phase{Float64}(\"ak135\", \"P\", 20.0, 110.0, 266.525, 11.5214, 36.9391, 58.063, Float64[], [0.0, 0.147386, 0.174248, 0.20124, 0.255622, 0.366009, 0.581664, 0.805968, 0.82544, 0.844979  …  19.7997, 19.8308, 19.8463, 19.854, 19.8618, 19.9165, 19.9571, 19.9773, 19.9875, 19.9976], [6261.0, 6251.0, 6249.19, 6247.37, 6243.74, 6236.44, 6222.49, 6208.41, 6207.2, 6206.0  …  6343.5, 6347.25, 6349.13, 6350.06, 6351.0, 6359.05, 6365.02, 6368.01, 6369.51, 6371.0]) \n TauPy.Phase{Float64}(\"ak135\", \"P\", 20.0, 110.0, 267.698, 9.21572, 28.731, 42.7498, Float64[], [0.0, 0.0847973, 0.100216, 0.115685, 0.146774, 0.209565, 0.331002, 0.455546, 0.466272, 0.47702  …  19.858, 19.8798, 19.8907, 19.8961, 19.9016, 19.9415, 19.971, 19.9858, 19.9931, 20.0005], [6261.0, 6251.0, 6249.19, 6247.37, 6243.74, 6236.44, 6222.49, 6208.41, 6207.2, 6206.0  …  6343.5, 6347.25, 6349.13, 6350.06, 6351.0, 6359.05, 6365.02, 6368.01, 6369.51, 6371.0])\n TauPy.Phase{Float64}(\"ak135\", \"P\", 20.0, 110.0, 268.261, 9.5515, 29.8818, 44.7109, Float64[], [0.0, 0.0908228, 0.10734, 0.123913, 0.157226, 0.224534, 0.3548, 0.488528, 0.500051, 0.511599  …  19.8505, 19.8734, 19.8848, 19.8905, 19.8963, 19.9381, 19.9691, 19.9845, 19.9923, 20.0], [6261.0, 6251.0, 6249.19, 6247.37, 6243.74, 6236.44, 6222.49, 6208.41, 6207.2, 6206.0  …  6343.5, 6347.25, 6349.13, 6350.06, 6351.0, 6359.05, 6365.02, 6368.01, 6369.51, 6371.0])   \n\n```\n\nIf you want to know the arrivals&rsquo; turning depths, then `turning_depth`\nis what you want:\n\n```julia\njulia> turning_depth.(p)\n5-element Array{Float64,1}:\n 465.716\n 406.874\n 410.0  \n 665.676\n 660.0  \n\n```\n\nIf you want to know the geographical coordinates of the path for an event and\nstation, then use the source and receiver geographical coordinates:\n\n```julia\njulia> event_lon, event_lat, sta_lon, sta_lat, dep = 0, 0, 10, 10, 100;\n\njulia> p = path(event_lon, event_lat, dep, sta_lon, sta_lat, \"S\")\n3-element Array{TauPy.PhaseGeog{Float64},1}:\n TauPy.PhaseGeog{Float64}(\"ak135\", \"S\", 0.0, 0.0, 10.0, 10.0, 100.0, 14.106, 350.494, 24.1935, 48.835, 83.5499, Float64[], [0.0, 0.542449, 1.36541, 1.40396, 1.44327, 3.49073, 5.54737, 5.58703, 5.62594, 6.45806  …  9.8127, 9.83169, 9.84117, 9.85065, 9.88817, 9.9256, 9.96294, 9.98158, 9.99089, 10.0002], [0.0, 0.550792, 1.38607, 1.42518, 1.46506, 3.53789, 5.60619, 5.64589, 5.68482, 6.51558  …  9.81819, 9.83664, 9.84585, 9.85506, 9.89149, 9.92782, 9.96405, 9.98213, 9.99116, 10.0002], [6271.0, 6262.21, 6251.8, 6251.4, 6251.0, 6240.69, 6251.0, 6251.4, 6251.8, 6262.21  …  6347.25, 6349.13, 6350.06, 6351.0, 6356.0, 6361.0, 6366.0, 6368.5, 6369.75, 6371.0])                   \n TauPy.PhaseGeog{Float64}(\"ak135\", \"S\", 0.0, 0.0, 10.0, 10.0, 100.0, 14.106, 364.733, 20.4703, 39.5658, 57.2195, Float64[], [0.0, 0.0878754, 0.192757, 0.19682, 0.200884, 0.315745, 0.431873, 0.52082, 0.666153, 0.762522  …  9.87896, 9.89135, 9.89753, 9.90372, 9.93078, 9.9578, 9.98476, 9.99822, 10.0049, 10.0117], [0.0, 0.0892309, 0.195729, 0.199855, 0.203982, 0.320611, 0.438522, 0.528832, 0.676382, 0.774215  …  9.88255, 9.89457, 9.90058, 9.90658, 9.93285, 9.95906, 9.98522, 9.99827, 10.0048, 10.0113], [6271.0, 6262.21, 6251.8, 6251.4, 6251.0, 6239.72, 6228.43, 6219.86, 6206.0, 6196.9  …  6347.25, 6349.13, 6350.06, 6351.0, 6356.0, 6361.0, 6366.0, 6368.5, 6369.75, 6371.0])\n TauPy.PhaseGeog{Float64}(\"ak135\", \"S\", 0.0, 0.0, 10.0, 10.0, 100.0, 14.106, 364.855, 20.7161, 40.1367, 58.304, Float64[], [0.0, 0.0916586, 0.201103, 0.205344, 0.209587, 0.32951, 0.450821, 0.543782, 0.695759, 0.796594  …  9.86447, 9.87716, 9.8835, 9.88984, 9.91746, 9.94502, 9.97253, 9.98627, 9.99313, 9.99999], [0.0, 0.0930724, 0.204205, 0.208511, 0.212819, 0.334588, 0.457761, 0.552145, 0.706439, 0.808803  …  9.86848, 9.8808, 9.88695, 9.8931, 9.91991, 9.94667, 9.97336, 9.98668, 9.99334, 9.99999], [6271.0, 6262.21, 6251.8, 6251.4, 6251.0, 6239.72, 6228.43, 6219.86, 6206.0, 6196.9  …  6347.25, 6349.13, 6350.06, 6351.0, 6356.0, 6361.0, 6366.0, 6368.5, 6369.75, 6371.0])  \n\n```\n\nSimilarly, `travel_time` also accepts five geographic arguments if you know the\nevent and station coordinates.\n\n### Calculation cache\n\nUnfortunately, Obspy's travel time and raypath calculations are somewhat\nslow.  To speed up repeated calculations of the same times and raypaths,\nTauPy implements a cache.  To set the size of the cache, use the\n`TauPy.set_cache_size_mb!(size_mb)` function.  The cache can be cleared\nusing `TauPy.clear_cache!()`.  These functions are not exported.\n\nTo disable the cache for individual calls to `path` or `travel_time`,\npass the keyword argument `cache=false`.\n\n## Related packages\n\nGlobal 1D traveltimes and raytracing are also available in the following\nalternative packages:\n\n- [TauP.jl](https://github.com/bvanderbeek/TauP.jl) wraps Phil Crotwell's\n  eminent [TauP](https://www.seis.sc.edu/taup/) Java suite of programs\n  under a LGPL-3 licence and using\n  [JavaCall.jl](https://github.com/JuliaInterop/JavaCall.jl)\n  to directly call the TauP library.\n",
        "createdAt": "2018-08-07T11:53:13.000Z",
        "updatedAt": "2025-11-25T12:26:04.000Z",
        "language": "Julia",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/anowacki/TauPy.jl/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "SeisSCOPED/seis_cloud",
        "url": "https://github.com/SeisSCOPED/seis_cloud",
        "description": "Cloud computing for seismology 101",
        "stars": 3,
        "forks": 1,
        "readme": "# Cloud Computing for seismology 101\n\nThis repository is a minimum seismo-code to run on the cloud and with cloud-hosted data (SCEDC/NCEDC).\n\n## Tutorial documentation and links\n\nPlease read the [High-Performance Seismology Book](https://seisscoped.org/HPS-book/chapters/cloud/AWS_101.html) for full instruction on how to use the code. ",
        "createdAt": "2022-05-31T17:43:54.000Z",
        "updatedAt": "2024-05-20T05:25:17.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/SeisSCOPED/seis_cloud/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Surendra-Bhattarai/wd_leuco_nano_seismology",
        "url": "https://github.com/Surendra-Bhattarai/wd_leuco_nano_seismology",
        "description": "WD pulsations",
        "stars": 0,
        "forks": 0,
        "readme": "See the [analysis.ipynb](https://github.com/Surendra-Bhattarai/wd_leuco_nano_seismology/blob/main/(testsuite%20run)_1M_pre_ms_to_wd/analysis.ipynb) file within the (testsuite run)_1M_pre_ms_to_wd folder.\n",
        "createdAt": "2025-05-14T18:03:55.000Z",
        "updatedAt": "2025-06-11T15:20:54.000Z",
        "language": "AMPL",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Surendra-Bhattarai/wd_leuco_nano_seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ese-msc-2021/environmental_data_week3",
        "url": "https://github.com/ese-msc-2021/environmental_data_week3",
        "description": "material for week 3 for the environmental data module (A. Lipp/rivers and B. Bell/seismology)",
        "stars": 0,
        "forks": 2,
        "readme": "# environmental_data_week3\nmaterial for week 3 for the environmental data module (A. Lipp/rivers and B. Bell/seismology)\n\n\nNB: the /data folder for \"Seismic reflection imaging\" was too big to pose on githup. You can access the data using the following Imperial File Exchange link: \n\n\n## Learning outcomes:\n\nOn successful completion of this module, students will be able to:\n1.\tUnderstand common data format and database structures specific to representative fields of environmental science \n2.\tDemonstrate technical competency in handling common data types routinely encountered in the environmental sciences and identify relevant open-source data repositories\n3.\tIdentify and design suitable data analysis strategies that consider data types, data distribution constraints, strength, benefits and limitations of statistical and modelling tools and environmental dynamics.\n4.\tUnderstand the limitation of available data and data analysis products. Understand sources of errors and demonstrate ability to comprehensively characterize uncertainties and interpret results in the context of these uncertainties, including measurement errors, environmental uncertainties as well as errors stemming from the analytical procedure itself (e.g. calibration of analysis using synthetic data/models). \n\n## An introduction to rivers, landscapes and fluvial data - Mon 29th Nov \n\n## Contents: \n\nRivers networks are extremely important parts of the natural and human environment. This module will introduce a number of foundational concepts that will allow you to understand the context of much of the fluvial data you may come across. We will be drawing from geomorphology, geochemistry and hydrology. We will also discuss where data about drainage networks comes from and how it can be accessed. You will gain first hand experience of generating drainage networks from topographic data in the practical. \n\nSpecific learning goals are: \n1. Understanding the basic scaling relationships underlying drainage networks. \n2. Developing a quantitative framework for how rivers modify landscapes on long-timescales. \n3. To know how to generate a drainage network from topographic data including limitations\n4. Understand the range of materials transported in rivers and associated data\n5. Consider the importance of drainage network topology in analysing fluvial data\n\n## Further reading: \n\nSpecific sources given as citations within the slides and more general reading is in the final slide.\n\n\n## Lecture schedule\n\n**The slides for the Lectures are available by following the links below**\n\n|Date                      | Lecture                             |Instructor |\n|--------------------------|-------------------------------------|------------|\n|2021-11-29 9:00-10:00 Mon | [Introduction to Rivers & Landscapes](https://imperiallondon-my.sharepoint.com/:p:/g/personal/agl18_ic_ac_uk/EcLL_IwcNpZKkIU7aox-nYcBRkHECInGFQzRRCyE9LwlZw?e=EbQ2h9)     | Alex Lipp  |\n|2021-12-02 14:00-15:00 Mon | [Data from within rivers](https://github.com/ese-msc-2021/environmental_data_week3/blob/main/rivers_lecture_pt2.pptx)    | Alex Lipp |     \n\n## Practical schedule\n|Date                      | Exercise                             |Instructor |\n|--------------------------|-------------------------------------|------------|\n|2021-11-29 10:00-12:00 Mon | [Extracting drainage from topography](https://github.com/ese-msc-2021/environmental_data_week3/tree/main/rivers_practicals)      | Alex Lipp  |\n|2021-12-02 15:00-17:00 Mon | [Tracking pollution through drainage networks](https://github.com/ese-msc-2021/environmental_data_week3/tree/main/rivers_practicals)   | Alex Lipp |     \n\n\n\n***\n\n\n## Seismic reflection imaging - Tue 30th Nov and Thur 2nd Dec\n\n## Description of contents:\n\nThis module will deliver the core knowledge and skills required for accessing, loading, viewing and analysing 3D post-stack seismic reflection data which provides images of the Earth's subsurface for environmental and geotechnical engineering purposes. In order for you to be able to make use of these data as data scientists we need to cover some geological and geophysical background. \n\nThis week, we will focus on: \n1. The fundamentals of the seismic reflection method and how seismic reflection data is collected and processed\n2. The standard data format for seismic reflection data (SEG-Y), how to access it and use it for environmental and geotechnical purposes\n3. How data science and machine learning could revolutionise seismic studies\n\nWe won't be able to go through these topics in detail, but it is hoped that the material covered will give you enough information to inspire you as Environmental data scientists to the great potential of seismic data\n\n\n\n## Supplementary/recommended reading and useful resources:\n* Key textbook for understanding the fundamentals of seismic reflection\n\nIntroduction to Geophysical Exploration, version 3. Kearey, Brooks and Hill. Oxford, John Wiley and Sons 2002\nhttps://library-search.imperial.ac.uk/discovery/search?query=any,contains,introduction%20to%20geophysical%20exploration&tab=Everything&search_scope=MyInst_and_CI&sortby=date_d&vid=44IMP_INST:ICL_VU1&facet=frbrgroupid,include,9030987754194472865&offset=0\n\n* Information on subsurface machine learning and seismic interpretation for data scientists\nhttps://agilescientific.com\n\nLinks to other useful articles and websites provided in the lecture slides\n\n\n\n## Lecture schedule\n\n|Date                      | Lecture                             |Instructor  |Moderator   |\n|--------------------------|-------------------------------------|------------|------------|\n|2021-11-30 9:00-12:00 Tue | Intro to seismic reflection data     | Rebecca Bell        | Raul Adriaensen    |\n|2021-12-02 9:00-12:00 Thur | Intro to the SEG-Y data format and interpretation     | Rebecca Bell        | Raul Adriaensen   |\n\n## Practical schedule\n\n|Date                      | Exercise                           |Instructor  |Moderator   |\n|--------------------------|-------------------------------------|------------|------------|\n|2021-11-30 14:00-17:00 Tue | 1: Synthetic seismic models, 2: NMO corrections     | Raul Adriaensen       | Rebecca Bell    |\n|2021-12-02 14:00-17:00 Thur | 3: Viewing SEG-Y data, 4: Seismic attributes     | Raul Adriaensen      | Rebecca Bell   |\n\n## Installing required python libraries\n\nFor facilitated setup it is recommended to create a virtual environment and install the required python packages. \nFor anaconda environments an environemnt.yml file is provided, for setup run:\n\n    conda env create -f environment.yml\n\nFrom within the base folder of the repository. \n\nFor other solutions a requirements file is provided, can be setup by running:\n\n    pip install -r requirements.txt\n\nFrom the base directory.\n\nMake sure to activate the environment after and run the notebooks from within this virtual environment! Lastly, make sure you have placed the memory intensive datafiles received via FileExchange under the data folder provided.\n\n## Assessment exercises\nAssessment will be 100% by coursework. It is all open book. \nThe assessment will involve 33% of the question on Landscapes and Drainage and 67% on Seismic reflection imaging\nExercises will be distributed and submitted via GitHub Classroom on Friday. SEG-Y data will be downloaded from a File Exchange link to be sent at 1pm on Friday. Data for the landscapes assessment will be provided with the assessment notebook.\n\n|Release Date         | Due Date            | Topic                             |\n|---------------------|---------------------|-----------------------------------|\n|2021-12-03 Fri 13:00 | 2021-12-03 17:00 Fri| Landscapes and Seismic reflection imaging |\n",
        "createdAt": "2021-11-23T13:22:43.000Z",
        "updatedAt": "2021-12-02T09:41:26.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ese-msc-2021/environmental_data_week3/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "lskk/ecn-synth",
        "url": "https://github.com/lskk/ecn-synth",
        "description": "Synthesize low-cost dense accelerometer seismic network signals from IRIS network seismometer signals",
        "stars": 3,
        "forks": 0,
        "readme": "# ecn-synth\n\nSynthesize low-cost dense accelerometer seismic network signals from IRIS network seismometer signals\n\nSupported input files are miniSEED files downloaded from USGS (usually IRIS Wilber3).\n\nminiSEED manipulation uses [obspy.mseed](http://docs.obspy.org/archive/0.10.2/packages/obspy.mseed.html) library.\n\nPython 3.6 is required.\n\nCreate the virtual environment using `python -m venv venv`\n\nMake sure to `pip install -r requirements.txt`\n\n## Overview\n\n[Slides](https://www.dropbox.com/s/26ywevun39kbzk9/Sintesis%20Data%20Gempa%20Bumi%20slides.pptx?dl=0)\n\n## Avicenna Dataset: Earthquakes used as initial case study\n\nThe datasets are named alphabetically (A..Z) using scientist names, starting from _Avicenna_.\n\nHendy stores this dataset in `E:\\project_amanah\\S3\\earthquake\\dataset-avicenna`.\n\nEach event contains:\n\n1. QuakeML\n2. miniSEED waveforms for selected stations (up to nearest 5 stations with signals)\n\n|    No.     |    Description                                         |    Time                         |    Location               |    Depth        |    MMI    |\n|------------|--------------------------------------------------------|---------------------------------|---------------------------|-----------------|-----------|\n|    1       |    [M 5.9 -   29km SW of Panyaungan Timur, Indonesia](https://earthquake.usgs.gov/earthquakes/eventpage/us2000cmwz)    |    2018-01-23   06:34:54 UTC    |    7.092°S   105.963°E    |    48.2   km    |    V      |\n|    2       |    [M 5.2 -   50km S of Sinarharapan, Indonesia](https://earthquake.usgs.gov/earthquakes/eventpage/us2000c551)         |    2017-12-16   00:22:30 UTC    |    7.884°S   106.820°E    |    44.0   km    |    III    |\n|    3       |    [M 6.5 -   1km E of Kampungbaru, Indonesia](https://earthquake.usgs.gov/earthquakes/eventpage/us2000c4v8)           |    2017-12-15   16:47:58 UTC    |    7.492°S   108.174°E    |    90.0   km    |    VII    |\n|    4       |    [M 5.7 -   109km SSW of Cibungur, Indonesia](https://earthquake.usgs.gov/earthquakes/eventpage/us20009lwc)          |    2017-06-11   23:15:06 UTC    |    8.321°S   106.259°E    |    7.0   km     |    IV     |\n|    5       |    [M 5.2 -   0km S of Cipatujah, Indonesia](https://earthquake.usgs.gov/earthquakes/eventpage/us10008k20)             |    2017-04-23   18:01:13 UTC    |    7.740°S   108.019°E    |    72.2   km    |    V      |\n\n## Signal Processing\n\n1. Differentiate from displacement/velocity into acceleration\n2. Attenuate (based on linear distance from hypocenter)\n3. Add noise - different levels of noise\n\n",
        "createdAt": "2018-05-30T23:13:42.000Z",
        "updatedAt": "2024-11-20T11:11:06.000Z",
        "language": "Python",
        "homepage": "https://ecn.pptik.id/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/lskk/ecn-synth/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "claudiodsf/nllgrid",
        "url": "https://github.com/claudiodsf/nllgrid",
        "description": "Python class for reading and writing NLLoc grid files.",
        "stars": 39,
        "forks": 13,
        "readme": "# NLLGrid\n\nPython class for reading and writing [NonLinLoc] grid files.\n\n(c) 2015-2025 Claudio Satriano, Natalia Poiata, Robert Pickle\n\n[![changelog-badge]][changelog-link]\n[![cf-badge]][cf-link]\n[![PyPI-badge]][PyPI-link]\n[![license-badge]][license-link]\n[![docs-badge]][docs-link]\n\n## Installation\n\n### Using Anaconda\n\nIf you use [Anaconda], the latest release of nllgrid is available via\n[conda-forge][cf-link].\n\nTo install, simply run:\n\n    conda install -c conda-forge nllgrid\n\n### Using pip and PyPI\n\nThe latest release of nllgrid is available on the\n[Python Package Index][PyPI-link].\n\nYou can install it easily through `pip`:\n\n    pip install nllgrid\n\n### From nllgrid GitHub releases\n\nDownload the latest release from the\n[releases page][releases-link],\nin `zip` or `tar.gz` format, then:\n\n    pip install nllgrid-X.Y.zip\n\nor\n\n    pip install nllgrid-X.Y.tar.gz\n\nWhere, `X.Y` is the version number (e.g., `1.3`).\nYou don't need to uncompress the release files yourself.\n\n### Installing a development snapshot\n\nIf you need a recent feature that is not in the latest release (see the\n`unreleased` section in [CHANGELOG][changelog-link]), you want to use the\nmore recent development snapshot from the\n[nllgrid GitHub repository][github-repo].\n\n#### Using pip\n\nThe easiest way to install the most recent development snapshot is to download\nand install it through `pip`, using its builtin `git` client:\n\n    pip install git+https://github.com/claudiodsf/nllgrid.git\n\nRun this command again, from times to times, to keep NLLGrid updated with\nthe development version.\n\n#### Cloning the NLLGrid GitHub repository\n\nIf you want to take a look at the source code (and possibly modify it 😉),\nclone the project using `git`:\n\n    git clone https://github.com/claudiodsf/nllgrid.git\n\nor, using SSH:\n\n    git clone git@github.com:claudiodsf/nllgrid.git\n\n(avoid using the \"Download ZIP\" option from the green \"Code\" button, since\nversion number is lost).\n\nThen, go into the `nllgrid` main directory and install the code in \"editable\nmode\" by running:\n\n    pip install -e .\n\nYou can keep your local NLLGrid repository updated by running `git pull`\nfrom times to times. Thanks to `pip`'s \"editable mode\", you don't need to\nreinstall NLLGrid after each update.\n\n## Getting Started\n\n### Reading a NLL grid\n\nA NLL grid is composed of two files (`.hdr` and `.buf`).\n\nTo read a NLL grid, do:\n\n```python\n>>> from nllgrid import NLLGrid\n>>> grd = NLLGrid('somegrid.hdr')\n```\n\nor, using the `.buf` filename:\n\n```python\n>>> grd = NLLGrid('somegrid.buf')\n```\n\nor even without any extension:\n\n```python\n>>> grd = NLLGrid('somegrid')\n```\n\nA grid description can be obtained by:\n\n```python\n>>> print(grd)\n```\n\nThe grid data array is accessed by `grd.array`.\nThe grid can be plotted doing:\n\n```python\n>>> grd.plot()\n```\n\nUse Python introspection (e.g. `dir(grd)`) to see all the available\nmethods and attributes.\n\n\n### Creating a NLL grid\n\nSuppose that you have a 3D data array stored into a NumPy array\ncalled `mydata`.\n\nFirst, create an empty NLL grid object:\n\n```python\n>>> from nllgrid import NLLGrid\n>>> grd = NLLGrid()\n```\n\nthen, add the data array and information on grid sampling and grid\norigin, e.g.:\n\n```python\n>>> grd.array = mydata\n>>> grd.dx = 0.5  #km\n>>> grd.dy = 0.5  #km\n>>> grd.dz = 0.5  #km\n>>> grd.x_orig = -10  #km\n>>> grd.y_orig = -20. #km\n>>> grd.z_orig = -1.  #km\n```\n\nOptionally, add a grid type and/or a geographic transformation:\n\n```python\n>>> grd.type = 'VELOCITY'\n>>> grd.orig_lat = 40.63\n>>> grd.orig_lon = 15.80\n>>> grd.proj_name = 'LAMBERT'\n>>> grd.first_std_paral = 38.\n>>> grd.second_std_paral = 42.\n>>> grd.proj_ellipsoid = 'WGS-84'\n```\n\nFinally, give a basename and write to disk:\n\n```python\n>>> grd.basename = 'mygrid'\n>>> grd.write_hdr_file()\n>>> grd.write_buf_file()\n```\n\nThis will create the two files `mygrid.hdr` and `mygrid.buf`.\n\nIf you want to save your grid in double precision (required for\ninstance by NLDiffLoc), change `grd.float_type` to `'DOUBLE'` before\nsaving the grid (default is `'FLOAT'`):\n\n```python\n>>> grd.float_type = 'DOUBLE'\n```\n\nNote that if you want to use your grid as input for NonLinLoc\n`Grid2Time` code, the grid type has to be `SLOW_LEN` and your grid\narray has to be transformed into slowness (in s/km) and multiplied\nby the grid step (in km).\n\n[changelog-badge]: https://img.shields.io/badge/Changelog-136CB6.svg\n[changelog-link]: CHANGELOG.md\n[cf-badge]: http://img.shields.io/conda/vn/conda-forge/nllgrid.svg\n[cf-link]: https://anaconda.org/conda-forge/nllgrid\n[PyPI-badge]: http://img.shields.io/pypi/v/nllgrid.svg\n[PyPI-link]: https://pypi.python.org/pypi/nllgrid\n[license-badge]: https://img.shields.io/badge/license-GPLv3-green\n[license-link]: https://www.gnu.org/licenses/gpl-3.0.html\n[docs-badge]: https://readthedocs.org/projects/nllgrid/badge/?version=latest\n[docs-link]: https://nllgrid.readthedocs.io/en/latest/?badge=latest\n\n[NonLinLoc]: http://alomax.free.fr/nlloc\n[Anaconda]: https://www.anaconda.com/products/individual\n[releases-link]: https://github.com/claudiodsf/nllgrid/releases\n[github-repo]: https://github.com/claudiodsf/nllgrid\n",
        "createdAt": "2017-06-07T09:49:12.000Z",
        "updatedAt": "2025-09-17T04:37:58.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/claudiodsf/nllgrid/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "seismo-learn/seismolearnpy",
        "url": "https://github.com/seismo-learn/seismolearnpy",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# seismolearnpy\n\n![GitHub](https://img.shields.io/github/license/seismo-learn/seismolearnpy)\n",
        "createdAt": "2022-05-15T07:53:30.000Z",
        "updatedAt": "2023-04-30T04:45:41.000Z",
        "language": "Makefile",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/seismo-learn/seismolearnpy/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "GeoscienceAustralia/hiperseis",
        "url": "https://github.com/GeoscienceAustralia/hiperseis",
        "description": "High Performance Seismologic Data and Metadata Processing System",
        "stars": 61,
        "forks": 25,
        "readme": "HiPerSeis: High Performance Software Package for Seismology Data/Metadata Processing and Analysis\n=================================================================================================\n\n.. image:: https://github.com/GeoscienceAustralia/hiperseis/actions/workflows/hiperseis_main_ci.yml/badge.svg\n   :target: https://github.com/GeoscienceAustralia/hiperseis/actions/workflows/hiperseis_main_ci.yml\n   :alt: Github Actions\n\n.. image:: https://codecov.io/gh/GeoscienceAustralia/hiperseis/branch/develop/graph/badge.svg\n   :target: https://codecov.io/gh/GeoscienceAustralia/hiperseis\n   :alt: codecov\n\n.. image:: https://readthedocs.org/projects/hiperseis/badge/?version=develop\n   :target: http://hiperseis.readthedocs.io/en/develop/\n   :alt: readthedocs\n\nHow to Cite\n===========\n\nIf you use this software in a scientific publication, we'd very much appreciate if you could cite the following papers:\n\n-  Hassan, R., Hejrani, B., Medlin, A., Gorbatov, A. and Zhang, F., 2020. High-performance seismological tools (HiPerSeis). In: Czarnota, K., Roach, I., Abbott, S., Haynes, M., Kositcin, N., Ray, A. and Slatter, E. (eds.) Exploring for the Future: Extended Abstracts, Geoscience Australia, Canberra, 1–4. https://ecat.ga.gov.au/geonetwork/srv/eng/catalog.search#/metadata/135095\n   \n\nOverview\n========\n\n- Home Page: https://github.com/GeoscienceAustralia/hiperseis\n\n- Documentation: http://hiperseis.readthedocs.io/en/develop/\n\n- Wiki Pages: https://github.com/GeoscienceAustralia/hiperseis/wiki\n\n\n\nCurrent Contacts\n================\n\n- Rakib Hassan: rakib.hassan@ga.gov.au \n\n- Alexei Gorbatov: alexei.gorbatov@ga.gov.au\n\n- Babak Hejrani: babak.hejrani@ga.gov.au\n\n\nSystem Requirements\n==========================\n\n- Python 3.6 (recommended)\n\nSetup Guide\n=================================\n\n1. First, obtain the source code from `Github repository <https://github.com/GeoscienceAustralia/hiperseis>`_\n\n-  ``git clone https://github.com/GeoscienceAustralia/hiperseis.git``\n- ``cd hiperseis``\n- ``git submodule update --init --recursive``\n\n2. HiPerSeis does not provide an installation script due to the number of dependencies involved, some of which require low-level libraries to be available on the host machine. Instead, shell scripts are provided in ``hiperseis/setup_scripts`` for Linux, OSX and Windows for installing dependencies through a combination of Conda and Pip. A shell script is provided for NCI GADI, tailored exclusively for the current list of low-level HPC libraries e.g. MPI, HDF5, etc. available on the system.\n\n3. To use HiPerSeis in the checked out location, you will need to add the root HiPerSeis folder to your PYTHONPATH variable. For example, if you checked out HiPerSeis to the folder `dev/hiperseis` relative to your home directory, then in a `bash` shell you need to execute the following shell command: ``export PYTHONPATH=$HOME/dev/hiperseis``.  This needs to be done for each command shell session, or added to ``.bashrc`` or its equivalent.\n\nThird Party Library Dependencies\n================================\n\nCertain modules require specific third party (non-Python) libraries to be installed\non the host system. For example, scripts that convert to sc3ml format also require Seiscomp3 to be\ninstalled and to be visible in the PATH.\n\nLicense\n===============\n\nHiPerSeis is licensed under the GPL version 3\n\n",
        "createdAt": "2017-04-05T03:57:13.000Z",
        "updatedAt": "2025-11-26T00:58:58.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/GeoscienceAustralia/hiperseis/develop/README.rst",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "seisboy/Seismology",
        "url": "https://github.com/seisboy/Seismology",
        "description": null,
        "stars": 0,
        "forks": 1,
        "readme": "# Seismology\nCodes and notes used in my research of seismology :eyes:\n## Self-developed code\n- [lsresp.pl](https://github.com/seisboy/Seismology/blob/master/Receiver%20Function/lsresp.pl): Whether for `instrument response` for single station.\n- [numENZ.pl](https://github.com/seisboy/Seismology/blob/master/Receiver%20Function/numENZ.pl): The number of `*.SAC.E`, `*.SAC.N`, `*.SAC.Z` for single station.\n- [runnumENZ.pl](https://github.com/seisboy/Seismology/blob/master/Receiver%20Function/runnumENZ.pl): Match usage with [numENZ.pl](https://github.com/seisboy/Seismology/blob/master/Receiver%20Function/numENZ.pl).\n## funclab\n[funclab](https://drive.google.com/file/d/0B2KG63pkFxf_aFRkYzJlVmFfYVE/view) is used for RFs process compiled by Language matlab.\n- [Notes.md](https://github.com/seisboy/Seismology/blob/master/funclab/Notes.md): Just some notes.\n- [run_all_examples.m](https://github.com/seisboy/Seismology/blob/master/funclab/run_all_examples.m)\n\n    * _Combination of 5 following scritps_\n\n    * _Same with source code_\n- [script01a_processPrfns.m](https://github.com/seisboy/Seismology/blob/master/funclab/script01a_processPrfns.m)\n\n    * _Deconvolution (PRFs)_\n\n    * _Modified based on source code, for 'space problem'_\n- [script01b_processSrfns.m](https://github.com/seisboy/Seismology/blob/master/funclab/script01b_processPrfns.m)\n\n    * _Deconvolution (SRFs)_\n\n    * _Underdevelopment_\n- [script02_checkPrfns.m](https://github.com/seisboy/Seismology/blob/master/funclab/script02_checkPrfns.m)\n\n    * _Quality control_\n\n    * _Same with source code_\n\n    * _`STATION_PREFIX` is correspond with `net`_\n- [script03a_plotGatherTimeStacks.m](https://github.com/seisboy/Seismology/blob/master/funclab/script03a_plotGatherTimeStacks.m): Underdevelopment.\n- [script03b_plotGatherDepthStacks.m](https://github.com/seisboy/Seismology/blob/master/funclab/script3b_plotGatherDepthStacks.m): Underdevelopment.\n",
        "createdAt": "2017-05-21T07:27:41.000Z",
        "updatedAt": "2023-10-24T11:59:43.000Z",
        "language": "Matlab",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/seisboy/Seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "bakerb845/ess412_introToSeismo",
        "url": "https://github.com/bakerb845/ess412_introToSeismo",
        "description": "Introduction to seismology for UW ESS 412/512 Winter 2018 class.",
        "stars": 1,
        "forks": 5,
        "readme": "# Introduction to Seismology Jupyter Notebooks\n\nThese are the Jupyter notebooks for the UW ESS 412/512 winter quarter introduction to seismology class.\n\nThese are intended to be run with Python 3.x where x can be 4 or 5.  These should also be accessible via [Microsoft Azure](https://notebooks.azure.com).\n\n## Prerequisites\n\nThese are Python3.5 notebooks.  They will subsequently require\n\n1. Python version 3.5.  Version 3.6 may also work.  The 2.x series is being deprecated so I'm trying to avoid it.\n2. [ObsPy](https://github.com/obspy/obspy/wiki).\n3. [Jupyter] notebooks.\n4. [Git](https://git-scm.com/).\n5. [basemap](https://matplotlib.org/basemap/)\n\nOptional - I strongly recommend installing Python with [Anaconda](https://conda.io/docs/user-guide/install/index.html).  In this instance you'll end up with Python3.6 and have to add\n\n    conda config --add channels conda-forge\n\nAlternatively, you can use these notebooks in a cloud environment via [Microsoft Azure](https://notebooks.azure.com).  However, this will require that you register with Microsoft.\n\n## Staying Up-To-Date\n\nThis repository will be populated as the class unfolds.\n\n### If working at a terminal\n\nTo stay up to date change your working directory to where the git repository is downloaded, e.g.,\n\n    cd /path/to/ess412_introToSeismo\n\nThen type\n\n    git pull\n\nThis should pull the latest notebooks and you'll up-to-date.  \n\n### If working on Azure\n\nTo stay up to date on Azure open a \"Terminal.\"  Then type\n\n    cd library\n    git pull\n\nThis should pull the latest notebooks and you'll be up-to-date. \n\n### Git is sassing you\n\nYou probably modified a file without changing the name/copying it to your local directory.  In git's complaints will likely be offending file names.  Let's say nisqually.ipynb is causing issues.  You can do a few things here.\n\n(1)  Change the offending file names\n\n    mv niqually.ipynb to nisqually_myName.ipynb\n\n(2)  Commit your change.  This may ultimately lead to a merge conflict but git can be pretty clever so give it a try.\n\n    git add nisqually.ipynb\n    git commit -m \"I've modified this file while doing my homework.  Maybe I'll follow the directions next time.\"\n\n## Usage\n\nHere are some ideas on usage for those working at a terminal and those working on Azure.\n\n### If working at a terminal\n\nIn this option locate the notebook in the git repo structure.  Change directories to where you are doing your work, e.g.,\n\n    cd /path/to/my/seismology/homework\n\nNow copy it to your working directory, e.g., \n\n    cp /path/to/ess412_introToSeismo/directory/notebook.ipynb ./notebook_myName.ipynb\n\nRenaming the file will prevent git from destroying your file during a git pull phase. \n\nThen run the notebook\n\n    /path/to/anaconda3/bin/jupyter notebook\n\nIn a web-browser you will see notebook\\_myName.ipynb.  Select it and click \"Run\".\n\n### If working on Azure\n\nIn this option locate the appropriate notebook in the library structure.  For example, let's call it notebook/notebook.ipynb.  Then rename notebook.ipynb by right-clicking and entering notebook\\_myName.ipynb.  This will prevent git from destroying your work during a git pull phase.\n\nNow select the renamed notebook and type \"Run\".\n\n## Directories\n\nThis is an overview directories and files contained within are described.\n\n1. introToPython contains a very simple introToPython.ipynb notebook which provides a very brief overview of Python.\n2. nisqually contains nisqually.ipynb - This is the Nisqually 2001 event recorded at station KEV.  It is an introduction to waveform interpretation and major seismic signals that we'll discuss in the class. \n3. reflectionCoeffs contains reflectionAndTransmissionCoefficients.ipynb and does simple solid-solid interface reflection coefficient calculations.\n\n",
        "createdAt": "2018-01-16T17:31:40.000Z",
        "updatedAt": "2020-10-07T00:12:54.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/bakerb845/ess412_introToSeismo/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ikahbasi/SeisRoutine",
        "url": "https://github.com/ikahbasi/SeisRoutine",
        "description": "SeisRoutine is a Python package for the routine inspection and processing of seismological data.",
        "stars": 0,
        "forks": 0,
        "readme": "# SeisRoutine\nSeisRoutine is a Python package for the routine inspection and processing of seismological data.\n",
        "createdAt": "2023-12-20T04:48:15.000Z",
        "updatedAt": "2025-12-04T13:15:18.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ikahbasi/SeisRoutine/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "hifayuna1982/Komputasi-Seismology",
        "url": "https://github.com/hifayuna1982/Komputasi-Seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# Komputasi-Seismology",
        "createdAt": "2017-10-10T04:02:46.000Z",
        "updatedAt": "2017-10-10T04:02:46.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/hifayuna1982/Komputasi-Seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "MIGG-NTU/SeisFigs_Examples",
        "url": "https://github.com/MIGG-NTU/SeisFigs_Examples",
        "description": "Examples of Plotting Seismic Figures",
        "stars": 0,
        "forks": 0,
        "readme": "# Examples of Plotting Seismic Figures\n\nThis repository contains examples of plotting seismic figures.\n\nWe will share plotting scripts and experience in this site, e.g.,\n\n- [GMT](https://www.generic-mapping-tools.org/)\n- [Python](https://matplotlib.org/)\n- [Illustrator](https://www.adobe.com/cn/products/illustrator.html)\n- [Blender](https://www.blender.org/)\n\n",
        "createdAt": "2020-11-20T11:52:42.000Z",
        "updatedAt": "2023-01-28T07:58:48.000Z",
        "language": "Python",
        "homepage": "https://migg-ntu.github.io/SeisFigs_Examples/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/MIGG-NTU/SeisFigs_Examples/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "vackar/BayesISOLA",
        "url": "https://github.com/vackar/BayesISOLA",
        "description": "Seismological toolbox for estimation of mechanism of a seismic source using moment tensor description.",
        "stars": 18,
        "forks": 8,
        "readme": "",
        "createdAt": "2021-09-14T10:34:31.000Z",
        "updatedAt": "2025-09-10T16:57:09.000Z",
        "language": "JavaScript",
        "homepage": "http://geo.mff.cuni.cz/~vackar/BayesISOLA",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/vackar/BayesISOLA/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "RongjiangWang/QSEIS_2006",
        "url": "https://github.com/RongjiangWang/QSEIS_2006",
        "description": "Synthetic seismograms based on a layered elastic halfspace model",
        "stars": 2,
        "forks": 1,
        "readme": "FORTRAN code for calculating synthetic seismograms based on a layered viscoelastic half-space earth model.\n\nLast update by Rongjiang Wang: Berlin, 29. July 2025\n\nHighlights:\n\n(1) orthonormal propagator algorithm for numerical stability (more efficient than the reflectivity method)\n\n(2) complex frequency technique for supressing the time-domain aliasing problem\n\n(3) wave-number-domain differential filter technique for suppressing numerical phases\n\n(4) various partial solution options\n\n(5) different shallow structures at source and receiver site\n\n(6) earth flattening transformation\n\nRelated codes\n\nMSEIS – for marine seismic application\n\nQSEIS2D - for quasi 2D structure model (only teleseismic applications)\n\nFor Windows user, the executable file is provided under folder \"WindowsEXE\". Linux user may compile the source codes with \"gfortran\" via a single command like, e.g.,\n\n~>cd .../SourceCode\n\n~>gfortran -o qseis2006 *.f -O3\n\nto get the excutable code qseis2006.\n\nAfter start the executable code, the program ask for an input file in the ASCII format. An example input file is provided under folder \"InputFile\". You may change the input data included in this file for your own applications.\n\nNotes to the co-ordinate convention used in QSEIS:\n\n(1) Define a local Cartesian co-ordinate system using Aki's convention, that is, x = northward, y = eastward and z = downward. The corresponding local cylindrical co-ordinate system has the same x axis and the azimuth (theta) increasing from north to east as commonly used in seismology.\n\n(2) Decompose your moment tensor or single force to their components in the local Cartesian co-ordinate system.\n\n(3) All Green's functions as outputs of QSEIS are given by their ZRT components in the cylindrical co-ordinate system for a unit source. To consider the effect of radiation pattern, you need to multiply each Green's function output with an azimuth factor, which is given in the documentation of example input file.\n\n(4) The final results for your application are then obtained by convoluting your source components with the corresponding Green's functions.\n\nReferences\n\nWang, R., (1999), A simple orthonormalization method for stable and efficient computation of Green's functions, Bulletin of the Seismological Society of America, 89(3), 733-741.\n\nWang, R., and H. Wang (2007), A fast converging and anti-aliasing algorithm for Green’s functions in terms of spherical or cylindrical harmonics, Geophysical Journal International, doi: 10.1111/j.1365-246X.2007.03385.x.\n",
        "createdAt": "2025-04-10T02:02:01.000Z",
        "updatedAt": "2025-11-24T03:01:24.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/RongjiangWang/QSEIS_2006/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "rizkiwuln1234/Seismology",
        "url": "https://github.com/rizkiwuln1234/Seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2023-11-07T08:21:22.000Z",
        "updatedAt": "2023-11-07T08:21:22.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "UMD-InSight/InSight-seismic-data-downloader",
        "url": "https://github.com/UMD-InSight/InSight-seismic-data-downloader",
        "description": "Scripts for download and processing of the InSight SEIS VBB data",
        "stars": 15,
        "forks": 3,
        "readme": "# InSight Seismic Data Downloader\n\n[![DOI](https://zenodo.org/badge/362388409.svg)](https://zenodo.org/badge/latestdoi/362388409)\n\nThis repository contains scripts for InSight seismic data download and processing. The scripts download the VBB SEIS instrument data for the seismic events that are listed in the Marsquake Catalog. The scripts are developed by the University of Maryland InSight group, whereas the instrument and marsquake events information are provided by working groups of the InSight team and should be cited appropriately, according to the information provided below.\n\n## Contributors\nFoivos Karakostas, Doyeon Kim, Ross Maguire, Aisha Khatib, Quancheng Huang, Angela Marusiak, Nicholas Schmerr, Ved Lekić - the University of Maryland, College Park, InSight group\n\n## Dependencies\n\nIn order to use this suite, a python3 installation in your system is necessary, as well as the [ObsPy](https://github.com/obspy/obspy/wiki) open sourced framework. Please acknowledge and cite the ObsPy references accordingly.\n\n## Contents\n\nThis repository contains:\n\n1. The 12th version of the Marsquakes Catalog. Citation: InSight Marsquake Service (2022). Use the link for the download and cite accordingly.\n2. The ELYSE station dataless file. Citation: InSight MARS SEIS Data Service (2019). Use the link for the download and cite accordingly.\n3. The python script **make_seismic_catalog.py** that creates the text file of the catalog, needed for the event data downloaders.\n4. The python script **eventdownloader.py**, which is used for downloading and processing of a unique event data.\n5. The python script **massivedownloader.py**, which is used for downloading and processing of events' data of a specific event Type and Quality (see Clinton et al., 2021 for details).\n\n## How to use\n\n**make_seismic_catalog.py**: Just run the script and the SeismicCatalog text file will be generated.\n\n**eventdownloader.py**: When asked to input the event, write the full event name. For example, to download the S1222a event, write *S1222a*.\n\n**massivedownloader.py**: When asked to input the Quality, write *A, B, C, or D*. When asked to input the Class, write the name of the class in the way that it is written in the SeismicCatalog text file, which means *LOW_FREQUENCY, BROADBAND, 2.4_HZ, HIGH_FREQUENCY, and VERY_HIGH_FREQUENCY*. For example, to download the A quality BROADBAND events, write *A* for the Quality input, and *BROADBAND* for the Class input.\n\n## Output files\n\nThe output files of the suite are saved in a directory that has the following format in your system: **DATA/Event_Type/Event_Quality/Event_Name/**. There are 4 mseed files:\n\n* The raw data mseed file, named **Event_Name.mseed**\n* The event data with instrument response removal, rotation to Z, N, E and the application of a Tukey window filter, with the 5% of the timeseries within the sinusoidal function, named **Event_Name_DISP.mseed** for displacement, **Event_Name_VEL.mseed** for velocity and **Event_Name_ACC.mseed** for acceleration.\n\nThe Event_Name is given in the Marsquake Catalog and is defined by an alphanumeric digit for the type of the Event (S for seismic events), 4 digits for the Sol (Martian Day of InSight operations) and one alphanumeric digit for the order of the event within the indicated sol.\n\nThe Event_Type and the Event_Quality are the Type and Quality of the event respectively, as defined by Clinton et al., 2021.\n\n## Citations and acknowledgements\n\nThe scripts of this repository download Mars InSight seismic data from IRIS. It is developped by Foivos Karakostas, Doeyeon Kim, Ross Maguire, Aisha Khatib, Quancheng Huang, Angela Marusiak, Nicholas Schmerr, Ved Lekić and the University of Maryland InSight group. Please acknowledge.\n\nWhen you use InSight SEIS Data, please follow the [citation instructions](https://www.seis-insight.eu/en/science/seis-data/seis-citation-information) that are also copied here:\n\n*SEIS data must be cited as reference in the following way:*\n\n*Citation in text :*\n\n*InSight Mars SEIS Data Service. (2019). SEIS raw data, Insight Mission. IPGP, JPL, CNES, ETHZ, ICL, MPS, ISAE-Supaero, LPG, MFSC. https://doi.org/10.18715/SEIS.INSIGHT.XB_2016*\n\n*In addition an acknowledgement must also be provided to the SEIS operators as follows:*\n\n*\"We acknowledge NASA, CNES, their partner agencies and Institutions (UKSA, SSO, DLR, JPL, IPGP-CNRS, ETHZ, IC, MPS-MPG) and the flight operations team at JPL, SISMOC, MSDS, IRIS-DMC and PDS for providing SEED SEIS data.\"*\n\n*Furthermore, the SEIS experiment paper (Lognonné et al., 2019) must be used as reference for describing the instrument, in addition to the SEIS team papers used by the user for the analysis. For a collection of SEIS papers, see Banerdt and Russel, 2017 and Banerdt and Russel, 2019.*\n\n## References\n\n1. Clinton, J. F., Ceylan, S., van Driel, M., Giardini, D., Stähler, S. C., Böse, M., … Stott, A. E. (2021). The Marsquake catalogue from InSight, sols 0–478. Physics of the Earth and Planetary Interiors, 310, 106595. https://doi:10.1016/j.pepi.2020.106595\n2. InSight Mars SEIS Data Service. (2019). SEIS raw data, Insight Mission. IPGP, JPL, CNES, ETHZ, ICL, MPS, ISAE-Supaero, LPG, MFSC. https://doi.org/10.18715/SEIS.INSIGHT.XB_2016\n3. InSight Marsquake Service (2022). Mars Seismic Catalogue, InSight Mission; V12 2022-10-01. ETHZ, IPGP, JPL, ICL, MPS, Univ. Bristol. https://doi.org/10.12686/a18\n4. Lognonné, P., Banerdt, W.B., Giardini, D. et al. (2019). SEIS: Insight’s Seismic Experiment for Internal Structure of Mars. Space Sci Rev 215, 12. https://doi.org/10.1007/s11214-018-0574-6\n \n",
        "createdAt": "2021-04-28T08:08:36.000Z",
        "updatedAt": "2025-04-07T02:51:57.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/UMD-InSight/InSight-seismic-data-downloader/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "davofis/computational_seismology",
        "url": "https://github.com/davofis/computational_seismology",
        "description": "Jupyter notebooks. Suplementary material for the book Computational Seismology,  A Practical Introduction - Heiner Igel",
        "stars": 32,
        "forks": 10,
        "readme": "# computational_seismology\n\nThis repository contains jupyter notebooks to be used as suplementary material for the book [Computational Seismology,A Practical Introduction](https://global.oup.com/academic/product/computational-seismology-9780198717409?cc=de&lang=en&) by Heiner Igel.\n\n## Code Execution:  [![Binder](http://mybinder.org/badge.svg)](http://mybinder.org:/repo/davofis/computational_seismology)\nThe code of this repository can immediately be executed by anyone. Please hit the badge [![Binder](http://mybinder.org/badge.svg)](http://mybinder.org:/repo/davofis/computational_seismology). This opens all notebooks in an executable environment using the [Binder](http://mybinder.org/) project.\n\nAll directories refer to an specific chapter according with the numbering in their names.\n\n##Tasks for jupyter notebooks:\n\n| Chapter | Notebook                          | Tasks                                                  |    Status   |\n|---------|-----------------------------------|--------------------------------------------------------|:-----------:|\n|    2    | Lambs Problem                     | Write notebook that runs the Fortran code              |     DONE    |\n|    2    | Double couple analytical solution | Improve the existing python code                       |     DONE    |\n|    5    | Fourier derivative                | Polish                                                 |     DONE    |\n|    5    | Chebyshev derivative              | Polish                                                 |     DONE    |\n|    5    | Fourier 1D acoustic               | proper source scaling,compare with analytical solution |     DONE    |\n|    5    | Chebyshev 1D elastic              | turn into notebook, clean up, add equations, etc       |     DONE    |\n|    5    | Fourier 2D                        | adapt to seismo-live make it _solution to, 5.22        |     DONE    |\n|    6    | Finte element static - relaxation |                                                        |     DONE    |\n|    6    | FE elastic 1D                     |                                                        |     DONE    |\n|    7    | spectral elements 1D elastic      |                                                        |     DONE    |\n|    8    | FV 1D advection scalar            |                                                        |     DONE    |\n|    8    | FV 1D elastic homogeneous         |                                                        |     DONE    |\n|    8    | FV 1D elastic heterogeneous       |                                                        |     DONE    |\n|    9    | DG 1D elastic homogeneous         |                                                        |     DONE    |\n|    9    | DG 1D elastic heterogeneous       |                                                        |     DONE    |\n\n\n",
        "createdAt": "2016-09-18T16:07:10.000Z",
        "updatedAt": "2025-06-25T15:52:23.000Z",
        "language": "Jupyter Notebook",
        "homepage": "https://global.oup.com/academic/product/computational-seismology-9780198717409?cc=de&lang=en&",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/davofis/computational_seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "JUNZHU-SEIS/Python_tutorial_for_seismology",
        "url": "https://github.com/JUNZHU-SEIS/Python_tutorial_for_seismology",
        "description": null,
        "stars": 0,
        "forks": 1,
        "readme": "",
        "createdAt": "2023-03-31T04:37:10.000Z",
        "updatedAt": "2023-09-13T13:05:37.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "seismo-netizen/Urban_Seismology",
        "url": "https://github.com/seismo-netizen/Urban_Seismology",
        "description": null,
        "stars": 2,
        "forks": 1,
        "readme": "",
        "createdAt": "2022-03-14T11:22:18.000Z",
        "updatedAt": "2022-03-17T09:50:12.000Z",
        "language": "MATLAB",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ebeauce/eqtempclust",
        "url": "https://github.com/ebeauce/eqtempclust",
        "description": "Package for the analysis of occupation probability and inter-event times in statistical seismology.",
        "stars": 1,
        "forks": 0,
        "readme": "# eqtempclust\n\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n![](https://img.shields.io/github/commit-activity/w/ebeauce/eqtempclust)\n![](https://img.shields.io/github/last-commit/ebeauce/eqtempclust)\n![](https://img.shields.io/github/stars/ebeauce/eqtempclust?style=social)\n\n\nPackage for the analysis of occupation probability and inter-event times in statistical seismology.<br>\n\nThis package accompanies the following manuscript:<br>\nEric Beaucé, **Measuring and modeling the occupation probability to characterize the temporal statistics of seismic sequences**, *Geophysical Journal International*, DOI: [https://doi.org/10.1093/gji/ggaf433](https://doi.org/10.1093/gji/ggaf433)<br>\n\n<p align=\"center\">\n<img src=\"data/readme.png\" width=400>\n</p><br><br><br><br>\n\n",
        "createdAt": "2023-04-27T15:56:15.000Z",
        "updatedAt": "2025-12-01T16:24:50.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ebeauce/eqtempclust/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ladominguez/Jupyter-Seismology",
        "url": "https://github.com/ladominguez/Jupyter-Seismology",
        "description": "Jupyter notebooks con diversos problemas de sismología. ",
        "stars": 1,
        "forks": 1,
        "readme": "# Jupyter-Seismology\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/ladominguez/Jupyter-Seismology/master)\nJupyter notebooks con diversos problemas de sismología. \n\n- Práctica 1 - Respuesta al instrumento.\n- Práctica 2 - Esfuerzos y deformaciones.\n- Práctica 3 - Ecuación de onda.\n",
        "createdAt": "2020-09-10T20:48:46.000Z",
        "updatedAt": "2023-01-31T15:22:28.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ladominguez/Jupyter-Seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "lowbontimp/stp-iris",
        "url": "https://github.com/lowbontimp/stp-iris",
        "description": "A client program to access the IRIS Web Services on the command-line interface",
        "stars": 1,
        "forks": 1,
        "readme": "# stp-iris.pl\n`stp-iris.pl` is a perl script for downloading seismic waveform data in the format of [SAC](http://ds.iris.edu/files/sac-manual/) \nfrom [IRIS Web Services](http://service.iris.edu) on the command-line interface.\n\n## Strength\n* It runs in command-line interface.\n* It is light. Minimal memory of computer is used.\n* It is fast. Raytracing is not performed to find a proper window.\n\n## Installation\n### 1. Perl library\nYou should install perl libraries below. \n\n* LWP::UserAgent\n* Date::Calc::XS\n\nPerl library can be easily installed using `cpan`\n\n```\n$) PERL_MM_USE_DEFAULT=1 perl -MCPAN -e 'install +LWP::UserAgent'\n$) PERL_MM_USE_DEFAULT=1 perl -MCPAN -e 'install +Date::Calc::XS'\n```\n\n\n### 2. Setting the path of `sac`\nGo to edit mode (`vi ./stp-iris.pl`) and set the line below properly for your system.\n```\nmy $sac = \"/opt/sac/bin/sac\" ;\n```\nYou can know where `sac` is by\n```\n$) which sac\n```\nIf not installed, `sac` should be preinstalled at [IRIS's Software Downloads](https://ds.iris.edu/ds/nodes/dmc/software/downloads/sac/).\n\n## Tutorial\n### Start\nStart `stp-iris.pl`.\n```\n$) chmod +x ./stp-iris.pl\n$) ./stp-iris.pl\n```\nPermit to make a working directory.\n```\n./.tmpdir.aJyrZ0RjID782P88 will be generated. [y/n]:\n```\nYou might see below.\n```\n+++++++++++++++++++++++++++\n|       stp-iris.pl       |\n|(http://service.iris.edu)|\n+++++++++++++++++++++++++++\n\nVersion 1.00 (Apr. 2021)\n\nType help(h) to see usage.\n\n---------------------------\n```\n\n### Downloading the sac files for an earthquake\n```\nSTP) evt 2015/09/08,08:19:53.790 -33.1147 -178.2046 12.77 5.5(mww) O(-200) O(1800) 7D % -- BHZ\n     1   2                       3        4         5     6        7       8       9  1011 12\n```\n```\n1: command name\n2: yyyy/mm/dd,hh:mm:ss(.sss)\n3: latitude\n4: longitude\n5: depth\n6: magnitude (class of magnitude)\n7: starting time in terms of origin time defined at '2'\n8: ending time in terms of origin time defined at '2'\n9: network (wild cards: % = * and _ = ? in Linux)\n10: station (same above)\n11: location (same above)\n12: channel (same above)\n```\nOutput looks like this.\n```\nsaved: ./RESP.7D.FS04D.--.BHZ\nsaved: ./20150908081953.7D.FS04D.--.BHZ.sac\nsaved: ./RESP.7D.FS08D.--.BHZ\nsaved: ./20150908081953.7D.FS08D.--.BHZ.sac\nsaved: ./RESP.7D.FS44D.--.BHZ\nsaved: ./20150908081953.7D.FS44D.--.BHZ.sac\nsaved: ./RESP.7D.G10D.--.BHZ\nsaved: ./20150908081953.7D.G10D.--.BHZ.sac\nsaved: ./RESP.7D.G33D.--.BHZ\nsaved: ./20150908081953.7D.G33D.--.BHZ.sac\nsaved: ./RESP.7D.G37D.--.BHZ\nsaved: ./20150908081953.7D.G37D.--.BHZ.sac\nsaved: ./RESP.7D.J20D.--.BHZ\nsaved: ./20150908081953.7D.J20D.--.BHZ.sac\nsaved: ./RESP.7D.M16D.--.BHZ\nsaved: ./20150908081953.7D.M16D.--.BHZ.sac\n```\n### Define the box where the stations are searched\nGo to edit mode (`vi ./stp-iris.pl`) and change the lines below for your purpose.\n```\nmy $box ;\n$box .= \"&minlat=38.8533\" ;\n$box .= \"&maxlat=51.149\" ;\n$box .= \"&minlon=-134.0732\" ;\n$box .= \"&maxlon=-122.3837\" ;\n```\nUse [IRIS's gmap](http://ds.iris.edu/gmap) to find a proper box.\n\n\n### Changing directory where files are saved\n```\nSTP) dir data01/001\nSTP) evt 2015/09/08,08:19:53.790 -33.1147 -178.2046 12.77 5.5(mww) O(-200) O(1800) 7D % -- BHZ\n```\nOutput looks like this.\n```\nsaved: ./data01/001/20150908081953.7D.FS04D.--.BHZ.sac\nsaved: ./data01/001/20150908081953.7D.FS08D.--.BHZ.sac\nsaved: ./data01/001/20150908081953.7D.FS44D.--.BHZ.sac\nsaved: ./data01/001/20150908081953.7D.G10D.--.BHZ.sac\nsaved: ./data01/001/20150908081953.7D.G33D.--.BHZ.sac\nsaved: ./data01/001/20150908081953.7D.G37D.--.BHZ.sac\nsaved: ./data01/001/20150908081953.7D.J20D.--.BHZ.sac\nsaved: ./data01/001/20150908081953.7D.M16D.--.BHZ.sac\n```\n\nSimilarly, the directory where the response files are saved can be changed.\n```\nSTP) dirresp resp01\n```\nOutput becomes\n```\nsaved: resp01/RESP.7D.FS04D.--.BHZ\nsaved: resp01/RESP.7D.FS08D.--.BHZ\nsaved: resp01/RESP.7D.FS44D.--.BHZ\nsaved: resp01/RESP.7D.G10D.--.BHZ\nsaved: resp01/RESP.7D.G33D.--.BHZ\nsaved: resp01/RESP.7D.G37D.--.BHZ\nsaved: resp01/RESP.7D.J20D.--.BHZ\nsaved: resp01/RESP.7D.M16D.--.BHZ\n```\n\n\n### Input list of commands\n```\nSTP) input filepath\n```\nwhere the _filepath_ contains multiple lines of the commands.\n\nAn example file is\n```\ndirresp resp01\nskip on\ndir data01/001\nevt 2015/09/08,08:19:53.790 -33.1147 -178.2046 12.77 5.5(mww) O(-200) O(1800) 7D % -- BHZ\nevt 2015/09/08,08:19:53.790 -33.1147 -178.2046 12.77 5.5(mww) O(-200) O(1800) 7D % -- HHZ\ndir data01/002\nevt 2015/09/09,07:05:44.490 -49.5321 -116.3317 10.0 5.9(mwb) O(-200) O(1800) 7D % -- BHZ\nevt 2015/09/09,07:05:44.490 -49.5321 -116.3317 10.0 5.9(mwb) O(-200) O(1800) 7D % -- HHZ\ndir data01/003\nevt 2015/09/11,21:19:18.000 -5.98 146.66 26.0 5.5(mwc) O(-200) O(1800) 7D % -- BHZ\nevt 2015/09/11,21:19:18.000 -5.98 146.66 26.0 5.5(mwc) O(-200) O(1800) 7D % -- HHZ\ndir data01/004\nevt 2015/09/11,21:19:18.660 -5.9797 146.6564 26.0 5.5(mww) O(-200) O(1800) 7D % -- BHZ\nevt 2015/09/11,21:19:18.660 -5.9797 146.6564 26.0 5.5(mww) O(-200) O(1800) 7D % -- HHZ\ndir data01/005\nevt 2015/09/12,13:38:01.000 41.9 142.65 49.0 5.5(mwc) O(-200) O(1800) 7D % -- BHZ\nevt 2015/09/12,13:38:01.000 41.9 142.65 49.0 5.5(mwc) O(-200) O(1800) 7D % -- HHZ\n!zip -r data01.zip data01\n!zip -r resp01.zip resp01\n```\n`skip on` lets `stp-iris.pl` skip redownloading and overwriting the sac files if they exist.\n`!` enables one to run a linux command.\n\n### Quit\n```\nSTP) quit\n```\n\n### Help\n```\nSTP) help\n```\n## Example 1: many teleseismic earthquakes using JWEED\nConvert the event file, _cascadia1234.events_ made by the program [JWEED](https://ds.iris.edu/ds/nodes/dmc/software/downloads/jweed/), to an input file of `stp-iris.pl`.\nGo to directory _ex01_ by `cd ex01`.\n```\n$) ./jweed2stp.pl > cmd01\n```\nAfter turning on `stp-iris.pl`,\n```\nSTP) input cmd01\n```\n\n## Example 2: many teleseismic earthquakes using FDSN web service\nSimilarly, make an event file and convert it to the input file.\n```\ncd ex02\n./fdsn2stp.pl > cmd01\n```\nAfter turning on `stp-iris.pl`,\n```\nSTP) input cmd01\n```\n\n## Example 3: similar to 2 but with quakeml format\n```\n./getevents.pl #output = events.xml\n./xml2txt.var.pl events.xml > events.txt\n./txt2stp.pl > cmd\n```\nSome FDSN event server supports only format=xml. This is for it.\n\n## Example 4: \n\n## Tip for running it in background\n[`nohup`](https://linux.die.net/man/1/nohup) makes `stp-iris.pl` run in background and immune to an unexpected shutdown of terminal. It is advantageous when one downloads many data for a long time.\n```\n$) nohup ./stp-iris.pl\nSTP) input cmd01\nCTRL+Z\n$) bg\n```\n\n## Self-controlling rate of connection\n[Guidelines for IRIS DMC services](http://ds.iris.edu/ds/nodes/dmc/services/usage/)\nare requiring no more than *5 concurrent connections* and no more than *10 connections per second*.\nAvoid running too many processes of `stp-iris.pl` simultaneously.\n`stp-iris.pl` sleeps for a while when the averaged number of connections exceeds a threshold.\nRemoving or changing this part in `stp-iris.pl` needs carefulness.\n\n## List of to do\n- [ ] Option to change 'O' into other phases, such as 'P' or 'S' (using raytracing program).\n- [ ] Custom format of name of file. You should modify a variable `$outputfilename` in the source code if you want to change it.\n\n\n",
        "createdAt": "2021-04-16T02:06:53.000Z",
        "updatedAt": "2021-12-16T02:54:48.000Z",
        "language": "Perl",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/lowbontimp/stp-iris/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "swiss-seismological-service/scrtdd",
        "url": "https://github.com/swiss-seismological-service/scrtdd",
        "description": "Double Difference Relocator for SeisComP",
        "stars": 38,
        "forks": 13,
        "readme": "<pre>\n/***************************************************************************\n *   Copyright (C) by ETHZ/SED                                             *\n *                                                                         *\n * This program is free software: you can redistribute it and/or modify    *\n * it under the terms of the GNU Affero General Public License as published*\n * by the Free Software Foundation, either version 3 of the License, or    *\n * (at your option) any later version.                                     *\n *                                                                         *\n * This program is distributed in the hope that it will be useful,         *\n * but WITHOUT ANY WARRANTY; without even the implied warranty of          *\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the           *\n * GNU Affero General Public License for more details.                     *\n *                                                                         *\n *                                                                         *\n *   Developed by Luca Scarabello, Tobias Diehl                            *\n *                                                                         *\n ***************************************************************************/\n</pre>\n\n[![DOI](https://zenodo.org/badge/246001157.svg)](https://zenodo.org/badge/latestdoi/246001157)\n\nPlease cite the code as:\n\n\"Luca Scarabello & Tobias Diehl (2021). swiss-seismological-service/scrtdd. Zenodo doi: 10.5281/zenodo.5337361\"\n\n# Description\n\nrtDD is a [SeisComP](<https://github.com/SeisComP>) extension module that implements\nDouble-Difference event relocation both in Real-Time, one event at the time, and classic\noffline mode, where an earthquake catalog is relocated as a whole.\n\nrtDD contains a C++ library for double-difference inversion which doesn't depend on\nSeisComP and can be used in other systems. See for example [pyrtdd](https://github.com/swiss-seismological-service/pyrtdd).\n\nrtDD acknowledges support from [Geothermica](http://www.geothermica.eu/)\n\n## Documentation\n\nYou can find the online documentation at https://docs.gempa.de/scrtdd/current/\n\n## Installation from binaries\n\nYou can find compiled version of this module at https://data.gempa.de/packages/Public/.\nThe installation file is a compressed tar archive containing the binary distribution of\nthis module, which can be extracted in your SeisComP installation folder.\n\n## Installation from source code\n\nIn order to use this module the sources have to be merged into the *SeisComP* sources,\nthen *SeisComP* can be compiled and installed as usual. Follow the up-to-date procedure\nat https://github.com/SeisComP/seiscomp\n\n<pre>\n#\n# get SeisComP (follow official documentation)\n#\ngit clone https://github.com/SeisComP/seiscomp.git seiscomp\ncd seiscomp/src/base\ngit clone https://github.com/SeisComP/common.git\ngit clone https://github.com/SeisComP/main.git\n[...etc...]\n\n#\n# Add rtdd into SeisComP\n#\ncd seiscomp/src/extras\ngit clone https://github.com/swiss-seismological-service/scrtdd.git\n\n#\n# Compile code as per SeisComP official documentation\n#\n...\n</pre>\n\nTo update rtDD to a new version:\n\n<pre>\n#\n# go to rtDD folder\n#\ncd seiscomp/src/extras/scrtdd\n\n#\n# fetch the changes that happened on rtDD repository\n#\ngit fetch origin -p --tags\n\n#\n# checkout the new version\n#\ngit checkout master\ngit rebase origin/master\n</pre>\n\n## Tests\n\nIf tests were enabled during the compilation of SeiComP then `scrtdd` can be tested\nrunning the following command in the build folder:\n.\n<pre>\nseiscomp/build$ ctest -R test_hdd\n</pre>\n\nAlternatively `scrtdd` can be tested together with the whole SeisComP test suite with:\n\n<pre>\nseiscomp/build$ make test\n</pre>\n\nSee SeisComP's [unit testing guide](https://docs.gempa.de/seiscomp/current/base/tests.html).\n\nPlease note that due to the size of the NonLinLoc grids, not all the transformation are\ntested. To let the tests cover all the implemented transformations the script \n`libs/hdd/test/data/nll/generate.ch` should be run, which will generate the missing grids.\n",
        "createdAt": "2020-03-09T10:04:31.000Z",
        "updatedAt": "2025-11-19T10:32:30.000Z",
        "language": "C++",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/swiss-seismological-service/scrtdd/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "anowacki/SeisSplit.jl",
        "url": "https://github.com/anowacki/SeisSplit.jl",
        "description": "Seismic shear wave splitting analysis in Julia",
        "stars": 8,
        "forks": 4,
        "readme": "# SeisSplit\n\nSeisSplit is a package for measuring shear wave splitting using the\nminimum eigenvalue method of Silver & Chan (1991), as modified by\nWalsh et al. (2013).\n\nIt uses the [Seis.jl](https://github.com/anowacki/Seis.jl) package which\nprovides fast, flexible seismic analysis.\n\nComputation of splitting using the rotation-correlation method\n(e.g., Bowman & Ando, 1987) is also possible.  This is computed\nautomatically by the main `splitting` function for quality control\npurposes.\n\n## Installing\n\n```julia\njulia> ] # Press ']' to get to package mode\n\n(@v1.5) pkg> add https://github.com/anowacki/Geodesics.jl https://github.com/anowacki/Seis.jl https://github.com/anowacki/SeisSplit.jl\n```\n\n## Using\n\nThe `splitting` function performs shear wave splitting analysis and returns a `SeisSplit.Result`\ntype containing information about the analysis.  Provide two `Seis.Trace`s, and\noptionally specify the maximum delay time and number of fast orientation and delay\ntime analysis points.\n\nWe can show an example of using SeisSplit on some synthetic test data,\nwhere the signal has had splitting of 1.4 s applied with a fast shear\nwave orientation of 40°.  Looking at the result `s`, the values of `s.phi_best`\nand `s.dt_best` show have recovered the expected splitting parameters:\n\n```julia\njulia> using Seis, SeisSplit\n\njulia> data_dir = joinpath(dirname(pathof(SeisSplit)), \"..\", \"test\", \"data\");\n\njulia> n, e = Seis.read_sac.(joinpath(data_dir, \"wave.BH\") .* (\"N\", \"E\"))\n(Seis.Trace(.SWAV..BHN: delta=0.05, b=0.0, nsamples=1999), Seis.Trace(.SWAV..BHE: delta=0.05, b=0.0, nsamples=1999))\n\njulia> s = splitting(n, e, 15, 35)\nSeisSplit.Result{Float32,Array{Float32,1}}:\n           phi: -90.0:1.0:90.0\n            dt: 0.0:0.1:4.0\n          lam1: 181×41 Array{Float32,2}: [68.06532, ..., 68.06532]\n          lam2: 181×41 Array{Float32,2}: [40.643196, ..., 40.643196]\n      phi_best: 40.0 °\n          dphi: 0.5 °\n       dt_best: 1.4 s\n           ddt: 0.025 s\n          spol: 10.00806 °\n         dspol: 0.22093524 °\n        trace1: Seis.Trace(.SWAV..BHN: delta=0.05, b=0.0, nsamples=1999)\n        trace2: Seis.Trace(.SWAV..BHE: delta=0.05, b=0.0, nsamples=1999)\n  window_start: 15 s\n    window_end: 35 s\n           ndf: 302\n     xcorr_phi: 90 Array{Float64,1}: [-90.0, ..., -88.0]\n      xcorr_dt: 81 Array{Float32,1}: [0.0, ..., 0.05]\n     xcorr_map: 81×90 Array{Float32,2}: [0.48693663, ..., 0.5155819]\nxcorr_phi_best: 42.0 °\n xcorr_dt_best: 1.35 s\n```\n\nThe following is the docstring for the `splitting` function:\n\n```\n    splitting(t1, t2, window_start=starttime(t1), window_end=endtime(t1); nphi=181, ndt=41, dt_max=4, xcorr=true) -> results\n\nPerform a search over a pair of Seis traces, `t1` and `t2`, for the smallest value of the\nminimum eigenvalue of the covariance matrix between the traces, for a set of `nphi`×`ndt`\nshear wave splitting operators, up to `dt_max` s.\n\n ## Output\n\n`results` is a `SeisSplit.Result` containing:\n\n- `phi`: The set of fast shear wave orientations in °\n- `dt`: The set of delays times in s\n- `lam1`: The larger eigenvalues at each [phi,dt] point\n- `lam2`: The smaller eigenvalues at each point\n- `phi_best` and `dphi`: The best ϕ and its 1σ uncertainty.  ϕ is measured\n  clockwise from local north (towards east) in °.\n- `dt_best` and `ddt`: The best δt and its 1σ uncertainty, in s\n- `spol` and `dspol`: The source polarisation and an estimate of its uncertainty for the\n  best-fitting ϕ and δt.  `spol` is given in ° clockwise of local north.\n- `trace1` and `trace2`, the original input traces, where `trace2` is clockwise of `trace1`\n- `window_start`, `window_end`, the analysis time window end points.\n- `ndf`, the number of degrees of freedom in the signal.\n\nIf `xcorr` is `true`, then the rotation correlation map for the pair\nof traces is also computed and the following additional fields are\npresent in `results`:\n\n- `xcorr_phi`: Angles over which rotation correlation was calculated (°)\n- `xcorr_dt`: Delay times over which rotation correlation was calculated (s)\n- `xcorr_map`: Cross correlation at each [phi,dt] point\n- `xcorr_phi_best`: Fast orientation of maximum cross correlation (°)\n- `xcorr_dt_best`: Delay time of maximum cross correlation (s)\n```\n\n### Plotting results\n\nYou can create a diagnostic plot of a `SeisSplit.Result` by loading\n[`Plots.jl`](https://github.com/JuliaPlots/Plots.jl) and calling `plot()` on the result:\n\n```julia\njulia> using Plots\n\njulia> plot(s)\n```\n\n![Example of a SeisSplit diagnostic plot](docs/images/diagnostic_plot_example.svg)\n\n(Note that Plots.jl must be included in the current environment; if it\nis not, then simply do `import Pkg; Pkg.add(\"Plots\")` first.)\n\n\n## Contributing\n\nTo report a bug, please\n[open an issue](https://github.com/anowacki/SeisSplit.jl/issues/new) and\nprovide as much detail as possible to reproduce the bug.  Ideally this\nshould be in the form of some code representing a\nminimal non-working example ([MWE](https://stackoverflow.com/help/minimal-reproducible-example)), but all reports are welcome.  I will\ntry and fix any bugs that are reported, but please note that this may\nnot be very quickly, depending on my other commitments.\n\nPull requests (especially if they implement bug fixes) are very welcome.\nFeature requests will be seriously considered.  Please make sure that\nnew code comes with tests, and that all tests pass with your addition.\n\n\n## References\n\n- Bowman, J. R., Ando, M., 1987. Shear-wave splitting in the\n  upper-mantle wedge above the Tonga subduction zone.  Geophys J R\n  Astr Soc 88, 25–41.\n  doi:[10.1111/j.1365-246X.1987.tb01367.x](https://doi.org/10.1111/j.1365-246X.1987.tb01367.x)\n- Silver, P.G., Chan, W.W., 1991. Shear-wave splitting and subcontinental mantle\n  deformation. J Geophys Res-Sol Ea 96, 16429–16454.\n  doi:[10.1029/91JB00899](https://doi.org/10.1029/91JB00899)\n- Walsh, E., Arnold, R., Savage, M.K., 2013. Silver and Chan revisited.\n  Journal of Geophysical Research: Solid Earth 118, 5500–5515.\n  doi:[10.1002/jgrb.50386](https://doi.org/10.1002/jgrb.50386)\n",
        "createdAt": "2019-03-11T20:25:11.000Z",
        "updatedAt": "2025-04-23T14:33:52.000Z",
        "language": "XC",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/anowacki/SeisSplit.jl/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Kayieni/thesis_app",
        "url": "https://github.com/Kayieni/thesis_app",
        "description": "The purpose of this thesis is to design a Web application so that it is possible to calculate the average focal mechanism of seismic events within predefined area sources, with the help of the Stressinverse software.",
        "stars": 3,
        "forks": 2,
        "readme": "# SICaMoT: Stress Inverssion Calcuator of Moment Tensors\nA Web application to calculate the stress inversion for a group of moment tensors of seismic events within predefined geographical areas, with the help of the STRESSINVERSE and Gisola softwares. The calculations are updated as new data are identified for each defined region. For the description of the calculation areas, area sources that have been proposed in the recent past for the Greek area are used, and there is also the possibility to manually design the desired surface, thus covering more possible applications for the needs of the individual researcher. Additionally, the desired group of moment tensors can be filtered by date, magnitude, depth and rake.\n\n## Screenshots\n\n![image](https://github.com/Kayieni/thesis_gisola_plugin/assets/44552188/7b67c96d-e88f-46a9-bedc-172c6beb3500)\n\n![image](https://github.com/Kayieni/thesis_app/assets/44552188/7136c764-ea1b-4e33-a924-01642e4f7e6c)\n\n![image](https://github.com/Kayieni/thesis_app/assets/44552188/038464cb-43d1-415b-888c-af9a64296858)\n\n![image](https://github.com/Kayieni/thesis_app/assets/44552188/3989ea5d-a1dc-42ba-8802-2bba94b31b1e)  ![image](https://github.com/Kayieni/thesis_app/assets/44552188/12cb545c-23f9-4b70-bb40-0f51b2266c09)\n\n\n\n",
        "createdAt": "2023-03-24T14:00:29.000Z",
        "updatedAt": "2024-09-05T03:49:15.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Kayieni/thesis_app/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "adjtomo/pysep",
        "url": "https://github.com/adjtomo/pysep",
        "description": "Seismogram Extraction and Processing: Seismic data retrieval and record sections ",
        "stars": 29,
        "forks": 18,
        "readme": "Python Seismogram Extraction and Processing\n===========================================\n\n[![PyPI Version](https://img.shields.io/pypi/v/pysep-adjtomo.svg)](https://pypi.python.org/pypi/pysep-adjtomo)\n[![SCOPED](https://img.shields.io/endpoint?url=https://runkit.io/wangyinz/scoped/branches/master/pysep)](https://github.com/SeisSCOPED/container/pkgs/container/pysep)\n\n`PySEP` uses ObsPy routines to request and standardize seismic data and metadata, returning uniform, consistent, and minimally processed waveforms for use in moment tensor and waveform inversions. \n\nThe package also contains core classes `RecSec` a **rec**ord **sec**tion plotter for rapid visualization of\nwaveform data, and `Declust`, for earthquake catalog declustering and geographical source-receiver weighting for waveform inversions.\n\n- Documentation can found ReadTheDocs: https://pysep.readthedocs.io/en/latest\n- Found a bug, requesting a new feature, or wanting to know how to use`PySEP`? [Feel free to open a GitHub issue](https://github.com/adjtomo/pysep/issues).\n- As part of the [SCOPED toolkit](https://github.com/SeisSCOPED), `PySEP` has been \n[containerized](https://github.com/SeisSCOPED/pysep/pkgs/container/pysep) \nusing Docker.\n",
        "createdAt": "2017-08-24T23:41:02.000Z",
        "updatedAt": "2025-10-17T21:40:22.000Z",
        "language": "Python",
        "homepage": "https://pysep.readthedocs.io",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/adjtomo/pysep/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "nadavwetzler/TDMTW",
        "url": "https://github.com/nadavwetzler/TDMTW",
        "description": "A full moment tensor solver",
        "stars": 9,
        "forks": 2,
        "readme": "# TDMTW\nA full moment tensor solver\n\nFocal mechanisms is computed using the full-waveform time-domain moment tensor (TDMT) technique (Dreger and Helmberger 1993), which implemented at the Geological Survey of Israel. Data are extracted by a magnitude dependent time windows starting 80 s before the event origin time, corrected for instrument response and the horizontal components are rotated to great circle path. Green’s functions are computed using the frequency-wavenumber integration code (FKPROG) of Saikia (1994) based on the Israel velocity model of Gitterman et al. (2002) with a 10 Hz sampling rate. This velocity model can be updated and recalculated using your own velocity model. See below for more details.\n\nGreen’s functions are band-pass filtered in the frequency band of 0.06–0.1 Hz for earthquake with magnitude 3.0≤MW≤4.0, and 0.5–1.0 Hz for MW≤2.9 in order to capture the high frequency seismic energy content of smaller magnitude earthquakes. This can be modified manually using -fmin and -fmax on the command line.\n\nThe best result is achieved through a grid-search on the depth and choosing the moment tensor solution and centroid depth for which the variance reduction (VR) is at maximum.  \n\nDreger, D.S., Helmberger, D. V., 1993. Determination of source parameters at regional distances with three- component sparse network data. J. Geophys. Res. 98, 8107–8125. https://doi.org/10.1029/93JB00023\n\nGitterman, Y., Pinsky, V., Shapira, A., Ergin, M., Kalafat, D., Gurbuz, G., Solomi, K., 2002. Improvement in detection, location, and identification of small events through joint data analysis by seismic stations in the Middle East/Eastern Mediterranean region 13. https://doi.org/DTRA01-00-C-0119\n\nSaikia, C.K., 1994. Modified frequency‐wavenumber algorithm for regional seismograms using Filon’s quadrature: modelling of Lg waves in eastern North America. Geophys. J. Int. 118, 142–158. https://doi.org/10.1111/j.1365-246X.1994.tb04680.x\n\nWetzler, N., Shalev, E., Göbel, T., Amelung, F., Kurzon, I., Lyakhovsky, V., Brodsky, E.E., 2019. Earthquake swarms triggered by groundwater extraction near the Dead Sea Fault. Geophys. Res. Lett. 46, 2019GL083491. https://doi.org/10.1029/2019GL083491\n\n# Please don't forget to cite if you use this code:\nWetzler, N., Sagy, A., Marco, S., Reches, Z., 2021. Asymmetry of faults and stress patterns within the Dead Sea basin as displayed by seismological analysis. Tectonophysics 229069. https://doi.org/10.1016/j.tecto.2021.229069\n\nor\n\nWetzler, N., Shalev, E., Göbel, T., Amelung, F., Kurzon, I., Lyakhovsky, V., Brodsky, E.E., 2019. Earthquake swarms triggered by groundwater extraction near the Dead Sea Fault. Geophys. Res. Lett. 46, 2019GL083491. https://doi.org/10.1029/2019GL083491\n\n# Run\n As a test case I chose one of the M4 along the main Dead Sea Transform - a left lateral strike-slip fault system.\n Use this line on your command line from your /MT folder:\n \n python3.6 tdmtw.py -ot 2021-06-15T23:08:54 -lat 30.099 -lon 35.178 -d 21 -m 4.1 -c GFZ\n \n -ot: Origine time (2021-06-15T23:08:54 - string)\n \n -lat: Latitude of the earthquake (30.099 - float)\n \n -lon: Longitude of the earthquake (35.178 - float)\n \n -d: Depth (4 - intiger)\n \n -m: Magnitude (4.7 - float)\n \n -c: the FDSN servers (GFZ,IRIS,... - you can use multiple providers by using commas)\n \n Optional parameters:\n \n -fmin: the low bp filter frequency\n \n -fmax: the upper bp filter frequency\n \n\n \n For more information about this event:\n https://earthquake.co.il/en/earthquake/feltInfo.php?ID=202106152307\n\n\n![Main](https://user-images.githubusercontent.com/88764899/129444678-5f9478a5-4dad-4169-b254-eb3101704fe5.png)\n\n# Solution\nTwo solutions are \n![Figure_2011](https://user-images.githubusercontent.com/88764899/129444688-54977b42-5c54-4845-a90a-ad0273cb503a.png)\n\n# Green's Functions\n1) sudo apt-get install csh\n\nyou can try to use the pre-compiled file in BIN_Linux or BIN_HighSierra\nIt is important to add the BIN_Linux or BIN_HighSierra folder to the .bashrc file\nedit the file:\ncd ~\nnano .bashrc\nexport GFBIN='pathto/TDMTW-main/' \nexport PATH=${PATH}:${GFBIN}/GF/BIN_Linux\n\nOr make exe file to run green functions:\nYou can make your own GF by running GF/Py/mk_Green_functions.py\n\n1) you need to install SAC from: http://ds.iris.edu/ds/nodes/dmc/software/downloads/sac/102-0/\n2) cd GF/FK-Integration\n3) make clean\n4) make\n5) cd UTILITIES\n6) make clean\n7) make\n\n\ncd to /GF/Py\npyhton3 mk_Green_function.py \n\nFor MacOS use:\nPath2_bin = pathlib.Path('../BIN_HighSierra').resolve()\n\nFor Linux use:\nPath2_bin = pathlib.Path('../BIN_Linux').resolve()\n\nVelocity model is used from GF/models/ .Use Gitt02.csv as a tamplate\n\nOr you can download the compressed Israel GF from \nhttps://figshare.com/articles/dataset/MSEED_GF_files/25567293\n\n# Customization\n\nLogo - please free to change to your preferred logo. The logo is placed in the /MT/Layers/Logo.png \n\nFaults - Global faults are ploted from the compilation of Bird (2002) http://peterbird.name/publications/2003_pb2002/2003_pb2002.htm . The layer is \na shapefile in /MT/Layers/PB2002_boundaries.shp \n\nDocumantation is still in progress... More shold be added in the near future\n\nPlease contact me directly if you have any questions: nadav.wetzler@gmail.com\n\nCheers,\n\nNadav\n\n",
        "createdAt": "2021-08-11T06:48:25.000Z",
        "updatedAt": "2025-09-26T07:59:55.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/nadavwetzler/TDMTW/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ashimrijal/MPI",
        "url": "https://github.com/ashimrijal/MPI",
        "description": "From computational seismology class",
        "stars": 0,
        "forks": 0,
        "readme": "# MPI\n",
        "createdAt": "2017-08-13T12:44:12.000Z",
        "updatedAt": "2021-02-28T12:04:25.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ashimrijal/MPI/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "aochihi/DepthStress",
        "url": "https://github.com/aochihi/DepthStress",
        "description": "Depth-dependent initial condition for earthquake simulations according to Aochi (2018) and Aochi & Tsuda (2023).",
        "stars": 0,
        "forks": 0,
        "readme": "# Depth depedent stress setting for dynamic rupture simulations\n\nThis repertory provides the programs calculating the depth-dependent stress accumulation on faults for earthquake simulations.\n\nThe formulations are based on the following papers, using Coulomb friction and Mohr circle. \n\n1. Aochi, H., Dynamic asymmetry of normal and reverse faults due to constrained depth-dependent stress accumulation, Geophys. J. Int., 215, 2134-3243, 2018. https://doi.org/10.1093/gji/ggy407\n\n2. Aochi, H. & K. Tsuda, Dynamic rupture simulatios based on depth-dependent stress accumulation, Geophys. J. Int., 233, 182-194, 2023. https://doi.org/10.1093/gji/ggac453\n\n# Programs (Aochi, 2018)\n\ngeneral4_distrib.f: Stress condition for a normal fault. \n\ngeneral4r_distrib.f: Stress condition for a reverse fault. \n\n## Compile: \n\ngfortran general4_distrib.f\n\n## Default setting: \n\n  fs = 0.6 : static frictional coefficient\n  \n  fd = 0.48 : dynamic frictional coefficient\n  \n  cohesive = 5.0 (MPa): cohesive force\n  \n  dip = 45.0: fault dip\n  \n  ( t-value = 1.0 implicitly: defining how large Mohr circle is between two friction lines according to Aochi and Ulrich, BSSA, 2015.)\n\n## Input: \n\nNone (all the parameters are to set in the main program inside)\n\n## Output: \"test.out\" (zdep, s1, s3, fd, tau0, tp, tr, sn0)\n\n  zdep (km) : variable.\n\n  s1 : maximum principal stress = vertical: calculated.\n\n  s3 : minimum prinicipal stress = horizontal: calculated. \n\n  fd : dynamic frictional coefficient : parameter\n  \n  tau0 : initial shear stress for a given fault : calculated\n  \n  tp : peak strength for a given fault: calculated\n  \n  tr : residual strengh for a given fault: calculated\n  \n  sn0 : initial normal stress for a given fault: calculated\n\n\n# Programs (Aochi & Tsuda, 2023)\n\nlayer5rev_distrib.f: reverse faulting (Figure 3 and Figure 10)\n\nlayer5ss_distrib.f: strike-slip faulting (Figure 4)\n\n## Compile\n\ngfortran layer5rev_distrib.f\n\n## Input:\n\ninfile = \"1d.txt\" or \"1d_teil.txt\"\n\n1st line: Total number of layers.\n\n2nd lines - : Depth of layer top (km), Vs (m/s), medium density (kg/m3)\n\n## Output 1: \"before_constrained.dat\" for layer5rev_distrib.f\n\nDepth (km), S_3=S_v (MPa), S_1=S_H (MPa), Delta sigma (MPa), rigidity (Pa), Delta epsilon\n\n## Output 2: \"constrained.dat\" for layer5rev_distrib.f\n\n1st-2nd lines: headers\n\nDepth (km), S_3=S_v (MPa), S_1=S_H (MPa), Delta sigma (MPa), ridigidy (Pa), Delta epsilon\n\n## Output 1ss: \"ss_before_constrained.dat\" for layer5s_distrib.f\n\nDepth (km), S_3=S_h (MPa), S_2=S_v (MPa), S_1=S_H (Mpa), Delta sigma (MPa), rigidity (Pa), Delta epsilon\n\n## Output 2ss: \"ss_constrained.dat\" for layer5ss_distrib.f\n\n1st-2nd lines: headers\n\nDepth (km), S_3=S_h (MPa), S_2=S_v (MPa), S_1=S_H (Mpa), Delta sigma (MPa), rigidity (Pa), Delta epsilon\n\n\n\n",
        "createdAt": "2022-06-20T12:28:05.000Z",
        "updatedAt": "2024-07-15T11:22:05.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/aochihi/DepthStress/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "RaulS22/helio-seismology",
        "url": "https://github.com/RaulS22/helio-seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# helio-seismology\n\n\n",
        "createdAt": "2024-10-25T15:32:51.000Z",
        "updatedAt": "2024-11-12T15:49:50.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/RaulS22/helio-seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ricardocaiza/SEISMOLOGY-",
        "url": "https://github.com/ricardocaiza/SEISMOLOGY-",
        "description": "This direction contains the code for seismological analysis using Obspy ",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2021-12-13T12:48:25.000Z",
        "updatedAt": "2021-12-13T12:53:07.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "pyrocko/idas-convert",
        "url": "https://github.com/pyrocko/idas-convert",
        "description": "Convert Silixa iDAS TDSM data to MiniSeed and other seismological formats",
        "stars": 8,
        "forks": 2,
        "readme": "# DAS Convert\n\nConvert and downsample distribute acoustic sensing (DAS) data acquired by Silixa iDAS to seismological data formats. Main purpose is to quickly convert and downsample massive amounts of high-resolution TDMS data to MiniSeed.\n\nTo handle the massive amount of data generated by DAS interrogators, the conversion tool is leveraging parallel IO and signal-processing processing. On production systems a throughput of 200 MB/s can be archived while converting and downsampling (from 1 kHz to 200 Hz).\n\nThe signal processing routines are based on the Pyrocko, mature and well tested seismological framework.\n\n## Installation\n\n```sh\npython3 setup.py install\n```\n\n## Documentation\n\nFind the online documentation at https://pyrocko.org/idas_convert/docs/.\n\n## Usage\n\nDump a config file\n\n```sh\ndas_convert dump_config\n```\n\n### Example Config\n```yaml\n--- !idas.iDASConvertConfig\n# Loading TDMS in parallel and process\nnthreads_loading: 1\nnthreads_processing: 8\nqueue_size: 32\nprocessing_batch_size: 8\n\n# Threads used for downsampling the data\nnthreads: 8\n\n# Input paths\npaths:\n- /home/isken/src/idas-convert\n\n# Out path, see pyrocko.io for details\noutpath: '%(tmin_year)s%(tmin_month)s%(tmin_day)s/%(network)s.%(station)s_%(tmin_year)s%(tmin_month)s%(tmin_day)s.mseed'\n\n# Overwrite mseed meta information\nnew_network_code: ID\nnew_channel_code: HSF\n\ndownsample_to: 200.0\n\n# MiniSeed record length\nrecord_length: 4096\n# MiniSeed STEIM compression\nsteim: 2\n\nplugins:\n\n# A plugin handling the communication with the GFZ tage file system\n- !das_convert.gfz_tapes.GFZTapesConfig\n  enabled: false\n  bytes_stage: 1T\n  waterlevel: 0.6\n  wait_warning_interval: 600.0\n  release_files: true\n  path_tapes_mount: /projects/ether/\n  path_tapes_prefix: /archive_FO1/RAW/\n\n# A Telegram bot to keep up-to-date with the process\n- !das_convert.telegram_bot.TelegramBotConfig\n  enabled: false\n  # Telegram API Token\n  token: 9e98b8c0567149eb861838a1d770be7d\n  # Telegram Chat ID\n  chat_id: -1237123123\n  # A status message will be dispatched every 3600 s\n  status_interval: 3600.0\n```\n\n### Start Conversion\n\n```sh\ndas_convert my_config.yml\n```\n\nSee `das_convert -h` for more options.\n\n### Citation\n\nDAS Convert - Convert distributed acoustic sensing data, Isken, Marius Paul; Christopher, Wollin; Heimann, Sebastian; Javier, Quinteros; Karl-Heinz, Jäckel; Philippe, Jousset (2021): DAS Convert - Convert distributed acoustic sensing data. V. 1.0. GFZ Data Services. https://doi.org/10.5880/GFZ.2.1.2021.005\n\n### Legal Stuff\n```\nLicence: GNU General Public License, Version 3, 29 June 2007\n\nCopyright © 2021 Helmholtz Centre Potsdam GFZ German Research Centre for Geosciences, Potsdam, Germany\n\nDAS Convert is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. DAS Convert is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see <http://www.gnu.org/licenses/>.\n```\n",
        "createdAt": "2021-09-21T09:24:40.000Z",
        "updatedAt": "2025-10-03T08:51:12.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/pyrocko/idas-convert/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "pyrocko/qseek",
        "url": "https://github.com/pyrocko/qseek",
        "description": "The earthquake detection and localization framework 🔥",
        "stars": 46,
        "forks": 8,
        "readme": "# Qseek\n\n*Data-driven Earthquake Detection*\n\n[![uv](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/uv/main/assets/badge/v0.json)](https://github.com/astral-sh/uv)\n[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)\n[![Pre-commit and Tests](https://github.com/pyrocko/qseek/actions/workflows/tests.yaml/badge.svg)](https://github.com/pyrocko/qseek/actions/workflows/tests.yaml)\n[![Python 3.10+](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://python.org/)\n[![PyPI - Version](https://img.shields.io/pypi/v/qseek)](https://pypi.org/project/qseek/)\n[![Documentation](https://img.shields.io/badge/read-documentation-blue)](https://pyrocko.github.io/qseek/)\n<!-- [![PyPI](https://img.shields.io/pypi/v/lassie)](https://pypi.org/project/lassie/) -->\n\nQseek is a an automatic, data-driven earthquake detection and localisation tool designed for large seismic data sets. It combines neural network phase annotations with a stacking-and-migration and an adaptive octree localisation approach.\n\nKey features are:\n\n* Earthquake phase detection using machine-learning model from [SeisBench](https://github.com/seisbench/seisbench), pre-trained on different data sets:\n  * [PhaseNet (Zhu and Beroza, 2018)](https://doi.org/10.1093/gji/ggy423)\n  * [EQTransformer (Mousavi et al., 2020)](https://doi.org/10.1038/s41467-020-17591-w)\n  * [OBSTransformer (Niksejel and Zahng, 2024)](https://doi.org/10.1093/gji/ggae049)\n  * LFEDetect\n* Ray tracers:\n  * Constant velocity\n  * 1D Layered velocity model\n  * 3D fast-marching velocity model (NonLinLoc compatible)\n* Earthquake magnitudes and other features:\n  * Local magnitudes (ML) with different attenuation models\n  * Moment Magnitudes (MW) based on modelled attenuation curves ([Dahm et al., 2024](https://doi.org/10.26443/seismica.v3i2.1205))\n  * Ground motion attributes (e.g. PGA, PGV, ...)\n* Station Corrections\n  * SST: station specific corrections\n  * SSST: source specific station corrections\n\nQseek is built on top of [Pyrocko](https://pyrocko.org).\n\n## Documentation\n\nOnline documentation is available at <https://pyrocko.github.io/qseek/>.\n\n## Installation\n\nFrom [PyPi](https://pypi.org/project/qseek/).\n\n```sh\npip install qseek\n```\n\nInstallation from GitHub.\n\n```sh\npip install git+https://github.com/pyrocko/qseek\n```\n\n## Project Initialisation\n\nPrint the default config with\n\n```sh\nqseek config\n```\n\nEdit the `my-project.json`\n\nStart the earthquake detection with\n\n```sh\nqseek search search.json\n```\n\n## Packaging\n\nThe simplest and recommended way of installing from source:\n\n### Development\n\nLocal development through pip.\n\n```sh\ncd qseek\nuv pip install -e .\n```\n\nThe project utilizes pre-commit for clean commits, install the hooks via:\n\n```sh\npre-commit install\n```\n\n## Citation\n\nPlease cite Qseek as:\n\n> Isken, M., Niemz, P., Münchmeyer, J., Büyükakpınar, P., Heimann, S., Cesca, S., Vasyura-Bathke, H., & Dahm, T. (2025). Qseek: A data-driven Framework for Automated Earthquake Detection, Localization and Characterization. Seismica, 4(1). <https://doi.org/10.26443/seismica.v4i1.1283>\n\n## License\n\nContribution and merge requests by the community are welcome!\n\nQseek was written by Marius Paul Isken and is licensed under the GNU GENERAL PUBLIC LICENSE v3.\n",
        "createdAt": "2023-04-27T17:04:06.000Z",
        "updatedAt": "2025-11-23T13:47:09.000Z",
        "language": "Python",
        "homepage": "https://pyrocko.github.io/qseek/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/pyrocko/qseek/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "phekdey-pheng/Earthquake_prediction_dawn_of_the_new_seismology",
        "url": "https://github.com/phekdey-pheng/Earthquake_prediction_dawn_of_the_new_seismology",
        "description": "Auto-generated repository for Earthquake_prediction_dawn_of_the_new_seismology",
        "stars": 0,
        "forks": 0,
        "readme": "# Earthquake_prediction_dawn_of_the_new_seismology\n\nThis repository was auto-generated. \n\n## Description\n\nThis is a placeholder README for the Earthquake_prediction_dawn_of_the_new_seismology repository.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details. \n\n## Author\n\nPhekdey PHORN \n\nCreated on 2025-08-25.\n\n## Contact\n\nFor any inquiries, please contact [Phekdey PHORN](+855 89755770). Thank you for visiting my GitHub profile!\n",
        "createdAt": "2025-08-25T12:52:29.000Z",
        "updatedAt": "2025-08-25T12:53:01.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/phekdey-pheng/Earthquake_prediction_dawn_of_the_new_seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "CelsoReyes/zmap7",
        "url": "https://github.com/CelsoReyes/zmap7",
        "description": "ZMAP Seismology Software. V 7.x has been updated to MATLAB R2018a.",
        "stars": 96,
        "forks": 46,
        "readme": "# Zmap version 7.X Readme\n\n## About\n\nZMAP version7.X represents a major rework of ZMAP.\n\n## Requirements\n\n### ZMAP requires:\n- __MATLAB R2018a__ (Version 9.4) or higher\n- Mapping Toolbox\n- Statistics and Machine Learning Toolbox\n\n### To leverage the parallel computing abilities:\n- Parallel Computing Toolbox\n\n### Some functions also require:\n- Optimization toolbox.\n\n### About ZMAP7\n\nNearly every aspect of the program has been modified, with the following goals in mind:\n\n*  Make ZMAP compatible with modern MATLAB installations\n*  Make it easer to add additional functionality\n*  Make the user interfaces more consistent and interactive.\n*  Make code more robust (leveraging MATLAB functions and objects)\n*  Make existing code more readable and maintainable (reducing code duplication)\n*  Retrieve event data from online services (FDSN web services)\n \n**Be aware, the changes in ZMAP are extensive**, and not all previously existing features may still exist. Also, you may not be able to load existing save files.\nZMAP7 should work from Matlab 2018a upwards.\n \n**Since this is an *alpha* version, expect that there are many areas that are still under construction**. Menu items will likely move around, and figures will change.  However, the basic functionality is there and should be able to help you start to explore your seismology data. We look forward to your feedback and idea for what we should include.\n \nIf (when) you run into bugs, feel free to report through creating a GitHub issue. (“Report a ZMAP issue” , under the “Help” menu on the ZMAP windows.  Please, do not send reports through email, as the issue list will be accessible by both whomever continues to maintain this program and the community.  The issues can be properly documented, prioritized, and addressed. \n \nTo get started with git, and ZMAP7, I created a few movies:\nhttps://www.youtube.com/playlist?list=PLXUrwVIXIt9wQ5gkCP5B96k8EHzAX6bJX \n[Note: these may already be severely out-of-date]\n\n\n## Getting Started\n\n### Quick Start\n\n1. Download or clone ZMAP 7 to your computer. \n1. Start MATLAB.\n1. Change directory to the zmap directory\n1. in the MATLAB command line, type \"zmap\"\n\n> some sample data for Switzerland can be found in the file `zmap/resources/sample/SED_fdsn_2000_on.mat`\n\n> Video: https://www.youtube.com/embed/gONcFBy4p8U?end=79\n\n### Work flow\n\nMost work will happen in the Main ZMAP screen.  \n\n### Loading data\n\nFrom the main ZMAP window, choose the `catalog` menu, then select `get/load catalog` where you will be presented with several options including:\n\n* `from (*.mat file)` : retreive a catalog saved into a matlab data file.  Some sample data can be found in zmap/resources/sample\n\n* `from FDSN service` : retrieve a catalog from an FDSN web service\n\n* `from other formatted file` : this contains mostly unmaintained functions to import from other sources.\n\n\n![Catalog Overview](docs/img/catalog_overview_20180216.png)\n\nClicking on the `see distributions` button will show a few histograms that may help you decide where to set your parameters\n![Catalog Overview With Distributions](docs/img/catoverview_dist_20180216.png)\n\n## The Main interfaces\n\n### Main Map Screen\n\nOnce a catalog is loaded, earthquakes will be plotted in the Main Window.\n![MainMapScreen](docs/img/ZmapMainWindow_20180216.png)\nThis is where most of the work will happen.  The screen is divided into several sections.  When first presented, all events will be hilighted, and the main map will take up the entirety of the left side of the window.\n\nThe plots on the right side of the screen will reflect statistics for the entire catalog.\n\n#### map features\n\nSeveral features are plotted on the map along with the earthquakes. Which ones are shown can be controlled from the `Display` menu.  From here, you an also choose whether to view the map in 3D or toggle its aspect ratio to more-or-less match the geographic region.\n\n### Selecting data of interest\n\n#### Select a region\n\nRegions can be selected in a few ways. Start by right-clicking in the map. Several options related to regions will be presented including :\n\n* `Select events in BOX` : select a rectangular region by clicking on two corners to define a box.\n* `Select events in POLYGON`: select a region by creating a polygon with the mouse.  Anything other than a \"normal\" click will close the polygon.\n* `Select events in CIRCLE` : Click and drag from the center of the circle out to some radius of interest.\n* `Delete polygon` : deletes the shape. all events are once again active.\n\nWhen any of the above choices have been made, only the events within the region (or _shape_) will be colored. All other events become grey dots.  The plots to the right will also change to reflect your selection.\n\nWhile defining a circle, you'll see the radius.  This circle is an oval because the map is distorted at this latitude.\n![Define a Circle - in progress](docs/img/circle_inprogress.png)\n\nOnce a shape is defined, then all other events fade into the background.\n![Define a Circle - done](docs/img/circleselected.png)\n\n#### Working with a region\n\nRegions can be modified : scaled, dragged, points added, etc. by interacting with the shape itself.  \nSee the menu item `about editing polygons` from the `Sampling` menu for a list of ways to interact with a polygon.\n\n\n### Cross Sections\n\n#### Selecting cross sections\n\nOne or more cross sections can be created. Cross sections are defined along\ngreat-circle arcs, and therefore may not appear as straight lines on the map.\nTo create a cross section, choose `Define X-section` from the map's context menu (that is, right click on the map). A dialog box will appear to allow the user to choose a width, specify cross-section labels, or override the color. Simple Labels are automatically generated.\n![Map with Cross Sections](docs/img/map_with_2p5xsec.png)\n\nThe primary map will then shrink to accomodate a cross-section plot that will\nappear beneath the map.  In the above image, two cross sections were already made, and the third `C-C'` is in progress.\n\nNotice, information about these cross sections appear on the plots to the right, with colors that match the cross section.\n\n#### Interacting with cross sections\n\nBoth the tabs and the cross-section plots are fully interactive.\nClicking on the tab for a cross-section provides the opportunity to see information about the cross section, or to change its width and color.\nThe option `Examine this area` will change the shape to encompass the cross section.  Deleting the cross section from this menu will also remove it from the map.\n\nRight-Clicking on the axes labels will allow the axes to be changed to any of the available data fields associated with a catalog.  Additionally, Right-clicking on the data will allow you to change the coloring and size schemes.\n\n### Histogram Plots\n\nThe binning for these plots can be changed through the axes' context menu.\n\n* `FMD` : frequency magnitude distribution plot. Information contained in here can be further analyzed via the context menu.\n\n### Cummulative plots\n\n* `Cumplot` : show cumulative event count through time\n* `Moment` : show cumulative moment through time\n* `Time-Mag` : plot of magnitudes through time. (does not reflect cross-sections)\n* `Time-Depth` : plot of depths through time (does not reflect cross-sections)\n\nThese plots can all be opened in another window, available for further analysis. To do so, right click on the data line of interest.  Additionally, the axes scaling for many plots can be toggled between linear and logarithmic.\n\n## Other help pages\n\n[Adding Functionality](ADDING_FUNCTIONALITY.md)\n\n[How Do I...?](HOWDOI.md)\n",
        "createdAt": "2018-08-31T06:21:34.000Z",
        "updatedAt": "2025-11-24T15:26:13.000Z",
        "language": "MATLAB",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/CelsoReyes/zmap7/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "emmanuel-portes/earthquake-tracker",
        "url": "https://github.com/emmanuel-portes/earthquake-tracker",
        "description": "This project is intended to find and show seismological information all around the world.",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2024-04-08T14:37:43.000Z",
        "updatedAt": "2025-12-03T22:53:31.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ppizarror/arias-intensity",
        "url": "https://github.com/ppizarror/arias-intensity",
        "description": "Function in matlab to calculate AI (Arias Intensity) parameter from a seismic accelerogram",
        "stars": 3,
        "forks": 1,
        "readme": "<h1 align=\"center\">\r\n  <img alt=\"Arias Intensity\" src=\"https://res.ppizarror.com/other/matlab.png\" width=\"200px\" height=\"200px\" />\r\n  <br /><br />\r\n  Arias Intensity</h1>\r\n<p align=\"center\">Function in matlab to calculate AI (Arias Intensity) parameter from a seismic accelerogram</p>\r\n<div align=\"center\"><a href=\"https://ppizarror.com\"><img alt=\"@ppizarror\" src=\"https://res.ppizarror.com/badges/author.svg\" /></a>\r\n<a href=\"https://www.gnu.org/licenses/old-licenses/gpl-2.0.html\"><img alt=\"GPL V2.0\" src=\"https://res.ppizarror.com/badges/licensegpl2.svg\" /></a>\r\n</div><br />\r\n\r\n## Usage\r\n\r\nThe arias intensity function is defined by:\r\n\r\n```matlab\r\nai = arias_intensity(t, acc)\r\n```\r\n\r\nWhere:\r\n\r\n| Variable | Description |\r\n| :-: | :--|\r\n| t | Time of the seismic accelerogram |\r\n| acc | Acceleration (g) of the seismic accelerogram |\r\n\r\n## Example\r\n\r\nLets suppose that a seismic registry is stored on *data/CNV_APED_201604162359_N_100.txt*, the file structure is like:\r\n\r\n```\r\n0.000000\t-6.329500\r\n0.010000\t2.539600\r\n0.020000\t12.822900\r\n0.030000\t9.435300\r\n0.040000\t-5.397100\r\n0.050000\t-14.233900\r\n...\r\n```\r\n\r\nThen:\r\n\r\n```matlab\r\nai = arias_intensity(t, acc);\r\n>> ai = 0.198232\r\n```\r\n\r\n## License\r\n\r\nThis project is licensed under GPLv2 [https://www.gnu.org/licenses/gpl-2.0.html]\r\n\r\n\r\n## Author\r\n<a href=\"https://ppizarror.com\" title=\"ppizarror\">Pablo Pizarro R.</a> | 2017\r\n",
        "createdAt": "2017-04-02T05:30:10.000Z",
        "updatedAt": "2025-06-07T01:12:58.000Z",
        "language": "MATLAB",
        "homepage": "https://mathworks.com/matlabcentral/fileexchange/108329-arias-intensity/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ppizarror/arias-intensity/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "uofuseismo/urts",
        "url": "https://github.com/uofuseismo/urts",
        "description": "UUSS Real-Time Seismology",
        "stars": 0,
        "forks": 0,
        "readme": "# About \n\nThe project documentation can be found [here](https://uofuseismo.github.io/urts).\n\nUtah Real-Time Seismology is the next generation real-time seismic system in development at University of Utah Seismograph Stations.  Its innovations are \n\n   1. Distributed, scalable seismic processing algorithms.\n   2. Modules that utilize machine learning.\n\n# Status\n\nThe software is currently under active development.  The release date is unclear.\n\n# Compiling\n\nCheck this [link](https://uofuseismo.github.io/urts/_topic_install.html).\n",
        "createdAt": "2022-10-24T15:26:55.000Z",
        "updatedAt": "2025-09-22T13:52:37.000Z",
        "language": "C++",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/uofuseismo/urts/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "seismology-hamburg/schippkus_hadziioannou_2022",
        "url": "https://github.com/seismology-hamburg/schippkus_hadziioannou_2022",
        "description": "Repository to accompany the paper entitled \"Matched Field Processsing for complex Earth structure\" by Schippkus & Hadziioannou",
        "stars": 10,
        "forks": 3,
        "readme": "# Matched Field Processsing for complex Earth structure\n\n[![DOI](https://zenodo.org/badge/438557201.svg)](https://zenodo.org/badge/latestdoi/438557201)\n\n<img align=\"left\" src=\"assets/mfp.png\" width=\"400px\">\n\nRepository to accompany the paper entitled \"Matched Field Processsing for complex Earth structure\" by Schippkus & Hadziioannou, published as pre-print on EarthArXiv ([doi.org/10.31223/X5492H](https://doi.org/10.31223/X5492H)) and submitted to Geophysical Journal International for peer review.\n\nHere, we provide all scripts and data necessary to reproduce all of our results.\n\n`/tutorial_notebook` contains a Python Jupyter notebook that introduces standard Matched Field Processing in a few easy steps.\n\n`/settings_files` contains yaml settings files used in the Matched Field Processing code developed for this paper (see [seismology-hamburg/matched_field_processing](https://github.com/seismology-hamburg/matched_field_processing)). These are the input files used to generate the synthetic tests and real-data results in the manuscript. Note that for all figures, multiple `.yml` files are provided. These correspond either directly to each subplots or the type of GF used.\n\n`/data_info` contains information on which stations and data exactly were used for the two real data examples in Figures 5 & 6 of our manuscript. For the Chino Hills earthquake, this includes data from 54 seismic stations of the CI network (estimated file size 40 MB). For the continous seismic data in February 2019, this includes data from 342 of the BE, BW, CH, CZ, DK, EI, FR, G, GB, GE, GR, GU, II, IM, IU, IV, LX, MN, NL, NO, OE, OX, PL, PM, RD, SX, TH, and WM networks (estimated file size 2.0 GB). For citations of these networks refer to our paper or [fdsn.org/networks](https://fdsn.org/networks).\n\n`/figure_scripts` contains Python scripts used to produce the figures in our manuscript from the output files of [seismology-hamburg/matched_field_processing](https://github.com/seismology-hamburg/matched_field_processing) using the settings files in `/settings_files`. Note that these scripts will require adapation to your file structure. They are provided as-is and are not well-documented.\n",
        "createdAt": "2021-12-15T08:46:24.000Z",
        "updatedAt": "2024-04-01T08:40:59.000Z",
        "language": "Jupyter Notebook",
        "homepage": "https://doi.org/10.31223/X5492H",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/seismology-hamburg/schippkus_hadziioannou_2022/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Dinhson287/seismology",
        "url": "https://github.com/Dinhson287/seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# DdWeb\n\nThis project was generated with [Angular CLI](https://github.com/angular/angular-cli) version 17.2.3.\n\n## Development server\n\nRun `ng serve` for a dev server. Navigate to `http://localhost:4200/`. The application will automatically reload if you change any of the source files.\n\n## Code scaffolding\n\nRun `ng generate component component-name` to generate a new component. You can also use `ng generate directive|pipe|service|class|guard|interface|enum|module`.\n\n## Build\n\nRun `ng build` to build the project. The build artifacts will be stored in the `dist/` directory.\n\n## Running unit tests\n\nRun `ng test` to execute the unit tests via [Karma](https://karma-runner.github.io).\n\n## Running end-to-end tests\n\nRun `ng e2e` to execute the end-to-end tests via a platform of your choice. To use this command, you need to first add a package that implements end-to-end testing capabilities.\n\n## Further help\n\nTo get more help on the Angular CLI use `ng help` or go check out the [Angular CLI Overview and Command Reference](https://angular.io/cli) page.\n",
        "createdAt": "2024-10-05T09:52:58.000Z",
        "updatedAt": "2024-10-12T03:27:20.000Z",
        "language": "TypeScript",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Dinhson287/seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "microquake/docker-python-seismology",
        "url": "https://github.com/microquake/docker-python-seismology",
        "description": null,
        "stars": 0,
        "forks": 1,
        "readme": "# seismic-docker",
        "createdAt": "2019-04-14T11:54:17.000Z",
        "updatedAt": "2020-08-28T14:28:16.000Z",
        "language": "Dockerfile",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/microquake/docker-python-seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ducspe/seismology_earthquake_prediction",
        "url": "https://github.com/ducspe/seismology_earthquake_prediction",
        "description": "This work was presented at the European Geosciences Union 2023 (EGU23).",
        "stars": 2,
        "forks": 0,
        "readme": "",
        "createdAt": "2023-08-29T06:57:49.000Z",
        "updatedAt": "2024-06-06T07:01:00.000Z",
        "language": "Jupyter Notebook",
        "homepage": "https://doi.org/10.5194/egusphere-egu23-1967",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "NoiseCIEI/ThreeStation",
        "url": "https://github.com/NoiseCIEI/ThreeStation",
        "description": "Seismic Ambient Noise Three-Station Interferometry",
        "stars": 42,
        "forks": 16,
        "readme": "ThreeStation\n============\n\n.. image:: https://github.com/NoiseCIEI/ThreeStation/blob/main/doc/source/schematic.jpg?raw=true\n    :alt: \"ThreeStation: A Python package for performing three-station interferometry.\"\n\nCode to perform **three-station interferometry**:\n\nShane Zhang, Lili Feng, Michael H Ritzwoller, Three-station interferometry and tomography: coda versus direct waves, *Geophysical Journal International*, Volume 221, Issue 1, April 2020, Pages 521–541, https://doi.org/10.1093/gji/ggaa046\n\nWe welcome any feedback and ideas!\nLet us know by submitting `pull requests <https://github.com/noiseciei/threestation/pulls>`__\nor `issues <https://github.com/noiseciei/threestation/issues>`__ on GitHub.\n\nDocumentation\n-------------\n\nhttps://threestation.readthedocs.io/en/latest\n\nVideo tutorial: https://www.youtube.com/watch?v=4gxWHNAcoPU\n\nReference\n---------\n\n**This code is used in:**\n\n- Shane Zhang, Hongda Wang, Mengyu Wu, Michael H Ritzwoller, **Isotropic and azimuthally anisotropic Rayleigh wave dispersion across the Juan de Fuca and Gorda plates and U.S. Cascadia from earthquake data and ambient noise two- and three-station interferometry**, Geophysical Journal International, Volume 226, Issue 2, August 2021, Pages 862–883, doi: `10.1093/gji/ggab142 <https://doi.org/10.1093/gji/ggab142>`__\n\n- Shane Zhang, Lili Feng, Michael H Ritzwoller, **Three-station interferometry and tomography: coda versus direct waves**, Geophysical Journal International, Volume 221, Issue 1, April 2020, Pages 521–541, doi: `10.1093/gji/ggaa046 <https://doi.org/10.1093/gji/ggaa046>`__\n\nPlease consider adding your work here\n(sumbit `pull requests on GitHub <https://github.com/noiseciei/threestation/pulls>`__)\nif this code is used.\n",
        "createdAt": "2020-02-23T03:32:59.000Z",
        "updatedAt": "2025-11-03T13:03:46.000Z",
        "language": "Jupyter Notebook",
        "homepage": "https://threestation.readthedocs.io/en/latest",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/NoiseCIEI/ThreeStation/main/README.rst",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "noiz-group/noiz",
        "url": "https://github.com/noiz-group/noiz",
        "description": "This is a publish only mirror of repository of Noiz project. The main repository is available at https://gitlab.com/noiz-group/noiz  ",
        "stars": 1,
        "forks": 0,
        "readme": ".. SPDX-License-Identifier: CECILL-B\n.. Copyright © 2015-2019 EOST UNISTRA, Storengy SAS, Damian Kula\n.. Copyright © 2019-2023 Contributors to the Noiz project.\n\n#####\nNoiz\n#####\n\n[![pipeline status](https://gitlab.com/noiz-group/noiz/badges/master/pipeline.svg)](https://gitlab.com/noiz-group/noiz/commits/master)\n[![coverage report](https://gitlab.com/noiz-group/noiz/badges/master/coverage.svg)](https://gitlab.com/noiz-group/noiz/commits/master)\n\nAmbient Seismic Noise processing application\n\nDocumentation can be found at https://noiz-group.gitlab.io/noiz\n\nAcknowledgements\n-----------------\n\nMe, Damian Kula, I would like to personally thank two people who had a significant influence on how Noiz was developed.\n\nI would like to thank **Alexandre Kazantsev** for countless hours spent on brainstorming, consultations and validation of the Noiz.\nAlex, you are the best.\n\nGeneral tree-like structure of how the processing is performed was inspired by **Maximilien Lehujeur** and software that he wrote during his stay at EOST that was named ``labex``.\nThank you, Max!\n\nOriginal Funding & Open Sourcing\n---------------------------------\n\nInitial version (up to ``0.5``) was developed by Damian Kula during his time at EOST, University of Strasbourg.\nThe first sketch of the Noiz was created in frame of collaboration between EOST, Storengy SAS and ES Geothermie.\nFurther developments were done in frame of collaboration between EOST and Storengy SAS.\nIn 2023 project was released to Open Source under CeCILL-B license.\nDevelopments since git commit tagged with ``0.5`` are thanks to community efforts.\n\nOpen-source dependencies\n------------------------\nNoiz uses Obspy (https://github.com/obspy/obspy and https://github.com/obspy) both as a dependency as well as a source of derived methods.\nThe latter derived methods are all located in the subpackage src/noiz/processing/obspy_derived\nNoiz devoloppers thank all the Obpsy contributors\n",
        "createdAt": "2023-05-03T17:18:11.000Z",
        "updatedAt": "2025-11-19T10:09:32.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/noiz-group/noiz/main/README.rst",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "wanghaoyucn/seismologybookMATLABcode",
        "url": "https://github.com/wanghaoyucn/seismologybookMATLABcode",
        "description": "a part of MATLAB code in Introduction to Seismology by Wan Yongge",
        "stars": 17,
        "forks": 6,
        "readme": "# seismologybookMATLABcode\na part of MATLAB code in Introduction to Seismology by Wan Yongge\n\n## 2022.1.27更新 / Update \n更新了一、二和四章。今天学长和我共享了题解以及程序包，并指出这些都可以扫描书后二维码下载，奈何我的版本没有书后二维码，过于尴尬。接下来会陆续上传其他章节，我先校对一下（bushi）。\n\n### 这是什么 / What's this\n万永革老师的《地震学导论》中附有很多的MATLAB程序，但使用电子版的同学无法获得电子版的代码，故我在此将一部分代码敲了下来并修改了一些小错误，目前是从第三章开始的，后续会陆续更新。\n\n* 如果你想获得这本书的电子版，请访问z-library\n\n### 大致内容 / overview\n##### 有的 / accessible\n* 随程序附带的还有可能会生成的图片，以供查看效果\n* 很少的注释\n##### 没有的 / inaccessible\n* 大部分的注释，因为书里都有，所以偷懒了\n* 每个程序作用的说明，计划之后补充（这样就可以当做单独的演示程序使用）\n\n### 其他 / other\nMATLAB本身没有自动补全，以及启动较慢，推荐使用（我自己在用）VsCode写代码，配置教程百度“vscode matlab”即可。\n\n之后计划更新本书习题的解答\n",
        "createdAt": "2022-01-23T15:06:10.000Z",
        "updatedAt": "2025-09-15T08:51:51.000Z",
        "language": "MATLAB",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/wanghaoyucn/seismologybookMATLABcode/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "SHB666/99999",
        "url": "https://github.com/SHB666/99999",
        "description": "seismology",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2022-02-24T09:19:12.000Z",
        "updatedAt": "2022-02-24T09:19:12.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "HerrMuellerluedenscheid/termology",
        "url": "https://github.com/HerrMuellerluedenscheid/termology",
        "description": "Seismology in a terminal",
        "stars": 0,
        "forks": 0,
        "readme": "Terminal Powered Seismology\n===========================\n\n# Requirements\n\n  * [Rust and Cargo](https://doc.rust-lang.org/cargo/getting-started/installation.html)\n\n# Installation\n\n    cargo install --path .\n\n# Usage\n\n    termology <filename.mseed>\n\nShortcuts:\n\n| Key        | Action           |\n|:----------:|------------------|\n| q          | quit             |\n| j          | Scroll down      |\n| k          | Scroll up        |\n| +          | Show more traces |\n| -          | Show fewer traces|\n\n![](assets/screenshot.png)\n",
        "createdAt": "2021-05-20T12:51:32.000Z",
        "updatedAt": "2022-12-25T13:20:08.000Z",
        "language": "Rust",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/HerrMuellerluedenscheid/termology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "paddy-seismic/retreat",
        "url": "https://github.com/paddy-seismic/retreat",
        "description": "RETREAT is a REal-time TREmor Analysis Tool. It performs f-k analysis on realtime (or archive) seismic array data to calculate the back azimuth and slowness with the aim of aiding in the location of volcanic tremor signals.",
        "stars": 26,
        "forks": 6,
        "readme": "# RETREAT - *RE*al-time *TRE*mor *A*nalysis *T*ool\n|||\n|---|---|\n|**RETREAT** is a **RE**al-time **TRE**mor **A**nalysis **T**ool written in python, making use of the [*ObsPy*](https://www.obspy.org/) framework. It performs beamforming on real-time (or optionally archive) seismic array data to calculate the back azimuth and slowness values in a given time window, with the aim of aiding in the location of volcanic tremor signals.|<img src=\"doc/retreat_trans96.png\" alt=\"logo\" width=\"300\"/>|\n[![DOI](https://zenodo.org/badge/263127922.svg)](https://zenodo.org/badge/latestdoi/263127922)\n\n## Background \n\nThe [EUROVOLC](https://eurovolc.eu/) project aims to promote an integrated and harmonised European volcanological community, and one of its main themes focuses on understanding sub-surface processes. Early identification of magma moving towards the surface is very important for mitigation of volcanic hazards, and joint research activities within the project aim to develop volcano pre-eruptive detection schemes. \n\nVolcanic tremor is a sustained seismic signal associated with eruptions and is often linked to movement of magmatic fluids in the subsurface. However, it can occur pre-, syn- and post-eruption, and signals with similar spectral content can be generated by several other processes (e.g. flooding, rockfalls). Hence one of the best ways of distinguishing between the processes underlying tremor generation is its 3D location. As tremor cannot be located using classical seismological methods, its source must be determined using alternatives such as amplitude-based techniques or seismic array analysis.\n\nDense, small-aperture arrays are particularly suited for analyzing volcanic tremor, yet costs associated with installation and maintenance have meant few long-term or permanent seismic arrays in use for routine monitoring. Therefore, to facilitate greater use of arrays in tracking volcanic tremor sources, we present **RETREAT** - a **RE**al-time **TRE**mor **A**nalysis **T**ool.\n\nThis software tool uses seismic array data and array processing techniques to help detect, quantify and locate volcanic tremor signals. It is a python-based tool that utilises existing routines from the open-source *ObsPy* framework to carry out analysis of seismic array data in real-time. The tool performs beamforming using f-k (frequency-wavenumber) analysis, or a Least-squares inversion, to calculate the back azimuth and slowness in overlapping time windows, which can be used to detect and track the location of volcanic tremor sources.  While primarily intended as a tool for utilizing seismic array data to locate and track volcanic tremor, RETREAT also has the capability to analyze infrasonic array data to track acoustic sources.\n\nVersion 2.x of RETREAT now also has the ability to analyse data from multiple arrays.\n\n## Installation\n\n#### Download\n\nIf you wish to use *git* you can download the latest version of the software by cloning the repository to a suitable location by typing:\n\n```git clone --recursive https://git.dias.ie/paddy/retreat```\n\nOr alternatively click on the download button (at the top of the page, below the project description) to download the source code as a zip, tar or compressed archive file:\n\n![](doc/screenshots/retreat-gitlab-download.png)\n\n#### Setup\n\nAfter you have cloned or downloaded the software, navigate to the repository directory, or unzip/untar the source code to a suitable directory. The contents of the repository should look something like this:\n\n```\nretreat\n├── CONTRIBUTORS.md\n├── doc\n│   ├── DIAS_logo.jpg\n│   ├── Eurovolc-header-1.jpg\n│   ├── EUROVOLC-logo-32.png\n│   ├── EUROVOLC-logo.jpg\n│   ├── IRIS-NORSAR1.jpg\n│   ├── IRIS-NORSAR2.jpg\n│   ├── retreat_trans128.png\n│   ├── retreat_trans192.png\n│   ├── retreat_trans48.png\n│   ├── retreat_trans96.png\n│   ├── screenshots\n│   │   ├── retreat-buttons.jpg\n│   │   ├── retreat-gitlab-download.png\n│   │   ├── retreat_GUI_2arrays_common.jpg\n│   │   ├── retreat_GUI_2arrays.jpg\n│   │   ├── retreat_GUI_figwindow.jpg\n│   │   ├── retreat_GUI.jpg\n│   │   ├── retreat_GUI_output.jpg\n│   │   ├── retreat_timeline_2arrays_arraysep.png\n│   │   ├── retreat_timeline_2arrays.png\n│   │   ├── retreat_WEB_bottom.jpg\n│   │   ├── retreat_WEB_figwindow.jpg\n│   │   ├── retreat_WEB_output.jpg\n│   │   └── retreat_WEB_top.jpg\n│   └── UR-array-map.png\n├── LICENSE.txt\n├── README.md\n├── requirements.txt\n└── retreat\n    ├── data\n    │   ├── array_preproc.py\n    │   ├── beamforming_lsqr.py\n    │   ├── check_for_gaps.py\n    │   ├── ew2st.py\n    │   ├── fdsn2st.py\n    │   ├── fdsn2st3.py    \n    │   ├── fix_times.py\n    │   ├── get_array_response.py\n    │   ├── get_meta.py\n    │   ├── get_nth.py\n    │   ├── __init__.py\n    │   ├── sds2st3.py\n    │   ├── slink2st3.py\n    │   └── stack.py\n    ├── defaults\n    │   ├── default_input_values_narrays.py\n    │   ├── default_input_values.py\n    │   ├── default_input_values.py.NO\n    │   ├── default_input_values.py.UR\n    │   └── __init__.py\n    ├── example_data\n    │   ├── dataless.seed.UR\n    │   ├── NO.xml\n    │   ├── NO.scnl\n    │   ├── UR.scnl\n    │   ├── VI.URA..HHZ.2014.246.00.00.mseed\n    │   ├── VI.URB..HHZ.2014.246.00.00.mseed\n    │   ├── VI.URD..HHZ.2014.246.00.00.mseed\n    │   ├── VI.URE..HHZ.2014.246.00.00.mseed\n    │   ├── VI.URF..HHZ.2014.246.00.00.mseed\n    │   ├── VI.URG..HHZ.2014.246.00.00.mseed\n    │   └── VI.URH..HHZ.2014.246.00.00.mseed\n    ├── gui\n    │   ├── get_param_gui.py\n    │   ├── gui_layout.py\n    │   ├── gui_layout_two.py                \n    │   ├── gui_sizes.py\n    │   └── __init__.py\n    ├── __init__.py\n    ├── __main__.py\n    ├── output\n    ├── plot\n    │   ├── add_logos.py\n    │   ├── __init__.py\n    │   ├── mapping.py\n    │   ├── rms_rmes.py\n    │   ├── set_font_sizes.py\n    │   ├── shiftedColorMap.py\n    │   ├── update_plot2.py\n    │   └── update_plot.py\n    ├── realtime.py\n    ├── start.py\n    ├── tools\n    │   ├── __init__.py\n    │   ├── KThread.py\n    │   ├── monitoring_routines.py\n    │   ├── processpool.py\n    │   └── time_to_wait.py\n    ├── update2.py\n    └── update.py\n```\n\nNext, check and install all the required software and packages by following the instructions in the next section.\n\n#### Requirements\n\nThis software requires python3. A list of required python modules is contained in the _requirements.txt_ file.\n\nThese are:\n\n- scipy (```python3-scipy```)\n- matplotlib (```python3-matplotlib```)\n- numpy (```python3-numpy```)\n- regex (```python3-regex```)\n- obspy (```python3-obspy```)\n- psutil (```python3-psutil```)\n- Pillow (```python3-pil```)\n- Cartopy (```python3-cartopy```)\n- PySimpleGUI\n- PySimpleGUIWeb\n\nMore information on *obspy* and *PySimpleGUI* is available from:\n\n[https://www.obspy.org/]() and [https://pysimplegui.readthedocs.io/en/latest/#install]()\n\nUbuntu/Debian package names are shown in brackets where available, and can be installed via: \n\n<code>sudo apt-get install *packagename*</code>\n\nTo install the required modules using **pip**, you can type the following:\n\n`pip3 install -r /path/to/requirements.txt`\n\nFinally, to start the software follow the instructions below.\n\n## Starting the software\n\nThe **RETREAT** package can be run in 3 modes:\n\n1. With a GUI interface, running in its own window - this is the default mode.\n2. With a web interface, where the input and output are displayed in a browser window\n3. (From the command line with no GUI or web interface)\n\n### GUI window\n\nThis the default mode. In a terminal window navigate to the directory you cloned or downloaded to and type:\n\n`python3 -m retreat`\n\nThis will start the software and open a GUI window that should look something like this:\n\n![GUI](doc/screenshots/retreat_GUI.jpg)\n\nwith the Input Parameters in the left hand pane, and the Control Buttons and Output Pane on the right hand side. \nFigures will appear in a *new window*.\n\n### Web interface\n\nTo run the software with a web interface in a browser, do the same as above, but simply give the ``-w`` command line flag, i.e. :\n\n`python3 -m retreat -w`\n\nThis will start the software and open a new tab in your browser and should look something like this:\n\n![GUI](doc/screenshots/retreat_WEB_top.jpg)\n\nwith the Input Parameters listed at the top of the page,\n\n![GUI](doc/screenshots/retreat_WEB_bottom.jpg)\n\nand the Control Buttons and Output Pane visible below if you scroll down the page.\nFigures will appear *below the Output Pane*.\n\n### Command-line interface (optional/advanced)\n<a name=\"command_line_mode\"></a> \nAlthough **specifically designed as a GUI tool**, a command-line mode allowing RETREAT to run from a shell or terminal without either a GUI or web interface, has now been implemented. This may be useful in certain circumstances, such as analysis of existing archive data and/or use in scripts. \n\nInput options will be read from the **default_input_values.py** file in the *retreat/defaults* directory (see [default values section](#default-values)) or other supplied defaults file. Output messages in the log file will be displayed in the terminal window, and saved output figures will NOT be displayed in the GUI (unless the [figure output](#figure-output-destination) is specified)\n\nTo run the software in command-line mode, do the same as above, but give the ``-c`` flag or argument, i.e.\n\n`python3 -m retreat -c`\n\n### Other command line options (optional/advanced)\n\nAs well as the options to specify a GUI, web or command line interface, RETREAT has several other command line options - including the ability to analyse [mutiple arrays](#multiple-arrays).\n\n#### Figure output destination\n\nWhen running in command line mode (see above), there is also the option to still display the figure output in either a GUI window or web interface, with the output remaining in your terminal. To do this use the `-f`  argument as specify the destination as \"gui\" for a GUI window or \"web\" to display in a browser window, i.e.:\n\n`python3 -m retreat -c -f gui`\n\nNote that this option is ONLY valid if also running in command line mode.\n\n#### Specify defaults file\n\nIf you wish to use a different [default values file](#default-values) other than the default location/filename (*retreat/defaults/default_input_values.py)* the `-d` can option can be used to specify the full path to the file, e.g:\n\n`python3 -m retreat -d /home/user/my_defaults.py`\n\n#### Multiple array mode \n\nVersion 2.x of RETREAT now has the ability to analyse multiple arrays. This option can be turned on by specifying the number of arrays with the command option `-n`, e.g.:\n\n`python3 -m retreat -n 2`\n\nSee [multiple arrays](#multiple-arrays) section for more details on using more than one array \n\n#### Summary\n\nAll command line options are also summarised in the Table below:\n\n| Option        | alternative           | Purpose                                                      |\n| ------------- | --------------------- | ------------------------------------------------------------ |\n| -h            | --help                | Show help message and command line options [flag]            |\n| -w            | --web                 | Run RETREAT using web interface in browser [flag]            |\n| -c            | --cmd                 | Run RETREAT using command line interface (no GUI or web interface) [flag] |\n| -f *FIGS*     | --figs *FIGS*         | Specify figure output destination. *FIGS* argument MUST be one of either \"gui\" or \"web\". e.g. -f gui. ONLY valid in command line mode (-c, --cmd) |\n| -d *DEFAULTS* | --defaults *DEFAULTS* | Specify path to defaults file if not using standard. *DEFAULTS* argument should be the full path to the file |\n| -n *NARRAYS*  | --narrays *NARRAYS*   | Use multiple arrays. *NARRAYS* argument is the (integer) number of arrays, e.g. -n 2 |\n\n## Description of Input Parameters\n\n### Input Data\n\nThese parameters define the source and properties of the input data. The fields are:\n\n* **Connection type** - Used for realtime data only. Can currently use the dropbox to choose from an FDSN, seedlink or earthworm/winston client.\n\n* **Client/Server** - Details of the server for the chosen connection type. For FDSN this *must* be either *IRIS* for the IRIS Federator or *EIDA* for the EIDAWS routing web service. For Seedlink or earthworm/winston servers this is the server URL:port, e.g. *rtserve.iris.washington.edu:18000* or *pubavo1.wr.usgs.gov:16022*\n\n* **SCNL** - These specify the data Station, Channel, Network and Location codes for the input data (wildcard \"*\" can be used)\n\n* **SCNL file** - checkbox to specify if you are supplying a text file containing a list of SCNL/SEED ids (i.e. if the station/channel list can't be expressed using wildcards). Input is a plain text file with one id per line, in ObsPy SEED\\_id format N.S.L.C, e.g. *NO.SPA0.00.HHZ* or *VI.URA..HHZ* (see example files in the example_data directory).\n\n* **SCNL filename** - Path and name of SCNL file (you can also use the *Browse* button to select)\n\n* **Inventory file** - checkbox to specify if you are supplying an inventory or metadata file (required if Connection type is **not** FDSN or if using archive data)\n\n* **Inventory filename** - Path and name of inventory file (you can also use the *Browse* button to select)\n\n* **File format** - Specify format of inventory file. You can use all formats supported by *obspy* (including: STATIONXML, dataless SEED, XSEED). RESP format is NOT supported as RESP files do not contain station coordinates. While a proper full inventory file is preferred, the only essential metadata required is the station locations. If you do not have an inventory for your network you can supply the station coordinates in a plain text file (choose ASCII format), with the following 4 columns: SEED_id (i.e. N.S.L.C), longitude, latitude, elevation. e.g.\n\n  `NO.SPA0.00.HHZ 16.36998 78.177711 323.0`\n\nNote that if ASCII format is selected then you are unable select the pre-processing option to remove the [instrument response](#remove_resp) (as no response information was supplied in the inventory). See also [this note](#coords) on coordinate specification.\n\n* **Replay mode** - checkbox for replay or archive data. Leave unchecked for real-time data.\n* **SDS directory** - path to the root of an SDS (Seiscomp Directory Structure - you can also use the *Browse* button to select)\n* **SDS type** - dropdown box to choose value for the *TYPE* field of the SDS\n* **Data format** - format of the waveform data. You can use all formats supported by *obspy* (including: MSEED, SAC, SEISAN, GCF)\n* **Custom Format** - checkbox to specify if you are using a non-standard SDS structure. If so, fill in your format in the box. Can be specified as {year}/{network}/{station}/{channel}/ etc. See the *obspy* [source code](https://docs.obspy.org/_modules/obspy/clients/filesystem/sds.html) for more details and examples.\n\n### Pre-processing\n\nThese parameters define any pre-processing applied to the data before the array analysis is carried out. The fields are:\n\n* **Use Z-components only** - checkbox to select only vertical (Z) components from the stream (checked by default)\n* **Demean** - checkbox to select whether to subtract the mean from each trace\n* **Linear detrend** - checkbox to select whether to remove a linear trend from the data\n<a name=\"remove_resp\"></a>\n* **Remove instrument response** - checkbox to select whether to remove the instrument response (output is velocity)\n* **Taper** -  checkbox to select whether to apply a taper to the data\n* **Taper fraction** - length of taper to apply (as a fraction of the window length)\n* **Decimate** - checkbox to select whether to decimate or downsample the data (less data, speeds up array processing)\n* **New sampling frequency** - specify the new sampling frequency to downsample to if the *Decimate* box is checked\n* **Pre-filter**- checkbox to select whether to Pre-filter the data\n* **Bandpass** - checkbox to select whether to Bandpass filter the data. The next 2 boxes specify the upper and lower frequency limits (in Hz) for the filter\n* **Check for gaps** - checkbox to select whether to check for and attempt to fill gaps in the input data stream. This function is designed to ensure RETREAT continues to run if small gaps or dropouts occur in the data. Gaps are currently filled with the trace mean or zero (if demean option selected).\n* **Minimum number of channels** - the minimum number of good (gap-free or suitably filled) channels required to proceed with the array analysis.\n* **Maximum fillable gap size** - the size of the maximum gap that should be filled (in seconds)\n* **Max. gap to fill at start/end** - the size of the maximum gap (in seconds) to be filled at the start or end of each trace, e.g. for channels that start late or finish early within the window\n\n### Timing\n\nThis set of parameters define the amount of data to be processed, by defining the length of the window and how often it is updated (real-time mode). The parameters are:\n\n* **Start Time** - specify the start time (UTC). This defaults to the current time (when software is started) when using real-time mode. It also accepts the keyword term '*now*'.\n* **Plot window** - length of the window to be plotted in the output figure timeseries (in seconds)\n* **Max realtime latency** - to account for latency of incoming data and to ensure real-time processing does not lag too much, you can specify the maximum number of seconds to allow for latency. If the delay between the end of the acquired data stream and the current time exceeds this value then processing will proceed immediately without waiting for the update interval. This parameter has no effect for archive data in replay mode.\n* **Window length** - amount of data to fetch on each update (in seconds)\n* **Update interval** - How often to update (fetch new data) - specified in seconds. If the processing for each update step takes longer than this update interval to complete, the software will warn you that realtime processing may lag. NOTE: for non-realtime/archive data this parameter is ignored and the next chunk of data is processed immediately.\n* **Pre-buffer** - amount to pre-buffer (in seconds) before the start time to ensure there are no gaps in the data stream. This is only relevant for real-time mode\n* **Fill window on start** - if this box is checked the software will fetch enough data to fill the entire window (specified by the *Plot window*) on the first update. Otherwise, it will fetch only *Window length* seconds and the window will grow with each update until it reaches the length of the *Plot Window*\n* **End time** - optionally specify the end time (UTC), Default=None. Note that this only applies for replay mode. RETREAT will attempt to continue beyond the end of the dataset (in case of large gaps/outages in all or multiple channels), and will continue to search forward for more data  indefinitely unless an end time is explicitly specified. Note that for real-time mode, the analysis continues indefinitely until stopped, and RETREAT will attempt to reconnect to the server/data source if the connection is lost.\n\n### Array Processing parameters\n\nSets parameters for the array processing, using the standard *array_analysis* routines in *obspy*. See the *obspy* documentation [here](https://docs.obspy.org/packages/autogen/obspy.signal.array_analysis.array_processing.html#obspy.signal.array_analysis.array_processing). The choice of these values will depend very much on the specific array being used. \n\n* The first set of 5 values define the **slowness grid** over which to perform the beamforming. These are the minimum and maximum slowness values in the *x*- and *y*-directions, and the desired slowness step (or resolution).\n\nThe next 2 parameters define the bandpass filter limits:\n\n* **Fmin** and **Fmax** - define the minimum and maximum frequency (in Hz) for the f-k analysis\n\nTo provide a timeseries output, the f-k analysis is performed by using shorter time windows and sliding these windows across the entire trace.\n\n* **Window length** - defines the sliding window length (in seconds)\n* **Overlap fraction** - defines the amount to overlap each window [0,1] (a higher value will increase the time resolution and hence processing time)\n\nOther parameters:\n\n* **Pre-whiten** - checkbox to select whether to Pre-whiten the data (unchecked/disabled by default)\n* **Velocity threshold** - Threshold for velocity for f-k analysis\n* **Semblance threshold** - Threshold for semblance for f-k analysis\n\nFinally, there is also an option to use a Least-Squares beamforming method as an alternative to f-k, e.g. for infrasound data where lower velocity/higher slowness values mean a large slowness grid is required which can impact the computation time. This method is described in [De Angelis et al. (2020)](https://doi.org/10.3389/feart.2020.00169), and allows for significantly faster computation. The python routine implementing the method in this software were adapted from the matlab code available [here](https://github.com/silvioda/Infrasound-Array-Processing-Matlab).\n\n* **LSQ beamforming** - checkbox to select whether to use Least-Squares beamforming instead of standard f-k analysis. Note that if checked, most of the array parameters are redundant, and only the **Window length** and **Overlap fraction** values still apply. Note that histogram output for any polar [plot](#results-and-plots) is now weighted by the correlation (MCCM), rather than being weighted by the power as in the f-k case. This method also returns timeseries of the errors in (apparent) velocity and back-azimuth - and the azimuth errors are used for the error cone in the optional map plot in this case.\n\n---\n\n<a name=\"coords\"></a> \nNote on **coordinates**: although the *obspy* array_processing module accepts coordinates as either Cartesian (*xy*) or latitude and longitude (*lonlat*), for convenience in plotting the array response function and map of the array, please note that latitude and longitude (*lonlat*) format is assumed. Therefore, *please ensure your station coordinates are specified as latitude and longitude* in your station metadata/inventory file.\n\n### Results and Plots\n\nThe parameters in this section define what you wish to plot as the output of the analysis as well as various settings for these figures. For more details and examples see the [Figures](#figures-and-output) section. The main timeseries figure can have up to 7 panels, with the desired output selected by the 7 checkboxes:\n\n* **Back azimuth** - checkbox to select to plot a timeseries of the calculated back azimuths (in degrees)\n* **Slowness** - checkbox to select to plot a timeseries of the calculated slowness (in s/km)\n* **Power/MCCM** - checkbox to select to plot a timeseries of the relative power [f-k] or MCCM (mean maximum correlation) [Least-squares]\n* **Seismogram** - checkbox to select to plot the (filtered) seismogram\n* **Spectrogram** - checkbox to select to plot a spectrogram of the data\n* **RMeS** - checkbox to select to plot the RMeS envelope (Root-Median-Square)\n\nRMeS is the envelope of the seismogram calculated using a Root-Median-Square sliding window. The parameters than control this are:\n\n* **RMeS Window** - length of the window to calculate RMeS in seconds\n* **Overlap fraction** - defines the amount to overlap each window [0,1] (a higher value will increase the time resolution and hence processing time)\n\nThe checkbox **Polar plot** creates a separate figure with the power from the beamforming analysis represented in *Polar* form as a histogram, with the *Back azimuth* on the *angular* axis, and the *slowness* on the *radial* axis. Note for Least-squares beamforming this is weighted by the MCCM rather than relative power.\n\n* **Save figures** - checkbox to select whether to save each figure. If checked each update will save the figure as a NEW file with a unique filename based on the timestamp. Otherwise, if unchecked, the same image file is overwritten on each update.\n* **Web Figures** - checkbox to select whether to display output figures in a web browser rather than a separate GUI window (**NOTE** this the default if the software is started in Web interface mode and cannot be changed)\n* **Plot logos** - checkbox for whether to add RETREAT and EUROVOLC logos to the output figures (enabled by default)\n\nThe next set of parameters define the size of the output figures, as pairs of *x* and *y* values. The sizes are specified in *pixels* (resolution is set to 100 DPI):\n\n* **Timeline plot dimensions** - size of main timeline figure (with up to 5 panels)\n* **Polar plot dimensions** - size of f-k polar form figure \n\nFixed *y*-axis limits for the various figures can also be set. If set to *auto* the figure will automatically scale the axes:\n\n* **Slowness plot(s) axis limits** - minimum and maximum slowness values for the *y*-axis of the timeline AND radial axis of power plot (in s/km)\n* **Back-azimuth plot axis limits** - minimum and maximum azimuth values for the *y*-axis (in degrees)\n\nYou can also define the resolution for the histogram in the polar representation of the results:\n\n* **Number of azimuth bins** - It is expected that 360 divided by this number is an integer\n* **Number of slowness bins** - Higher values increase the resolution but take longer to process\n\nOther options include:\n\n* **Plot timestamp** - if checked this will print the timestamp onto each figure (*current* time if in real-time mode OR stream start time if in archive mode)\n* **Use stack for plots** - if checked a *stack* of the traces in the array will be used as the *seismogram* plot. Otherwise the first station in the stream is used. The stack used is the beam corresponding to the slowness and backazimuth values derived from the maximum relative power.\n* **Normalized histogram** - if checked normalizes the power or MCCM in the histogram for the Polar plot, based on the maximum possible value (i.e. if there was perfect correlation/coherence for every value in the timeseries)\n* **RMeS limits** - minimum and maximum velocity values for the RMeS panel *y*-axis (in m/s). If set to *auto* the figure will automatically scale the axes\n* **Seismogram amplitude limits** - minimum and maximum velocity values for the *y*-axis (in m/s). Again, if set to *auto* the figure will automatically scale the axes\n* **Power/MCCM limits** - minimum and maximum velocity values for the *y*-axis. Again, if set to *auto* the figure will automatically scale the axes.\n\nThe next set of values control the (optional) plot of the array response function:\n\n* **Array response function** - checkbox to select whether to produce a plot of the array response function. Note that *only one of* the array response function or map can be selected.\n\n* **Elevation in [m]** - checkbox to determine if elevation values are given in metres. If unchecked elevation values in kilometres are assumed.\n\nThe wavenumber grid over which to evaluate the array response must also be defined:\n\n* **wavenumber limit** - limit or maximum wavenumber to analyze (assumed symmetric grid: *-klim* to *+klim* in both *x* and *y*-directions)\n* **wavenumber step** - wavenumber step or resolution of grid\n\nsee [here](https://docs.obspy.org/tutorial/code_snippets/array_response_function.html) for the *obspy* documentation and an example of plotting the array response function.\n\n* **Plot dimensions** - size of the output figure (*x* and *y* values) specified in *pixels*.\n\nThe final set of values control the (optional) map of the array, overlain by the calculated back azimuths:\n\n* **Map display** - checkbox to select whether to produce a map using data from [OpenTopoMap](https://wiki.openstreetmap.org/wiki/OpenTopoMap). A working internet connection is required to download the relevant tiles. Note that *only one of* the array response function or map can be selected.\n* **Array at centre** - checkbox to select whether to have the array at the centre of the map\n* **Radius from array** - If array is at the centre of the map, this value defines a radius (in km) to automatically determine the map extent.\n* **Plot dimensions** - size of the map figure (*x* and *y* values) specified in *pixels*.\n\nThe *x*- and *y*-axis limits of the map extent can also be manually specified. If the array at centre checkbox is **NOT** checked, these values **MUST** be given. Else, if set to *auto* the figure will automatically scale the axes.\n\n* **Latitude limits** - manually specify the latitude axis limits (min, max) in degrees.\n* **Longitude limits**- manually specify the latitude axis limits (min, max) in degrees.\n\nIf set to *auto* the figure will automatically scale the axes (for array at centre checked)\n\n#### Spectrogram \n\nA separate section contains the settings for controlling the spectrogram figure (if selected to plot):\n\n* **Fmin** - minimum limit for the frequency (*y*-) axis.\n* **Fmax** - maximum limit for the frequency (*y*-) axis.\n\nThe length and overlap of the FFT window used are controlled by:\n\n* **Window length** - length of the window to calculate FFT in seconds\n* **Overlap fraction** - defines the amount to overlap each window [0,1]\n\nThe colormap and limits of the colour scale for the spectrogram are given by:\n\n* **Colormap limits** - minimum and maximum values (normalised)\n* **Colormap** - name of colormap to use for the spectrogram. For a list of colormaps available for matplotlib, see [https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html](https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html).\n\n### Output\n\nThese settings control where the output produced by the software is placed on your system. The parameters are:\n\n* **Figure output path** - full path to where to store output figures and plots. (default is: *retreat/output/*). The next 4 boxes are the **filenames** for the different output figures: *MainTimeline*, *fkpolar*, *arrayresp*, and *map* (in order). Note that if the **Save Figures** checkbox is selected (see above) then these names will be appended by a unique string based on the timestamp.\n\n* **Log file path** - full path to where to store the logfile. The accompanying box specifies the log file **filename**.\n\n* **Save data** - checkbox to select whether raw data from each update of the array processing is saved to disk. If checked then on each update an ASCII text file is written containing 3 columns: 1) *time* (in matplotlib date format - see [here](https://matplotlib.org/api/dates_api.html#matplotlib-date-format)), 2) *backazimuth* (in degrees) and 3) *slowness* (in  s/km)). The name of the output text files is specified by the **filename**, appended by a unique string based on the network and timestamp. Note that the files are saved in the same directory as specified by **Figure output path**.\n\n### Default values\n\nAll parameters that can be set using the GUI or web interface can also be defined in advance of runtime. This is controlled by the **default_input_values.py** file in the *retreat/defaults* directory, which contains a simple python dictionary comprising of pairs of parameters and their default values. e.g:\n\n```\ndefaults = dict(\n        #########################\n        # DATA SOURCES - REALTIME\n        myclient=\"IRIS\",\n        connection=['FDSN', 'seedlink'],\n        ...\n        ...\n        ...\n        arrayfigname=\"arrayresp\",\n        mapfigname=\"map\",\n        ########################\n    )\n```\nThis file can be easily edited to change the default values as desired.\n\nThe repository also contains two examples files containing default values that can be used to run the two examples described in the [Examples](#examples) section. These are:\n\n>```default_input_values.py.NO```\n\nwhich contains values to run the *real-time* data example, using data from the SPITS/NORSAR (NO) array\n\nand\n\n>```default_input_values.py.UR```\n\nwhich contains values to run the *archive* data example, using data from the B&aacute;r&eth;arbunga (UR) array.\n\n#### Command line input\n\nNote that the full path to an alternative defaults file (must have a .py extension), can be supplied using the ``-d`` command line option. For example:\n\n>```python3 -m retreat -d /path/to/defaults_file.py```\n\n## Control Buttons\n\nThere are 3 simple buttons to control the software:\n\n![](doc/screenshots/retreat-buttons.jpg)\n\n* **Start** - pressing this will start the data acquisition (either real-time or from archive files), carry out the array processing, produce the output figures and then update/repeat as specified. This continues indefinitely until stopped.\n\n* **Stop** - pressing this button stops the computation\n\n* **Exit** - Closes the window and exits from the program\n\n## Figures and Output\n\n### Output window\n\nIn both GUI and web mode, the interface displays an output pane that shows the output produced once the **Start** button is pressed. The text displayed on screen is the same as the messages that are written to the log file. Note that the logfile will be overwritten by default when the software is run, unless a different name is specified in the input parameters.\n<!--This is achieved using the *pygtail* module, which updates the output pane by printing log file lines that have not been read. -->\n\n![output-pane](doc/screenshots/retreat_WEB_output.jpg)\n\n### Figure window\n\nIn both GUI and web mode, the interface also has a figure window that shows the output figures that are produced once the **Start** button is pressed.\n\nIn GUI mode this will open as a *new* window:\n\n![GUI-figwindow](doc/screenshots/retreat_GUI_figwindow.jpg)\n\nwhereas in web mode, figures are displayed in the *same* browser window, *below* the output pane:\n\n![web-figwindow](doc/screenshots/retreat_WEB_figwindow.jpg)\n\n## Examples\n\nTwo example configuration files are included with the source code to demonstrate how the tool can be used for both real-time and archive data.\n\n### 1. *Real-time* mode using data from SPITS array\n\nThe first example uses real-time data from the small-aperture SPITS array in Spitsbergen, part of the larger [NORSAR](https://www.fdsn.org/networks/detail/NO/) array:\n\n![IRIS-Norsar-info](doc/IRIS-NORSAR1.jpg)\n\nData is acquired in real-time from [IRIS](https://www.iris.edu/hq/) via the *obspy* [FDSN](https://docs.obspy.org/packages/obspy.clients.fdsn.html) client.\n\nTo run the example, simply copy the appropriate default values file (NO array) and overwrite the default values:\n\n> ```cd retreat/defaults/```\n\n>```cp default_input_values.py.NO default_input_values.py```\n\nand [start](#starting-the-software) the software. This should begin analysis of real-time data, with results similar to those shown [here](#figure-window).\n\n#### Seedink\nNote: this example configuration should also work using the IRIS seedlink server (*rtserve.iris.washington.edu:18000*) rather than the FDSN client as the data source. In this case the **Inventory file** checkbox must be checked as metadata cannot be retrieved automatically. The default for this configuration example is the *NO.xml* file supplied in the *retreat/example_data* directory.\n\n### 2. *Archive* mode using data from FUTUREVOLC\n\nThe second example uses archive data from the 2014 eruption at B&aacute;r&eth;arbunga volcano/Holuhraun in Iceland, collected as part of the FUTUREVOLC project. Several hours of data from the UR array between 00:00 and 08:00 UTC on 03 September 2014 are included with the distribution in the *example_data* folder. This corresponds to part of the time period analysed in [Eibl et al., (2017a)](https://doi.org/10.1038/ngeo2906). The map below shows the location of the seven station UR array in Iceland, relative to the erupted lava flow field in Holuhraun (red) and Bárdarbunga volcano (B, black dot) along with the approximate path of the intruded dyke in grey.\n\n![UR-array-map](doc/UR-array-map.png)\n<!--<p align=\"center\">-->\n<!--  <img src=\"doc/UR-array-map.png\">-->\n<!--</p>-->\n\nAgain, to run this example, simply copy the appropriate default values file (UR array) and overwrite the default values:\n\n`cd retreat/defaults/`\n\n`cp default_input_values.py.UR default_input_values.py`\n\nand [start](#starting-the-software) the software. This should begin analysis of the archive data.\n\n<!--**CHECK - what can we make available here?? and HOW!?**-->\n\n## Multiple arrays\n\nA new feature in version 2.x of RETREAT is the ability to process and analyse data from more than one array. If the `-n` command line option is selected, RETREAT is enabled in multiple array mode, with the number of arrays specified as an argument. This means RETREAT is now able to analyse input data from multiple arrays and display the results together in the output window. Note that in this configuration RETREAT does not (currently) jointly invert the data, but simply performs the beamforming separately for each array.\n\n### Format of defaults file\n\nTo accomodate the fact that there are now two (or more arrays), the format of the **default_input_values.py** file for multiple array mode is slightly different. Rather than a single dictionary of parameters, the file now contains a dictionary of common parameters that are common to all arrays:\n```\n    # default parameter values\n    #############################\n    ### Start with values/parameters common to all arrays\n    defaults_common = dict(\n        #########################\n        # TIMING\n        plot_window=3600,\n        window_length=900,\n        update_interval=900,\n        ...\n        savedata=False,\n        datafile=\"array_output\",\n        ########################\n        )\n```\nas well as two (or more) separate dictionaries - one for each array - for parameters that can be different for each array:\n\n    ## ARRAY 1    \n    defaults_array_one = dict(\n        #########################\n        # DATA SOURCES - REALTIME\n        myclient=\"IRIS\",\n        ...\n        clim_max=1.0,\n        cmap=\"jet\",\n    )\n    \n    ## ARRAY 2    \n    defaults_array_two = dict(\n        #########################\n        # DATA SOURCES - REALTIME\n        myclient=\"GFZ\",\n        ...\n        clim_max=2.0,\n        cmap=\"jet\",\n    )\n\nAn example input file for use with multiple arrays is included with the RETREAT distribution: *retreat/defaults/default_input_values_narray.py* that can be modified as required, such as adding additonal dictionaries for more arrays.\n\n### Extra parameter\n\nMost of the input parameters in multiple array mode are identical to those used for single array processing. However there is one addtional input parameter that must be specified:\n\n- **Separate timeline plots**  - this parameter specifies whether the timeseries output plots (back azmiuth, slowness, seismogram etc.) from each array should be displayed as separate panels or overlaid in a single plot. Note that this parameter is labelled *arraysep* in the default values input file dictionary\n\nIf **unchecked**, the default is to superimpose the output results into a single plot, as shown in the example below:\n\n![retreat_timeline_2arrays](doc/screenshots/retreat_timeline_2arrays.png)\n\nIf **checked**, each timeline variable selected to be plotted will have two separate panels - one for each array - as shown below:\n\n![retreat_timeline_2arrays_arraysep](doc/screenshots/retreat_timeline_2arrays_arraysep.png)\n\n### GUI interface\n\n#### Two arrays\n\nIf the *narrays* argument is equal to 2, then a GUI interface can be used. Starting retreat with the appropriate  arguments:\n\n`python3 -m retreat -n 2 -d retreat/defaults/default_input_values_narrays.py`\n\nshould start the GUI window and look something like this:\n\n![GUI_2arrays](doc/screenshots/retreat_GUI_2arrays.jpg)\n\nEach of the two arrays has its own set of parameters that can be different for each array, including: connection, station/channel information, pre-processing, array processing and spectrogram parameters.\n\nCommon parameters that are the same for both arrays,  e.g. timing, plotting and output options, are specified beneath these and can be accessed via the vertical scroll bar.\n\n#### More than two arrays\n\nTo avoid the size of the GUI becoming too large and complicated, if more than two arrays are to be used, then RETREAT *must* be run in [command line mode](#command_line_mode), specified with the `-c` option. Output figures can still be displayed in a GUI window or web browser if desired by setting the `-f` option and specifying *gui* or *web*. See: [command line options](#summary) for more details on these options.\n\n## References\n\nDe Angelis S, Haney MM, Lyons JJ, Wech A, Fee D, Diaz-Moreno A and Zuccarello L (2020). Uncertainty in Detection of Volcanic Activity Using Infrasound Arrays: Examples From Mt. Etna, Italy. Front. Earth Sci. 8:169. doi:[10.3389/feart.2020.00169](https://doi.org/10.3389/feart.2020.00169)\n\nEibl, Eva P. S., Bean, C.J., Vogfjörd, K.S., Ying, Y., Lokmer, I., Möllhoff, M., O’Brien, G.S., & Pálsson, F.  (2017a). Tremor-rich shallow dyke formation followed by silent magma flow at Bárðarbunga in Iceland. Nature Geoscience volume 10, pages 299–304, doi:[10.1038/ngeo2906](https://doi.org/10.1038/ngeo2906).\n\nEibl, E. P. S., Bean, C. J., Jónsdóttir, I., Höskuldsson, A., Thordarson, T., Coppola, D., Witt, T., and Walter, T. R. (2017b), Multiple coincident eruptive seismic tremor sources during the 2014–2015 eruption at Holuhraun, Iceland, J. Geophys. Res. Solid Earth, 122, 2972– 2987, doi:[10.1002/2016JB013892](https://doi.org/10.1002/2016JB013892).\n\nSmith, P. J. and Bean, C. J., (2020), RETREAT: A REal-Time TREmor Analysis Tool for Seismic Arrays, With Applications for Volcano Monitoring. Front. Earth Sci. 8:586955. [https://doi.org/10.3389/feart.2020.586955](doi:10.3389/feart.2020.586955)\n\n## Release History\n* 0.0.1\n    * First release. Work in progress\n* 1.0.0 Snapshot of (working) single array version of RETREAT\n* 1.0.1 Updated README\n* 2.0.0 Added multiple array capabilities\n\n## Licensing\n\nCopyright (c) 2021, Patrick Smith and all persons listed in [CONTRIBUTORS.md](https://git.dias.ie/paddy/retreat/blob/master/CONTRIBUTORS.md). This project is licensed under the EUPL, v1.2. See [LICENSE.txt](https://git.dias.ie/paddy/retreat/blob/master/LICENSE.txt) for more information.\n\n## Meta\n Patrick Smith – psmith@cp.dias.ie\n<!-- [@YourTwitter](https://twitter.com/dbader_org)-->\n\n<!--![](doc/DIAS_logo.jpg)-->\n[![DIAS_LOGO](doc/DIAS_logo.jpg)](https://www.dias.ie/)\n\n[https://git.dias.ie/paddy/retreat](https://git.dias.ie/paddy/retreat)\n\nThis software was developed and made available as part of the **EUROVOLC** project. \n\nFor more information see [https://eurovolc.cp.dias.ie/index.php](https://eurovolc.cp.dias.ie/index.php/Open_software)\n\n<!--![](doc/EUROVOLC-logo.jpg)-->\n[![EUROVOLC_LOGO](doc/EUROVOLC-logo.jpg)](https://eurovolc.eu)\n\n## Cite this code\n[![DOI](https://zenodo.org/badge/263127922.svg)](https://zenodo.org/badge/latestdoi/263127922)\n\nSmith, P. J. and Bean, C. J., (2020), RETREAT: A REal-Time TREmor Analysis Tool for Seismic Arrays, With Applications for Volcano Monitoring. Front. Earth Sci. 8:586955. [https://doi.org/10.3389/feart.2020.586955](doi:10.3389/feart.2020.586955)\n\nPatrick Smith. (2020, May 11). RETREAT - a REal-time TREmor Analysis Tool (Version v1.0). Zenodo. http://doi.org/10.5281/zenodo.3820867\n\n<!--## Contributing-->\n\n<!--1. Fork it (<https://github.com/yourname/yourproject/fork>)-->\n<!--2. Create your feature branch (`git checkout -b feature/fooBar`)-->\n<!--3. Commit your changes (`git commit -am 'Add some fooBar'`)-->\n<!--4. Push to the branch (`git push origin feature/fooBar`)-->\n<!--5. Create a new Pull Request-->\n\n<!--<!-- Markdown link & img dfn's -->\n<!--[npm-image]: https://img.shields.io/npm/v/datadog-metrics.svg?style=flat-square-->\n<!--[npm-url]: https://npmjs.org/package/datadog-metrics-->\n<!--[npm-downloads]: https://img.shields.io/npm/dm/datadog-metrics.svg?style=flat-square-->\n<!--[travis-image]: https://img.shields.io/travis/dbader/node-datadog-metrics/master.svg?style=flat-square-->\n<!--[travis-url]: https://travis-ci.org/dbader/node-datadog-metrics-->\n<!--[wiki]: https://github.com/yourname/yourproject/wiki-->\n",
        "createdAt": "2020-05-11T18:43:55.000Z",
        "updatedAt": "2025-04-01T05:51:01.000Z",
        "language": "Python",
        "homepage": "https://git.dias.ie/paddy/retreat",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/paddy-seismic/retreat/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "rizac/stream2segment",
        "url": "https://github.com/rizac/stream2segment",
        "description": "A Python project to download, process and visualize medium-to-massive amount of seismic waveforms and metadata",
        "stars": 23,
        "forks": 8,
        "readme": "# <img align=\"left\" height=\"30\" src=\"https://www.gfz-potsdam.de/fileadmin/gfz/medien_kommunikation/Infothek/Mediathek/Bilder/GFZ/GFZ_Logo/GFZ-Logo_eng_RGB.svg\"> Stream2segment <img align=\"right\" height=\"50\" src=\"https://www.gfz-potsdam.de/fileadmin/gfz/GFZ_Wortmarke_SVG_klein_en_edit.svg\">\n\n|Jump to: | [Usage](#usage) | [Installation](#installation) | [Development and Maintenance](#development-and-maintenance) | [Citation](#citation) |\n| - | - | - | - | - |\n\nA Python library and command line application to download, process and visualize \nevent-based seismic waveform  segments, specifically designed to manage big \nvolumes of data.\n\nThe key aspects with respect to widely-used similar applications is the use of\na Relational database management system (RDBMS) to store downloaded data and \nmetadata. The main advantages of this approach are: \n\n* **Storage efficiency**: no huge amount of files, no complex, virtually \n  unusable directory structures. Moreover, a database prevents data and metadata \n  inconsistency by design, and allows more easily to track what has already \n  been downloaded in order to customize and improve further downloads\n\n* **Simple Python objects representing stored data and relationships**, easy \n  to work with in any kind of custom code accessing the database. For instance, a \n  segment is represented by a `Segment` object with its data, metadata and related \n  objects easily accessible through its attributes, e.g., `segment.stream()`, \n  `segment.maxgap_numsamples`, `segment.event.magnitude`, \n  `segment.station.network`, `segment.channel.orientation_code` and so on.\n  \n* **A powerful segments selection** made even easier by means of a simplified\n  syntax: map any attribute described above to a selection expression\n  (e.g. `segment.event.magnitude: \"[4, 5)\"`) and with few lines you can compose \n  complex database queries such as e.g., *\"get all downloaded segments within a \n  given magnitude range, with well-formed data and no gaps, \n  from broadband channels only and a given specific network\"*\n\n\n\n## Usage\n\nFor full details, please consult the [wiki page](https://github.com/rizac/stream2segment/wiki)\n\nStream2segment is a Python library and command line application available \nafter installation via the command `s2s` on the terminal. By typing `s2s --help` you\nwill see all available subcommands for downloading \nand managing data, launching Python processing functions, creating class labels for segments \nannotation, or producing graphical output, as shown below:\n\n![S2s GUI](https://raw.githubusercontent.com/wiki/rizac/stream2segment/images/screenshot_gui.png)\n\n<!--\n<table>\n\t<tr>\n\t\t<td align=\"center\"><img width=\"90%\" src=\"https://geofon.gfz-potsdam.de/software/stream2segment/processgui.png\"/></td>\n\t\t<td align=\"center\"><img width=\"90%\" src=\"https://geofon.gfz-potsdam.de/software/stream2segment/s2s_dinfogui.png\"/></td>\n\t</tr>\n\t<tr>\n\t\t<td>The <code>s2s show ...</code> command opens a GUI in the browser where downloaded data and customizable plots are shown</td>\n\t\t<td> The <code>s2s dl dstats ...</code> command opens an HTML page in the browser where download statistics can be shown</td>\n\t</tr>\n</table>\n\n\n<sub>Both image linked from https://geofon.gfz-potsdam.de/software/stream2segment/</sub>\n-->\n\nYou start the program via the command `init` ( \n`s2s init --help` for details) to create several fully documented\nexamples files that you can immediately start to configure and modify\n(see the **[gitHub wiki page](https://github.com/rizac/stream2segment/wiki)** for details).\nIn a nutshell: \n\n 1. **A download configuration file** in YAML syntax. Edit the file \n    (all documentation is provided in the file as block comments) and \n    start downloading by typing:\n   \n    ```console\n    s2s download -c <config_file> ...\n    ```\n   \n    > **Note** the path of the database where to store the downloaded data\n      must be input in the config file. The supported database types are SQLite \n      and Postgres: for massive downloads (as a rule of thumb: &ge; 1 million segments)\n      we suggest to use Postgres. In any case, we **strongly** suggest running the program \n      on computers with at least **16GB** of RAM.\n\n    > **Note**  massive downloads are time-consuming operations where it is likely to miss\n      some data due to any kind of temporary connection problems. Consequently, **it is advisable\n      to perform the same massive download at least twice with the same configuration**  \n      (subsequent runs will be faster as data will not be re-downloaded unnecessarily)\n\n 2. **A Jupyter notebook tutorial with examples for processing downloaded data**,\n    for user who prefer this approach instead of the processing module described\n    below (online version **[here](https://github.com/rizac/stream2segment/wiki/Using-Stream2segment-in-your-Python-code)**)\n\n 3. **Two Python modules** (with relative configuration in YAML syntax):\n \n    1. `paramtable.py`: process downloaded data and produce a tabular output (CSV, HDF) by executing the \n       module as script (see code block after `if __name__ == \"__main__\"` in the module for details):\n       ```console\n       python paramtable.py ...\n       ```\n         \n    2. `gui.py`: visualize downloaded data in the user browser via the plots defined in the module (an example in the figure above):\n       ```console\n       s2s show -d download.yaml -p gui.py -c gui.yaml ...\n       ```\n       (Type `s2s show --help` for details).\n       \n    > **Note**: the associated YAML files (`paramtable.yaml`, `gui.yaml`) are not \n      mandatory but enforce the good practice of separating configuration settings (YAML)\n      and the actual Python code. This way you can experiment \n      the same code with several settings by only creating different YAML files\n\n\n## Installation\n\nThis program has been installed and tested on Ubuntu (14 and later) and macOS\n(El Capitan and later).\n\nIn case of installation problems, we suggest you to proceed in this order:\n\n 1. Look at [Installation Notes](#installation-notes) to check if the problem\n    has already been observed and a solution proposed\n 2. Google for the solution (as always)\n 3. [Ask for help](https://github.com/rizac/stream2segment/issues)\n\n\n### 1 Requirements\n\nIn this section we assume that you already have Python (**3.5 or later**) \nand the required database software. The latter should not be needed if you use\n[SQLite](https://docs.python.org/3/library/sqlite3.html) or if the\ndatabase is already installed remotely, so basically you are concerned only if you\nneed to download data locally (on your computer) on a Postgres database.\n\n\n#### 1.1 macOS\n\nOn macOS (El Capitan and later) all required software is generally already\npreinstalled. We suggest you to go to the next step and look at the\n[Installation Notes](#installation-notes) in case of problems\n(to install software on macOS, we recommend to use [brew](https://brew.sh/)).\n\n<details>\n<summary>Details</summary>\n\nIn few cases, on some computers we needed to run one or more of the following\ncommands (it's up to you to run them now or later, only those really needed):\n\n```\nxcode-select --install\nbrew install openssl\nbrew install c-blosc\nbrew install git\n```\n\n</details>\n\n#### 1.2 Ubuntu\n\nUbuntu does not generally have all required packages pre-installed. The bare minimum\nof the necessary packages can be installed with the `apt-get` command:\n\n```\nsudo apt-get install git python3-pip python3-dev  # python 3\n```\n\n<details>\n<summary>Details</summary>\n\nIn few cases, on some computers we needed to run one or more of the following\ncommands (it's up to you to run them now or later, only those really needed):\n\nUpgrade `gcc` first:\n\t\n```\nsudo apt-get update\nsudo apt-get upgrade gcc\n```\n\nThen:\n\n```\nsudo apt-get update\nsudo apt-get install libpng-dev libfreetype6-dev \\\n\tbuild-essential gfortran libatlas-base-dev libxml2-dev libxslt-dev python-tk\n```\n\n\n</details>\n\n### 2 Clone repository\n\nGit-clone (basically: download) this repository to a specific folder of your choice:\n```\ngit clone https://github.com/rizac/stream2segment.git ./stream2segment\n```\nand move into the repository root:\n```\ncd stream2segment\n```\n\n### 3 Install and activate Python virtualenv\n\nWe strongly recommend to use Python virtual environment to avoid conflicts\nwith already installed packages on your operating system (if you already\nhave a virtual environment, just activate it and go to the next section).\n\nConda users (e.g. Anaconda, Miniconda) can skip this section and check the [Conda documentation](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html) instead.\n\nMake virtual environment in a \"stream2segment/env\" directory (env is a convention,\nbut it's ignored by `git commit` so better keeping it. You can also use \".env\"\nwhich makes it usually hidden in Ubuntu. Also on Ubuntu, you might need to install\n`venv` first via `sudo apt-get install python3-venv`)\n```\npython3 -m venv ./env\n```\n\nTo activate your virtual environment, type:\n\n ```\n source env/bin/activate\n ```\nor `source env/bin/activate.csh` (depending on your shell)\n\n> <sub>Activation needs to be done __each time__ we will run the program.</sub>\n> <sub>To check you are in the right env, type: `which pip` and you should see it's\n  pointing inside the env folder</sub>\n\n\n### 4 Install Stream2segment\n\n**Important reminders before installing**: \n  - From now on you are supposed to be in the stream2segment directory,\n     (where you cloned the repository) with your Python virtualenv activated\n  - In case of errors, check the [Installation notes below](#installation-Notes)\n\nInstall the required packages with the tested versions listed in `requirements.txt` \n(if you are working on an existing environment with stuff \nalready installed in it, **please read the [first installation note](#installation-notes) below** \nbefore proceeding):\n```console\npip install --upgrade pip setuptools wheel && pip install -r ./requirements.txt\n```\n > <sub>type `requirements.dev.txt` instead of `requirements.txt` if you want to install also test packages, e.g., you want to contribute to the code and/or run tests</sub>\n\nInstall this package:\n```console\npip install -e .\n```\n\n(optional) install jupyter notebook or jupyterlab \n(see [Jupyter page for details](https://jupyter.org/install)), e.g.:\n```console\npip install jupyterlab\n```\n\nThe program is now installed. To double-check the program functionalities,\nyou can run tests (see below) and report the problem in case of failure.\nIn any case, before reporting a problem remember to check first the\n[Installation Notes](#installation-notes)\n\n\n### 5 Installation Notes\n\n- in case of a message like `ERROR: No matching distribution found for <package_name>`,\n  try to skip the requirements file:\n  ```console\n  pip install --upgrade pip setuptools wheel && pip install -e .\n  ```  \n  This will install packages satisfying a *minimum* required \n  version instead of the *exact* version passing tests: while less safe in general, this approach\n  lets `pip` handle the best versions to use, with more chance of\n  success in this case. **You can choose this strategy not only in case of mismatching distributions, \n  but also while working on a virtual environment with already installed packages, \n  because it has less chance of breaking existing code.**\n\n- In older ObsPy version, numpy needs to be installed first. If you see an error \n  like \"you need to install numpy first\", open \"requirements.txt\" and copy the \n  line which starts with numpy. Supposing it's `numpy==0.1.12`, then run \n  `pip install numpy==0.1.12` before re-running the `pip install ...` command \n  above\n\n- When installing the program (`pip install -e .`), `-e` is optional and \n  makes the package editable, meaning that you can edit the repository and make all \n  changes immediately available, without re-installing the package. This is useful \n  when, e.g., `git pull`-ing new versions frequently.\n  \n- (update January 2021) On macOS (version 11.1, with Python 3.8 and 3.9):\n\n  - if the installation fails with a lot of printout, and you spot a\n    \"Failed building wheel for psycopg2\", see  \n    <!--, try to execute:\n    ```\n    export LIBRARY_PATH=$LIBRARY_PATH:/usr/local/opt/openssl/lib/ && pip ./installme-dev\n    ```\n    (you might need to change the path of openssl below). Credits \n    -->\n    [here](https://stackoverflow.com/a/61159643/3526777) and\n    [here](https://stackoverflow.com/a/39800677/3526777)\n \n  - If the error message is \"Failed building wheel for tables\",\n    then try to install c-blosc (on macOS,  `brew install c-blosc`) <!-- and re-run `installme-dev` installation command\n    (with the `export` command above, if needed) -->\n \n\n- If you see (we experienced this while running tests, thus we can guess you should see\n  it whenever accessing the program for the first time):\n  ```\n  This system supports the C.UTF-8 locale which is recommended.\n  You might be able to resolve your issue by exporting the\n  following environment variables:\n\n    export LC_ALL=C.UTF-8\n    export LANG=C.UTF-8\n  ```\n  Then edit your `~/.profile` (or `~/.bash_profile` on Mac) and put the two lines starting\n  with 'export', and execute `source ~/.profile` (`source ~/.bash_profile` on Mac) and\n  re-execute the program.  \n\n- On Ubuntu 12.10, there might be problems with libxml (`version libxml2_2.9.0' not found`). \n  Move the file or create a link in the proper folder. The problem has been solved looking\n  at http://phersung.blogspot.de/2013/06/how-to-compile-libxml2-for-lxml-python.html\n\nAll following issues should be solved by installing all dependencies as described in\nthe section [Prerequisites](#prerequisites). If you did not install them, here the solutions\nto common problems you might have and that we collected from several Ubuntu installations:\n\n- For numpy installation problems (such as `Cannot compile 'Python.h'`) , the fix \n  has been to update gcc and install python3-dev (python2.7-dev if you are using Python2.7,\n  discouraged): \n  ```\n  sudo apt-get update\n  sudo apt-get upgrade gcc\n  sudo apt-get install python3-dev\n  ```\n   For details see [here](http://stackoverflow.com/questions/18785063/install-numpy-in-python-virtualenv)\n \n - For scipy problems, `build-essential gfortran libatlas-base-dev` are required for scipy.\n   For details see [here](http://stackoverflow.com/questions/2213551/installing-scipy-with-pip/3865521#3865521)\n \n - For lxml problems, `libxml2-dev libxslt-dev` are required. For details see [here](http://lxml.de/installation.html)\n \n - For matplotlib problems (matplotlib is not used by the program but from imported libraries),\n   `libpng-dev libfreetype6-dev` are required. For details see\n   [here](http://stackoverflow.com/questions/25593512/cant-install-matplotlib-using-pip) and\n   [here]( http://stackoverflow.com/questions/28914202/pip-install-matplotlib-fails-cannot-build-package-freetype-python-setup-py-e)\n\n\n\n## Development and Maintenance\n\n### 1 Run tests\n\nStream2segment has been highly tested (current test coverage is above 90%)\non Python version >= 3.5+. Although automatic continuous integration (CI) systems are not\nin place, we do our best to regularly tests under new Python and package versions. \nRemember that tests are time-consuming (some minutes currently).\nHere some examples depending on your needs:\n\n```\npytest -xvvv -W ignore ./tests/\n```\n\n```\npytest -xvvv -W ignore --dburl postgresql://<user>:<password>@localhost/<dbname> ./tests/\n```\n\n<!--\n```\npytest -xvvv -W ignore --cov=./stream2segment --cov-report=html ./tests/\n```\n-->\n\n```\npytest -xvvv -W ignore --dburl postgresql://<user>:<password>@localhost/<dbname> --cov=./stream2segment --cov-report=html ./tests/\n```\n\nWhere the options denote:\n\n- `-x`: stop at first error\n- `-vvv`: increase verbosity,\n- `-W ignore`: do not print Python warnings issued during tests. You can omit the `-W`\n  option to turn warnings on and inspect them, but consider that a lot of redundant\n  messages will be printed: in case of test failure, it is hard to spot the relevant error\n  message. Alternatively, try `-W once` - warn once per process - and `-W module` -warn\n  once per calling module.\n- `--cov`: track code coverage, to know how much code has been executed during tests, and\n  `--cov-report`: type of report (if html, you will have to opened 'index.html' in the\n  project directory 'htmlcov')\n- `--dburl`: Additional database to use.\n  The default database is an in-memory sqlite database (e.g., no file will be created),\n  thus this option is basically for testing the program also on postgres. In the example,\n  the postgres is installed locally (`localhost`) but it does not need to.\n  *Remember that a database with name `<dbname>` must be created first in postgres, and\n  that the data in any given postgres database will be overwritten if not empty*\n\n\n> <sub>Note on coding: although PEP8 recommends 79 character length, the program used initially a 100\n  characters max line width, which is being reverted to 79 (you might see mixed\n  lengths in the modules). It seems that [among new features planned for Python 4 there is\n  an increment to 89.5 characters](https://charlesleifer.com/blog/new-features-planned-for-python-4-0/).\n  If true, we might stick to that in the future</sub>\n  \n  \n### 2 Updating dependencies\n\nIn the absence of Continuous Integration in place, from times to times, it is necessary\n  to update the dependencies, to make `pip install` more likely to work (at least for\n  some time). The procedure is:\n  ```\n\tpip install -e .\n\tpip freeze > ./requirements.tmp\n\tpip install -e \".[dev]\"\n\tpip freeze > ./requirements.dev.tmp\n  ```\n**Remember to comment the line of stream2segment\n  from all requirements** (as it should be installed as argument of pip:\n  `pip install <options> .`, and not inside the requirements file).\n\n  Run tests (see above) with warnings on: fix what might go wrong, and eventually you can\n  replace the old `requirements.txt` and `requirements.dev.txt` with the `.tmp` file\n  created. \n\n### 3 Updating wiki\n  \n  Requirements (to be done once):\n   - `jupyter` installed.\n   - The git repository `stream2segment.wiki` which you can clone from the \n     stream2segment/wiki URL on the GitHub page. The repository must\n     be cloned next to (on the same parent directory of) the\n     stream2segment repository\n     \n  The wiki is simply a git project composed of Markdown (.md) files, where\n  `Home.md` implements the landing page of the wiki on the browser, and thus\nusually hosts the table of contents with links to other markdown files `.md` \n  in the directory. Currently, two of those `.md` files are generated from the \n  notebooks `.ipynb` inside stream2segment:\n  \n  - ./resources/templates/\n    - Using-Stream2segment-in-your-Python-code.ipynb\n    - The-Segment-object.ipynb\n  \n#### 3.1 Update existing notebook\n\n1. Edit the notebook in stream2segment/resources/templates:\n  `jupyter notebook stream2segment/resources/templates`\n  Execute the whole notebook to update it, then `git push` as usual\n   \n2. Create `.md` versions of the notebook for the wiki. From the stream2segment \n   repository as `cwd`:\n   ```bash\n    F='Using-Stream2segment-in-your-Python-code';jupyter nbconvert --to markdown ./stream2segment/resources/templates/$F.ipynb --output-dir ../stream2segment.wiki \n   ```\n   (repeat for every notebook file, e.g. `The-Segment-object`. Note only the file name,\n   no file extension needed)\n   \n3. Commit and push to the `stream2segment.wiki` repo:\n   `cd ../stream2segment.wiki`, then as usual `git commit` and `git push`. One line command:\n   `(cd ../stream2segment.wiki && git commit -am 'updating wiki' && git push)`\n    \n#### 3.2 Add a new notebook\n  \nCreate the notebook (`jupyter notebook stream2segment/resources/templates`). \n**Choose a meaningful file name: use upper case when needed, type hyphens '-'\ninstead of spaces**: the file name will be used as title to show the page\nonline (replacing hyphens with spaces).\nOnce the notebook is created and executed:\n     \n- (optional) If you want to include the notebook also as example in the `s2s init` command,\n     look at `stream2segment/cli.py`  \n  \n- Make the notebook being executed during tests (see examples in `tests/misc/test_notebook.py`)\n     and run tests to check everything works.\n  \n- Make the notebook visible in the wiki by adding a reference to it\n     (the notebook URL is the file name with no extension, I guess case- insensitive). \n     A reference can be added in several places:\n     - In the file `_Sidebar.md` (in the wiki repository)\n       which will show it in the sidebar on GitHub\n     - In `Home.md`\n     - In some other notebook (see example in\n       `Using-stream2segment-in-you-Python-code.ipynb`). In this case, note that\n       you might need to update also the referencing notebook\n       (see points 2-3 [above](#to-update-one-of-those-existing-notebooks))\n\n- Create the markdown file and commit to the wiki (see points 2-3 above under\n     `To update one of those existing notebooks`)\n\n\n\n## Citation\n\n**Software:**\n> Zaccarelli, Riccardo (2018): Stream2segment: a tool to download, process and visualize event-based seismic waveform data. GFZ Data Services.  [http://doi.org/10.5880/GFZ.2.4.2019.002](http://doi.org/10.5880/GFZ.2.4.2019.002)\n\n\n**Research article:**\n> Riccardo Zaccarelli, Dino Bindi, Angelo Strollo, Javier Quinteros and Fabrice Cotton. Stream2segment: An Open‐Source Tool for Downloading, Processing, and Visualizing Massive Event‐Based Seismic Waveform Datasets. *Seismological Research Letters* (2019). [https://doi.org/10.1785/0220180314](https://doi.org/10.1785/0220180314)\n\n",
        "createdAt": "2016-02-10T16:46:45.000Z",
        "updatedAt": "2025-07-23T09:09:08.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/rizac/stream2segment/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "geophydog/Awesome_Building_Figures_with_Python",
        "url": "https://github.com/geophydog/Awesome_Building_Figures_with_Python",
        "description": "Jupyter notebook of  map-making with Python",
        "stars": 6,
        "forks": 1,
        "readme": "## Awesome_Building_Figures_with_Python\n```\nJupyter notebook of  map-making with Python and you will find what you want if you're lucky!\n```\n\n***\n\n## Structures\n- Jupyter Lab [(Beautiful_Figures_by_Python.ipynb)](https://github.com/geophydog/Awesome_Building_Figures_with_Python/blob/master/Beautiful_Figures_by_Python.ipynb)\n- HTML [(Beautiful_Figures_by_Python.html)](https://github.com/geophydog/Awesome_Building_Figures_with_Python/blob/master/Beautiful_Figures_by_Python.html)\n\n***\n\n## Tips\n```\nYou can look for some examples in files above. However, I recommend you to search some examples \nin file \"Beautiful_Figures_by_Python.html\" if you want to do it quickly.\n```\n",
        "createdAt": "2019-08-20T13:05:39.000Z",
        "updatedAt": "2025-01-07T15:15:38.000Z",
        "language": "HTML",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/geophydog/Awesome_Building_Figures_with_Python/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "DIG-Kaust/Seismology-Projects",
        "url": "https://github.com/DIG-Kaust/Seismology-Projects",
        "description": "A collection of projects performed by my students in the ErSE 210 Seismology course",
        "stars": 9,
        "forks": 2,
        "readme": "![Seismology](https://github.com/DIG-Kaust/Seismology-Projects/blob/main/logo.png)\n\nA collection of projects performed by my students in the ErSE 210 Seismology course. \n\nAll projects have been carried out on the open [Volve dataset](https://www.equinor.com/energy/volve-data-sharing). Our ambition is to apply an extensive set of processing techniques to the dataset during the years and provide a one-stop-shop for instructors and researchers willing to use the Volve dataset for their teaching or researcher.\n\n\n## Projects\n\n### 2021\n\n- Juan Daniel Romero [jdromerom](https://github.com/jdromerom): [Wavelet estimation](jromero_waveletestimation)\n\n### 2022\n\n- Danilo Chamorro Riascos [dchamorror](https://github.com/dchamorror): [Semblance analysis and NMO correction](dchamorro_nmo)\n\n- Ning Wang [WANGN0E](https://github.com/WANGN0E): [Up/Down wavefield separation](nwang_wavsep)\n\n- Eyad Babtain: [Kirchhoff Depth Migration](ebabtain_kirchhoff)\n\n\n### 2023\n\n- Arturo Ruiz [arturoruizs](https://github.com/arturoruizs): [3D Deblending](aruiz_3ddebl)\n\n- Muhammad Iqbal Khatami [iqbalkhatami16](https://github.com/iqbalkhatami16): [Multiple removal using parabolic radon transform](ikhatami_radondem)\n \n- Xiao Ma: [Kirchoff Time Migration](xma_kirchoofftime)\n\n\n### 2024\n\n- Emerald Olango: [Prestack Kirchoff Time Migration](eolango_prestackkirchoofftime)\n\n- Sophia Manzo Vega: [Dead trace identification](smanzo_deadtraces)\n\n- Amnah Samarin - Ulises Berman [Amnah2020](https://github.com/Amnah2020)-[uber30](https://github.com/uber30): [Refraction tomography](https://github.com/Amnah2020/RefrTomo/tree/Volvo)\n\n- Charbel Sayan: [Predictive Decon](csayan_predictivedecon)\n\n- Cristhian Valladares: [Time to Depth Conversion](cvalladares_time2depth)\n \n \n## Data\n\nIn most cases, our projects start from one of the openly available data in the Volve village. In this case, we refer to this [notebook](https://github.com/PyLops/pylops_notebooks/blob/master/developement/SeismicInversion-Volve.ipynb) where the entire procedure to download data from the Volve data village is explained in details.\n\nWe also provide a script in the `data_preparation` directory, which generates two subsets of the main dataset used in some of the projects.\n\nFinally, in some cases we used derived datasets from other projects of ours or available on the web. In this case, we provide details on how to obtain such data in the project folder directly.\n\n\n## Contribute\n\nNote that these projects are the results of a few weeks of work and therefore may be unfinished; nevertheless, they provide good starting points for whoever interested to work with the Volve dataset at different stages of processing.\n\nContributions to improve any of the projects are most welcome! Please simply open a GitHub issue if you find any bug or want to add any feature to the current material. We will be happy to discuss with you and work together on making a PR at any time.",
        "createdAt": "2023-01-17T07:31:32.000Z",
        "updatedAt": "2025-08-27T13:22:39.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/DIG-Kaust/Seismology-Projects/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "kwinkunks/sidx",
        "url": "https://github.com/kwinkunks/sidx",
        "description": "Seismc indexing",
        "stars": 1,
        "forks": 0,
        "readme": "",
        "createdAt": "2017-02-03T19:18:39.000Z",
        "updatedAt": "2022-12-09T11:02:07.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "wecassidy/glimer-map",
        "url": "https://github.com/wecassidy/glimer-map",
        "description": "An interactive map of reciever functions for seismic stations across the world from Stéphane Rondenay's GLImER project.",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2019-05-25T14:02:26.000Z",
        "updatedAt": "2019-07-29T02:12:11.000Z",
        "language": "JavaScript",
        "homepage": "http://stephanerondenay.com/glimer-map/map.html",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "mnky9800n/Python-course-materials-for-seismology-students",
        "url": "https://github.com/mnky9800n/Python-course-materials-for-seismology-students",
        "description": "a library of various tools and notebooks for catalog analysis",
        "stars": 32,
        "forks": 15,
        "readme": "# Seismology Tools\nThis is a collection of notebooks for a python short course for scientists. These notebooks are designed to be useful both in class and to students outside of class. Any suggestions for making them better is helpful.\n\nThe learning goals for these notebooks are:\n\n1. Students should feel comfortable with standard data types in python (list, string, float, int, dict, etc)\n2. Students should be able to import data and export data using pandas\n3. Students should be able to make maps in python that include data such as points and heat maps\n4. Students should be able to make plots in python that have horizontal and vertical axes of different datas\n5. Students should be able to manipulate histograms in python and create visualizations of histograms\n6. Students should be comfortable with jupyter notebooks\n7. Students should be able to perform \"exploratory data analysis\" and generate a hypothesis from it\n\n# Getting started\nTo get started we recommend using Anaconda: https://www.anaconda.com/download/\n\nThe environment.yml file is provided to install all of the dependencies required for these notebooks.\n\n## Steps to getting started\n\n1. Download Anaconda (we recommend python 3.X version, 64-bit): https://www.continuum.io/downloads\n2. Follow installation instructions for anaconda\n3. create your new environment `conda env create -f environment.yml` : http://conda.pydata.org/docs/using/envs.html#create-a-separate-environment\n4. activate the environment\n5. download the notebooks or clone the repository and get started!\n\nNote: if you have trouble installing shapely please try [this](https://anaconda.org/scitools/shapely)\n\n### Please cite our article about this collection of notebooks if you use them in your teaching or research\nJohn M. Aiken, Chastity Aiken, Fabrice Cotton; A Python Library for Teaching Computation to Seismology Students. Seismological Research Letters ; 89 (3): 1165–1171. doi: https://doi.org/10.1785/0220170246\n",
        "createdAt": "2017-06-28T09:56:16.000Z",
        "updatedAt": "2025-05-21T06:56:10.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/mnky9800n/Python-course-materials-for-seismology-students/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "schipp/matched_field_processing_simple",
        "url": "https://github.com/schipp/matched_field_processing_simple",
        "description": "Matched Field Processing to locate sources of seismic signal near a seismic array, following Umlauf & Korn (2019)",
        "stars": 7,
        "forks": 1,
        "readme": "# Matched Field Processing (MFP)\n\n# THIS REPOSITORY IS SUPERSEDED BY [https://github.com/schipp/fast_beamforming](https://github.com/schipp/fast_beamforming)\n\n[![DOI](https://zenodo.org/badge/268340026.svg)](https://zenodo.org/badge/latestdoi/268340026)\n\nDetermine nearby sources of seismic signal from dense array data using \"Matched Field Processing\". The plane-wave-assumption of classical beamforming is violated in cases where sources are near or inside the seismic array. This method is essentially 3D-beamforming, where a grid-search is performed on a 3D-grid (here x,y,z) instead of 2D in classical beamforming (usually backazimuth and velocity). Green's Function spectra are computed for a medium with constant velocity at each grid point and compared to recorded (i.e., synthetic for now) spectra. The beampower is computed using the Bartlett processor.\n\nFor now, this is a synthetic demonstration. Eventually, I will expand this to real data.\n\nThis code follows the methodology detailed in Umlauft & Korn (2019).\n\n> Umlauft, J., & Korn, M. (2019). 3-D fluid channel location from noise tremors using matched field processing. Geophysical Journal International, 219(3), 1550–1561. [doi](http://doi.org/10.1093/gji/ggz385)\n\n## requirements\n\n- tqdm\n",
        "createdAt": "2020-05-31T18:30:57.000Z",
        "updatedAt": "2024-08-01T06:09:36.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/schipp/matched_field_processing_simple/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ThomasLecocq/msnoise-tomo",
        "url": "https://github.com/ThomasLecocq/msnoise-tomo",
        "description": "The Ambient Noise Tomography Plugin for MSNoise",
        "stars": 24,
        "forks": 18,
        "readme": "# MSNoise-Tomo\n\n[![Build Status](https://travis-ci.org/ThomasLecocq/msnoise-tomo.png)](https://travis-ci.org/ThomasLecocq/msnoise-tomo)\n[![Build status](https://ci.appveyor.com/api/projects/status/hkaxi68hikwu6any/branch/master?svg=true)](https://ci.appveyor.com/project/ThomasLecocq/msnoise-tomo/branch/master)\n\n\nMSNoise-Tomo is a plugin to the MSNoise framework, allowing to branch from the [REF stack](http://msnoise.org/doc/workflow/006_stack.html) to compute FTAN, dispersion curves and 2D period maps. \n\nMSNoise-Tomo is developed by Thomas Lecocq (Royal Observatory of Belgium, ROB) based on original codes from A. Mordret and M. Landès (IPGP, 2013).\n\nReferences\n----------\n* Mordret, A., Landès, M., Shapiro, N.M., Singh, S.C., Roux, P., Barkved, O.I., 2013. Near-surface study at the Valhall oil field from ambient noise surface wave tomography. Geophys. J. Int. 193, 1627–1643. https://doi.org/10.1093/gji/ggt061\n* Barmin, M.P., Ritzwoller, M.H., Levshin, A.L., 2001. A Fast and Reliable Method for Surface Wave Tomography. Pure and Applied Geophysics 158, 1351–1375. https://doi.org/10.1007/PL00001225\n\nDocumentation\n-------------\n\nThe documentation is available at http://msnoise.org/plugins/msnoise-tomo/doc/\n\nInstallation\n------------\n\nCreate a new environment using conda:\n\n* conda create -n tomo -c conda-forge python=3.7 obspy=1.1.1 msnoise shapely pyproj\n* conda activate tomo\n\nThen, the code can be installed from scratch using this repository, or using precompiled python wheels:\n\n* Windows:http://msnoise.org/plugins/msnoise-tomo/msnoise_tomo-0.1b0-cp37-cp37m-win_amd64.whl \n* Linux: http://msnoise.org/plugins/msnoise-tomo/msnoise_tomo-0.1b0-cp37-cp37m-linux_x86_64.whl \n* MacOS:http://msnoise.org/plugins/msnoise-tomo/msnoise_tomo-0.1b0-cp37-cp37m-macosx_10_9_x86_64.whl\n\nRemember, always consider the current GitHub *master* as not stable!\n\nCommercial Usage\n----------------\nIf you plan to use MSNoise-TOMO for commercial purposes, please contact Thomas\nLecocq directly.",
        "createdAt": "2017-03-02T14:02:33.000Z",
        "updatedAt": "2025-10-29T03:28:45.000Z",
        "language": "Python",
        "homepage": "http://www.msnoise.org",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ThomasLecocq/msnoise-tomo/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "vsilwal/IITH_seismo",
        "url": "https://github.com/vsilwal/IITH_seismo",
        "description": "Basic Seismological Tools",
        "stars": 2,
        "forks": 0,
        "readme": "IITH_seismo\n==============\n\nThis is a private repository for the members of IITH-SeismicSource Group. \nIt can be downloaded using the following command at the terminal:\n```\ngit clone https://github.com/vsilwal/IITH_seismo\n```\nLet me know if there is any permission issue..\n\nThis repository contains utilities that could be of use for other members of the group \n(like: meshing, data processing, plotting, etc). \nMost packages start like this until they are ready to be shared publicly or become an independent repository of its own.\n\nFor now it contains three repositories:\n- CUBIT - For building geometry and mesh in Cubit\n- matlab_util - For matlab functions \n- python_util - For python functions\n\nSimilarly you can add perl, C, etc as you feel the need.\n\nHere is a brief introduction to github:\nhttp://rogerdudler.github.io/git-guide/\n",
        "createdAt": "2019-03-28T08:16:23.000Z",
        "updatedAt": "2023-07-22T13:33:21.000Z",
        "language": "MATLAB",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/vsilwal/IITH_seismo/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "bvanderbeek/TauP.jl",
        "url": "https://github.com/bvanderbeek/TauP.jl",
        "description": "A Julia wrapper for the TauP Toolkit, a Java Package for the calculation of seismic travel-times, ray paths, and related quantities through 1D spherical Earth velocity models.",
        "stars": 6,
        "forks": 1,
        "readme": "# TauP.jl\nA Julia wrapper for the [TauP Toolkit](https://www.seis.sc.edu/taup/), a Java Package for the calculation of seismic travel-times, ray paths, and related quantities through 1D spherical Earth velocity models ([GitHub Repo](https://github.com/crotwell/TauP)).\n\nGoal of this project is to provide TauP's functionality inside Julia. At present, this package includes Julia wrappers to `taup_time` (calculation of seismic travel-times, ray parameters, incidence and take-off angles) and `taup_path` (calculation of ray paths and along-ray travel-time information) and supports the placement of receivers below Earth's surface. Additionaly, Julia wrappers to TauP's geodsic methods and functions for reading and writing TauP model files are provided. Wrappers are built using [JavaCall.jl](https://github.com/JuliaInterop/JavaCall.jl).\n\nFor examples on how to use TauP.jl, see `tutorial.jl`.\n\nNote that another wrapper package for TauP named [TauPy.jl](https://github.com/anowacki/TauPy.jl) is available. Note that `TauPy` is a Julia wrapper of a Python-implemented version of TauP [ObsPy](https://docs.obspy.org/) package. The motivation for this project was to create an updated and pure Julia wrapper for the original TauP Java program. Those familiar with ObsPy may want to consider `TauPy` depending on their use case.\n\n\n# Installation\nThe package is part of the Julia General Registetry and can be istalled via the package manager:\n```julia\njulia> import Pkg\njulia> Pkg.add(\"TauP\")\n```\n\n`TauP.jl` ships with the latest version of TauP that has been tested with the wrappers (currently v2.6.1). By default, this local version of TauP will be used by the wrappers unless the environnment variable `TAUP_JAR` is defined. In this case, `TAUP_JAR` should hold the the path to the desired TauP jar-file. This can be defined from a julia session prior to loading the TauP.jl package as follows:\n```julia\njulia> ENV[\"TAUP_JAR\"] = \"path to TauP jar-file\"\n```\n\n## Attention Non-Windows Users!\nFor JavaCall to function properly, please set the environment variable `JULIA_COPY_STACKS = 1` before starting Julia. See [JavaCall documentation](https://github.com/JuliaInterop/JavaCall.jl) for more details. However, this may break Julia's multi-threading (e.g., [this issue](https://github.com/JuliaLang/julia/issues/44589))\n\nAdditionally, MacOS users must start Julia with the flag `julia --handle-signals=no` to avoid Java-related segmentation faults.\n\n# Quick Start Guide\nFor those familiar with TauP, using the Julia wrappers should be intuitive. For example, the following calculates travel-times through the IASP91 reference model for direct P and S phases for a source-receiver distance of 55 degrees and a source depth of 100 km,\n\n```julia\njulia> using TauP\njulia> TimeTauP = taup_time([\"P\",\"S\"], 55.0, 100.0; model = \"iasp91\", verbose = true)\n    Model: iasp91 \n    Depth: 100.0 (km) \n Distance: 55.0 (deg) \n Phase Name   Travel-time (s)   Ray Param (s/deg)   Takeoff (deg)   Incident (deg)\n ---------------------------------------------------------------------------------\n P            560.843           7.202               31.98           22.07         \n S            1015.837          13.365              33.27           23.82\n```\n\nThe option `verbose = true` enables TauP-like screen output of the results which are stored in the TimeTauP structure.\n\nIf performance is important or cleaner output is desired, a lower-level syntax designed for single-phase calculations can be used. For example,\n```julia\njulia> TimeObj = buildTimeObj(\"prem\") # Build TauP Time object for PREM model\njulia> set_taup_phase!(TimeObj, \"P\") # Set calculation parameters\njulia> set_taup_source_depth!(TimeObj, 100.0)\njulia> set_taup_receiver_depth!(TimeObj, 0.0)\njulia> t, p, i, j = taup_time!(TimeObj, 55.0) # Call taup_time\n```\nHere, `TimeObj` is a structure corresponding to TauP's Time object that contains all the relevant information for running `taup_time`. A tuple of arguments is returned with the travel-time `t`, ray parameter `p`, take-off angle `i`, and incidence angle `j`.\n\nNote that `TimeObj` is modified in the call to `taup_time!` to contain the relevant travel-time information. However, the same `TimeObj` may still be used on subsequent calculations and its calculation parameters may be updated via various set_* functions.\n\nFor convenience, a variety of methods for different input arguments (e.g., geographic source and receiver positions) are also implemented. See function help for details.\n\nFor more examples, see `tutorial.jl`.\n\n\n# Contributing\nAny comments on how the package may be improved or any additions to the package (e.g., new wrappers for other TauP methods such as taup_pierce) are welcome. Please open a GitHub issue or pull request.\n",
        "createdAt": "2023-04-27T15:10:14.000Z",
        "updatedAt": "2025-05-16T10:57:56.000Z",
        "language": "Java",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/bvanderbeek/TauP.jl/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "antongrin/eikonal-tutorial",
        "url": "https://github.com/antongrin/eikonal-tutorial",
        "description": "A simple realization of eikonal equation",
        "stars": 8,
        "forks": 6,
        "readme": "# eikonal-tutorial\nA simple realization of eikonal equation\n\n\nInspired by https://github.com/malcolmw/pykonal\n\n![](imgs/Figure_2.png)\n",
        "createdAt": "2017-12-01T13:01:14.000Z",
        "updatedAt": "2024-12-18T16:04:50.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/antongrin/eikonal-tutorial/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "RongjiangWang/MSEISFK_2008",
        "url": "https://github.com/RongjiangWang/MSEISFK_2008",
        "description": "A variant of MSEIS, for calculating synthetic f-k spectrum (dispersion curves) based on a layered elastic halfspace model with water as the surface layer",
        "stars": 0,
        "forks": 0,
        "readme": "A variant of MSEIS (QSEIS), for calculating synthetic f-k spectrum (dispersion curves) based on a layered elastic halfspace model with water as the surface layer.\n\nFor Windows user, the executable file is provided under folder \"WindowsEXE\". Linux user may compile the source codes with \"gfortran\" via a single command like, e.g.,\n\n~>cd .../SourceCode\n\n~>gfortran -o mseisfk *.f -O3\n\nto get the excutable code mseisfk.\n\nAfter start the executable code, the program ask for an input file in the ASCII format. An example input file is provided under folder \"InputFile\". You may change the input data included in this file for your own applications.\n\nA similar application example is given in QSEISFK_2011.\n\nReferences\n\nWang, R., (1999), A simple orthonormalization method for stable and efficient computation of Green's functions, Bulletin of the Seismological Society of America, 89(3), 733-741.\n",
        "createdAt": "2025-06-26T01:55:31.000Z",
        "updatedAt": "2025-06-26T02:15:49.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/RongjiangWang/MSEISFK_2008/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "seismiq-net/documentation",
        "url": "https://github.com/seismiq-net/documentation",
        "description": "Online Sensor Documentation",
        "stars": 0,
        "forks": 0,
        "readme": "seismiq Sensor Documentation\n================\n\n```shell\nnpm install\nnpm run docs:build\nnpm run docs:preview\n```\n",
        "createdAt": "2025-05-26T11:57:23.000Z",
        "updatedAt": "2025-07-21T15:33:45.000Z",
        "language": null,
        "homepage": "https://docs.seismiq.net",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/seismiq-net/documentation/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "crockonuts/hypoellipse",
        "url": "https://github.com/crockonuts/hypoellipse",
        "description": "HYPOELLIPSE Earthquake Location Program",
        "stars": 4,
        "forks": 0,
        "readme": "",
        "createdAt": "2018-05-31T02:59:48.000Z",
        "updatedAt": "2024-01-30T19:29:38.000Z",
        "language": "Fortran",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "stefanmaar/psysmon",
        "url": "https://github.com/stefanmaar/psysmon",
        "description": "A seismological prototyping software.",
        "stars": 1,
        "forks": 1,
        "readme": "",
        "createdAt": "2020-02-02T10:23:34.000Z",
        "updatedAt": "2025-10-23T09:22:55.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "pyrocko/pyrocko",
        "url": "https://github.com/pyrocko/pyrocko",
        "description": "An official read-only mirror of https://git.pyrocko.org/pyrocko/pyrocko. A seismology toolkit for Python.",
        "stars": 237,
        "forks": 85,
        "readme": "# Pyrocko\n### _A seismology toolkit for Python_\n\n[![Build Status](https://drone.pyrocko.org/api/badges/pyrocko/pyrocko/status.svg?ref=refs/heads/master)](https://drone.pyrocko.org/pyrocko/pyrocko)\n[![Anaconda-Server Badge](https://anaconda.org/pyrocko/pyrocko/badges/version.svg)](https://conda.anaconda.org/pyrocko)\n[![PyPI](https://img.shields.io/pypi/v/pyrocko.svg)](https://pypi.python.org/pypi/pyrocko/)\n\n\n## Installation\n\nPyrocko can be installed on various operating systems and in many different\ninstallation styles. Please consult the [Pyrocko Installation Manual](https://pyrocko.org/docs/current/install/) for details.\n\n### System wide installation from source\n\n```\ngit clone https://git.pyrocko.org/pyrocko/pyrocko.git\ncd pyrocko\npython install.py deps system\npython install.py system\n```\n\n### User installation from source\n\n```\ngit clone https://git.pyrocko.org/pyrocko/pyrocko.git\ncd pyrocko\npip install .  # only install into isolated environments like this!\n```\n\n### Installation with Anaconda\n\nAnaconda3 packages are available for Linux, OSX and Windows ([details](https://pyrocko.org/docs/current/install/packages/anaconda.html)).\n\n```\nconda install -c pyrocko pyrocko\n```\n\n### User installation with Python pip\n\nBinary pip packages are available for Linux and Windows ([details](https://pyrocko.org/docs/current/install/packages/pip.html)).\n\n```\npip install --user pyrocko\npip install --user --only-binary :all: PyQt5\n```\n\n## Documentation\n\nDocumentation and usage examples are available online at https://pyrocko.org/docs/current\n\n## Community Support\n\nCommunity support at [https://hive.pyrocko.org](https://hive.pyrocko.org/signup_user_complete/?id=9edryhxeptdbmxrecbwy3zg49y).\n\n## Citation\nThe recommended citation for Pyrocko is: (You can find the BibTeX snippet in the\n[`CITATION` file](CITATION.bib)):\n\n> Heimann, Sebastian; Kriegerowski, Marius; Isken, Marius; Cesca, Simone; Daout, Simon; Grigoli, Francesco; Juretzek, Carina; Megies, Tobias; Nooshiri, Nima; Steinberg, Andreas; Sudhaus, Henriette; Vasyura-Bathke, Hannes; Willey, Timothy; Dahm, Torsten (2017): Pyrocko - An open-source seismology toolbox and library. V. 0.3. GFZ Data Services. https://doi.org/10.5880/GFZ.2.1.2017.001\n\n[![DOI](https://img.shields.io/badge/DOI-10.5880%2FGFZ.2.1.2017.001-blue.svg)](https://doi.org/10.5880/GFZ.2.1.2017.001)\n\n## License\nGNU General Public License, Version 3, 29 June 2007\n\nCopyright © 2017 Helmholtz Centre Potsdam GFZ German Research Centre for Geosciences, Potsdam, Germany\n\nPyrocko is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\nPyrocko is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.\nYou should have received a copy of the GNU General Public License along with this program. If not, see <http://www.gnu.org/licenses/>.\n\n## Contact\n* Sebastian Heimann;\n  sebastian.heimann@gfz-potsdam.de\n\n* Marius Isken;\n  marius.isken@gfz-potsdam.de\n\n* Marius Kriegerowski;\n  marius.kriegerowski@gfz-potsdam.de\n\n```\nHelmholtz Centre Potsdam German Research Centre for Geoscienes GFZ\nSection 2.1: Physics of Earthquakes and Volcanoes\nHelmholtzstraße 6/7\n14467 Potsdam, Germany\n```\n",
        "createdAt": "2010-03-11T08:19:55.000Z",
        "updatedAt": "2025-12-04T10:46:21.000Z",
        "language": "Python",
        "homepage": "https://pyrocko.org",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/pyrocko/pyrocko/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "wbm06/seismology",
        "url": "https://github.com/wbm06/seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2018-06-07T13:49:04.000Z",
        "updatedAt": "2018-06-07T13:49:04.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "arjundatta23/cc_kern_inv",
        "url": "https://github.com/arjundatta23/cc_kern_inv",
        "description": "A seismological code in Python: this is a package for full waveform ambient seismic noise inversion for sources or structure.",
        "stars": 6,
        "forks": 2,
        "readme": "# cc_kern_inv\nA seismological code in Python: this is a package for full waveform ambient noise inversion for sources or structure. Overview of directory structure:\n\n- the source inversion code is in \"anseicca\". \n- structure inversion code is under development.\n- modules that are common to source and structure inversion, are in \"modules_common\". \n\nReferences\n\nFor ANSEICCA (source inversion):\n\nDatta, A., Shekar, B., Kumar P.L. (2023). Acoustic full waveform inversion for 2-D ambient noise source imaging. Geophysical Journal International, 234(3), 1628-1639. https://doi.org/10.1093/gji/ggad158\n",
        "createdAt": "2022-02-07T08:15:08.000Z",
        "updatedAt": "2025-06-04T19:36:04.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/arjundatta23/cc_kern_inv/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "guyomd/clock_drifts",
        "url": "https://github.com/guyomd/clock_drifts",
        "description": "Estimation of seismological station clock drifts from the differences of inter-event arrival times",
        "stars": 0,
        "forks": 0,
        "readme": "# clock_drifts\nEvaluation of seismological station clock drifts errors.\nThis approach is based on the analysis of differential P- & S-wave arrival times, and is particularly adapted to datasets composed of earthquake clusters.\nInput data consists of: \n- differential P and S arrival times for each pair of earthquakes located in the same cluster, and\n- a constant Vp/Vs value, which should hold for the whole cluster area, in particular at depth.\n\nMore details can be found in Daniel & Arroucau, 2022 (submitted to BSSA)\n",
        "createdAt": "2022-01-10T09:05:17.000Z",
        "updatedAt": "2023-01-26T13:52:16.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/guyomd/clock_drifts/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "jwellik/STALTA_tuner",
        "url": "https://github.com/jwellik/STALTA_tuner",
        "description": "Interactive configuration of STA/LTA algorithms for seismology.",
        "stars": 6,
        "forks": 5,
        "readme": "# STALTA_tuner\nInteractive configuration for STA/LTA algorithms. Applied to seismology. Based on codes in [ObsPy](https://docs.obspy.org/tutorial/code_snippets/trigger_tutorial.html).\n\nSTA/LTA settings can have a major affect on the results from automatic event detection across a network. This program allows you to interactively adjust different STA/LTA parameters to improve your network processing. There are no right answers for tuning your STA/LTA algorithm. Keep adjusting until you find something that represents your data well.\n\nThis program allows you to load seismic data and adjust common STA/LTA parameters to see how it will affect triggering on your data.\n\nCheck out the [Wiki](https://github.com/jwellik/STALTA_tuner/wiki) for more details :-)\n\n## Installation\nDownload the [zip file](https://github.com/jwellik/STALTA_tuner/archive/master.zip) or use `git` to clone the entire repository to your working directory (e.g., `/Users/jaywellik/PYTHON/stalta_tuner/`). All scripts will be run from this directory.\n\nSTALTA_tuner runs on Python 3.7. The program is interactive and will run in your default browser. It is powered by the [Bokeh server](https://docs.bokeh.org/en/latest/docs/user_guide/server.html) service. All dependencies can be easily installed via [Anaconda](https://www.continuum.io/) on the command line. I *highly* recommend using a virtual environment in order to guarantee that STALTA_tuner runs properly. Follow the directions below to install STALTA_tuner with a pre-defined virtual environment.\n\nFirst, change directories to /stalta_tuner. You can create the virtual environment with the provided yml file, or you can create the environment manually.\n\nTo create the environment from a yml file, do the following steps:\n```\n$ conda config --add channels conda-forge\n$ conda env create --file stalta37.yml\n```\n\nTo create the environment manually, do this:\n```\n$ conda config --add channels conda-forge\n$ conda env create --name stalta37\n$ conda activate stalta37\n$ conda install numpy\n$ conda install scipy\n$ conda install obspy\n$ conda install matplotlib\n$ conda install pandas\n$ conda install bokeh\n```\n\n## Usage\n\nFirst, activate the virtual environment\n```\n$ conda activate stalta37\n```\n\nNow, run the shell script to execute the code. Specify the configuration file without the extension. E.g.,\n```\n$ ./run.sh MSH_lite\n```\n\nThe interactive tuner will open in your default browser. Change the STA/LTA algorithm, STA window, LTA window, and trigger thresholds to tune your STA/LTA settings. You can also change the timestamp to study. Only 30' are displayed at a time.\n\n## Configuration file\n\nConfiguration files exist in ./configs. They are simple. Look at ./configs/MSH.py as an example. Set the server, stations (nslc), and the number of stations required to exceed the STA/LTA threshold to produce a trigger. Note, when you call the configuration file when running the program, do *not* include the extension, '.py'. See 'Usage'.\n\nCheck out the [Wiki](https://github.com/jwellik/STALTA_tuner/wiki) for more details.\n",
        "createdAt": "2020-04-04T00:14:43.000Z",
        "updatedAt": "2025-10-29T18:41:29.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/jwellik/STALTA_tuner/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "OpenSeismology/OSSR",
        "url": "https://github.com/OpenSeismology/OSSR",
        "description": "Open source seismology repository",
        "stars": 0,
        "forks": 0,
        "readme": "# OSSR\nOpen source seismology repository\n",
        "createdAt": "2018-10-17T12:11:40.000Z",
        "updatedAt": "2018-11-19T05:57:22.000Z",
        "language": null,
        "homepage": "http://ossr.github.io/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/OpenSeismology/OSSR/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "seismiq-net/torch-models",
        "url": "https://github.com/seismiq-net/torch-models",
        "description": "TorchScript neural network models for QuakeSaver smart seismic sensors.",
        "stars": 2,
        "forks": 0,
        "readme": "# QuakeSaver Neural Network Models\n\n[![Export TorchScript models](https://github.com/QuakeSaver/torch-models/actions/workflows/export-models.yml/badge.svg)](https://github.com/QuakeSaver/torch-models/actions/workflows/export-models.yml)\n<a href=\"https://github.com/psf/black\"><img alt=\"Code style: black\" src=\"https://img.shields.io/badge/code%20style-black-000000.svg\"></a>\n\nThis repository assembles the neural network models for QuakeSaver seismic sensors.\n\nThe [pytorch](https://pytorch.org/) models are assembled in [TorchScript](https://pytorch.org/docs/stable/jit.html), which are executed on-the-edge on the QuakeSaver sensor's firmware.\n\n## Available Models\n\nWe are actively working on bringing neural networks for seismology and building health monitoring to the edge.\n\n### PhaseNet\n\nPhaseNet phase detector for P- and S- wave phase picks trained by [SeisBench](https://github.com/seisbench/seisbench) on various large data sets:\n\n* ethz\n* geofon\n* instance\n* iquique\n* lendb\n* neic\n* original\n* scedc\n* stead\n\nFor more information see\n\n> Weiqiang Zhu, Gregory C Beroza, PhaseNet: a deep-neural-network-based seismic arrival-time picking method, Geophysical Journal International, Volume 216, Issue 1, January 2019, Pages 261–273, <https://doi.org/10.1093/gji/ggy423>\n\n> Jack Woollam, Jannes Münchmeyer, Frederik Tilmann, Andreas Rietbrock, Dietrich Lange, Thomas Bornstein, Tobias Diehl, Carlo Giunchi, Florian Haslinger, Dario Jozinović, Alberto Michelini, Joachim Saul, Hugo Soto; SeisBench—A Toolbox for Machine Learning in Seismology. Seismological Research Letters 2022;; 93 (3): 1695–1709. doi: <https://doi.org/10.1785/0220210324>\n\n## Development Installation\n\nLocal installation of the helper repositoy\n\n```sh\npip3 install .\n```\n\n### Development\n\nWe utilize `pre-commit` for clean commits.\n\n```sh\npre-commit install\n```\n\n## Distribution of Models\n\nDistribution of the TorchScript models is carried out by GitHub actions. See the `.github/workflow` folder for details on continuous deployment. The sensors fetch models from the released artifacts.\n",
        "createdAt": "2023-04-01T20:15:24.000Z",
        "updatedAt": "2023-04-03T14:53:29.000Z",
        "language": "Python",
        "homepage": "https://quakesaver.net",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/seismiq-net/torch-models/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "cbell/geology-base",
        "url": "https://github.com/cbell/geology-base",
        "description": "Base notebook for seismological data analysis. ",
        "stars": 0,
        "forks": 0,
        "readme": "# geology-base\nBase notebook for seismological data analysis. \n\nPackages that are installed: \n--- \n* https://github.com/obspy/obspy/wiki\n* https://paudetseis.github.io/Telewavesim/\n* https://github.com/trichter/rf\n* https://pyrocko.org/\n* https://github.com/matplotlib/jupyter-matplotlib\n* https://github.com/lckr/jupyterlab-variableInspector\n\n\n\nNot installed:\n---\n\n* http://amaggi.github.io/waveloc/index.html\n\nNote: This may be attempted again to install after the notebook is in use and the requested package is needed again. However due to the age of the package, and issues with use and running in the notebook - it is currently on hold. \n",
        "createdAt": "2019-12-10T20:09:46.000Z",
        "updatedAt": "2022-01-10T21:53:05.000Z",
        "language": "Dockerfile",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/cbell/geology-base/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "yunndlalala/Python_tools",
        "url": "https://github.com/yunndlalala/Python_tools",
        "description": "Tools for seismology study",
        "stars": 7,
        "forks": 1,
        "readme": "",
        "createdAt": "2020-04-19T09:20:42.000Z",
        "updatedAt": "2025-09-24T08:09:20.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "PauloHjoe/seismology",
        "url": "https://github.com/PauloHjoe/seismology",
        "description": "Seismology stuffs.",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2020-04-12T17:56:41.000Z",
        "updatedAt": "2020-04-12T17:56:41.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "pyrocko/contrib-snufflings",
        "url": "https://github.com/pyrocko/contrib-snufflings",
        "description": "An official read-only mirror of https://git.pyrocko.org/pyrocko/contrib-snufflings. User contributed snufflings collection",
        "stars": 14,
        "forks": 16,
        "readme": "User Contributed Snufflings\n===========================\n\n**Snufflings** are plugins for seismological trace viewer **Snuffler** which is\npart of the Python framework [Pyrocko](http://pyrocko.org).\n\nAn overview of available snufflings is given at the end of this page.\n\nInstallation\n------------\nClone this repository and change directory:\n\n    git clone https://github.com/pyrocko/contrib-snufflings.git\n    cd contrib-snufflings\n\nCreate symbolic links pointing from this directory into `$HOME/.snufflings`.\nThe included `setup.py` script provides a shortcut for that:\n\n    python setup.py link [arguments]\n\nIf no `arguments` are given, all available snufflings will be linked. You will\nfind the new snufflings under *panels* or *run* in Snuffler's menu.\n\nUpdate\n------\nPull updates from the repository:\n\n    git pull origin master\n\nHelp\n----\nMost snufflings include documentation which can be found in the *Help* menu in\nSnuffler.\n\nContribute\n----------\nAfter cloning this repository, add your own snufflings and send a pull request.\n\nWe recommend to add a doc string at the top of each snuffling right\nbeneath the snuffling's class name. This text will be shown when pressing the\n*Help* button on the snuffling's panel. It should give an overview of the\nfunctionalities. Wrapping this text in html code can be used to pretty up the\ndocumentation.\n\nIf you miss some feature in a snuffling, discover a bug or would like to\ndiscuss an idea for a new snuffling click on *Issues* and open up a\n*New Issue*.\n\n\n-------------------------------------------------------------------------------\n\nExamples\n========\n\nPlot PSD\n--------\n\nPlot power spectral densities\n\nfile: [psd.py](psd.py)\n\n![screenshot](screenshots/psd.png)\n\nCross correlation relocation\n----------------------------\n\nRelocate events by cross correlating waveforms\n\nfile: [cc\\_relocation.py](cc_relocation.py)\n\n![screenshot](screenshots/cc_relocation.png)\n\nCross correlation search\n------------------------\n\nFind repeating events\n\nfile: [corrsearch.py](corrsearch.py)\n\n![screenshot](screenshots/corrsearch.png)\n\nCross correlation matrix\n------------------------\n\nCross correlate selected events. Results, including cross-correlation factor\nand time lags between maxima of the cross correlation can be stored in YAML\nformat to ease later analysis.\n\ndirectory: [cc\\_matrix](cc_matrix)\n\n![screenshot](screenshots/cc_matrix.png)\n\nListen to seismograms\n---------------------\n\nExport seismograms to .wav files or listen to seismological recordings. Direct\nplayback requires the PyQt4 bindings for Phonon.\n(E.g. on Debian and ubuntu available through: `apt-get install python-qt4-phonon`)\n\nfile: [audio.py](audio.py)\n\n![screenshot](screenshots/SeiSound.png)\n\nTime Line\n---------\n\nTemporal overview of catalog data.\n\nfile: [time_line.py](time_line.py)\n\n![screenshot](screenshots/timeline.png)\n\nExtract Events\n--------------\n\nSave waveforms for time windows around selected events as MSEED. This is a\nhybrid Snuffling which can be run from the command-line as well.\n\nfile: [extract_events.py](extract_events.py)\n\n![screenshot](screenshots/extract_events.png)\n\nExport waveforms\n----------------\n\nExport selected/visible waveforms as MSEED, ASCII, SAC or YAFF files.\n\nfile: [export_waveforms.py](export_waveforms.py)\n\nDistance projected waveform plots\n---------------------------------\n\nApplying a reduction velocity allows to 'shrink' the time domain.\nFigures can be exported in various image file formats like .png or .pdf.\nIn order to improve the visual perception of small wiggles, positive amplitudes can be plotted in filled mode.\n\nfile: [plot_traces.py](plot_traces.py)\n\n![screenshot](screenshots/plot_traces.png)\n\nSpectrogram\n-----------\n\nfile: [spectrogram.py](spectrogram.py)\n\n![screenshot](screenshots/spectrogram.png)\n\nGeodetic forward modelling\n--------------------------\n\nVisualize and output of a data trace for a rectangular dislocation source in an elastic halfspace.\nYou will need to go to the okada dir and type make in order to compile the C-Code.\n\ndirectory: [okada](okada)\n\n![screenshot](screenshots/okada.png)\n\nParticle Motion\n---------------\n\nPlot combinations of vertical and horizontal channels of selected stations.\n\nfile: [particle_motion.py](particle_motion.py)\n\n![screenshot](screenshots/particle_motion.png)\n",
        "createdAt": "2013-09-20T12:29:20.000Z",
        "updatedAt": "2024-04-02T19:19:14.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/pyrocko/contrib-snufflings/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Rayn93/Seismology_Application",
        "url": "https://github.com/Rayn93/Seismology_Application",
        "description": "Project of seismology application for my Master Thesis",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2017-11-03T17:08:31.000Z",
        "updatedAt": "2018-03-30T14:31:35.000Z",
        "language": "HTML",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "siddharth1712/Devito_Simulations_GLEE",
        "url": "https://github.com/siddharth1712/Devito_Simulations_GLEE",
        "description": "2-D and 3-D Models in Devito for the Seismology payload of GLEE",
        "stars": 0,
        "forks": 0,
        "readme": "# Devito_Simulations_GLEE\n2-D and 3-D Models in Devito for the Seismology payload of GLEE\n",
        "createdAt": "2021-02-13T11:22:34.000Z",
        "updatedAt": "2021-02-13T11:37:47.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/siddharth1712/Devito_Simulations_GLEE/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "raspishake/rscu",
        "url": "https://github.com/raspishake/rscu",
        "description": "Raspberry Shake Charting Utility for Windows, Mac, and Linux",
        "stars": 10,
        "forks": 1,
        "readme": "# rscu - Raspberry Shake Charting Utility\n\nwritten and contributed by David Fowler\n\n## About\n\nrscu (pronounced \"rascue\") is a GUI-based charting utility built to make custom plotting of [Raspberry Shake](https://raspberryshake.org) data easier.\n\n## Installation\n\n### via Anaconda/Miniconda (recommended)\n\nDownload and install [Miniconda3](https://docs.conda.io/en/latest/miniconda.html)\n1. In a Terminal (Mac OS or Linux) or Anaconda Prompt (Windows), tell conda to update itself:\n\n    ```\n    conda update conda -y\n    ```\n\n1. Next, tell conda to create an environment with the proper requirements, and activate it:\n\n    ```\n    conda create --name rscu obspy\n    conda activate rscu\n    ```\n\n1. Now, install rscu directly from github (you may need [git](https://git-scm.com/) for this):\n\n    ```\n    pip install git+https://github.com/raspishake/rscu\n    ```\n\n1. You're ready to run rscu!\n\n    ```\n    rscu\n    ```\n\n### on Debian Linux without Anaconda\n\n1. Install requirements via apt and pip\n\n    ```\n    sudo apt install python3-numpy python3-lxml python3-pip\n    pip install obspy\n    ```\n\n1. Now, install rscu directly from github (you may need [git](https://git-scm.com/) for this):\n\n    ```\n    pip install git+https://github.com/raspishake/rscu\n    ```\n\n1. You're ready to run rscu!\n\n    ```\n    rscu\n    ```\n",
        "createdAt": "2020-08-08T17:00:33.000Z",
        "updatedAt": "2025-04-25T17:09:39.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/raspishake/rscu/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "kura-okubo/NoiseJul.jl",
        "url": "https://github.com/kura-okubo/NoiseJul.jl",
        "description": "A Julia package allowing for seismological processing and analysis.",
        "stars": 1,
        "forks": 0,
        "readme": "# NoiseJul\n\n[![Stable](https://img.shields.io/badge/docs-stable-blue.svg)](https://kura-okubo.github.io/NoiseJul.jl/stable)\n[![Dev](https://img.shields.io/badge/docs-dev-blue.svg)](https://kura-okubo.github.io/NoiseJul.jl/dev)\n[![Build Status](https://travis-ci.com/kura-okubo/NoiseJul.jl.svg?branch=master)](https://travis-ci.com/kura-okubo/NoiseJul.jl)\n[![Codecov](https://codecov.io/gh/kura-okubo/NoiseJul.jl/branch/master/graph/badge.svg)](https://codecov.io/gh/kura-okubo/NoiseJul.jl)\n\nA Julia package of modules for seismological processing and analysis.\n\n# Installation\n\nFrom the Julia command prompt:\n\n1. Press ] to enter pkg.\n2. Type `add xxx/NoiseJul.jl`\n3. Press backspace to exit pkg.\n4. Type `using NoiseJul`\n\n!!! note\n      If you have an error when adding NoiseJul, try\n\n> using Pkg; Pkg.pkg\"add https://github.com/xtyangpsp/SeisConvert.jl.git https://github.com/kura-okubo/SeisDownload.jl.git https://github.com/jaredbryan881/SeisXcorrelation.jl.git https://github.com/kura-okubo/SeisRemoveEQ.jl.git https://github.com/kura-okubo/SeisBeamforming.git\";\n\n!!! note\n\n      Since it imports large number of submodules, precompile process takes a minute.\n\n# Modules\n\n- [SeisIO](https://github.com/jpjones76/SeisIO.jl): Julia language support for geophysical time series data\n\n   Author: [jpjones76](https://github.com/jpjones76)\n\n- [SeisNoise](https://github.com/tclements/SeisNoise.jl): Ambient Noise Cross-Correlation in Julia\n\n   Author: [Tim Clements](https://github.com/tclements/)\n\n- [SeisDownload](https://github.com/kura-okubo/SeisDownload.jl.git): Mass download of Seismic data with SeisIO\n\n   Author: [Kurama Okubo](https://github.com/kura-okubo)\n\n- [SeisConvert](https://github.com/xtyangpsp/SeisConvert.jl): Data convertor from SAC format\n\n   Author: [Xiaotao Yang](https://github.com/xtyangpsp)\n\n- [SeisXcorrelation](https://github.com/jaredbryan881/SeisXcorrelation.jl.git) (under developing): High-order CC with SeisNoise\n\n  Author: [Jared Bryan](https://github.com/jaredbryan881)\n\n- [SeisRemoveEQ](https://github.com/kura-okubo/SeisRemoveEQ.jl.git): Removing earthquake and tremor from raw data using kurtosis & STA/LTA\n\n   Author: [Kurama Okubo](https://github.com/kura-okubo)\n\n- [SeisBeamforming](https://github.com/kura-okubo/SeisBeamforming.git): Compute Beampower for Beamforming analysis\n\n   Author: [Kurama Okubo](https://github.com/kura-okubo)\n\n# Example\n\n`S = NoiseJul.IO.SeisData()`\n\n`F = NoiseJul.Noise.FFTData()`\n\nMore information can be found in each github repository.\n\n---\nCopyright (c) 2019 Harvard University Earthquake Seismology Group. All Rights Reserved.\n",
        "createdAt": "2019-08-04T16:13:11.000Z",
        "updatedAt": "2019-08-30T01:29:56.000Z",
        "language": "Julia",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/kura-okubo/NoiseJul.jl/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Chouyuhin/Tools_seismo",
        "url": "https://github.com/Chouyuhin/Tools_seismo",
        "description": "Utils for seismology coding",
        "stars": 0,
        "forks": 0,
        "readme": "# Tools_seismo\n\nUtils for seismology coding\n### Tool lists\n\n- cut mseed file to mseed in given time window\n- convert the npz to hdf5",
        "createdAt": "2025-11-28T22:26:59.000Z",
        "updatedAt": "2025-11-28T22:39:01.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Chouyuhin/Tools_seismo/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "earthinversion/STADIUM-Py",
        "url": "https://github.com/earthinversion/STADIUM-Py",
        "description": "STADIUM-Py is a Python software tool that fully automates the receiver function technique (RF) and shear wave splitting (SKS) measurement analysis.",
        "stars": 21,
        "forks": 11,
        "readme": "# Receiver Function and SKS automatic measurement - Seismological Tools Automated Download, processing & Imaging Using Mostly Python (STADIUM - Py)\n![visitors](https://visitor-badge.glitch.me/badge?page_id=earthinversion.STADIUM-Py) \n<a href=\"https://doi.org/10.5281/zenodo.4686103\"><img src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.4686103.svg\" alt=\"DOI\"></a>\n\n\n\n\n- By Cédric P Legendre (@cplegendre) and Utpal Kumar (@utpalrai)\n- Based on RF analyses by: Tom Eulenfeld (@trichter); https://github.com/trichter/rf\n- Based on SKS analyses by: Jack Walpole (@JackWalpole); https://github.com/JackWalpole/splitwavepy\n- Based on ObsPy: https://github.com/obspy/\n\nThe User's manual for STADIUM-Py can be downloaded from [here](wikiimage/User_Manual_STADIUM-py.pdf).\n\n\n## Installation\nInstall the anaconda Python 3 environment `rfsksenv` by running the following command (Note: It requires preinstalled anaconda)\n\n\n- __OSX:__\n\n```conda env create -f environment_osx_10_14_6.yml```\n\n- __Linux:__\n\n```conda env create -f environment_UbuntuXIX.yml```\n\n- __Windows:__\n\nrequires installation of Anaconda Python followed by packages in the environement. \n\n__To activate the anaconda environement__:\n\n```conda activate rfsksenv```\n\n\n#### PS: \n- All these libraries can be installed separately in \"non-anaconda\" environment as well.\n\n- If having some issues with Cartopy, one simple fix may be:\n\n```pip uninstall shapely; pip install --no-binary :all: shapely```\n- In __Ubuntu__, there were conflicts in the different required libraries. Therefore, some of the necessary dependencies needed to be installed manually:\n\n  - cartopy\n      -- ```conda install -c conda-forge cartopy```\n  - h5py\n      -- ```conda install h5py```\n  - obspyh5\n      -- ```pip install obspyh5``` or\n      -- ```conda install obspyh5```\n  - rf\n      -- ```pip install rf```\n  - splitwavepy\n      -- ```pip install splitwavepy```\n\n## Run:\n```python stadium.py```\n\n### User's input:\nA total of four files controls the run of STADIUM-Py:\n- `input_file.yaml` (select region)\n- `Settings/stepwise.yaml` (toggle the steps)\n- `Settings/advRFparam.yaml` (fine tune the RF parameters)\n- `Settings/advSKSparam.yaml` (fine tune the SKS parameters)\n\nBelow, you will find a list of the parameters, with possible values and its description.\n\n### __`input_file`__\n\n- `project\\_name`         |default|       Define the name of the project directory where all results will be stored.\n- `fresh_start`\t\t| 0/1\t|\tDelete the 'default' folder and start fresh\n- `makeRF`\t\t| 0/1\t|\tRun the code to calculate the Reciever Functions\n- `makeSKS`\t\t|0/1\t|\tRun the code to calculate the shear-wave splitting of SKS phase\n\nIn addition, 4 boundary parameters are used to __select the region of interest__:\n\n1. `mnlong`\t\t|-130\t|\tMinimum longitude of the region of interest\n2. `mxlong`\t\t|-60\t|\tMaximum longitude of the region of interest\n3. `mnlat`\t\t\t|35\t|\tMinimum latitude of the region of interest\n4. `mxlat`\t\t\t|50\t|\tMaximum latitude of the region of interest\n\n\n### __`stepwise`__\n\nThis file is provided in case user want to run specific part of the code (for testing or parameters adjustments).\n\n\n__data settings__\n- `client`\t\t|IRIS\t|\tEnter all the clients for data download separated by commas. List of ObsPy clients: https://docs.obspy.org/packages/obspy.clients.fdsn.html . Current list -- 24 items: BGR, EMSC, ETH, GEONET, GFZ, ICGC, INGV, IPGP, IRIS, ISC, KNMI, KOERI, LMU, NCEDC, NIEP, NOA, ODC, ORFEUS, RASPISHAKE, RESIF, SCEDC, TEXNET, USGS, USP.\n- `network`\t\t|*\t|\tList of networks (default = all)\n- `station`\t\t|*\t|\tList of stations (default = all)\n- `locations`\t\t|\"\",\"00\"|\tList of locations (default = \"\",\"00\")\n\n\n__plot settings__\n- `plot_stations`\t\t|0/1\t|\tCreate a stations map\n- `plot_events`\t\t|0/1\t|\tCreate an events map\n- `plot_all_retrieved_events_stations`\t\t|0/1\t|\tCreate a stations and event map\n\n\n__RF stepwise__\n- `obtain_inventory_RF`\t| 0/1\t|\tList all the stations available\n- `download_data_RF`\t| 0/1\t|\tDownload the waveforms to calculate the Reciever Functions\n- `compute_plot_RF`\t| 0/1\t|\tPlot receiver functions?\n- `plot_ppoints`\t\t|0/1\t|\tPlot the piercing points (for Reciever Functions)\n- `plot_RF_profile`\t|0/1\t|\tPlot the vertical profiles (for Reciever Functions)\n\n\n__SKS stepwise__\n- `obtain_inventory_SKS`\t|0/1\t|\tList all the stations available (for SKS)\n- `download_data_SKS`\t|0/1\t|\tDownload the waveforms to calculate the shear-wave splitting of SKS phase\n- `plot_traces_ENZ`\t|0/1\t|\tPlot the waveforms (for SKS)\n- `plot_traces_RTZ`\t|0/1\t|\tPlot the rotated waveforms (for SKS)\n- `plot_SKS_measure`\t|0/1\t|\tPlot the grid search for phase and delay time.\n- `plot_SKS`\t\t|0/1\t|\tPlot the results (for SKS)\n- `picking_SKS`\t\t|0/1\t|\tPicking of the SKS phase\n- `plot_traces`\t\t|0/1\t|\tPlot the traces that may contain SKS phase\n- `plot_trigger`\t\t|0/1\t|\tPlot the automatic picking of the SKS phase\n\n\n### __`advRFparam.yaml`: RF parameters__\n\n__filenames__\n- `invRFfile` \t\t\t\t|rf_stations.xml\t\t|\tstation xml\n- `RFsta`\t\t\t\t\t|all_stations_RF.txt\t\t|\tstation text catalog\n- `retr_stations`\t\t\t\t|all_stations_rf_retrieved.txt\t|\tretrived stations list file\n- `data_rf_suffix`\t\t\t|rf_profile_data\t\t|\trf data file name: {net}-{stn}-rf_profile_data.h5\n- `events_map_suffix`\t\t\t|RF-events_map\t\t\t|\tevents map filename suffix {net}-{stn}-RF-events_map.png\n- `retr_station_prefix`\t\t\t|RF_stations\t\t\t|\tretrieved stations prefix\n- `rf_compute_data_suffix`\t\t|rf_profile_rfs\t\t\t|\trf computation result file name: network-station-rf_profile_rfs.h5\n- `rfprofile_compute_result_prefix`\t|rf_profile_profile\t\t|\trf profile computation result file name: rf_profile_profile{azimuth}_*.h5\n\n__H - K settings__\n- `h_kappa_res_file`\t|h-kappa-values.txt\t|\tFile name for the H-K results\n- `plot_h`\t\t|0/1\t\t\t|\tPlot Moho map\n- `plot_kappa`\t\t|0/1\t\t\t|\tPlot Vp/Vs ratio\n\n__RF profile settings__\n- `num_profile_divs_lat`\t|2\t|\tAmount of EW profiles\n- `num_profile_divs_lon`\t|3\t|\tAmount of NS profiles\n- `ppdepth`\t\t|70\t|\tChosen depth for piercing point calculation\n\n\n__RF event search settings__\n- `minradiusRF`\t\t|30\t|\tMinimum epicentral distance (for Reciever Functions)\n- `maxradiusRF`\t\t|90\t|\tMaximum epicentral distance (for Reciever Functions)\n- `minmagnitudeRF`\t|5.5\t|\tMinimum magnitudes of events (for Reciever Functions)\n- `maxmagnitudeRF`\t|9.5\t|\tMaximum magnitudes of events (for Reciever Functions)\n\n__RF filter settings__\n- `minfreq`\t\t|0.5\t|\tstream minfreq for bandpass\n- `maxfreq`\t\t|2.0\t|\tstream maxfreq for bandpass\n\n\n__RF display settings__\n- `trace_height`\t\t|0.1\t|\theight of one trace in inches\n- `trim_min`\t\t|-5\t|\ttrim stream relative to onset before plotting\n- `trim_max`\t\t|20\t|\ttrim stream relative to onset before plotting\n- `rf_info`\t\t|default|\tadditional axes for RF plot, None for no additional axes\n\n\n### __`advSKSparam`: SKS parameters__ \n__File names__\n- `invSKSfile`\t\t|sks_stations.xml\t\t|\tstation xml\n- `SKSsta`\t\t|stations_SKS.txt\t\t|\tstation text catalog\n- `retr_stations`\t\t|all_stations_sks_retrieved.txt\t|\tretrived stations list file\n- `data_sks_suffix`\t|sks_profile_data\t\t|\tsks data file name: {net}-{stn}-sks_profile_data.h5\n- `events_map_suffix`\t|SKS-events_map\t\t\t|\tevents map filename suffix {net}-{stn}-SKS-events_map.png\n- `retr_station_prefix`\t|SKS_stations\t\t\t|\tretrieved stations prefix\n- `sks_meas_indiv`\t|sks_measurements.txt\t\t|\tsks measurements file suffix for individual stations\n- `sks_measure_map`\t|SKS_station_Map\t\t|\tfilename of sks measurements map\n\n__SKS event search settings__\n- `minradiusSKS`\t\t|90  \t|\tMinimum epicentral distance (for SKS)\n- `maxradiusSKS`\t\t|120\t|\tMinimum epicentral distance (for SKS)\n- `minmagnitudeSKS`\t|5.5\t|\tMinimum magnitudes of events (for SKS)\n- `maxmagnitudeSKS`\t|9.5\t|\tMaximum magnitudes of events (for SKS)\n\n\n__SKS filter settings__\n- `minfreq`\t\t|0.01\t|\tstream minfreq for bandpass\n- `maxfreq`\t\t|0.6\t|\tstream maxfreq for bandpass\n\n\n__SKS picking__\n- `trimstart`\t\t|30\t|\ttrim the traces for sks picking trace starttime+trimstart  to starttime+trimend\n- `trimend`\t\t|110\t|\ttrim the traces for sks picking trace starttime+trimstart  to starttime+trimend\n\n__SKS picking algorithm__\n- `sks_picking_algo`\t|recursive_sta_lta\t|\tpicking algorithm for sks phase...other options are classic_sta_lta, z_detect, carl_sta_trig, delayed_sta_lta\n- `sks_picking_algo_thr0`\t|2.5\t\t\t|\tstarting threshold for sks picking algorithm\n- `sks_picking_algo_thr1`\t|0.65\t\t\t|\tend threshold for sks picking algorithm\n\n__SKS measurements constraints__\n\n - `sel_param`: lam12 #options: snr, lam12; selection parameter of the measurements: either use signal to noise ratio, snr or use the eigenvalue ratio (lambda1/lambda2), lam12\n\n##### sel_param_settings:\n - `snr_ratio`\t\t|2\t|\tminimum signal to noise ratio of the traces for filtering good measurements\n - `lam12fast_threh`\t|1.1\t|\tthreshold for the lambda1/lambda2 for fast direction pick\n - `lam12lag_threh`\t|1.1\t|\tthreshold for the lambda1/lambda2 for lag time pick\n##### lag_settings:\n - `minlag`\t\t|0\t|\tminimum allowed lag time in sks measurements\n - `maxlag`\t\t|3\t|\tmaximum allowed lag time in sks measurements\n - `maxdlag`\t\t|1.5\t|\tmaximum allowed error in the lag time\n##### fast_dir_settings:\n - `maxdfast`\t\t|7\t|\tmaximum allowed error in the fast direction\n\n__error_plot_toggles__:\n - `error_plot_indiv`\t|0\t|\tmake 1 to plot the error profiles of fast direction and lag time for each measurements\n - `error_plot_all`\t|1\t|\tmake 1 to plot the error profiles of fast direction and lag time for each measurements\n\n\n__PS__:\nThese parameters should be modified with caution by the users.\n\n### __Procedure__:\n\n- Search the events that satisfy some critera (defined input_file.yaml).\n\n- Search all stations for which data are available (defined input_file.yaml).\n\n- Download the waveforms.\n\n- This is an automated procedure that may be time consuming if a large dataset is selected.\n\n\n## Process the data following:\n\n### RF\n- Filter and rotate the trace into the LQ domain.\n- Deconvolve the radial and tangential components by the vertical component.\n- Calculate the piercing points for each event.\n- Stacks the reciever functions before plotting.\n- Plot the reciever functions for L and Q components, sorted by back azimuth (or distance).\n- Create some vertical profile for all stations in selected regions.\n\n### SKS\n- Filter and rotate the trace into the radial/tangential referencial.\n- Minimize the energy on the transverse components.\n- Automatically pick the SKS phase.\n- Invert for phase and delay time.\n- Plot the results.\n\n\n## Display parameters:\n\n### Station Map\n\n\n<img src=\"wikiimage/STU_map.png\" width=\"200\" alt=\"Station Map\">\n\n\n### Events Map\n\n#### For RF:\n\n\n<img src=\"wikiimage/STU_EventsRF.png\" width=\"300\" alt=\"Events Map for RF\">\n\n\n#### For SKS:\n<img src=\"wikiimage/STU_EventsSKS.png\" width=\"300\" alt=\"Events Map for SKS\">\n\n\n#### Reciever Functions\n\n * Single event RF\n\n<img src=\"wikiimage/STU_RF.png\" width=\"300\" alt=\"RF - single event\">\n\n\n * Piercing points\n\n<img src=\"wikiimage/STU_pp.png\" width=\"200\" alt=\"RF - piercing points\">\n\n\n\n * Single station profile\n\n<img src=\"wikiimage/STU_Q.png\" width=\"150\" alt=\"profile - Q\">\n<img src=\"wikiimage/STU_L.png\" width=\"150\" alt=\"profile - L\">\n\n\n#### Reciever Functions - Multiple station profile\n\n<img src=\"wikiimage/STU_Q_profile.png\" width=\"300\" alt=\"N090E profile\">\n\n\n#### Reciever Functions - H - Kappa measurements on individual RF\n\n<img src=\"wikiimage/RFHK.png\" width=\"300\" alt=\"N090E profile\">\n\n\n#### Reciever Functions - H - Kappa maps\n\n<img src=\"wikiimage/HK.png\" width=\"300\" alt=\"N090E profile\">\n\n\n\n### SKS\n * Read the station HDF5 file containing all the seismic traces recovered for this station.\n\n<img src=\"wikiimage/STU_dataSKS-ZNE.png\" width=\"200\" alt=\"ZNE data\">\n\n\n * Filter and rotate the trace into the radial / tangential referencial.\n            ```trace1.rotate('NE->RT')```\n\n<img src=\"wikiimage/STU_dataSKS-ZRT.png\" width=\"200\" alt=\"ZRT data\">\n\n\n * Minimize the energy on the transverse components.\n\n * Automatically pick the SKS phase.\nWe implemented several picking options from [ObsPy](https://github.com/obspy/obspy/wiki) to attempt to pick the SKS phase.\n\n<img src=\"wikiimage/STU_pick.png\" width=\"200\" alt=\"Automatic picking\">\n\n\n * Invert for phase and delay time.\n                ```measure = sw.EigenM(data)```\n\n<img src=\"wikiimage/SKS_gridsearch.png\" width=\"300\" alt=\"Grid search\">\n\n\n * Get all the potential SKS measurements for each station\n\n<img src=\"wikiimage/SKSSummary.png\" width=\"300\" alt=\"individual station\">\n\n\n * Plot the results.\n <p align=\"left\">\n<img src=\"wikiimage/SKSMap.png\" width=\"150\" alt=\"All stations results\">\n\n<img src=\"wikiimage/SKSMap2.png\" width=\"150\" alt=\"All stations with data\">\n </p>\n\n## Cite as\nKumar, Utpal, & Legendre, Cédric P. (2021, January 16). STADIUM-Py: Python Command-line Interface for automated Receiver Functions and Shear-Wave Splitting Measurements (Version 1.0). Zenodo. http://doi.org/10.5281/zenodo.4686103\n",
        "createdAt": "2019-09-23T05:55:38.000Z",
        "updatedAt": "2025-02-06T07:55:40.000Z",
        "language": "Python",
        "homepage": "https://www.earthinversion.com/stadiumpy/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.4686103",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.4686103",
            "dataCite": "10.5281/zenodo.4686103",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/earthinversion/STADIUM-Py/master/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.4686103",
            "title": "STADIUM-Py: Python Command-line Interface for automated Receiver Functions and Shear-Wave Splitting Measurements",
            "journal": "Zenodo",
            "dateReleased": "2021-01-16T00:00:00.000Z",
            "abstract": "STADIUM-Py is an open-source command-line tool designed for automated computation of receiver functions (RFs) and shear-wave splitting (SWS) measurements. It uses many useful tools from Obspy for data retrieval and processing and integrates them with existing open-source seismological packages for achieving complete automation. STADIUM-Py employs the rf-package algorithm to perform the moveout correction (for Ps phase by default) via the common conversion point (CCP) technique using the IASP91 velocity model. Besides, STADIUM-Py also implements the H-κ technique developed by Zhu &amp; Kanamori (2000) to determine the crustal thickness (H) and the average Vp/Vs ratios (κ) under the seismic station. <br> STADIUM-Py adopts the Python package SplitWavePy developed by Jack Walpole to measure the splitting parameters. It uses the eigenvalue method of Silver and Chan (1991) for estimating the splitting parameters – the azimuth angle of the fast shear wave (φ) and delay time (δt) between the two split shear waves. The User's manual for STADIUM-Py is provided in the package.<br> In addition to computing the RFs and SWS measurements, STADIUM-Py also performs the quality checks at each step. Finally, the results are stored locally for each step of the analysis in the form of figures and tables for further analysis and result verification.",
            "citationsArray": []
        },
        "publications": [
            {
                "doi": "10.1186/s40623-021-01495-0",
                "name": "Crustal structure and upper mantle anisotropy of the Afar triple junction",
                "source": "Zenodo",
                "authorNames": [
                    "Kumar, U.",
                    "Legendre, C. P.",
                    "Huang, B. S."
                ],
                "url": [
                    "http://ui.adsabs.harvard.edu/#abs/2021EP&S...73..166K",
                    "http://doi.org/10.1186/s40623-021-01495-0"
                ]
            }
        ]
    },
    {
        "source": "GitHub",
        "name": "JandyZ22/sacsnr",
        "url": "https://github.com/JandyZ22/sacsnr",
        "description": "Signal-to-noise ratio calculations in seismology",
        "stars": 0,
        "forks": 0,
        "readme": "# sacsnr\n\nGet SNR of SAC files in a specified time window.\n\n$S N R=10 \\log _{10}\\left(\\frac{A_s}{A_N}\\right)^2$\n\nCalculation of SNR of radial and tangential seismic records before estimating Receiver Functions(RFs) \n\n## SAC I/O functions\n\n- `sacio.h`: Head file for SAC file format, and prototype for SAC IO functions.\n- `sacio.c`: Definitions of several SAC IO functions.\n  - `read_sac_head`: read SAC header\n  - `read_sac`: read SAC binary data\n  - `read_sac_xy`: read SAC binary XY data\n  - `read_sac_pdw`: read SAC data in a partial data window (cut option)\n  - `write_sac`: write SAC binary data\n  - `write_sac_xy`: write SAC binary XY data\n  - `new_sac_head`: create a minimal SAC header\n  - `sac_head_index`: return the index of a SAC head field\n  - `issac`: Check if a file in in SAC format\n\n## SAC Utilities\n\n- [sacsnr](#sacsnr): Get SNR of SAC files in a specified time window.\n\n### `sacsnr`\n```\nUsage:\n  sacsnr [-Ttmark/s0/s1/n0/n1] sacfiles\n  tmark  sac hearder t0~9 \n  s0-s1  signal time window \n  n0-n1  noise time window  \n  eg:  sacsnr -T7/0/50/-50/0 CD2.BHN.2007.294.210257.SAC\nOptions:\n  -h    show usage.\n```\n\n## Compile and Use\nUse the command in this folder:\n```\ngcc -o sacsnr sacsnr.c sacio.c sacio.h -lm\n```\nPlace the tool directly into the $SACHOME/bin.\n\nSACHOME refers to the folder where the SAC is installed",
        "createdAt": "2023-08-29T09:58:09.000Z",
        "updatedAt": "2023-08-29T10:02:18.000Z",
        "language": "C",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/JandyZ22/sacsnr/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "mpanning/OnDeckInSight",
        "url": "https://github.com/mpanning/OnDeckInSight",
        "description": "Data and processing software for \"On-deck seismology: Lessons from InSight for future planetary seismology\"",
        "stars": 2,
        "forks": 2,
        "readme": "# OnDeckInSight\nProcessing software for \"On-deck seismology: Lessons from InSight for future planetary seismology\"\n\nAll raw data used in the paper is available for download either at the IRIS Data Management Center, or through the [Mars SEIS Data service MSDS](https://www.seis-insight.eu/en/science/science-summary).  This data is read using the [obspy python package](https://www.obspy.org).\n\nThe authoritative source for the archived InSight data, including all instruments is the Planetary Data System.  SEIS data is archived at the [Geosciences node](https://pds-geosciences.wustl.edu/missions/insight/index.htm).  Wind data from the TWINS sensor is archived at the [Atmospheres node](https://atmos.nmsu.edu/data_and_services/atmospheres_data/INSIGHT/insight.html).\n\nTHe following python codes are included for producing figures in the paper:\n\n`make_ppsd_panelA_fig.py` - Panel A of figure 2\n\n`make_paper_ppsd_fig.py` - Panel B of figure 2\n\n`windspeed_comp.py` - Figure 3 and panel C of figure 1 (Note that the summary RMS data produced by this script is saved in the file WS_SPviking_rms.csv)\n\n`generate_atten_curve.py` - Generate interpolated attenutation curves using Instaseis databases previously published used in figures 5-7.  The Mars Instaseis databases can be found at [http://instaseis.ethz.ch/marssynthetics/](http://instaseis.ethz.ch/marssynthetics/) and are explained in Clinton et al. (2017) [doi: 10.1785/0220170094](http://doi.org/10.1785/0220170094).  The Europa Instaseis databases can be found at [http://instaseis.ethz.ch/icy_ocean_worlds/](http://instaseis.ethz.ch/icy_ocean_worlds/) and are explained in Staehler et al. (2018) [doi: 10.1002/2017JE005338](http://doi.org/10.1002/2017JE005338).  `planet` variable and `db_short` need to be set in the code to create files for all models used in the paper.\n\n`plot_atten_curve.py` - Figure 5 (Note that this uses interpolated attenuation curves from the `atten_curves` subdirectory generated by `generate_atten_curve.py`)\n\n`event_prob_matrix_ondeck.py` - Generates data files for figure 6 using attenuation curves already discussed\n\n`plot_prob_ondeck.py` - Panels A and B of figure 6 (uses data from probability matrix csv files in this directory generated by `event_prob_matrix_ondeck.py`)\n\n`event_prob_matrix_europa.py`, `event_prob_matrix_europa_lownoise.py`, and `event_prob_matrix_europa_selfnoise.py` - Generates the input files for figure 7\n\n`plot_prob.py`, `plot_prob_selfnoise.py`, `plot_prob_lownoise.py` - plotting panels of figure 7\n\nThe specific data used in these codes including specific filenames can be downloaded at the NASA Open Data Portal archive preserved at [this archive](http://doi.org/10.25966/na0g-cr11).  This data should be saved expanded as a subdirectory called `datafiles` in order to use the python codes as written.\n\nAll software in this repository is licensed using GNU General Public License version 3 as described in the file `COPYING`\n\nAffiliation: Jet Propulsion Laboratory, California Institute of Technology\nThe research was carried out at the Jet Propulsion Laboratory, California Institute of Technology, under a contract with the National Aeronautics and Space Administration (80NM0018D0004).  This research is in support of InSight Contribution Number 120.\n",
        "createdAt": "2019-12-23T16:36:23.000Z",
        "updatedAt": "2022-01-05T15:25:02.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/mpanning/OnDeckInSight/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "balessio/seismology",
        "url": "https://github.com/balessio/seismology",
        "description": "Collection of programs written for downloading seismological signals and earthquake metadata from online databases and making useful visualizations.",
        "stars": 0,
        "forks": 0,
        "readme": "# seismology\nCollection of programs written for downloading seismological signals and earthquake metadata from online databases and making useful visualizations.\n\nDescriptions included with scripts\n",
        "createdAt": "2021-06-22T21:13:58.000Z",
        "updatedAt": "2021-06-22T21:33:58.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/balessio/seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "gege1997/seismology_project",
        "url": "https://github.com/gege1997/seismology_project",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2017-10-07T16:39:08.000Z",
        "updatedAt": "2017-10-07T16:39:08.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "elygeo/coseis",
        "url": "https://github.com/elygeo/coseis",
        "description": "Computational Seismology Tools",
        "stars": 22,
        "forks": 8,
        "readme": "# COSEIS\n\n# Computational Seismology Tools\n\n[github.com/gely/coseis](https://github.com/gely/coseis/)  \n[elygeo.net/coseis](http://elygeo.net/coseis/)  \n\n![](figs/Bigten.jpg)\n\n\n## Summary\n\nCoseis is a toolkit for earthquake simulation featuring:\n\n- The Support Operator Rupture Dynamics ([SORD](#sord)) code for modeling\n  spontaneous rupture and 3D wave propagation.\n\n- SCEC Community Velocity Models (CVM) codes, with MPI parallelization for\n  [Magistrale version](https://scec.usc.edu/scecpedia/CVM-S4) (CVM-S), and new\n  [geotechnical layer implementation](http://elygeo.net/2016-Vs30GTL-Ely+4.html)\n  for the [Harvard version](http://scec.usc.edu/scecpedia/CVM-H) (CVM-H).\n\n- Utilities for mesh generation, coordinate projection, and visualization.\n\nThe primary interface is through a Python module which (for high-performance\ncomponents) wraps Fortran parallelized with hybrid OpenMP and MPI.\n\nCoseis is written by [Geoffrey Ely](http://elygeo.net/) with contributions from\nSteven Day, Bernard Minster, Feng Wang, Zheqiang Shi, and Jun Zhou.  It is\nlicensed under [BSD](http://opensource.org/licenses/BSD-2-Clause) terms.\n\n<div class=\"warn\">\n**NOTICE**: Coseis is currently only lightly maintained.  You likely will need\nto dig in, understand the code, and fix things to use it.  If you have (or know\nof) means to support planned improvements or custom requirements, please be in\ntouch.\n</div>\n\n\n## Install MacOS Dependencies\n\nInstall [Xcode](http://itunes.apple.com/us/app/xcode/id497799835) from the App\nStore followed by the Xcode the Command Line Tools with:\n\n    xcode-select --install\n\nInstall [Homebrew](http://brew.sh/) and use it to install GCC (for Fortran),\nPython:\n\n    brew install gcc python3 \n\nFor analyisis and graphics also install VTK and usueful Python libs:\n\n    brew install vtk\n    python -m pip install mayavi matplotlib pyproj obspy\n\n\n## Install Coseis\n\nClone the source code from the [Coseis GitHub\nrepository](http://github.com/gely/coseis):\n\n    git clone git://github.com/gely/coseis.git\n\nSetup python to be able to find the `cst` package:\n\n    cd coseis\n    python -m cst.setup\n\n\n\n## Test\n\nTo run the test suite interactively:\n\n    cd tests\n    python test_runner.py --run=exec\n\nOr, submit a job for batch processing:\n\n    python test_runner.py --run=submit\n\nAfter completion, a report is printed to the screen (or saved in\n`run/test_suite/test_suite.output`):\n\n    PASSED: cst.tests.hello_mpi.test()\n    PASSED: cst.tests.point_source.test()\n    PASSED: cst.tests.pml_boundary.test()\n    PASSED: cst.tests.kostrov.test()\n\n\n## SORD\n\nThe Support Operator Rupture Dynamics (SORD) code simulates spontaneous rupture\nwithin a 3D isotropic viscoelastic solid. Wave motions are computed on a\nlogically rectangular hexahedral mesh, using the generalized finite difference\nmethod of support operators. Stiffness and viscous hourglass corrections are\nemployed to suppress suppress zero-energy grid oscillation modes. The fault\nsurface is modeled by coupled double nodes, where the strength of the coupling\nis determined by a linear slip-weakening friction law. External boundaries may\nbe reflective or absorbing, where absorbing boundaries are handled using the\nmethod of perfectly matched layers (PML). The hexahedral mesh can accommodate\nnon-planar ruptures and surface topography\n\nSORD simulations are configured with Python scripts. Underlying computations\nare coded in Fortran 95 and parallelized for multi-processor execution using\nMessage Passing Interface (MPI) and OpenMP. The code is portable and tested\nwith a variety of Fortran 95 compilers, MPI implementations, and UNIX-like\noperating systems (Linux, MacOS, IBM AIX, etc.).\n\n\n## Background\n\nThe formulation, numerical algorithm, and verification of the SORD method are\ndescribed by @2008-GJI-Ely+2 for wave propagation, and @2009-GJI-Ely+2 for\nspontaneous rupture. @2010-BSSA-Ely+2 present an application to simulating\nearthquakes in southern California.\n\n\n## User Guide\n\n### Quick test\n\nRun a simple point source explosion test and plot a 2D slice of particle\nvelocity:\n\n    cd scripts\n    python SORD-Example-sim.py\n    python SORD-Example-ploy.py\n\nPlotting requires Matplotlib, and the result should look like this:\n\n![](figs/SORD-Example.svg)\n\n### Python Scripting\n\nThe general procedure is to import the `cst` module, create a dictionary of\nparameters, and pass that dictionary to the `cst.sord.run()` function.\nParameters are either job-control or simulation parameters. Defaults for these\ntwo types of parameters are given in [cst.job.defaults](cst/job.py) and\n[cst.sord.parameters](cst/sord.py), respectively. Machine specific job-control\nparameters may also be present in the `conf` directory that supersede the\ndefaults.\n\nIt maybe be helpful to look through example applications in the\n`scripts` directory, and return to this document for further description\nof the simulation parameters.\n\n### Field I/O\n\n[Note about a change from previous versions: The `fieldio` parameter has been\nremoved, and instead each field I/O parameter is a separate list.]\n\nMulti-dimensional field arrays may be accessed for input and out through a list\nof operations that includes reading from and writing to disk, as well as\nassigning to scalar values or time-dependent functions. In the quick test\nabove, `rho`, `vp`, `vs`, `v1`, and `v2` are examples of 3- and 4-D fields. The\nfull list of available fields is given in the\n[cst.sord.fieldnames](cst/sord.py) dictionary.\n\nField variables are categorized in four ways: (1) static vs. dynamic, (2)\nsettable vs. output only, (3) node vs. cell registration, and (4) volume vs.\nfault surface. For example, density `rho` is a static, settable, cell, volume\nvariable. Slip path length `sl` is a dynamic, output, node, fault variable.\n\nField operations may specify a subregion of the array by giving slicing indices\nfor each dimension. The 0-based indices can be either, a single index, empty\nbrackets `[]` as shorthand for the entire array, of arguments to the python\n`slice()` function, which can be either [start], [start, stop] or [start, stop,\nstep]. Here are some examples:\n\n    [10, 20, 1, []]             # Single cell, full time history\n    [10, 20, 1, -1]             # Single node, last time step\n    [[], [], [], -1]            # Full 3D volume, last time step\n    [10, [], [], [0, None, 10]] # j=10 node surface, every 10th time step\n\nFIXME: this section is unfinished.\n\n    f = val                         # Set f to value\n    f = ([], '=', val)              # Set f slice to value\n    f = ([], '+', val)              # Add value to f slice\n    f = ([], '=', 'rand', val)      # Random numbers in range (0, val)\n    f = ([], '=', 'func', val, tau) # Time function with period tau, scaled by val\n    f = ([], '<=', 'filename')      # Read filename into f\n    f = ([], '=>', 'filename')      # Write f into filename\n\nA dot (`.`) indicates sub-cell positioning via weighted averaging. In this case\nthe spatial indices are single logical coordinates that may vary continuously\nover the range. The fractional part of the index determines the weights. For\nexample, an index of 3.2 to the 1D variable f would specify the weighted\naverage: 0.8 \\* f(3) + 0.2 \\* f(4).\n\nReading and writing to disk uses flat binary files where j is the fastest\nchanging index, and t is the slowest changing index. Mode 'R' extrapolates any\nsingleton dimensions to fill the entire array. This is useful for reading 1D or\n2D models into 3D simulations, obviating the need to store (possibly very\nlarge) 3D material and mesh coordinate files.\n\nFor a list of available time functions, see the `time_function` subroutine in\n[util.f90](cst/sord/util.f90). The routine can be easily modified to add new\ntime functions. Time functions can be offset in time with the `tm0` initial\ntime parameter.\n\n### Boundary Conditions\n\nBoundary conditions for the six faces of the model domain are specified by the\nparameters `bc1` (near-size, x, y, and z faces) and `bc2` (far-side, x, y, and\nx faces). The symmetry boundary conditions can be used to reduce computations\nfor problems where they are applicable. These are not used for specifying\ninternal slip boundaries. However, for problems with symmetry across a slip\nsurface, the fault may be placed at the boundary and combined with an\nanti-mirror symmetry condition. The following BC types are supported:\n\n`free`: Vacuum free-surface. Stress is zero in cells outside the boundary.\n\n![](figs/SORD-BC0.svg)\n\n`rigid`: Rigid surface. Displacement is zero at the boundary.\n\n![](figs/SORD-BC3.svg)\n\n`+node`: Mirror symmetry at the node. Normal displacement is zero at the\nboundary. Useful for a boundary corresponding to (a) the plane orthogonal to\nthe two nodal planes of a double-couple point source, (b) the plane normal to\nthe mode-III axis of a symmetric rupture, or (c) the zero-width axis of a 2D\nplane strain problem.\n\n![](figs/SORD-BC1.svg)\n\n`-node`: Anti-mirror symmetry at the node. Tangential displacement is zero\nat the boundary. Useful for a boundary corresponding to (a) the nodal planes of\na double-couple point source, (b) the plane normal to the mode-II axis of a\nsymmetric rupture, or (c) the zero-width axis of a 2D antiplane strain problem.\n\n![](figs/SORD-BC-1.svg)\n\n`+cell`: Mirror symmetry at the cell. Same as type 1, but centered on the cell.\n\n![](figs/SORD-BC2.svg)\n\n`-cell`: Anti-mirror symmetry at the cell. Same as type -1, but centered on the\ncell. Can additionally be used when the boundary corresponds to the slip\nsurface of a symmetric rupture.\n\n![](figs/SORD-BC-2.svg)\n\n`pml`: Perfectly match layer (PML) absorbing boundary.\n\nExample: a 3D problem with a free surface at Z=0, and PML absorbing boundaries\non all other boundary faces:\n\n    shape = [50, 50, 50, 100]\n    bc1 = ['pml', 'pml', 'free']\n    bc2 = ['pml', 'pml', 'pml']\n\nExample: a 2D antiplane strain problem with PML absorbing boundaries. The\nnumber of nodes is 2 for the zero-width axis:\n\n    shape = [50, 50, 2, 100]\n    bc1 = ['pml', 'pml', '-node']\n    bc2 = ['pml', 'pml', '-node']\n\n### Defining the fault rupture surface\n\nFault rupture always follows a surface of the (possibly non-planar) logical\nmesh. The orientation of the fault plane is defined by the `faultnormal`\nparameter. This can be either 1, 2, or 3 corresponding to surfaces normal to\nthe j, k, or l logical mesh directions. Any other value (typically 0) disables\nrupture altogether. The location of the rupture plane with in the mesh is\ndetermined by the `ihypo` parameter, which has a dual purpose of also defining\nthe nucleation point. So, the indices of the collocated fault double nodes are\ngiven by `int(ihypo[faultnormal])`, and `int(ihypo[faultnormal]) + 1`. For\nexample, a 3D problem of dimensions 200.0 x 200.0 x 200.0, with a fault plane\nlocated at z = 100.0, and double nodes at l = (21, 22), may be set up as such:\n\n    delta = [5.0, 5.0, 5.0, 0.1]\n    faultnormal = 3\n    shape = [41, 41, 42, 100]\n    hypocenter = [20.0, 20.0, 20.5]\n    bc1 = ['free', 'free', 'free']\n    bc2 = ['free', 'free', 'free']\n\nFor problems with symmetry across the rupture surface (where mesh and material\nproperties are mirror images), the symmetry may be exploited for computational\nsavings by using an appropriate boundary condition and solving the elastic\nequations for only one side of the fault. In this case, the fault double nodes\nmust lie at the model boundary, and the and the cell-centered anti-mirror\nsymmetry condition used. For example, reducing the size of the previous example\nto put the rupture surface along the far z boundary:\n\n    shape = [41, 41, 22, 100]\n    hypocenter = [20.0, 20.0, 20.5]\n    bc1 = ['free', 'free', 'free']\n    bc2 = ['free', 'free', '-cell']\n\nAlternatively, put the rupture surface along the near z boundary:\n\n    shape = [41, 41, 22, 100]\n    hypocenter = [20.0, 20.0, 1.5]\n    bc1 = ['free', 'free', '-cell']\n    bc2 = ['free', 'free', 'free']\n\nFurther symmetries may present. If our previous problem has slip only in the x\ndirection, then we may also use node-centered mirror symmetry along the\nin-plane axis, and node-centered anti-mirror symmetry along the anti-plane\naxis, to reduce computations eight-fold:\n\n    shape = [21, 21, 22, 100]\n    hypocenter = [20.0, 20.0, 20.5]\n    bc1 = ['free', 'free', 'free']\n    bc2 = ['anti-n', 'mirror-n', 'anti-c'\n\n\n## Memory Usage and Scaling\n\n23 single precision (four-byte) memory variables are required per mesh point.\nOn current hardware, computation time is on the order of the one second per\ntime step per one million mesh points. SORD scalability has been benchmarked up\nto 64 thousand processors at ALCF.\n\n![](figs/SORD-Benchmark.svg)\n\n: **Figure.** Weak-scaling benchmarks.\n\n## References {.bib}\n\n",
        "createdAt": "2010-07-24T06:01:27.000Z",
        "updatedAt": "2025-11-18T12:07:13.000Z",
        "language": "Python",
        "homepage": "http://elygeo.net/coseis/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/elygeo/coseis/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ikahbasi/read_V1",
        "url": "https://github.com/ikahbasi/read_V1",
        "description": "this is simple python code to read ascii acceleration data (V1 file) in seismology ",
        "stars": 1,
        "forks": 0,
        "readme": "# read_V1\nThis is simple python code to read ascii acceleration data (V1 file) in seismology \n\nThis code is too simple :) but work enough for me\n\nI just try to make first work in github\n\n    This code has 3 simple function for read,write and plot V1-file in v1.py\n    (file.V1 is strong motion or acceleration data of earthquake)\n    \n        1)read_v1\n            >>> path = 'path-of-file/namefile'\n            \n            # then\n            >>> st = read_v1(path,method = 'obspystream') # if you have obspy modul\n            # or\n            >>> data = read_v1(path,method = 'ascii')\n            # or\n            >>> data,st = read_v1(path,method = 'both') # if you have obspy modul\n\n           \n            \n            # data has all 3 component\n            # data = [[comp1, time1, acc1],[comp2, time2, acc2],[comp3, time3, acc3]]\n        \n        2)v1_write_2column_file\n            >>> v1_write_2column_file(data)\n            # write 3 files with name of 'name_input_file+component' in current directory \n    \n        3)plot_v1\n            >>> plot_v1(data)\n            # save image 'output.png' in current directory\n            \n        Please send me any file.V1 that makes error to improve improve efficiency.\n",
        "createdAt": "2019-07-15T18:48:59.000Z",
        "updatedAt": "2024-04-13T14:29:16.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ikahbasi/read_V1/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "stephenhernandez/paramo-python",
        "url": "https://github.com/stephenhernandez/paramo-python",
        "description": "Various Python routines for Stephen's daily seismological workflow",
        "stars": 0,
        "forks": 0,
        "readme": "# paramo-python\n\nVarious Python routines, dubbed Páramo-Python, are translations of code \noriginally written in Matlab from the project Páramo-Matlab.\n\nThe name \"páramo\" refers to the tropical alpine tundra common in the\nEcuadorian Andes. Most of this code was written during my tenure at the\nInstituto Geofísico in Quito, Ecuador for the daily processing of primarily \nseismological, event catalog, and infrasound data.\n",
        "createdAt": "2019-07-23T13:28:55.000Z",
        "updatedAt": "2019-07-23T22:10:20.000Z",
        "language": null,
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/stephenhernandez/paramo-python/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "anowacki/MomentTensors.jl",
        "url": "https://github.com/anowacki/MomentTensors.jl",
        "description": "Seismic moment tensors in Julia",
        "stars": 10,
        "forks": 0,
        "readme": "# MomentTensors.jl\n\n[![Build Status](https://github.com/anowacki/MomentTensors.jl/workflows/CI/badge.svg)](https://github.com/anowacki/MomentTensors.jl/actions)\n[![Coverage status](https://codecov.io/gh/anowacki/MomentTensors.jl/branch/master/graph/badge.svg?token=knbujQ671A)](https://codecov.io/gh/anowacki/MomentTensors.jl)\n\n## What is MomentTensors.jl?\nA [Julia](http://julialang.org) package for dealing with [seismic moment\ntensors](https://earthquake.usgs.gov/learn/glossary/?term=moment%20tensor).\n\nIt is currently very limited, and useful for three main things:\n\n1. Calculating the radiation pattern for a moment tensor;\n2. Decomposing tensors into their istropic, double-couple and CLVD\n   components; and\n3. Rotating moment tensors.\n\nI wrote this module because these are by far the most common things I need to do\nin my day-to-day global seismologist life.\n\n## What it isn't\nCurrently, no plotting is performed, nor conversion between conventions.\n\nThe module internally assumes the Harvard/Global CMT convention (see the module\ninteractive help for details), though nothing is stopping you using another\nconvention with it as long as you remember which indices correspond to which\ndirections.\n\nInput/output is limited to reading the CMTSOLUTION format used by\n[SPECFEM3D](https://github.com/geodynamics/specfem3d) and the\n[NDK](http://www.ldeo.columbia.edu/~gcmt/projects/CMT/catalog/allorder.ndk_explained)\nformat used by the [GlobalCMT project](https://globalcmt.org).\n\n## How to install\nMomentTensors.jl can be added to your Julia install like so:\n\n```julia\njulia> import Pkg; Pkg.add(\"MomentTensors\")\n```\n\n\n## How to use\n### MT type\nMomentTensors.jl represents moment tensors using the `MT` type.\n\n#### Construction\nThey can be constructed by:\n- passing a set of six moment tensor components, in the order\n  _M<sub>rr</sub>, M<sub>θθ</sub>, M<sub>φφ</sub>,\n  M<sub>rθ</sub>, M<sub>rφ</sub>, M<sub>θφ</sub>_),\n- a vector of length six containing these components,\n- a 3 × 3 matrix _M<sub>ij</sub>_ with\n  _i,j_ ∈ {_r,θ,φ_}, or\n- the strike, dip and rake (in Aki & Richards convention) plus a moment\n  in N.m.\n\nThe element type of an `MT{T} where T` instance is determined\nautomatically from the values supplied, or one can specify a\ndesired element type explicitly with e.g. `MT{Float32}(1, 2, 3, 4, 5, 6)`.\n\nSee the docstring for `MT` for more details of construction.\n\n#### Indexing\nTo retrieve an individual component of the `MT` `m`, you can:\n- access the _i,j_ component with `m[i,j]` where _i,j_ ∈ {1,2,3};\n- get the named components with `m[:rθ]` or equivalently `m[:rt]`\n  (see the docstring for `getindex`); and\n- get the _I_<sup>th</sup> component of the six-element vector with\n  `m[I]`.\n\n\n## Exported functions\n\n- `MT`: Construct a new moment tensor.\n- `amplitude_v_azimuth`: Compute the P, SV and SH amplitudes, and polarisation angle,\n  for a particular takeoff angle at a range of azimuths.\n- `cmtsolution`: Construct a new moment tensor from a string in the SPECFEM3D 'CMTSOLUTION'\n   format\n- `decompose`: Decompose a moment tensor into its isotropic, double-couple\n  and CLVD components, and report the relative proportion of the isotropic,\n  deviatoric and double-couple parts, plus their associated moments.\n- `eps_non_dc`: Calculate the non-double-couple component of an MT as\n  defined by Giardini.\n- `m0`: Return the scalar moment, given a moment magnitude.\n- `mw`: Return the moment magnitude, given a scalar moment.\n- `ndk`: Construct a new moment tensor from a string in the 'NDK' format used by\n  the Global CMT project.\n- `radiation_pattern`: Compute the P, SV and SH amplitude, and S polarisation angle,\n  along a specific takeoff angle and azimuth.\n- `rotate`: Rotate an MT.\n\n\n## Getting help\nFunctions are documented, so at the REPL type `?` to get a `help?>` prompt,\nand type the name of the function:\n\n```julia\nhelp?> MT\nsearch: MT mtime SymTridiagonal Meta Method match Matrix mktemp methods matchall\n\n  MT(rr, θθ, ϕϕ, rθ, rϕ, θϕ) -> ::MT\n  MT(M::Vector(6)) -> ::MT\n  MT(M::Array(3,3)) -> ::MT\n  MT(strike, dip, rake, M0) -> ::MT\n\n  Construct a new MT (moment tensor) in the native frame used by MomentTensors:\n\n    •    Radial (r) upwards\n      \n    •    Colatitude (θ or t) southwards\n      \n    •    Longitude (ϕ or p) eastwards)\n      \n\n  Several forms exist to construct a moment tensor:\n\n    •    Supply individual components as a list of arguments\n      \n    •    Supply a 6-vector\n      \n    •    Give a 3×3 matrix\n      \n    •    Specify a strike, dip and rake in degrees, and scalar moment (N.m)\n      \n\n  One may access the values of a moment tensor M in two ways:\n\n    1.   M[i,j] yields the elements of M.m as if they were a two-tensor\n      \n    2.   M[::Symbol] yields the elements by name; see getindex(::MT) for details\n```\n\n## Contributing\nIf you find a bug with MomentTensors or have suggestions for improvement,\nplease\n[open an issue](https://github.com/anowacki/MomentTensors.jl/issues/new/choose)\ngiving as much information as possible on how to reproduce the bug or problem.\n\n[Pull requests](https://github.com/anowacki/MomentTensors.jl/compare)\nto add new features are welcome and will be seriously\nconsidered.  Please note that the package aims to be lightweight and\nrely on few external dependencies.\n",
        "createdAt": "2016-10-19T10:59:45.000Z",
        "updatedAt": "2025-11-25T12:25:31.000Z",
        "language": "Julia",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/anowacki/MomentTensors.jl/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "INGV/geopsy-docker",
        "url": "https://github.com/INGV/geopsy-docker",
        "description": "Run the geopsy software with Docker!",
        "stars": 2,
        "forks": 0,
        "readme": "# geopsy-docker\n\n## Getting started\n\n### For bulding docker container:\n```sh\ndocker build -t geopsy .\n```\n\n### MacOSX *only*\nDownload and install **XQuartz**:\n- https://www.xquartz.org\n\nEnable flag: **Preferences** -> **Security** -> **Allow connections from network clients**.\n\n### Export display:\n```sh\nMYIP=\"`ifconfig | grep -w inet | egrep -v -w \"127.0.0.1\" | awk '{print $2}' | head -n 1`\"\nxhost +${MYIP} || exit\n```\n\n### Run the *container*:\n```sh\ndocker run --rm -it \\\n    -e DISPLAY=${MYIP}:0 \\\n    --mount type=bind,source=/tmp/.X11-unix,target=/tmp/.X11-unix \\\n    -v /tmp/your-data:/opt/data \\\n    geopsy:latest geopsy\n```\n\n## Contribute\nThanks to your contributions!\n\nHere is a list of users who already contributed to this repository: \\\n<a href=\"https://github.com/ingv/geopsy-docker/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=ingv/geopsy-docker\" />\n</a>\n",
        "createdAt": "2024-06-19T09:55:47.000Z",
        "updatedAt": "2025-05-12T12:34:22.000Z",
        "language": "Dockerfile",
        "homepage": "http://www.geopsy.org",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/INGV/geopsy-docker/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "rafaelfsilva/seismology-workflow",
        "url": "https://github.com/rafaelfsilva/seismology-workflow",
        "description": null,
        "stars": 0,
        "forks": 2,
        "readme": "# Seismology Workflow\n\nThis workflow performs seismogram deconvolutions to estimate earthquake source time functions (STFs) for the 2013 Craig, Alaska Earthquake. Signals in the subdirectory `input/EGF` are deconvolved from the corresponding signals in the subdirectory `input/MShock`.\n\n<img src=\"docs/images/seismology-workflow.png?raw=true\" width=\"600\">\n\n### Description\n\n__`sG1IterDecon`__: receives a pair of signals, one from `input/EGF` and another from `input/MShock`, and computes seismogram deconvolutions to estimate earthquake source time functions (STFs). The output file is in the SAC (Seismic Analysis Code) format.\n\n__`siftSTFByMisfit`__: receives all STFs generated from the `sG1IterDecon` jobs and sifts the data by misfit. Only the good fits are kept and compressed into a single `.tar.gz` file.\n\n## Generating the Workflow\nThe `generate_dax.sh` script creates a Pegasus DAX workflow using the signals found in the subdirectories `input/EGF` and `input/MShock`.\nThe number of `sG1IterDecon` jobs will depend on the number of EGF signals and their corresponding signals in MShock. The command should be executed as follows:\n\n```\n./generate_dax.sh seismology.dax\n```\nThis command will generate a `seismology.dax` file, which is the Pegasus workflow containing all jobs (with their corresponding executables) and their dependencies (data dependency in this case).\n\n## Running the Seismology Workflow\nTo run the workflow, execute the following command:\n```\n./plan_dax.sh seismology.dax\n```\nOnce the workflow execution is completed, the compressed output file with the good fits, will be available in `output/good-fit.tar.gz`.\n\n",
        "createdAt": "2016-07-06T14:07:53.000Z",
        "updatedAt": "2020-04-15T21:28:51.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/rafaelfsilva/seismology-workflow/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "anangsahroni/stmkgxhmgi_longcourse",
        "url": "https://github.com/anangsahroni/stmkgxhmgi_longcourse",
        "description": "Repositori untuk menyimpan material Long Course STMKGxHMGI tentang Geophysical Python for Seismic Data Analysis",
        "stars": 5,
        "forks": 7,
        "readme": "![header_image](./figures/longcourse_header_cropped_sm.png)\n# Long Course\n## *\"Geophysical Python for Seismic Data Analysis\"*\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5758361.svg)](https://doi.org/10.5281/zenodo.5758361)\n\n**Instruktur:**\nDr.rer.nat. Wiwit Suryanto, M.Si\n\n**Dipersiapkan oleh:**\nAnang Sahroni\n\n\n**Waktu:**\n\nSesi 1: 18 September 2021\n\nSesi 2: 25 September 2021\n\n**Tempat:**\nZoom Meeting\n\n**Agenda:**\nMemberikan wawasan kepada mahasiswa Geofisika dalam pengolahan data Geofisika: pemrosesan data seismik menggunakan python.\n\n## Luaran\n1. Peserta dapat melakukan instalasi Python\n2. Peserta dapat membuat dan menggunakan Jupyter Notebook\n3. Peserta dapat membaca, memfilter, dan mengeplot peta dan statistik gempa bumi menggunakan modul umum Python seperti `numpy`, `scipy`, dan `matplotlib`\n4. Peserta dapat menentukan parameter gempa menggunakan metode yang sederhana pada Python memanfaatkan modul seismologi seperti `obspy`\n\n## Peralatan untuk peserta\nLaptop ataupun *Personal Computer* (PC) yang terkoneksi dengan internet.  \nJika hendak menjalankan kode tanpa instalasi bisa melalui: [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/anangsahroni/stmkgxhmgi_longcourse/HEAD)\n\n## Data:\n1. [Katalog Gempa Bumi](https://github.com/anangsahroni/geoscope-geohazard-workshop/blob/main/data/demo_data_BMKG_Mamuju.csv) Badan Meteorologi Klimatologi dan Geofisika (BMKG)\n2. Titik-titik Stasiun untuk berbagai jaringan seismometer\n\n## Jadwal\n| **Topik** |\n|:-----------|\n| **PRESESI: 17 September 2021** |\n| *[Instalasi Python dalam Miniconda](https://nbviewer.jupyter.org/github/anangsahroni/hmgi_longcourse_python/blob/main/0_Instalasi_Miniconda_dan_Modul.ipynb?flush_cache=true)* atau [PDF](https://github.com/anangsahroni/hmgi_longcourse_python/blob/main/pdf/0_Instalasi_Miniconda_dan_Modul%20-%20Jupyter%20Notebook.pdf)|\n| 1. Instalasi Miniconda pada Windows, Linux, ataupun MacOS |\n| 2. Menjalankan Python Console melalui Anaconda Prompt |\n| 3. Menulis kode dalam editor (Integrated Development Environment/IDE) kode dan menjalankannya melalui Anaconda Prompt\n| 4. Pengenalan IDE dan beberapa contohnya\n| 5. Menginstall `pandas`, `numpy`, `matplotlib`, `scipy`, `Cartopy`, dan `notebook` menggunakan Anaconda Prompt pada *virtual environment* \n| 6. Menjalankan kode sederhana di Jupyter Notebook\n| 7. Memanggil fungsi bawaan python (`math`), mencoba, dan memanggil bantuan (`help`) untuk masing-masing fungsi\n| 8. Memberikan catatan dan gambar dalam bentuk `Markdown` di Jupyter Notebook\n| 9. Menyimpan notebook pada repositori Github dan menambahkan ke Binder\n| 10. Mengupdate notebook dan melakukan commit ke repositori\n| **EXERCISE:** Membuat panduan instalasi Miniconda pada Jupyter Notebook dan menambahkannya di repositori Github individu. |\n||\n| **SESI 1: 18 September 2021** |\n| *Introduction to geophysical programming using python: basic python for seismology [Materi 1](https://nbviewer.jupyter.org/github/anangsahroni/hmgi_longcourse_python/blob/main/1_Basic_Python_for_Seismology_1.ipynb) ([PDF](https://github.com/anangsahroni/hmgi_longcourse_python/blob/main/pdf/1_Basic_Python_for_Seismology_1%20-%20Jupyter%20Notebook.pdf)/[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1GSL2MVX8t2rygqAB7P6j11bmZ5licw4j?usp=sharing)) dan [Materi 2](https://nbviewer.jupyter.org/github/anangsahroni/hmgi_longcourse_python/blob/main/1_Basic_Python_for_Seismology_2.ipynb) ([PDF](https://github.com/anangsahroni/hmgi_longcourse_python/blob/main/pdf/1_Basic_Python_for_Seismology_2%20-%20Jupyter%20Notebook.pdf)/[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1rC16cn_VtpWrbum9WpGhltdfv6lurLBC?usp=sharing))* atau [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/anangsahroni/stmkgxhmgi_longcourse/HEAD)|\n| 1. Membaca data katalog menggunakan `pandas` |\n| 2. Membedakan jenis-jenis data antar kolom pada katalog (`String`, `Integer`, dan `Float`) |\n| 3. Mengambil salah satu kolom ke dalam bentuk `List` dan mempelajari metode-metode pada `List` (`indexing`, `slicing`, `append`, dan lain sebagainya) |\n| 4. Menggunakan `for` *loop* untuk mengkonversi format `String` menjadi `datetime` untuk waktu kejadian |\n| 5. Menggunakan `conditional` untuk memfilter katalog berdasarkan besar magnitudo atau waktu |\n| 6. Membuat fungsi untuk memfilter katalog berdasarkan kedalaman dan menyimpannya menjadi modul siap impor |\n| 7. Membuat plot magnitudo dengan jumlah kejadian dan waktu kejadian (dapat berupa G-R Plot atau plot sederhana) |\n| 8. Mengkombinasikan `List` latitude dan longitude untuk mengeplot episenter |\n| 9. Mengintegrasikan kolom magnitude untuk membedakan ukuran titik titik plot |\n| 10. Mengintegrasikan kolom kedalaman untuk membedakan warna titik plot |\n| 11. Menambahkan *basemap* pada plot Menggunakan `Cartopy` |\n| **EXERCISE:** Membaca file titik stasiun, memfilter berdasarkan network, dan mengeplotnya bersama dengan titik-titik gempa. |\n||\n| **SESI 2: 25 September 2021** |\n| *Source Mechanism and processing seismic data with python : Determine earthquake epicenter, hypocenter, and type of P Wave*|\n| Jika menggunakan komputer lokal silahkan install modul yang dibutuhkan pada sesi dua dengan cara: `conda install -c conda-forge xarray rasterio tqdm`|\n| 1. Menentukan episenter dengan metode lingkaran [Materi](https://nbviewer.jupyter.org/github/anangsahroni/stmkgxhmgi_longcourse/blob/main/2_Earthquake_Source_Simple_1.ipynb)|\n| 2. Menentukan hiposenter dengan metode Geiger dan probabilistik [Materi 1](https://nbviewer.jupyter.org/github/anangsahroni/stmkgxhmgi_longcourse/blob/main/2_Earthquake_Source_Simple_2_1.ipynb), [Materi 2](https://nbviewer.jupyter.org/github/anangsahroni/stmkgxhmgi_longcourse/blob/main/2_Earthquake_Source_Simple_2_2.ipynb)|\n| 3. Pengenalan pengolahan waveform dengan `obspy` [Materi](https://nbviewer.jupyter.org/github/anangsahroni/stmkgxhmgi_longcourse/blob/main/2_Earthquake_Source_Simple_3.ipynb)|\n||\n\n\n## Software untuk diinstall\n1. **Miniconda**. Instalasi Python akan dilakukan menggunakan Anaconda Distribution dalam bentuk *lite* yaitu Miniconda. Dengan Miniconda instalasi paket atau modul pendukung untuk Python akan lebih mudah dan tertata. [Unduh installer Miniconda](https://docs.conda.io/en/latest/miniconda.html), pilih untuk versi Python 3.8.\n2. Editor teks agar penulisan kode lebih mudah karena biasanya sudah disertai pewarnaan kode  (*syntax highlighting*) dan indentasi otomatis. Editor teks dapat menggunakan **Notepad++**, **SublimeText**, atau menggunakan IDE yang lebih kompleks seperti **PyCharm** dan **Visual Studio Code**.\n\nSoftware-software yang dibutuhkan tersebut **sudah harus diinstall sebelum proses pemberian materi dimulai** karena ukurannya cukup besar.\n\n## Akun Github\nPeserta workshop dianjurkan mendaftarkan akun GitHub melalui [Daftar Github](http://github.com)\n\n## Bacaan Tambahan:\nPeserta dapat belajar pada Lesson di [Software Carpentry](https://software-carpentry.org/lessons/) dengan materi yang mendalam dan metode yang sama yaitu learning by doing.\n\n## Referensi\nPanduan ini disusun terinspirasi dari materi pada [Software Carpentry](https://software-carpentry.org/lessons/), materi inversi hiposenter probabilistik Igel & Geßele di [Seismo Live](https://krischer.github.io/seismo_live_build/html/Seismic%20Inverse%20Problems/Earthquake%20Location/el_hypocenter_solution_wrapper.html),panduan workshop Leonardo Uieda pada [repositori](https://github.com/leouieda/python-hawaii-2017), Lisa Itauxe [Python for ES Student](https://github.com/ltauxe/Python-for-Earth-Science-Students), dan Suryanto et al (HAGI ANT Workshop 2021, *not publicly available*).\n\n",
        "createdAt": "2021-09-17T05:06:13.000Z",
        "updatedAt": "2025-10-15T15:02:35.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.5758361",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.5758361",
            "dataCite": "10.5281/zenodo.5758361",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/anangsahroni/stmkgxhmgi_longcourse/main/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.5758361",
            "title": "anangsahroni/stmkgxhmgi_longcourse: Course Material: SMTKGxHMGI Long Course - Geophysical Python for Seismic Data Analysis",
            "journal": "Zenodo",
            "dateReleased": "2021-12-04T00:00:00.000Z",
            "abstract": "Rilis ini merupakan kumpulan material Long Course STMKGxHMGI tentang Geophysical Python for Seismic Data Analysis",
            "citationsArray": []
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "konstak98/AmbientNoisePy",
        "url": "https://github.com/konstak98/AmbientNoisePy",
        "description": "Ambient-Noise Seismology Package for Analysis",
        "stars": 0,
        "forks": 0,
        "readme": "\n",
        "createdAt": "2023-10-11T10:07:57.000Z",
        "updatedAt": "2024-07-04T10:43:34.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/konstak98/AmbientNoisePy/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "dulcegonzalez0711/seismology",
        "url": "https://github.com/dulcegonzalez0711/seismology",
        "description": "Pasantía en sismología en INSIVUMEH",
        "stars": 0,
        "forks": 0,
        "readme": "# seismology\nPrácticas supervisadas primer semestre 2024. INSIVUMEH. \n\nEste repositorio contiene los scripts y datos utilizados durante la realización de mis prácticas profesionales en INSIVUMEH. \n\nArchivos incluidos: \n- reporte_info_all_2023.csv\n- reporte_info_all_2023_clasificados.csv\n- clasificacion_sismos.py\n\nAutoría y Contribuciones:\n- Autora: Dulce Abril González\n- Supervisor: Ing. Diego Castro\n",
        "createdAt": "2024-02-23T15:15:04.000Z",
        "updatedAt": "2025-07-03T00:12:20.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/dulcegonzalez0711/seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "seisman/seisman.info.posts",
        "url": "https://github.com/seisman/seisman.info.posts",
        "description": "SeisMan博客的内容部分，引擎部分见 https://github.com/seisman/blog.seisman.info",
        "stars": 63,
        "forks": 31,
        "readme": "---\ntitle: SeisMan博客\nauthor: '[seisman](https://seisman.info)'\n---\n\n# 地震学基础\n\n## 基础知识\n\n- [体力与等效体力](/post/2013-09-16-body-force-equivalents.md)\n- [地震学中的坐标系](/post/2017-01-12-coordinate-systems-in-seismology.md)\n- [地震学断层的定义](/post/2013-09-26-faults-in-seismology.md)\n- [标量矩与矩震级转换关系](/post/2013-07-02-scalar-moment-and-moment-magnitude.md)\n- [辐射花样的计算与震源球的绘制](/post/2014-05-01-radiation-pattern-and-beach-ball.md)\n- [慢度与射线参数](/post/2015-02-05-slowness-and-ray-parameter.md)\n- [震相走时的计算](/post/2015-05-02-traveltime-calculation.md)\n- [球状模型下的 Tau 积分](/post/2015-05-15-tau-integral-in-spherical-earth.md)\n\n## 仪器响应\n\n- [地震学的仪器响应](/post/2013-06-23-instrumental-response.md)\n- [仪器响应的物理细节](/post/2013-06-26-instrumental-response-physical-details.md)\n- [仪器响应文件 RESP](/post/2013-06-27-instrumental-response-resp.md)\n- [仪器响应文件 SAC PZ](/post/2013-06-28-instrumental-response-sacpz.md)\n- [用 RESP 和 PZ 去除仪器响应的差别](/post/2013-06-29-resp-sacpz-difference.md)\n- [仪器响应实例分析](/post/2013-06-30-instrumental-response-details.md)\n- [freqlimits 的选取](/post/2014-01-16-how-to-choose-freqlimits.md)\n\n# 地震学软件\n\n## TauP {#taup-toolkit}\n\n- [走时计算软件包](/post/2014-03-29-traveltime-packages.md)\n- [走时计算软件TauP](/post/2013-07-10-taup.md)\n- [TauP 的安装](/post/2014-10-08-taup-install.md)\n- [利用 TauP 计算射线穿透点](/post/2014-11-07-taup-calculate-pierce-points.md)\n- [利用 TauP 为 SAC 文件标记理论到时](/post/2014-11-10-taup-mark-traveltime.md)\n- [用 taup_time 计算震相走时及射线信息](/post/2015-01-24-taup-calculate-traveltime.md)\n- [TauP 自定义模型](/post/2016-05-21-taup-custom-models.md)\n- [识别 TauP 输出中的多个 P 震相](/post/2016-05-22-taup-identify-multiple-P-phases.md)\n- [计算任意深度处下表面反射波走时](/post/2017-03-31-taup-calculate-underside-reflection-traveltime.md)\n\n## SAC\n\n- [SAC 参考手册中文版](/post/2013-07-06-sac-manual.md)\n- [用 SAC 快速拾取震相](/post/2016-02-19-faster-ppk.md)\n- [SAC 文件读写模块 sacio_Fortran](/post/2016-07-19-sacio-fortran90.md)\n- [SAC 文件修改事件经纬度后震中距的自动计算](/post/2013-08-03-calculate-gcarc-after-adding-event-location.md)\n- [SAC 不同格式间的转换](/post/2013-08-04-conversion-of-different-sac-formats.md)\n- [脚本中调用 SAC 时不显示版本信息](/post/2013-12-26-sac-display-copyright.md)\n- [SAC 修改绘图窗口的背景色](/post/2014-06-15-change-background-color-of-window-in-sac.md)\n- [SEED 格式转 SAC 格式](/post/2015-06-19-convert-seed-to-sac.md)\n- [SAC 中将位移记录转换成速度记录](/post/2016-01-12-sac-convert-displacement-to-velocity.md)\n- [PPK 标记震相到时的一些注意事项](/post/2013-08-20-ppk-notes.md)\n- [判断 SAC 数据是否已去除仪器响应](/post/2016-03-13-instrumental-response-removed-or-not.md)\n\n## GMT\n\n- [GMT 4.5.17 在 Linux 下的安装](/post/2013-11-07-install-gmt4-under-linux.md)\n- [GMT 4 在 Mac 下的安装](/post/2015-09-05-install-gmt4-under-mac.md)\n- [GMT4 与 GMT5 双版本共存](/post/2013-11-09-multiple-versions-of-gmt.md)\n- [Windows 下使用 GMT 的正确姿势](/post/2014-12-10-how-to-use-gmt-under-windows.md)\n- [GMT4 脚本风格指南](/post/2014-05-13-gmt4-style-guide.md)\n- [在 GMT 中使用 LaTeX](/post/2013-10-24-gmt-latex.md)\n- [GMT 绘制地理坐标与笛卡尔坐标混合体](/post/2014-04-26-mix-geographical-coordinate-with-cartesian-coordinate.md)\n- [等震中距线的绘制](/post/2014-05-21-plot-equal-distance-lines.md)\n- [GMT 绘制双 Y 轴](/post/2014-06-12-double-y-axis-plot.md)\n- [不同比例尺曲线的画法](/post/2015-02-01-plot-multiple-lines-in-different-scales.md)\n- [GMT 中添加注释和标注](/post/2014-12-28-add-annotations-in-gmt.md)\n- [修改 Y 轴的坐标标注的方向](/post/2015-03-11-change-orientation-of-y-axis-annotations.md)\n- [GMT5 自定义坐标轴](/post/2015-06-06-gmt5-custom-axes.md)\n- [GMT 绘制无刻度轴](/post/2015-07-15-axes-without-ticks.md)\n- [GMT 区域填色](/post/2015-08-05-area-fill.md)\n- [GMT 绘制图中图](/post/2015-08-16-gmt-insert-map.md)\n- [GMT 添加断层名](/post/2015-08-21-plot-fault-names.md)\n- [绘制事件与台站间的连线](/post/2015-11-19-raypath-between-events-and-stations.md)\n- [在地图上绘制特定的经纬线](/post/2015-12-11-plot-specified-gridline-on-maps.md)\n- [绘制颜色渐变的线条](/post/2016-01-18-plot-lines-with-gradient-colors.md)\n- [在极坐标下绘制深度剖面](/post/2016-03-12-depth-profile-in-polar-coordinates.md)\n- [用海岸线裁剪区域](/post/2013-10-30-coastline-clip.md)\n- [计算某点离海岸线的距离](/post/2014-02-21-calculate-distance-to-coast.md)\n- [从 3D 数据中截取剖面](/post/2015-04-08-profile-from-3d-data.md)\n- [判断点在多边形内](/post/2015-10-10-points-inside-polygons.md)\n\n## pssac\n\n- [安装 pssac](/post/2013-08-04-pssac-install.md)\n- [pssac 使用教程](/post/2015-07-17-pssac-notes.md)\n- [在地图上绘制波形](/post/2015-07-18-plot-traces-on-maps.md)\n- [绘制波形对比图](/post/2013-09-15-waveform-comparison-plot.md)\n- [pssac2 的安装](/post/2013-08-09-pssac2-install.md)\n- [pssac2 使用教程](/post/2015-07-19-pssac2-notes.md)\n\n## fk\n\n- [fk3.2 编译](/post/2013-09-01-fk-install.md)\n- [fk 用法笔记](/post/2015-02-28-fk-notes.md)\n\n## 其他\n\n- [hk1.3 编译](/post/2013-09-08-hk-install.md)\n- [MoPaD:地震矩绘制和分析工具](/post/2013-08-27-mopad.md)\n- [由矩张量计算双力偶断层参数](/post/2013-07-16-moment-tensor-to-double-couple.md)\n- [gCAP 的安装](/post/2014-06-13-gcap-install.md)\n- [震中距、方位角和反方位角的计算](/post/2013-07-03-calculate-dist-az-baz.md)\n- [JPlotResp：绘制地震仪器响应](/post/2013-07-19-jplotresp.md)\n- [CCP1.0 编译](/post/2013-11-29-ccp-install.md)\n- [CPS330 : Computer Programs in Seismology](/post/2014-01-01-cps330.md)\n- [CPS330 的安装](/post/2015-05-12-cps330-install.md)\n- [安装 evt2sac](/post/2015-07-31-evt2sac-install.md)\n- [rdseed 的安装](/post/2014-10-07-rdseed-install.md)\n- [rdseed 用法笔记](/post/2015-10-13-rdseed-notes.md)\n- [SOFI2D 笔记](/post/2015-12-08-sofi2d-notes.md)\n- [在Google Earth上绘制震源球](/post/2018-03-03-beachballs-on-google-earth.md)\n\n# 地球物理相关资源\n\n## Hinet台网\n\n- [日本高密度地震台网 Hi-net](/post/2014-08-25-hinet.md)\n- [Hi-net 连续波形数据](/post/2014-08-27-hinet-continuous-waveform-data.md)\n- [Hi-net 连续数据申请的源码分析](/post/2014-08-29-hinet-continuous-waveform-data-source-code.md)\n- [Hi-net WIN32 格式](/post/2014-09-04-hinet-win32-format.md)\n- [Hi-net Channel Table 文件](/post/2014-09-05-hinet-channel-table.md)\n- [Hi-net 的仪器响应](/post/2014-09-06-hinet-instrumental-response.md)\n- [Hi-net win32tools](/post/2014-09-07-hinet-win32tools.md)\n- [HinetPy: Hi-net 数据申请与处理模块](/post/2017-04-01-hinetpy.md)\n\n## 地震波形数据\n\n- [地震数据申请](/post/2015-10-27-seismic-waveform-data-request.md)\n- [地震波形数据格式](/post/2014-01-10-seismic-data-formats.md)\n- [地震数据的命名规则](/post/2014-03-05-seismic-file-name-convections.md)\n- [IRIS 数据申请工具: BREQ_FAST](/post/2013-07-23-breq-fast.md)\n- [用 Wilber 3 申请地震波形数据](/post/2015-09-28-wilber3.md)\n- [SOD 入门教程](/post/2016-12-24-sod-notes.md)\n- [使用 Web Services 获取地震数据](/post/2016-08-14-web-service-clients.md)\n- [利用 Web Service Fetch scripts 申请和下载数据](/post/2013-08-04-web-service-fetch-scripts.md)\n- [IRIS 的理论地震图生成引擎](/post/2016-05-26-iris-syngine.md)\n- [IRIS FTP 数据下载的几个方法](/post/2014-01-24-download-data-from-iris-ftp.md)\n- [IRIS FTP 数据下载脚本](/post/2014-01-25-perl-script-for-downloading-iris-ftp-data.md)\n\n## 地球物理数据\n\n- [全球地形起伏数据总结](/post/2013-09-30-global-relief-models.md)\n- [全球地形起伏模型etopo5](/post/2013-08-10-etopo5.md)\n- [全球地形起伏模型 ETOPO1](/post/2013-08-11-etopo1.md)\n- [全球地形起伏模型 ETOPO2](/post/2013-08-11-etopo2.md)\n- [全球数字高程模型 GTOPO30](/post/2013-08-11-gtopo30.md)\n- [高精度地形网格数据 SRTM](/post/2013-09-29-srtm.md)\n- [全球地形起伏数据：SRTM30_PLUS](/post/2013-12-31-srtm30-plus.md)\n- [全球地形起伏数据：SRTM15_PLUS](/post/2015-04-07-srtm15-plus.md)\n- [全球数字高程数据：ASTER GDEM](/post/2014-01-12-aster-gdem.md)\n- [全球水深数据 GEBCO](/post/2014-02-05-gebco.md)\n- [全球地壳模型 crust 1.0](/post/2013-10-03-crust1.md)\n- [中国行政区划数据下载](/post/2013-11-23-china-administrative-areas-data.md)\n- [洋壳年龄数据](/post/2014-05-17-ocean-floor-crustal-age.md)\n- [板块边界数据集](/post/2014-05-18-plate-boundary-datasets.md)\n\n## 其他\n\n- [地震学入门简易指南](/post/2013-08-08-simple-guide-to-seismology.md)\n- [Global CMT 信息整理](/post/2013-07-01-global-cmt.md)\n- [全球地震目录 PDE](/post/2014-01-15-global-earthquake-catalog-pde.md)\n\n# 编程\n\n## Linux\n\n- [CentOS 7 配置指南](/post/2014-07-14-centos7-setup.md)\n- [CentOS 7 配置指南 — 安装篇](/post/2014-07-15-centos7-setup-1.md)\n- [CentOS 7 配置指南 — 开发环境篇](/post/2014-07-15-centos7-setup-2.md)\n- [CentOS 7 配置指南 — 脚本语言篇](/post/2014-07-15-centos7-setup-3.md)\n- [CentOS 7 配置指南 — 效率软件篇](/post/2014-07-15-centos7-setup-4.md)\n- [CentOS 7 配置指南 — 日常软件篇](/post/2014-07-15-centos7-setup-5.md)\n- [制作 Linux USB 安装镜像](/post/2018-04-07-linux-usb-installer.md)\n- [Linux 下安装 NVIDIA 显卡驱动](/post/2014-07-13-install-nvidia-drivers-under-linux.md)\n- [CentOS 7 下的软件安装方法及策略](/post/2014-11-23-how-to-install-softwares-under-centos-7.md)\n- [CentOS 7.0 下安装小小输入法](/post/2014-07-10-install-yong-chinese-input-method-under-centos-7.md)\n- [CentOS 7 安装 fcitx 中文输入法](/post/2014-09-20-fcitx-for-centos7.md)\n- [CentOS 7 下安装 WPS Office](/post/2014-10-01-wps-office-for-centos7.md)\n- [Intel 非商业开发工具](/post/2013-09-10-intel-non-commercial-software.md)\n- [彻底卸载 Intel Parallel Studio](/post/2014-07-16-uninstall-intel-parallel-studio-completely.md)\n- [Linux 下安装 TeXLive 2017](/post/2013-07-11-texlive-install.md)\n- [判断字节序的多种方法](/post/2013-11-13-linux-endian.md)\n- [使用 xeCJK 解决中文问题的最小模板](/post/2014-02-28-xeCJK-mini-template.md)\n- [跟我一起写 Makefile (PDF 重制版)](/post/2014-03-07-how-to-write-makefile.md)\n- [Linux 下合并 PDF](/post/2014-09-18-merge-pdf.md)\n- [Firefox 安装 Java 插件](/post/2013-07-11-firefox-java-plugin.md)\n\n## Python\n\n- [Python 发送邮件到 BREQ_FAST](/post/2014-08-23-python-send-mail.md)\n- [Python 多版本共存之 pyenv](/post/2013-10-04-pyenv.md)\n- [Python科学计算发行版--Anaconda](/post/2014-05-14-anaconda.md)\n- [USTC 网络通登录脚本 Python 版](/post/2014-06-16-python-ustc-wlt-login.md)\n\n## Perl\n\n- [Perl 发送邮件到 BREQ_FAST](/post/2013-07-26-perl-send-email.md)\n- [Perl 的单引号字符直接量](/post/2013-07-30-perl-single-quoted-string-literals.md)\n- [Perl 进度条模块](/post/2013-10-13-perl-progressbar.md)\n- [Perl 中的时间加法](/post/2013-10-16-perl-timespan.md)\n- [Perl 多版本共存之 plenv](/post/2013-11-03-plenv.md)\n- [Perl 如何找出两个数组的交集、并集](/post/2013-11-18-find-intersection-and-difference-of-two-arrays.md)\n\n## C\n\n- [闰年的判断](/post/2013-08-04-leap-year.md)\n- [计算某日是一年中的第几天](/post/2014-01-18-calculate-jday.md)\n\n## 优质软件\n\n- [强大的跨平台 PDF 处理工具：cpdf](/post/2015-01-27-cpdf.md)\n- [图像格式转换工具 convert](/post/2013-09-27-imagemagick-convert.md)\n- [PDF 合并和分割工具--PDFtk](/post/2013-10-31-pdftk.md)\n\n## macOS\n\n- [用 Mac 打造合适的科研环境](/post/2018-01-29-macOS.md)\n\n# 其他\n\n## 博客相关\n\n- [Hello World!](/post/2013-06-21-hello-world.md)\n- [博客托管的一些调整](/post/2014-07-17-blog-hosts.md)\n\n## 杂类\n\n- [一些说明](/post/2015-09-27-declarations.md)\n- [一些产品的推广链接](/post/2015-04-11-my-referral-links.md)\n- [我所使用的软件/服务列表](/post/2014-08-05-personal-preferences.md)\n- [征集令](/post/2016-04-14-you-are-wanted.md)\n- [地球物理相关软件](/post/2014-02-20-geo-software.md)\n- [地球物理学家列表](/post/2015-07-23-geophysicist.md)\n- [地学学术期刊](/post/2016-07-04-journals.md)\n- [文件管理与备份](/post/2016-08-24-file-organization.md)\n- [参考文献管理最佳实践](/post/2018-05-12-manage-references.md)\n",
        "createdAt": "2016-05-15T02:15:49.000Z",
        "updatedAt": "2025-11-25T03:38:58.000Z",
        "language": "C",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/seisman/seisman.info.posts/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "monisobs/zmap",
        "url": "https://github.com/monisobs/zmap",
        "description": "mapping seismological parameters",
        "stars": 0,
        "forks": 0,
        "readme": "# zmap\nmapping seismological parameters\n",
        "createdAt": "2018-06-08T10:14:37.000Z",
        "updatedAt": "2018-06-08T10:14:39.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/monisobs/zmap/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ds-modules/EPS130-21521-SP23",
        "url": "https://github.com/ds-modules/EPS130-21521-SP23",
        "description": "UC Berkeley EPS 130 (Strong Motion Seismology) Spring 2023",
        "stars": 0,
        "forks": 0,
        "readme": "# EPS-130-SP23\n\nEPS 130 - Strong Motion Seismology - Doug Dreger - Spring 2023\n\nOpen this repository  [![Datahub](https://img.shields.io/badge/Launch-UCB%20Datahub-blue.svg)](https://datahub.berkeley.edu/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fds-modules%2FEPS130-21521-SP23&urlpath=tree%2FEPS130-21521-SP23%2F)\n\nHomework 1 [![Datahub](https://img.shields.io/badge/Launch-UCB%20Datahub-blue.svg)](https://datahub.berkeley.edu/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fds-modules%2FEPS130-21521-SP23&branch=main&urlpath=tree%2FEPS130-21521-SP23%2FEPS130_Homework1%2Feps130_hw1_gutenberg_richter_v3.0.ipynb)\n\n",
        "createdAt": "2023-01-04T14:07:59.000Z",
        "updatedAt": "2025-07-07T21:22:27.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ds-modules/EPS130-21521-SP23/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Ilmosal/NorLyst",
        "url": "https://github.com/Ilmosal/NorLyst",
        "description": "Simple database managing tool for analysts in ISUH(Institute of Seismology, University of Helsinki).",
        "stars": 0,
        "forks": 0,
        "readme": "# NorLyst\nSimple database managing tool for analysts in ISUH(Institute of Seismology, University of Helsinki).\n",
        "createdAt": "2019-01-18T12:22:39.000Z",
        "updatedAt": "2019-09-23T09:24:20.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Ilmosal/NorLyst/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "lam1aa/pyseis",
        "url": "https://github.com/lam1aa/pyseis",
        "description": "PySeis is a Python package for environmental seismology, a scientific field that studies the seismic signals emitted by Earth surface processes.",
        "stars": 3,
        "forks": 0,
        "readme": "<h1 align=\"center\">Pyseis</h1>\n\n<p align=\"center\">\n  <a href=\"#dart-project-overview\">Overview</a> &#xa0; | &#xa0; \n  <a href=\"#sparkles-features\">Features</a> &#xa0; | &#xa0;\n  <a href=\"#rocket-tools\">Tools</a> &#xa0; | &#xa0;\n  <a href=\"#white_check_mark-installation\">Installation</a> &#xa0; | &#xa0;\n  <a href=\"#book-documentation\">Documentation</a> &#xa0; | &#xa0;\n  <a href=\"#memo-license\">License</a> &#xa0; | &#xa0;\n  <a href=\"#black_nib-citation\">Citation</a> &#xa0; | &#xa0;\n  <a href=\"#notebook_with_decorative_cover-contributing-to-pyseis\">Contributing</a> &#xa0; | &#xa0;\n</p>\n\n\n## :dart: Project Overview ##\n\nPySeis is a `Python` package for environmental seismology, a scientific field that studies the seismic signals emitted by Earth surface processes. This package provides a suite of tools to facilitate the reading, writing, preparation, analysis, and visualization of seismic data, drawing inspiration from the functionality of the [eseis](R/dietze2018_R_eseis.pdf) package in `R`. \n\n## :question: Why Pyseis ##\n\nWhile the eseis package in `R` offers tools for seismological data analysis, there was a gap in equivalent functionality within the `Python` ecosystem. PySeis aims to bridge this gap by providing `Python` users with a comparable suite of tools, ensuring they can perform environmental seismology tasks efficiently within a `Python` environment.\n\n## :sparkles: Features ##\n\n:heavy_check_mark: Fluvial data inversion\\\n:heavy_check_mark: Reference model creation\\\n:heavy_check_mark: Spatial distance calculation\\\n:heavy_check_mark: Spatial signals migration\\\n:heavy_check_mark: Spatial data clipping\\\n:heavy_check_mark: Coordinate conversion\\\n:heavy_check_mark: Source location detection\\\n:heavy_check_mark: Source tracking\\\n:heavy_check_mark: Spectrum modeling\n\n## :rocket: Tools ##\n\nThe following tools were used in this project:\n\n- [Python](https://www.python.org)\n- [NumPy](https://pypi.org/project/numpy/)\n- [Pandas](https://pypi.org/project/pandas/)\n- [Geopandas](https://pypi.org/project/geopandas/)\n- [Matplotlib](https://pypi.org/project/matplotlib/)\n- [Scipy](https://pypi.org/project/scipy/)\n- [Shapely](https://pypi.org/project/shapely/)\n- [Rasterio](https://pypi.org/project/rasterio/)\n\n\n\n\n## :white_check_mark: Installation ##\n\nBefore starting :checkered_flag:, you need to have [Git](https://git-scm.com) and [Python](https://www.python.org) installed.\n\n\n```bash\n# Clone this project\n$ git clone https://gitup.uni-potsdam.de/tautz1/pyseis.git\n\n# Access\n$ cd pyseis\n\n# Activate your virtual environment (optional)\n# Install the pyseis package\n$ pip install .\n\n# To see the data analysis workflow, install snakemake and run\n$ snakemake -s workflow/snakefile -j 1 --latency-wait 60\n\n```\n\n## :book: Documentation ##\n\nTo see Pyseis documentation open [this](docs/index.html) in your browser.\n\n## :memo: License ##\n\n[MIT](LICENSE)\n\n## :black_nib: Citation ##\n\nIf you use this software, please cite it using [this](CITATION.cff) file.\n\n## :notebook_with_decorative_cover: Contributing to Pyseis ##\n\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg)](CONDUCT.md)\n\nAll contributions, bug reports, bug fixes, documentation improvements, enhancements, and ideas are welcome.\n\nA detailed overview on how to contribute can be found in the [contributing guide](CONTRIBUTING.md). Feel free to create new issues and start working on it!\n\nAs contributors and maintainers to this project, you are expected to abide by Pyseis' code of conduct. \n\nMore information can be found at: [Code of Conduct](CONDUCT.md)\n\n&#xa0;\n\n<a href=\"#top\">Back to top</a>\n",
        "createdAt": "2024-12-04T18:22:22.000Z",
        "updatedAt": "2025-01-18T07:59:15.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/lam1aa/pyseis/main/README.md",
        "mainPaper": {
            "doi": "",
            "title": "Pyseis - contributing to environmental seismology data analysis in python",
            "dateReleased": "1970-01-01T00:00:00.000Z"
        },
        "repoDoi": "",
        "publications": [
            {
                "doi": "",
                "name": "Pyseis - contributing to environmental seismology data analysis in python",
                "source": "",
                "authorNames": [],
                "publicationDate": "1970-01-01T00:00:00.000Z"
            }
        ]
    },
    {
        "source": "GitHub",
        "name": "armeanuiulia/VolcanoSeismologyProjectIA",
        "url": "https://github.com/armeanuiulia/VolcanoSeismologyProjectIA",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# Volcano Seismology Project \n**Student:** Iulia-Ștefania Armeanu \n\n## Notebooks:\n\n  1. **`backazimuth.ipynb`**\n\n     - **Purpose:** Calculate back azimuth (BAZ) from horizontal rotation rate data.\n     - **Key Function:** `calculate_baz_horiz()`\n     - **Output:** Time vector, BAZ vector, and visualizations.\n\n  2. **`plot events in time.ipynb`**\n\n     - **Purpose:** Visualize seismic events over time.\n     - **Processes:** Reads event data, calculates P-S phase differences, and plots event occurrences.\n\n  3. **`waveform_visualisation_allday.ipynb`**\n\n     - **Purpose:** Visualize daily seismic waveforms.\n     - **Output:** Continuous waveform plots for an entire day.\n\n  4. **`waveform_visualisation_combined.ipynb`**\n\n     - **Purpose:** Combine and visualize multiple waveform datasets.\n     - **Output:** Overlaid waveform plots for comparison.\n\n  5. **`waveforms_visualisation.ipynb`**\n\n     - **Purpose:** Visualize and analyze seismic waveforms.\n     - **Output:** Detailed waveform plots with analysis.\n",
        "createdAt": "2024-07-11T10:45:53.000Z",
        "updatedAt": "2024-08-01T21:43:23.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/armeanuiulia/VolcanoSeismologyProjectIA/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "earthinversion/GMT_tutorial_for_beginners",
        "url": "https://github.com/earthinversion/GMT_tutorial_for_beginners",
        "description": "This package contains the scripts and commands to plot all types of high resolution figures using Generic Mapping Tools (GMT)",
        "stars": 15,
        "forks": 5,
        "readme": "# GMT Tutorial for Beginners\n## Requirements:\n* Pre-installed GMT-5, check by typing `gmt` in terminal.\n* Pre-installed netcdf-5, check by typing `ncdump`.\n* Pre-installed ghostview package, check by typing `gv`.\n* Downloaded `ETOPO1_Bed_g_gmt4.grd` in the __Data__ directory from the [NOAA website](https://www.ngdc.noaa.gov/mgg/global/relief/ETOPO1/data/bedrock/grid_registered/netcdf/).\n\n## Installing GMT\nIn Ubuntu: `sudo apt-get install gmt gmt-dcw gmt-gshhg`\n\nIn Mac: `brew install gmt`\n\nFor other operating systems, check [GMT website](http://gmt.soest.hawaii.edu/projects/gmt/wiki/Installing)\n\n## Description of the Package:\nThe package consists of three directories: Data, Scripts and Figures.\n\n* The __Data__ directory contains the data files required to run the scripts in the __Scripts__ directory.\n* The __Scripts__ directory consists of all the bash scripts numbered from 1-8. \n    - `1linearPlots.sh`: Contains commands for making basemap for linear projections including the log-log plot. It also explains how to add title, xlabel, ylabel, tick-marks, background-color to the plot.\n    - `2PlottingMaps.sh`: This explains how to plot the Mercator projection, Alber's projection, Orthographic projection, Eckert projection.\n    - `3PlottingLinesSymbols.sh`: This script explains the use of `psxy` command to plot the lines and symbols. It also contains the commands to plot the earthquake epicenter with colors representing depths and symbol size representing magnitude.\n    - `4PlottingTexts.sh`: This script explains how to type texts onto the plots. The user can even type mathematical equations.\n    - `5Plottingcontours.sh`: This bash script explains how to plot the contour lines using the command `grdcontour`. It also explains how to cut the large data set using the `grdcut` command and obtain the information about it using the `grdinfo`. It also explains how to do interpolation of data (__nearest neighbour__ and __spline__).\n    - `6Manipulating_Images.sh`: It contains the description of how to make the cpt files, and plot the colorbars using `psscale` command. It also explains plotting the relief data.\n    - `7multiD_maps.sh`: This script explains how to plot the multidimensional netcdf data in GMT. \n    - `8three-DPlots.sh`: It includes how to plot the data as 3D plots using two methods: mesh plot, color-coded surface. \n* The __Figures__ directory consists of all the example plots from 1-24.",
        "createdAt": "2017-09-13T10:29:59.000Z",
        "updatedAt": "2025-10-27T06:51:52.000Z",
        "language": "Shell",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/earthinversion/GMT_tutorial_for_beginners/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "michaelgrund/GMT-plotting",
        "url": "https://github.com/michaelgrund/GMT-plotting",
        "description": "Collection of GMT (Generic Mapping Tools) scripts, jupyter notebooks (using PyGMT) and files (including digitized map content, colormaps, grid files etc.)",
        "stars": 120,
        "forks": 28,
        "readme": "# GMT-plotting\n> [!NOTE]  \n> Updates for PyGMT based notebooks are carried out at irregular intervals. The GMT bash scripts are not maintained anymore.\n> \n> Did you find a bug or have suggestions for improvements? Simply open a new [issue](https://github.com/michaelgrund/GMT-plotting/issues) or [pull request](https://github.com/michaelgrund/GMT-plotting/pulls).\n\n\nCollection of **GMT** ([Generic Mapping Tools](https://www.generic-mapping-tools.org/)) scripts and files (including digitized map content, colormaps, grid files etc.). All scripts should run with GMT versions **>= 5.2.1** and **< 6.0.0** . Each directory in this repository represents a single stand-alone application (individual manuals in pdf format are included as well):\n\nSome directories include [Jupyter Notebooks](https://jupyter.org/) in which I use [**PyGMT**](https://www.pygmt.org), a Python interface for GMT ([**_Tian et al., 2025_**](https://zenodo.org/records/15628725)) to generate the individual maps and plots (see the :arrow_forward: symbol and the examples below).\n\n- [001_map_equidist_EQ](https://github.com/michaelgrund/GMT-plotting/tree/master/001_map_equidist_EQ): plotting global seismicity between 1960 and 2025 on equidistant map :arrow_forward: [Jupyter Notebook](https://github.com/michaelgrund/GMT-plotting/tree/master/001_map_equidist_EQ/pygmt_jupyter_notebook/pygmt_plot_equidist_EQs.ipynb)\n- [002_map_equidist_EQ_GCMT](https://github.com/michaelgrund/GMT-plotting/tree/master/002_map_equidist_EQ_GCMT): plotting focal mechanisms (\"beach balls\") of the global seismicity between 1964 and 2019 on equidistant map\n- [003_map_seafloor_ages](https://github.com/michaelgrund/GMT-plotting/tree/master/003_map_seafloor_ages): plotting the ages of oceanic lithosphere on a global map :arrow_forward: [Jupyter Notebook](https://github.com/michaelgrund/GMT-plotting/tree/master/003_map_seafloor_ages/pygmt_jupyter_notebook/pygmt_seafloor_ages.ipynb)\n- [004_map_splitting_database](https://github.com/michaelgrund/GMT-plotting/tree/master/004_map_splitting_database): plotting the content of the shear-wave splitting data base on a global map\n- [005_map_equidist_siberia](https://github.com/michaelgrund/GMT-plotting/tree/master/005_map_equidist_siberia): plotting an equidistant map centered on NW Siberia (and raypaths)\n- [006_map_SKS_SKKS_areas](https://github.com/michaelgrund/GMT-plotting/tree/master/006_map_SKS_SKKS_areas): visualizing _SKS_-_SKKS_ pierce point areas in the D\" layer :arrow_forward: [Jupyter Notebook](https://github.com/michaelgrund/GMT-plotting/blob/master/006_map_SKS_SKKS_areas/pygmt_jupyter_notebook/pygmt_SKS_SKKS_areas.ipynb) \n- [007_map_SKS_SKKS_pierce_points](https://github.com/michaelgrund/GMT-plotting/tree/master/007_map_SKS_SKKS_pierce_points): reproducing _SKS_-_SKKS_ pierce point figures of [**_Grund & Ritter (2019)_**](https://doi.org/10.1130/G45514.1), Geology\n- [008_map_scan_tectonic](https://github.com/michaelgrund/GMT-plotting/tree/master/008_map_scan_tectonic): visualizing geological and tectonic content of Fennoscandia :arrow_forward: [Jupyter Notebook](https://github.com/michaelgrund/GMT-plotting/tree/master/008_map_scan_tectonic/pygmt_jupyter_notebook/pygmt_map_tectonic_fenno.ipynb)\n- [009_paper_GR2020](https://github.com/michaelgrund/GMT-plotting/tree/master/009_paper_GR2020): :arrow_forward: Jupyter Notebooks to reproduce some figures presented in our paper [**_Grund & Ritter (2020)_**](https://doi.org/10.1093/gji/ggaa388), GJI\n- [010_paper_RFSG2022](https://github.com/michaelgrund/GMT-plotting/tree/master/010_paper_RFSG2022): :arrow_forward: Jupyter Notebook to reproduce figure 1 presented in our paper [**_Ritter, Fröhlich, Sanz Alonso & Grund (2022)_**](https://doi.org/10.1007/s10950-022-10112-w) Journal of Seismology\n\n---\nAlthough I now work outside academia as data scientist, I provide these PyGMT/GMT examples and notebooks in the hope that they will be useful for other students, scientists or interested hobbyist map creators. Some new content may come up in the near future. Stay tuned!\n\n<table>\n  <thead>\n    <tr>\n      <th colspan=\"4\">Click on the images to directly access the corresponding Jupyter Notebooks</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>\n        <a href=\"https://github.com/michaelgrund/GMT-plotting/blob/main/006_map_SKS_SKKS_areas/pygmt_jupyter_notebook/pygmt_SKS_SKKS_areas.ipynb\" target=\"_blank\">\n          <img src=\"https://github.com/michaelgrund/GMT-plotting/blob/main/006_map_SKS_SKKS_areas/pygmt_jupyter_notebook/PLOT_sks_skks_areas.png\" width=\"400\">\n        </a>\n      </td>\n      <td>\n        <a href=\"https://github.com/michaelgrund/GMT-plotting/blob/main/001_map_equidist_EQ/pygmt_jupyter_notebook/pygmt_plot_equidist_EQs.ipynb\" target=\"_blank\">\n          <img src=\"https://github.com/michaelgrund/GMT-plotting/blob/main/001_map_equidist_EQ/pygmt_jupyter_notebook/PLOT_EQglob_BFO.png\" width=\"300\">\n        </a>\n      </td>\n      <td>\n        <a href=\"https://github.com/michaelgrund/GMT-plotting/blob/main/008_map_scan_tectonic/pygmt_jupyter_notebook/pygmt_map_tectonic_fenno.ipynb\" target=\"_blank\">\n          <img src=\"https://github.com/michaelgrund/GMT-plotting/blob/main/008_map_scan_tectonic/pygmt_jupyter_notebook/PLOT_map_Fenno.png\" width=\"300\">\n        </a>\n      </td>\n      <td>\n        <a href=\"https://github.com/michaelgrund/GMT-plotting/blob/main/009_paper_GR2020/pygmt_jn_fig_s8/GR_2020_Fig_S8.ipynb\" target=\"_blank\">\n          <img src=\"https://raw.githubusercontent.com/michaelgrund/GMT-plotting/main/009_paper_GR2020/pygmt_jn_fig_s8/PLOT_figs8.png\" width=\"400\">\n        </a>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        <a href=\"https://github.com/michaelgrund/GMT-plotting/blob/main/003_map_seafloor_ages/pygmt_jupyter_notebook/pygmt_seafloor_ages.ipynb\" target=\"_blank\">\n          <img src=\"https://github.com/michaelgrund/GMT-plotting/blob/main/003_map_seafloor_ages/pygmt_jupyter_notebook/PLOT_sf_ages.png\" width=\"400\">\n        </a>\n      </td>\n      <td>\n        <a href=\"https://github.com/michaelgrund/GMT-plotting/blob/main/010_paper_RFSG2022/RFSG_2022_Fig_01.ipynb\" target=\"_blank\">\n          <img src=\"https://github.com/michaelgrund/GMT-plotting/blob/main/010_paper_RFSG2022/PLOT_fig01_map_URG.png\" width=\"350\">\n        </a>\n      </td>\n      <td>\n        <a href=\"https://github.com/michaelgrund/GMT-plotting/blob/main/009_paper_GR2020/pygmt_jn_fig_s18/GR_2020_Fig_S18.ipynb\" target=\"_blank\">\n          <img src=\"https://raw.githubusercontent.com/michaelgrund/GMT-plotting/main/009_paper_GR2020/pygmt_jn_fig_s18/PLOT_figS18.png\" width=\"300\">\n        </a>\n      </td>\n      <td>\n        <a href=\"https://github.com/michaelgrund/GMT-plotting/blob/main/009_paper_GR2020/pygmt_jn_fig_14/GR_2020_Fig_14.ipynb\" target=\"_blank\">\n          <img src=\"https://raw.githubusercontent.com/michaelgrund/GMT-plotting/main/009_paper_GR2020/pygmt_jn_fig_14/PLOT_fig14.png\" width=\"350\">\n        </a>\n      </td>\n    </tr>\n  </tbody>\n</table>\n\n---\n\nIf you make use of the content in this repository please acknowledge GMT (e.g. [**_Wessel et al., 2013_**](https://doi.org/10.1002/2013EO450001); [**_2019_**](https://doi.org/10.1029/2019GC008515)), PyGMT ([**_Tian et al., 2025_**](https://zenodo.org/records/15628725)), our published papers and/or my PhD thesis in whose framework several of the scripts and notebooks were developed:\n\n- **_Ritter, J.R.R., Fröhlich, Y., Sanz Alonso, Y. & Grund, M. (2022)_**, Short-scale laterally varying SK(K)S shear wave splitting at BFO, Germany – implications for the determination of anisotropic structures, *Journal of Seismology*, 26, 1137-1156, https://doi.org/10.1007/s10950-022-10112-w.\n- **_Grund, M. & Ritter, J.R.R. (2020)_**, Shear-wave splitting beneath Fennoscandia - Evidence for dipping structures and laterally varying multilayer anisotropy, *Geophysical Journal International*, 223, 1525–1547, https://doi.org/10.1093/gji/ggaa388.\n- **_Grund, M. & Ritter, J.R.R. (2019)_**, Widespread seismic anisotropy in Earth’s lowermost mantle beneath the Atlantic and Siberia, *Geology*, 47(2), 123–126, \nhttps://doi.org/10.1130/G45514.1.\n- **_Grund, M. (2019)_**, Exploring geodynamics at different depths with shear wave splitting, Dissertation, *Karlsruhe Institute of Technology (KIT)*, https://doi.org/10.5445/IR/1000091425. \n\n---\n\n### References\n\n- **_Tian, D., Uieda, L., Leong, W. J., Fröhlich, Y., Grund, M., Schlitzer, W., Jones, M., Toney, L., Yao, J., Jing-Hui, T., Magen, Y., Materna, K., Belem, A., Newton, T., Anant, A., Ziebarth, M., Quinn, J., & Wessel, P. (2025)_**, PyGMT: A Python interface for the Generic Mapping Tools, v0.16.0, *Zenodo*, https://zenodo.org/records/15628725.\n- **_Wessel, P., Luis, J. F., Uieda, L., Scharroo, R., Wobbe, F., Smith, W. H. F. & Tian, D. (2019)_**, The generic mapping tools version 6. *Geochemistry, Geophysics, Geosystems*, 20(11), 5556-5564, https://doi.org/10.1029/2019GC008515.\n- **_Wessel, P., Smith, W. H. F., Scharroo, R., Luis, J., & Wobbe, F. (2013)_**, Generic mapping tools: improved version released. *Eos, Transactions American Geophysical Union*, 94(45), 409-410, https://doi.org/10.1002/2013EO450001.\n",
        "createdAt": "2019-03-21T15:31:00.000Z",
        "updatedAt": "2025-11-12T08:53:03.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/michaelgrund/GMT-plotting/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "INGV/RESTORE",
        "url": "https://github.com/INGV/RESTORE",
        "description": "Missing earthquake data reconstruction in the space-time-magnitude domain",
        "stars": 7,
        "forks": 1,
        "readme": "\n![alt text](https://github.com/angystallone/Seismology_Stuff/blob/main/figures/RESTORE_logo.png?raw=true)\n\n\n<h2>About</h2>\n\n`RESTORE` is a Python tool tackling the short-term aftershock incompleteness issue (STAI).\nIt is based on a stochastic gap-filling procedure which reconstructs the missing events in the space-time-magnitude domain based on empirical earthquake properties. \nThe subsets of the catalog affected by the STAI issue are automatically detected.\n\n<h2>To run</h2>\n\nSet the input parameters in the `input_file.txt` file:\n\n- size: moving-window size (in number of events per window) --> 1000 by default (Mignan and Woessner, 2012).\n- step: moving-window step (in number of events per step) --> 250 by default (Mignan and Woessner, 2012).\n- st_dev_multiplier: multiplies the Mc standard deviation (Mc Sigma), controls the confidence level for STAI gaps identification; STAI gaps are windows where mc >= mc_ok + n*sigma, where n = st_dev_multiplier; increasing the value of st_dev_multiplier results in a more conservative approach in detecting temporary deviations of Mc.\n- Sigma: smoothing distance of the Gaussian kernel, controls the spread of the smoothing; smaller values will result in sharper and more localized smoothing.\n- sbin: bin in the latitude and longitude direction (degrees), controls the grid resolution\n- fault_length_multiplier: multiplies the fault length rupture, controls the areal extent of the subcatalog where inferences about Mc trend with time are made; smaller values will result in higher resolution of STAI gaps detection, as local seismicity is less diluted.\n- t_end_quiet: ending time of the seismically quiescent period.\n- b: b-value of the Gutenberg-Richter law (alternatively, it can be estimated with the function provided in RESTORE).\n- alpha: significance level for the Lilliefors test.\n- mc: reference value for the magnitude of completeness, if set by the user (by default, it is estimated with the function provided in RESTORE).\n- depth_distribution: [`scipy.optimize.curve_fit`] distribution to fit to hypocenter depths, available options are: normal, lognormal, beta, bimodal.\n- p0: [`scipy.optimize.curve_fit`] initial guess for the parameters of the hypocenter depth distribution: 'mu', 'sigma' (normal), 'mu', 'sigma' (lognormal), 'a', 'b' (beta), 'mu1', 'sigma1', 'A1', 'mu2', 'sigma2', 'A2' (bimodal).\n\n\nRun `RESTORE` using the following command:\n\n```bash\npython Run_RESTORE.py\n```\n\n<h2>Synthetic_Test [v 2.0.0 only]</h2>\n\n`Run_Synthetic_Test.py` --> runs the synthetic test (uses `ETAS_incomplete.txt` as input dataset)\n\n`ETAS_complete.txt` --> synthetic dataset (before STAI modeling)\n\n`ETAS_incomplete.txt` --> the synthetic dataset (after STAI modeling)\n\nCompare the replenished catalog with `ETAS_complete.txt`, to check how the missing events are reconstructed by RESTORE\n\n<h2>Required external modules</h2>\n\n`mc_lilliefors` (download it <a href=\"https://gitlab.com/marcus.herrmann/mc-lilliefors\">here</a>)\n\n<h2>How to cite</h2>\n\nIf you use `RESTORE` in your research, please cite using the following citation:\n\n```bash\n@software{Stallone_RESTORE,\nauthor = {Stallone, Angela and Falcone, Giuseppe},\ntitle = {{RESTORE}},\nurl = {https://github.com/INGV/RESTORE}\n}\n```\n\n**ZENODO**\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10809571.svg)](https://doi.org/10.5281/zenodo.10809571)\n\n**Linked article**\n\nStallone, A., & Falcone, G. (2021). *Missing earthquake data reconstruction in the space‐time‐magnitude domain*. Earth and Space Science, 8(8), e2020EA001481.\n<a href=\"https://doi.org/10.1029/2020EA001481\">https://doi.org/10.1029/2020EA001481</a>\n\nFor any comment, question or suggestion write to:\n<angela.stallone@ingv.it>\n\n\n<h2>Acknowledgements</h2>\n\nThis project has been founded by the Seismic Hazard Center\n(Centro di Pericolosità Sismica, CPS, at the Istituto Nazionale di Geofisica e Vulcanologia, INGV)\n\n\n\n\n",
        "createdAt": "2020-07-20T08:59:34.000Z",
        "updatedAt": "2025-03-28T14:42:45.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.10809571",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.10809571",
            "dataCite": "10.5281/zenodo.10809571",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/INGV/RESTORE/master/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.10809571",
            "title": "INGV/RESTORE: RESTORE 2.1.0",
            "journal": "Zenodo",
            "dateReleased": "2024-03-12T00:00:00.000Z",
            "abstract": "Simulates hypocenter depth of missing earthquakes.\n\n\n\nLoads users' parameters directly from an input file.\n\n\n\nAutomatically filters events in the catalog falling within a meaningful region surrounding the large shock (filtered data saved in 'Filtered_Catalog.txt'). This is essential for a proper estimation of Mc fluctuation over time and, consequently, for a proper detection of STAI gaps. Differently from the older version, where users needed to provide a catalog pertaining to the earthquake's region, RESTORE 2.1.0 autonomously extracts the area of interest from an indefinitely large catalog, using a multiple of the fault rupture length. The user can customize the multiplier, which should be calibrated on the event magnitude. Opting for smaller areas will enhance the resolution of STAI gaps detection, as it minimizes the dilution of local seismicity.\n\n\n\nGives full control to the user on the Gaussian kernel parameters, i.e. Sigma (smoothing distance) and sbin (grid resolution).\n\n\n\nAdded several useful prints.",
            "citationsArray": []
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "krischer/jane",
        "url": "https://github.com/krischer/jane",
        "description": "Jane - Document Database for Seismology",
        "stars": 27,
        "forks": 10,
        "readme": "# Jane - *Document Database for Seismology*\n\n[![Build Status](https://travis-ci.org/krischer/jane.svg?branch=master)](https://travis-ci.org/krischer/jane) [![codecov](https://codecov.io/gh/krischer/jane/branch/master/graph/badge.svg)](https://codecov.io/gh/krischer/jane) [![license](https://img.shields.io/badge/license-GPLv3-F57F17.svg)](http://www.gnu.org/licenses/gpl.html) [![python-version](https://img.shields.io/badge/python-3.4,3.5-blue.svg)](http://python.org)\n[![django-version](https://img.shields.io/badge/django-1.9-850A8B.svg)](https://www.djangoproject.com/)\n\n**Documentation:** http://krischer.github.io/jane\n",
        "createdAt": "2014-06-06T14:04:59.000Z",
        "updatedAt": "2024-06-29T15:34:44.000Z",
        "language": "Python",
        "homepage": "http://krischer.github.io/jane",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/krischer/jane/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "zml72062/seismology",
        "url": "https://github.com/zml72062/seismology",
        "description": null,
        "stars": 0,
        "forks": 1,
        "readme": "# 地震概论 2022 秋周五班笔记\n",
        "createdAt": "2022-12-16T08:33:57.000Z",
        "updatedAt": "2022-12-16T08:33:57.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/zml72062/seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "jamiemccann/BSM2022",
        "url": "https://github.com/jamiemccann/BSM2022",
        "description": "Scripts for poster plots for British Seismological Meeting 2022",
        "stars": 0,
        "forks": 0,
        "readme": "# BSM2022\nScripts for poster plots for British Seismological Meeting 2022\n",
        "createdAt": "2022-07-26T14:28:37.000Z",
        "updatedAt": "2022-07-26T14:31:25.000Z",
        "language": "Shell",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/jamiemccann/BSM2022/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "bclswl0827/slgo",
        "url": "https://github.com/bclswl0827/slgo",
        "description": "SeedLink server protocol basic implementation in pure Go.",
        "stars": 7,
        "forks": 3,
        "readme": "# slgo\n\nSeedLink protocol basic implementation in pure Go.\n\n## Preview\n\n![Swarm Screenshot](https://raw.githubusercontent.com/bclswl0827/slgo/master/preview/swarm.png)\n\n## Quick Start\n\nSee [example](https://github.com/bclswl0827/slgo/tree/master/example) for more details.\n",
        "createdAt": "2024-08-24T16:28:01.000Z",
        "updatedAt": "2025-11-04T17:22:49.000Z",
        "language": "Go",
        "homepage": "https://pkg.go.dev/github.com/bclswl0827/slgo",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/bclswl0827/slgo/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ldelaroque/WaveEquationSolver1D-SEM",
        "url": "https://github.com/ldelaroque/WaveEquationSolver1D-SEM",
        "description": "Small exercise of computing the 1D elastic wave equation with the spectral element method",
        "stars": 1,
        "forks": 0,
        "readme": "# Solving the elastic wave equation by the spectral elements method\n\nComputing of the 1D Wave Equation with the absorbing boundary conditions. \n\nStart the computing by lauching:\n```\npython main.py\n```\nNot operational yet!\n",
        "createdAt": "2023-08-24T19:04:42.000Z",
        "updatedAt": "2025-03-12T13:33:02.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ldelaroque/WaveEquationSolver1D-SEM/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Seisgirl/Basic-Knowledge-of-Seismology",
        "url": "https://github.com/Seisgirl/Basic-Knowledge-of-Seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# Basic-Knowledge-of-Seismology\nI will introduce some basic concepts of Seismology.\nThey will be useful for beginners.\n",
        "createdAt": "2017-10-05T03:27:25.000Z",
        "updatedAt": "2018-01-22T12:18:06.000Z",
        "language": null,
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Seisgirl/Basic-Knowledge-of-Seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "DAS-RCN/awesome-das",
        "url": "https://github.com/DAS-RCN/awesome-das",
        "description": "A curated list of DAS tools and resources.",
        "stars": 51,
        "forks": 7,
        "readme": "# Awesome DAS [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)\n\n> A curated list of awesome resources for distributed acoustic sensing (DAS).\n\n## Contents\n\n<!-- toc -->\n\n* [Data Management](#data-management)\n* [Data repositories](#data-repositories)\n* [Processing](#processing)\n* [Visualisation](#visualisation)\n* [Modelling](#modelling)\n\n<!-- tocstop -->\n\n## Data Management\n\n* [das-convert](https://git.pyrocko.org/pyrocko/das-convert) - Convert and downsample DAS data sets efficiently to established seismological data formats.\n* [dastools](https://git.gfz-potsdam.de/javier/dastools) - tools to read, manipulate and convert seismic waveforms generated by DAS systems.\n\n## Processing\n\n* [distpy](https://github.com/Schlumberger/distpy) - An Open Source python module for rapid prototyping Distributed Acoustic Sensing (DAS) processing flows\n* [lightguide](https://github.com/pyrocko/lightguide) - Tools and modelling for distributed acoustic sensing data. Advanced de-noising filtering techniques. Integrates into Pyrocko.\n* [mldas](https://github.com/DAS-RCN/mldas) - Machine Learning for distributed acoustic sensing.\n* [jDAS](https://github.com/martijnende/jDAS) - Coherence-based Deep Learning denoising of DAS data.\n\n## Visualisation\n\n* [pyrocko](https://pyrocko.org) - Pyrocko's snuffler can handle large DAS data sets efficiently and visualize waterfall plots interactively.\n\n## Modelling\n\n* [pyrocko.gf](https://pyrocko.org) - Pyrocko-GF can forward model strain and strain-rate along the fiber's trajectory.\n\n## Data repositories\n\nList of public DAS data repositories:\n\n* PoroTomo Experiment at Brady Hot Springs ([GDR OpenEI](https://gdr.openei.org/submissions/849))\n* FORGE Phase 2C ([GDR OpenEI](https://gdr.openei.org/submissions/1185))\n* Garner Valley ([GDR OpenEI](https://gdr.openei.org/submissions/614))\n* Belgium DAS ([Caltech](https://data.caltech.edu/records/1296))\n* Monterrey Bay Dark Fiber ([GitHub](https://github.com/njlindsey/Photonic-seismology-in-Monterey-Bay-Dark-fiber1DAS-illuminates-offshore-faults-and-coastal-ocean))\n* RCA Shore Station outward along the Cascadia Margin ([Announcement/FTP Link](https://oceanobservatories.org/2022/02/distributed-acoustic-sensing-lays-groundwork-for-earthquake-tsunami-warnings-and-more/))\n* PubDAS a PUBlic Distributed Acoustic Sensing datasets repository for geosciences ([Globus] (https://app.globus.org/file-manager?origin_id=706e304c-5def-11ec-9b5c-f9dfb1abb183&origin_path=%2F)) \n\nSelected earthquake data:\n\n* SAFOD DAS array ([GitHub](https://github.com/ariellellouch/DASDetection))\n* Stanford Phase 1 experiment ([GitHub](https://github.com/eileenrmartin/FiberOpticEarthquakes))\n* Fairbanks Farmers Loop ([GitHub](https://github.com/eileenrmartin/FiberOpticEarthquakes))\n\n![CC0](https://licensebuttons.net/p/zero/1.0/88x31.png \"CC0 1.0 Universal (CC0 1.0)\")\n",
        "createdAt": "2021-08-27T13:23:08.000Z",
        "updatedAt": "2025-12-03T15:41:33.000Z",
        "language": "Ruby",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/DAS-RCN/awesome-das/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "tsonpham/ObsSeisHUS2025",
        "url": "https://github.com/tsonpham/ObsSeisHUS2025",
        "description": "Workshop \"Introduction to Observational Seismology\" hosted at Hanoi University of Science from 21 to 25 April 2025.",
        "stars": 3,
        "forks": 2,
        "readme": "# Introduction to Observational Seismology Workshop\n\n<!-- WARNING: This website is under active constructions. Its content might be changed without notice! -->\n\nLecturer:\n- Dr. Phạm Thành Sơn (ANU)\n\nOrganizers:\n- A/Prof. Phó Đức Tài (HUS)\n- A/Prof. Lê Hồng Phương (HUS)\n- Dr. Nguyễn Thị Minh Huyền (HUS)\n- A/Prof. Trần Thanh Tuấn (HUS)\n\nVenue: VNU Hanoi University of Science, 334 Nguyễn Trãi, Thanh Xuân, Hà Nội.\n\nTime: April 21-25, 2025\n\n### Workshop Description\nThis intensive one-week workshop, Introduction to Observational Seismology, is designed to equip learners with a basic understanding of concepts in observational seismology and hands-on experience with modern research tools for seismological data processing. It serves as an engaging introduction for beginners, but provides resources and materials to facilitate advanced study. \n\n### Aims\n- Promote observational seismology as a computational science of passive seismic data.\n- Introduce basic seismological concepts and modern practices for retrieving waveforms and metadata, as well as performing research-standard analysis. \n- Hands-on experience with seismological research tools: Python, Jupyter Notebook, Google Colab, ObsPy, SeisBench. \n\n### Key contents and expected learning outcomes:\n- Introduction to geographical mapping skills + visualize scientific data, including skills to (1) draw a geophysical map for a region of interest, (2) plot scientific data on a map.\n- Introduction to digital seismic data, including (1) principles of seismometry, (2) seismic data as digital signals, (3) global seismic databases, and (4) basic data processing tools.\n- Introduction to inverse problem theory with demonstration for (1) polynomial parameter estimation and (2) earthquake location determination.\n- Introduction to cross-correlation techniques (1) theory and example of autocorrelation for shallow Earth imaging, and (2) single-event global correlogram.\n- A brief introduction to machine learning in seismology, including (2) the PKIKP onset picker and (2) automatic earthquake detection with the Seisbench framework.\n\n### About the faciliator\n[Dr. Phạm](https://www.tsonpham.net/) is an observational seismologist, who uses seismic waves to understand the Earth’s interior structures and seismic energy sources using mathematical tools, such as signal processing, numerical modeling, and geophysical inference. He is particularly interested in structures and processes a few kilometers beneath the surface, such as polar ice sheets, down to the Earth’s deepest shell, including its cores. To date, one of his visible contributions is to help understand better the architecture of the seismic wavefield several hours after large earthquakes and use it to decipher several long-lasting puzzles regarding the Earth’s inner core. In current and near-future research, he aims to expand my seismological toolbox to advance research on the topics, focusing on understanding the structures and dynamics of the polar ice sheets in Antarctica and Greenland in the changing climate. \n\n## Recommended pre-class reading list\nHere I compile a list of some reading materials about some useful tools in observational seismology:\n- What's inside the Earth: Interactive poster [link](https://www.earthscope.org/inside-the-earth-poster/)\n- Coding environment: Jupyter Notebook [link to tutorial](https://colab.research.google.com/notebooks/intro.ipynb#scrollTo=GJBs_flRovLc)\n- Free cloud server: Google Colab [link to tutorial](https://colab.research.google.com/notebooks/intro.ipynb#scrollTo=5fCEDCU_qrC0)\n- Basic mapping tool: Basemap [link to tutorial](https://matplotlib.org/basemap/stable/users/geography.html)\n- Theoretical travel time and ray paths: Obspy Taup [link to tutorial](https://docs.obspy.org/packages/obspy.taup.html)\n- Access to seismic data servers: Obspy FDSN Client [link to tutorial](https://docs.obspy.org/packages/obspy.clients.fdsn.html#module-obspy.clients.fdsn)\n- Efficient Bayesian sampler: emcee [link to tutorial](https://emcee.readthedocs.io/en/stable/tutorials/line/)\n- Google machine learning crash course [link to tutorial](https://developers.google.com/machine-learning/crash-course/linear-regression)\n- Machine Learning Cơ Bản by Vũ Hữu Tiệp [link to ebook](https://github.com/tiepvupsu/ebookMLCB)\n- Seisbench: A toolbox for machine learning in seismology [link](https://seisbench.readthedocs.io/en/stable/)\n\nMore up-to-date reading list can be found in this [Google Docs](https://docs.google.com/document/d/1o0qxpMSplIhqRjPVi1Lvl_tMI04KH6avsbPa5C-PZC4/edit?usp=drive_link).\n\n## In-class activities\n\n*Module 1: Introduction, geographical mapping*\n<!-- * [Notes](Day1/notes.md) -->\n* Lecture slides [PDF](https://drive.google.com/file/d/1KUkcooDYMBD843-AA1lpRnQ8i88LBAO_/view?usp=sharing) [PPTX](https://anu365-my.sharepoint.com/:p:/r/personal/u5883665_anu_edu_au/Documents/HUS2025/Module1.pptx?d=w397b024a68eb4e7f817f38c052061bf5&csf=1&web=1&e=Io1eim)\n* In-class exercise: Plotting Maps: Seismograph and Seismicity in Vietnam [![Open In Colab](https://img.shields.io/badge/open%20in-Colab-b5e2fa?logo=googlecolab&style=flat-square&color=ffd670)](https://colab.research.google.com/github/tsonpham/ObsSeisHUS2025/blob/master/Day1/D1_Lab.ipynb)\n* Self-practice exercise: Exploring seismic stations in Antarctica [![Open In Colab](https://img.shields.io/badge/open%20in-Colab-b5e2fa?logo=googlecolab&style=flat-square&color=ffd670)](https://colab.research.google.com/github/tsonpham/ObsSeisHUS2025/blob/master/Day1/D1_Prac.ipynb)\n\n*Module 2: Ray theory, seismometry, seismic databases*\n<!-- * [Overview](Day2/notes.md) -->\n* Lecture slides [PPTX (download to view)](https://docs.google.com/presentation/d/1a3Esj2lMUnZUAi9Tt2DM-P9fcKZ78ww8/edit?usp=drive_link&ouid=115613731196113719130&rtpof=true&sd=true)\n* In-class exercise: Ray theoretical travel times and paths\n [![Open In Colab](https://img.shields.io/badge/open%20in-Colab-b5e2fa?logo=googlecolab&style=flat-square&color=ffd670)](https://colab.research.google.com/github/tsonpham/ObsSeisHUS2025/blob/master/Day2/D2_Lab.ipynb)\n* Self-practice exercise: Triangulation of M5.2 Kon Tum 28/07/2024 earthquake [![Open In Colab](https://img.shields.io/badge/open%20in-Colab-b5e2fa?logo=googlecolab&style=flat-square&color=ffd670)](https://colab.research.google.com/github/tsonpham/ObsSeisHUS2025/blob/master/Day2/D2_Prac.ipynb)\n\n*Module 3: Geophysical inverse problem*\n<!-- * [Notes](Day3/notes.md) -->\n* Lecture slides [PPTX (download to view)](https://docs.google.com/presentation/d/1aSIX8qNMxjs41evKr-xIcPzySY_q-wlB/edit?usp=drive_link&ouid=115613731196113719130&rtpof=true&sd=true)\n* In-class exercise: Linear regression [![Open In Colab](https://img.shields.io/badge/open%20in-Colab-b5e2fa?logo=googlecolab&style=flat-square&color=ffd670)](https://colab.research.google.com/github/tsonpham/ObsSeisHUS2025/blob/master/Day3/D3_Lab.ipynb)\n* Self-practice exercise: Earthquake location as an inverse problem [![Open In Colab](https://img.shields.io/badge/open%20in-Colab-b5e2fa?logo=googlecolab&style=flat-square&color=ffd670)](https://colab.research.google.com/github/tsonpham/ObsSeisHUS2025/blob/master/Day3/D3_Prac.ipynb)\n* Advanced exercise: Seismic moment tensor inversion (by Julien Thurin) [![Open In Colab](https://img.shields.io/badge/open%20in-Colab-b5e2fa?logo=googlecolab&style=flat-square&color=ffd670)](https://colab.research.google.com/drive/1UJWOompBz9MlJN0B6SoVKzF8Whz_1nPp?usp=sharing#scrollTo=n8Gxw3DPkxAb)\n\n*Module 4: Seismic interferometry and study of the Earth’s structures*\n<!-- * [Notes](Day4/notes.md) -->\n* Lecture slides [PPTX (download to view)](https://docs.google.com/presentation/d/1bWzEXppb5zZ2RlFcG0IK9pZA0-Gv71w2/edit?usp=drive_link&ouid=115613731196113719130&rtpof=true&sd=true)\n* In-class exercise: Teleseismic *P*-wave coda autocorrelation [![Open In Colab](https://img.shields.io/badge/open%20in-Colab-b5e2fa?logo=googlecolab&style=flat-square&color=ffd670)](https://colab.research.google.com/github/tsonpham/ObsSeisHUS2025/blob/master/Day4/D4_Lab.ipynb)\n* Self-practice exercise: Single-event global correlogram [![Open In Colab](https://img.shields.io/badge/open%20in-Colab-b5e2fa?logo=googlecolab&style=flat-square&color=ffd670)](https://colab.research.google.com/github/tsonpham/ObsSeisHUS2025/blob/master/Day4/D4_Prac.ipynb)\n\n*Module 5: Machine learning in Seismology*\n<!-- * [Notes](Day5/notes.md) -->\n* Lecture slides [PPTX (download to view)](https://docs.google.com/presentation/d/1du1_Z1jidPnA3t1n1VAMcqPfHtNOEKRf/edit?usp=sharing&ouid=115613731196113719130&rtpof=true&sd=true)\n* In-class exercise: Convolutional neural network for PKIKP onset phase picker [![Open In Colab](https://img.shields.io/badge/open%20in-Colab-b5e2fa?logo=googlecolab&style=flat-square&color=ffd670)](https://colab.research.google.com/github/tsonpham/ObsSeisHUS2025/blob/master/Day5/D5_Lab.ipynb)\n* Self-practice exercise: Introduction to [seisbench](https://seisbench.readthedocs.io/en/stable/index.html): A toolbox for machine learning in seismology [![Open In Colab](https://img.shields.io/badge/open%20in-Colab-b5e2fa?logo=googlecolab&style=flat-square&color=ffd670)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/01b_model_api.ipynb)\n* Bonus project: Automatic picking of micro icequakes [![Open In Colab](https://img.shields.io/badge/open%20in-Colab-b5e2fa?logo=googlecolab&style=flat-square&color=ffd670)](https://colab.research.google.com/github/tsonpham/ObsSeisHUS2025/blob/master/Day5/D5_Project.ipynb)",
        "createdAt": "2025-03-16T22:58:14.000Z",
        "updatedAt": "2025-10-12T03:47:48.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/tsonpham/ObsSeisHUS2025/master/readme.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "crotwell/hobcawbelle",
        "url": "https://github.com/crotwell/hobcawbelle",
        "description": "web site for BELLE at Hobcaw Barony",
        "stars": 0,
        "forks": 0,
        "readme": "# hobcawbelle\n\nWeb site for seismometer installed as BELLE at Hobcaw Barony, Georgetown, SC.\n\nhttps://eeyore.seis.sc.edu/scsn/hobcawbelle/\n",
        "createdAt": "2025-02-13T17:13:12.000Z",
        "updatedAt": "2025-11-27T14:30:21.000Z",
        "language": "TypeScript",
        "homepage": "https://eeyore.seis.sc.edu/scsn/hobcawbelle/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/crotwell/hobcawbelle/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "lermert/noisi",
        "url": "https://github.com/lermert/noisi",
        "description": "A tool for modeling and source inversion of auto- and cross-correlations of ambient seismic \"noise\".",
        "stars": 74,
        "forks": 24,
        "readme": "## Noisi - ambient noise cross-correlation modeling and inversion\n\nThis tool can be used to simulate noise cross-correlations and sensitivity kernels to noise sources.\n\n### Installation\n\nInstall requirements (easiest done with anaconda)\n- [obspy](https://docs.obspy.org/)\n- PyYaml\n- pandas\n- mpi4py\n- geographiclib\n- cartopy\n- h5py\n- jupyter\n- pytest\n\nAdditionally, install [instaseis](http://instaseis.net/), if you plan to use it for Green's functions.\nInstall jupyter notebook if you intend to run the tutorial (see below).\n\nIf you encounter problems with mpi4py, try removing it and reinstalling it using pip (`pip install mpi4py`).\n\n#### Install pre-packaged\nAfter installing the dependencies, run \n`pip install noisi`\n\n#### Install editable for further development\nClone the repository with git:\n`git clone https://github.com/lermert/noisi.git`\n\nChange into the `noisi_v1/` directory. Call `pip install -v -e .` here.\n\nAfter installation, change to the `noisi/noisi` directory and run `pytest`. If you encounter any errors (warnings are o.k.), we'll be grateful if you let us know. \n\n### Getting started\nTo see an overview of the tool, type `noisi --help`.\nA step-by-step tutorial for jupyter notebook can be found in the `noisi/noisi` directory.\nExamples on how to set up an inversion and how to import a wavefield from axisem3d are found in the noisi/examples directory.\n\n\n",
        "createdAt": "2020-02-11T14:20:00.000Z",
        "updatedAt": "2025-11-03T07:28:15.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/lermert/noisi/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ntguojiarui/Introduction-to-Seismology",
        "url": "https://github.com/ntguojiarui/Introduction-to-Seismology",
        "description": "Review materials for Introduction to Seismology",
        "stars": 6,
        "forks": 0,
        "readme": "# Introduction-to-Seismology\nReview materials for Introduction to Seismology\n\nCopyright Guo Jiarui, Peking University, all rights reserved\n\n## What is it about\nThis project can be review materials for students who are to take the final exam of the course Introduction to Seismology. It can also served as a project for those who want to get more knowledge in this field. \n\nThere is a pdf document of notes and a pdf document of calculating problems. It is worth noticing that a number of calculating problems will necessarily appear on the test paper in the coming exam. Also, you must take a look at the notes in this project to guarantee a good performance in the exam. \n\n## Who contributes to this project\nThe project is mostly completed by Guo Jiarui, Peking University. We also appreciate those who have assisted us in this project. We convey our best gratefulness to you. \n\n## Who can use the documents in this project\nAnyone, once he/she is willing to indicate the source is allowed to use them for free. However, we reserve the right to blame those who sell these materials for profit. \n\n\n### Editor\nGuo Jiarui\n\n6th January, 2021\n\n### Updated at 8th January, 2021\n* 2.2.e  ''天然地震的额震源破裂''应为''天然地震的震源破裂''. \n* 4.2  ''地震点额首波''应为''地震点的首波''.\n* 5.8  式中第二个lg应直接去掉.\n* 5.11  ''我国最大的水库地震时''应为''我国最大的水库地震是''.\n* 8.2  ''建筑物的抗震设计通常是在一定地震的前提''应为''建筑物的抗震设计通常是在一定地震烈度的前提''.\n\nWe apologize for our carelessness in our project. \n",
        "createdAt": "2021-01-06T10:01:34.000Z",
        "updatedAt": "2023-08-26T13:23:50.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ntguojiarui/Introduction-to-Seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "senli1073/SeisT",
        "url": "https://github.com/senli1073/SeisT",
        "description": "[TGRS] SeisT: A Foundational Deep-Learning Model for Earthquake Monitoring Tasks",
        "stars": 21,
        "forks": 5,
        "readme": "\n\n[![TGRS](https://img.shields.io/badge/IEEE_TGRS_(2024)-5908215-blue)](https://doi.org/10.1109/TGRS.2024.3371503)\n[![arXiv](https://img.shields.io/badge/arXiv-2310.01037-b31b1b)](https://arxiv.org/abs/2310.01037)\n![License](https://img.shields.io/github/license/senli1073/SeisT)\n![LastCommit](https://img.shields.io/github/last-commit/senli1073/SeisT)\n------------------\n\n- [SeisT Architecture](#seist-architecture)\n- [Introduction](#introduction)\n- [Usage](#usage)\n  - [Data preparation](#data-preparation)\n  - [Training](#training)\n  - [Fine-tuning](#fine-tuning)\n  - [Testing](#testing)\n- [Citation](#citation)\n- [Reporting Bugs](#reporting-bugs)\n- [Acknowledgement](#acknowledgement)\n- [License](#license)\n\n## SeisT Architecture\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/senli1073/SeisT/main/images/SeisT_Architecture.png\">\n</p>\n\n## Introduction\nSeisT is a backbone for seismic signal processing, which can be used for multiple seismic monitoring tasks such as earthquake detection, seismic phase picking, first-motion polarity classification, magnitude estimation, back-azimuth estimation, and epicentral distance estimation.\n\nThis repository also provides some baseline models implemented by Pytorch under `./models`, such as PhaseNet, EQTransformer, DitingMotion, MagNet, BAZ-Network, and distPT-Network. \n\nNOTE: The model weights included in this repository serve as the basis for performance evaluation in the paper. They have been evaluated using identical training/testing data and a consistent training strategy, thereby affirming the effectiveness of SeisT. Nevertheless, if you intend to employ these models in practical engineering applications, it is crucial to retrain the SeisT models with larger datasets to align with the specific demands of engineering applications.\n\n## Usage\n\n### Data Preparation\n\n- **For training and evaluation**\n  \n  Create a new file named `yourdata.py` in the directory `dataset/` to read the metadata and seismograms of the dataset. Then the `@register_dataset` decorator needs to be used to register your dataset. \n\n  (Please refer to the code samples, such as `datasets/DiTing.py` and `datasets/PNW.py`)\n\n- **For model deployment**\n\n  Follow the steps in `demo_predict.py` and rewrite the `load_data` function.\n\n### Training\n\n- **Model**<br/>\n  Before starting training, please make sure that your model file is in the directory `models/` and is registered by using `@register_model`. The  available models in the project can be inspected using the following method: \n  ```Python\n  >>> from models import get_model_list\n  >>> get_model_list()\n  ['eqtransformer', 'phasenet', 'magnet', 'baz_network', 'distpt_network', 'ditingmotion', 'seist_s_dpk', 'seist_m_dpk', 'seist_l_dpk', 'seist_s_pmp', 'seist_m_pmp', 'seist_l_pmp', 'seist_s_emg', 'seist_m_emg', 'seist_l_emg', 'seist_s_baz', 'seist_m_baz', 'seist_l_baz', 'seist_s_dis', 'seist_m_dis', 'seist_l_dis']\n  ```\n  The task names and their abbreviations in this project are shown in the table below:\n\n  <table><tbody>\n\n  <th valign=\"bottom\">Task</th>\n  <th valign=\"bottom\">Abbreviation</th>\n\n  <tr><td align=\"left\">Detection & Phase Picking</td>\n  <td align=\"left\">dpk</td>\n\n  <tr><td align=\"left\">First-Motion Polarity Classification</td>\n  <td align=\"left\">pmp</td>\n\n  <tr><td align=\"left\">Back-Azimuth Estimation</td>\n  <td align=\"left\">baz</td>\n\n  <tr><td align=\"left\">Magnitude Estimation</td>\n  <td align=\"left\">emg</td>\n\n  <tr><td align=\"left\">Epicentral Distance Estimation</td>\n  <td align=\"left\">dis</td>\n\n  </tbody></table>\n\n- **Model Configuration**<br/>\n  The configurations of the loss functions, labels, and the corresponding models are in `config.py` which also provides a detailed explanation of all the fields.\n\n\n- **Start training**<br/>\n  To start training with a CPU or a single GPU, please use the following command to start training:\n  ```Shell\n  python main.py \\\n    --seed 0 \\\n    --mode \"train_test\" \\\n    --model-name \"seist_m_dpk\" \\\n    --log-base \"./logs\" \\\n    --device \"cuda:0\" \\\n    --data \"/root/data/Datasets/Diting50hz\" \\\n    --dataset-name \"diting\" \\\n    --data-split true \\\n    --train-size 0.8 \\\n    --val-size 0.1 \\\n    --shuffle true \\\n    --workers 8 \\\n    --in-samples 8192 \\\n    --augmentation true \\\n    --epochs 200 \\\n    --patience 30 \\\n    --batch-size 500\n  ```\n  \n  To start training with multiple GPUs, please use `torchrun` to start training:\n  ```Shell\n  torchrun \\\n    --nnodes 1 \\\n    --nproc_per_node 2 \\\n    main.py \\\n      --seed 0 \\\n      --mode \"train_test\" \\\n      --model-name \"seist_m_dpk\" \\\n      --log-base \"./logs\" \\\n      --data \"/root/data/Datasets/Diting50hz\" \\\n      --dataset-name \"diting\" \\\n      --data-split true \\\n      --train-size 0.8 \\\n      --val-size 0.1 \\\n      --shuffle true \\\n      --workers 8 \\\n      --in-samples 8192 \\\n      --augmentation true \\\n      --epochs 200 \\\n      --patience 30 \\\n      --batch-size 500\n  ```\n  \n  There are also a variety of other custom arguments which are not mentioned above. Use the command `python main.py --help` to see more details.\n\n  \n### Fine-tuning\n\nThe following table provides the pre-trained checkpoints used in the paper:\n<table><tbody>\n\n<th valign=\"bottom\">Task</th>\n<th valign=\"bottom\">Train set</th>\n<th valign=\"bottom\">SeisT-S</th>\n<th valign=\"bottom\">SeisT-M</th>\n<th valign=\"bottom\">SeisT-L</th>\n\n\n<tr><td align=\"left\">Detection & Phase Picking</td>\n<td align=\"left\">DiTing</td>\n<td align=\"center\"><a href=\"https://raw.githubusercontent.com/senli1073/SeisT/main/pretrained/seist_s_dpk_diting.pth\">download</a></td>\n<td align=\"center\"><a href=\"https://raw.githubusercontent.com/senli1073/SeisT/main/pretrained/seist_m_dpk_diting.pth\">download</a></td>\n<td align=\"center\"><a href=\"https://raw.githubusercontent.com/senli1073/SeisT/main/pretrained/seist_l_dpk_diting.pth\">download</a></td>\n\n<tr><td align=\"left\">First-Motion Polarity Classification</td>\n<td align=\"left\">DiTing</td>\n<td align=\"center\"><a href=\"https://raw.githubusercontent.com/senli1073/SeisT/main/pretrained/seist_s_pmp_diting.pth\">download</a></td>\n<td align=\"center\"><a href=\"https://raw.githubusercontent.com/senli1073/SeisT/main/pretrained/seist_m_pmp_diting.pth\">download</a></td>\n<td align=\"center\"><a href=\"https://raw.githubusercontent.com/senli1073/SeisT/main/pretrained/seist_l_pmp_diting.pth\">download</a></td>\n\n<tr><td align=\"left\">Back-Azimuth Estimation</td>\n<td align=\"left\">DiTing</td>\n<td align=\"center\"><a href=\"https://raw.githubusercontent.com/senli1073/SeisT/main/pretrained/seist_s_baz_diting.pth\">download</a></td>\n<td align=\"center\"><a href=\"https://raw.githubusercontent.com/senli1073/SeisT/main/pretrained/seist_m_baz_diting.pth\">download</a></td>\n<td align=\"center\"><a href=\"https://raw.githubusercontent.com/senli1073/SeisT/main/pretrained/seist_l_baz_diting.pth\">download</a></td>\n\n<tr><td align=\"left\">Magnitude Estimation</td>\n<td align=\"left\">DiTing</td>\n<td align=\"center\"><a href=\"https://raw.githubusercontent.com/senli1073/SeisT/main/pretrained/seist_s_emg_diting.pth\">download</a></td>\n<td align=\"center\"><a href=\"https://raw.githubusercontent.com/senli1073/SeisT/main/pretrained/seist_m_emg_diting.pth\">download</a></td>\n<td align=\"center\"><a href=\"https://raw.githubusercontent.com/senli1073/SeisT/main/pretrained/seist_l_emg_diting.pth\">download</a></td>\n\n<tr><td align=\"left\">Magnitude Estimation</td>\n<td align=\"left\">PNW</td>\n<td align=\"center\"><a href=\"https://raw.githubusercontent.com/senli1073/SeisT/main/pretrained/seist_s_emg_pnw.pth\">download</a></td>\n<td align=\"center\"><a href=\"https://raw.githubusercontent.com/senli1073/SeisT/main/pretrained/seist_m_emg_pnw.pth\">download</a></td>\n<td align=\"center\"><a href=\"https://raw.githubusercontent.com/senli1073/SeisT/main/pretrained/seist_l_emg_pnw.pth\">download</a></td>\n\n<tr><td align=\"left\">Epicentral Distance Estimation</td>\n<td align=\"left\">DiTing</td>\n<td align=\"center\"><a href=\"https://raw.githubusercontent.com/senli1073/SeisT/main/pretrained/seist_s_dis_diting.pth\">download</a></td>\n<td align=\"center\"><a href=\"https://raw.githubusercontent.com/senli1073/SeisT/main/pretrained/seist_m_dis_diting.pth\">download</a></td>\n<td align=\"center\"><a href=\"https://raw.githubusercontent.com/senli1073/SeisT/main/pretrained/seist_l_dis_diting.pth\">download</a></td>\n\n</tbody></table>\n\nUse the \"--checkpoint\" argument to pass in the path of the pre-training weights.\n\n### Testing\n  To start training with a CPU or a single GPU, please use the following command to start testing:\n\n  ```Shell\n  python main.py \\\n    --seed 0 \\\n    --mode \"test\" \\\n    --model-name \"seist_m_dpk\" \\\n    --log-base \"./logs\" \\\n    --device \"cuda:0\" \\\n    --data \"/root/data/Datasets/Diting50hz\" \\\n    --dataset-name \"diting\" \\\n    --data-split true \\\n    --train-size 0.8 \\\n    --val-size 0.1 \\\n    --workers 8 \\\n    --in-samples 8192 \\\n    --batch-size 500\n  ```\n  \n  To start training with multiple GPUs, please use `torchrun` to start testing:\n  ```Shell\n  torchrun \\\n    --nnodes 1 \\\n    --nproc_per_node 2 \\\n    main.py \\\n      --seed 0 \\\n      --mode \"test\" \\\n      --model-name \"seist_m_dpk\" \\\n      --log-base \"./logs\" \\\n      --data \"/root/data/Datasets/Diting50hz\" \\\n      --dataset-name \"diting\" \\\n      --data-split true \\\n      --train-size 0.8 \\\n      --val-size 0.1 \\\n      --workers 8 \\\n      --in-samples 8192 \\\n      --batch-size 500\n  ```\n\n  It should be noted that the `train_size`, `val_size`, and `seed` in the test phase must be consistent with that training phase. Otherwise, the test results may be distorted.\n\n## Citation\n\nPaper: https://doi.org/10.1109/TGRS.2024.3371503\n\nIf you find this repo useful in your research, please consider citing:\n\n```\n@ARTICLE{10453976,\n  author={Li, Sen and Yang, Xu and Cao, Anye and Wang, Changbin and Liu, Yaoqi and Liu, Yapeng and Niu, Qiang},\n  journal={IEEE Transactions on Geoscience and Remote Sensing}, \n  title={SeisT: A Foundational Deep-Learning Model for Earthquake Monitoring Tasks}, \n  year={2024},\n  volume={62},\n  pages={1-15},\n  doi={10.1109/TGRS.2024.3371503}\n}\n```\n\nThe baseline models used in this paper:\n\n- **PhaseNet**<br/>\n  *Zhu, W., & Beroza, G. C. (2019). PhaseNet: A deep-neural-network-based seismic arrival-time picking method. Geophysical Journal International, 216(1), 261-273.*\n\n- **EQTransformer**<br/>\n  *Mousavi, S. M., Ellsworth, W. L., Zhu, W., Chuang, L. Y., & Beroza, G. C. (2020). Earthquake transformer—an attentive deep-learning model for simultaneous earthquake detection and phase picking. Nature communications, 11(1), 3952.*\n\n- **DiTingMotion**<br/>\n  *Zhao, M., Xiao, Z., Zhang, M., Yang, Y., Tang, L., & Chen, S. (2023). DiTingMotion: A deep-learning first-motion-polarity classifier and its application to focal mechanism inversion. Frontiers in Earth Science, 11, 1103914.*\n\n- **MagNet**<br/>\n  *Mousavi, S. M., & Beroza, G. C. (2020). A machine‐learning approach for earthquake magnitude estimation. Geophysical Research Letters, 47(1), e2019GL085976.*\n\n- **BAZ-Network** <br/>\n  *Mousavi, S. M., & Beroza, G. C. (2020). Bayesian-Deep-Learning Estimation of Earthquake Location From Single-Station Observations. IEEE Transactions on Geoscience and Remote Sensing, 58(11), 8211-8224.*\n\n\n## Reporting Bugs\nReport bugs at https://github.com/senli1073/SeisT/issues.\n\nIf you are reporting a bug, please include:\n\n- Operating system version.\n- Versions of Python and libraries such as Pytorch.\n- Steps to reproduce the bug.\n\n\n## Acknowledgement\nThis project refers to some excellent open source projects: [PhaseNet](https://github.com/AI4EPS/PhaseNet), [EQTransformer](https://github.com/smousavi05/EQTransformer), [DiTing-FOCALFLOW](https://github.com/mingzhaochina/DiTing-FOCALFLOW), [MagNet](https://github.com/smousavi05/MagNet), [Deep-Bays-Loc](https://github.com/smousavi05/Deep-Bays-Loc), [PNW-ML](https://github.com/niyiyu/PNW-ML), and [SeisBench](https://github.com/seisbench/seisbench).\n\n\n## License\nCopyright S.Li et al. 2023. Licensed under an MIT license.\n\n\n\n",
        "createdAt": "2023-10-07T09:02:28.000Z",
        "updatedAt": "2025-11-20T07:28:49.000Z",
        "language": "Python",
        "homepage": "https://doi.org/10.1109/TGRS.2024.3371503",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/senli1073/SeisT/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "xtyangpsp/SeisStack",
        "url": "https://github.com/xtyangpsp/SeisStack",
        "description": "Collection of codes of vector stacking methods used in seismology",
        "stars": 0,
        "forks": 0,
        "readme": "# SeisStack\nCollection of vector stacking methods and codes used in seismology. Codes are all in three languages: MATLAB, julila, Python.\n\n##  Directories\n1. **src**: source codes organized by methods.  \n    a). The stacking code is saved under: `src/METHOD`. For example, for Robust stacking, the code path is: `src/Robust`.  \n    b). Other codes may be saved under: `src/Utilities`.  \n2. **data**: test data sets.\n\n\n",
        "createdAt": "2019-11-02T03:24:45.000Z",
        "updatedAt": "2024-06-28T06:22:47.000Z",
        "language": "MATLAB",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/xtyangpsp/SeisStack/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "martijnende/AI4Seismology",
        "url": "https://github.com/martijnende/AI4Seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# AI4Seismology",
        "createdAt": "2025-04-27T12:01:58.000Z",
        "updatedAt": "2025-04-28T13:17:58.000Z",
        "language": "Batchfile",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/martijnende/AI4Seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Cuda-Chen/ms2fft",
        "url": "https://github.com/Cuda-Chen/ms2fft",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# ms2fft\nRead miniSEED data then make FFT on it.\n\n# Dependencies\n- libmseed\n- fftw\n\n# How to Compile and Run\nFirst clone this repo with this command:\n```\n$ git clone --recursive https://github.com/Cuda-Chen/ms2fft.git\n```\n\nThen type:\n```\n$ make\n$ ./ms2fft [mseedfile]\n```\n\n# Output Format\n```\n<fs> <real> <imag>\n```\n\n# What is the meaning of out[0]?\n- http://www.fftw.org/fftw3_doc/Complex-One_002dDimensional-DFTs.html#Complex-One_002dDimensional-DFTs\n    - The DFT results are stored in-order in the array out, with the zero-frequency (DC) component in out[0]. \n- Zero-frequency component\n    - https://www.quora.com/What-is-a-zero-Hz-frequency-component\n    - Short answer: the zero Hz component is the average of the signal in the time-domain. Also called its DC value.\n",
        "createdAt": "2020-03-19T02:12:56.000Z",
        "updatedAt": "2024-12-27T18:05:13.000Z",
        "language": "C",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Cuda-Chen/ms2fft/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "geophystech/GroundMotion.jl",
        "url": "https://github.com/geophystech/GroundMotion.jl",
        "description": "The ground motion evaluation module (earthquake seismology)",
        "stars": 4,
        "forks": 5,
        "readme": "# GroundMotion.jl\n<!-- vim-markdown-toc GFM -->\n\n* [Install](#install)\n* [Abbreviations](#abbreviations)\n* [Basic Principles of Ground Motion Assessment](#basic-principles-of-ground-motion-assessment)\n    * [GRID case](#grid-case)\n    * [Without grid](#without-grid)\n    * [GMPE correction by weighted averaging method](#gmpe-correction-by-weighted-averaging-method)\n* [End-to-end examples](#end-to-end-examples)\n* [How to get VS30 grid](#how-to-get-vs30-grid)\n    * [Read and Write data grids](#read-and-write-data-grids)\n* [Earthquake location data](#earthquake-location-data)\n* [GMPE Models](#gmpe-models)\n    * [Abrahamson and Silva 2008 GMPE Model (AS2008)](#abrahamson-and-silva-2008-gmpe-model-as2008)\n    * [Si and Midorikawa 1999 GMPE Model (simidorikawa1999)](#si-and-midorikawa-1999-gmpe-model-simidorikawa1999)\n    * [Morikawa and Fujiwara 2013 GMPE Model (gmpe_mf2013)](#morikawa-and-fujiwara-2013-gmpe-model-gmpe_mf2013)\n        * [About Dl variable](#about-dl-variable)\n* [Appendix 1. Seismic scale intensity (Russian GOST R 57546-2017).](#appendix-1-seismic-scale-intensity-russian-gost-r-57546-2017)\n* [Citation](#citation)\n* [LICENSE](#license)\n\n<!-- vim-markdown-toc -->\n\nThe ground motion evaluation module (earthquake seismology).\n\nBuild Status\n\n![CI](https://github.com/geophystech/GroundMotion.jl/actions/workflows/ci.yml/badge.svg)\n[![codecov](https://codecov.io/gh/geophystech/GroundMotion.jl/branch/master/graph/badge.svg)](https://codecov.io/gh/geophystech/GroundMotion.jl)\n\n## Install\n\n```julia\n]\n(v1.6) pkg> add GroundMotion.jl\n```\n\n## Abbreviations\n\n`PGA` peak ground acceleration.\n\n`PGV` peak ground velocity.\n\n`PSA` peak spectral acceleration.\n\n`VS30` the time-averaged shear-wave velocity to 30 m depth,\n[data](https://earthquake.usgs.gov/data/vs30/).\n\n`SSI` seismic scale intensity (Russian GOST R 57546-2017, see Appendix 1).\n\n## Basic Principles of Ground Motion Assessment\n\nNames of GMPE functions looks like: `gmpe_{Name_of_gmpe_function}`. For\nexample: `gmpe_as2008`, where `as2008` is Abrahamson and Silva 2008 GMPE Model.\nThe configuration for a model (see `examples/*.conf`) has `ground_motion_type`\nthat can be `PGA`,`PGV`,`PSA` and define the type of output data points.\n\nEach GMPE function has at least 2 methods: for calculation based on input\nVS30-grid or without any grid (where VS30 is considered to be constant).\n\nAlthough the GMPE field is obtained by modeling using earthquake source\nparameters, it can be corrected using observed intensity data such as\nmeasurements at seismic sites or felt reports. To this end, the `gmpe_corr`\nroutine is used to calculate a new GMPE PGA/intensity grid using simple the\nweighted average method. For technical details see\n[preprint](https://preprints.ru/article/1013) (Russian).\n\n### GRID case\n\nThe GMPE function for each grid's point calculates `{pga/pgv/psa}` values using\n`latitude`, `longitude` [degrees for WGS84 ellipsoid] and `VS30` [m/s]. The\noutput data has return in custom type (depends by config) where latitude and\nlongitude are copy from input grid and `pga/pgv/pgd/psa` is calculated by\nfunction.\n\nFor example: the function `gmpe_as2008` with parameters\n\n```julia\npga_as2008(\n    eq::Earthquake,\n    config_as2008::Params_as2008,\n    grid::Array{Point_vs30};\n    min_val::Number\n)\n```\n\nwhere `ground_motion_type = \"PGA\"` at `config`, returns 1-d is\n`Array{Point_pga_out}` with points based on input grid and `pga > min_val`\n(`pga` is Acceleration of gravity in percent (%g) rounded to `ggg.gg`).\n\n### Without grid\n\nIn case of without any grid GMPE functions return simple 1-d `Array{Float64}`\nwith `{pga/pgv/pgd/psa}` data. It calculates from epicenter to `distance` with\n`1` [km] step perpendicularly to the epicenter.\n\n```julia\npga_as2008(\n    eq::Earthquake,\n    config::Params_as2008;\n    VS30::Number=350,\n    distance::Int64=1000\n)\n```\n\nwhere `ground_motion_type = \"PGA\"` at `config`, return is `Array{Float64}` with\n`1:distance` values of `pga` (also rounded to `ggg.gg`).\n\n### GMPE correction by weighted averaging method\n\nThe `gmpe_corr` function fan in a computed GMPE filed in `Array{Point_pga_out`\nformat and measurements in `Array{Point_felt_report}` type. The measurements\ncan be of two types: exact PGA by an seismic station or aggregated felt report.\n\n```julia\nfunction gmpe_corr(\n    config::Params_gmpe_corr,\n    grid::Array{Point_pga_out},\n    intensity_measures::Array{Point_felt_report};\n    intensity_in_pga::Bool=true,\n    pga_out::Bool=true\n)\n```\n\nInput:\n* `config` configuration of weighted averaging procedure\n* `grid` GMPE field modeling by any of `gmpe_*` method\n* `intensity_measures` Intensity measures by felt reports or stations\n* `intensity_in_pga` is the intensity measures in PGA %g.gg units? Otherwise intensity in SSI is assumes.\n* `pga_out` if set to `true` function will return corrected GMPE filed in %g.gg units\n\nOutput:\nCorrected GMPE field in `Array{Point_pga_out}` or `Array{Point_ssi_out}`.\n\nSee `examples/gmpe-corr.conf` for parameters setup.\n\n## End-to-end examples\n\nSimple `as2008` GMPE modeling:\n\n```julia\nusing GroundMotion\n# init model parameters\ninclude(\"GroundMoution.jl/examples/as2008.conf\")\n# load vs30 grid\ngrid = read_vs30_file(\"Downloads/web/testvs30.txt\") # see How to get VS30 grid\n# set earthquake location\neq = Earthquake(143.04,51.92,13,6)\n# run AS2008 PGA modeling on GRID\nout_grid = gmpe_as2008(eq,config_as2008,grid)\n# run AS2008 PGA FOR PLOTTING with VS30=30 [m/s^2], distance=1000 [km] by default.\nsimulation = pga_as2008(eq,config_as2008)\n```\n\nGMPE `sm2013` modeling with felt report correction:\n\n```julia\nusing GroundMotion, DelimitedFiles\n# init model parameters\ninclude(\"GroundMoution.jl/examples/morikawa-fujiwara-2013.conf\")\ninclude(\"GroundMoution.jl/examples/gmpe-corr.conf\")\n# load vs30 grid\ngrid = read_vs30_file(\"Downloads/web/testvs30.txt\") # see How to get VS30 grid\n# set earthquake location\neq = Earthquake(143.6,50.5,13,6)\n# MF2013 on grid, M6, ASID false, Dl - constant\nout_grid = gmpe_mf2013(eq, config_mf2013_crustal_pga, grid)\n# load felt reports in SSI\nintensity_mesuares_ssi = read_intensity_file(\n    \"GroundMoution.jl/test/felt_reports.txt\"\n)\n# convert to PGA\nintensity_mesuares_pga = convert_from_ssi_to_pga(intensity_mesuares_ssi)\n# obtain corrected grid\nout_grid_corr = gmpe_corr(\n    config_gmpe_corr_base, out_grid, intensity_mesuares_pga,\n    intensity_in_pga=true, pga_out=true\n)\n# convert result and save to dlm\nto_file = convert_to_float_array(out_grid_corr)\nwritedlm(\"Downloads/M6_lon_lat_g.txt\", to_file)\n```\n\n## How to get VS30 grid\n\nDownload GMT grd file from [USGS Vs30 Models and Data\npage](https://earthquake.usgs.gov/data/vs30/) Unzip it. It takes around 2,7G\ndisk space for one file:\n\n```bash\nunzip global_vs30_grd.zip\n...\nls -lh global_vs30.grd\n-rw-r--r--  1 jamm  staff   2,7G  8 сен  2016 global_vs30.grd\n```\n\nUse `GMT2XYZ` [man\npage](https://www.soest.hawaii.edu/gmt/gmt/html/man/grd2xyz.html) from\n[GMT](https://www.soest.hawaii.edu/gmt/) to convert grd data to XYZ text file:\n\n```bash\n# example:\ngrd2xyz global_vs30.grd -R145.0/146.0/50.0/51.0 > test_sea.txt\n# number of rows:\ncat test_sea.txt |wc -l\n   14641\n```\n\n### Read and Write data grids\n\nUse `read_vs30_file` to read data from vs30 file:\n\n```julia\ngrid = read_vs30_file(\"Downloads/web/somevs30.txt\")\n```\n\nAfter some `gmpe_*` function on grid done, you will get\n`Array{Point_{pga,pgv,pgd,psa}_out}`. Use `convert_to_float_array` to convert\n`Array{Point_{pga,pgv,pgd,psa}_out}` to `Nx3` `Array{Float64}`:\n\n```julia\ntypeof(A)\n#--> Array{GroundMoution.Point_pga_out,1}\nlength(A)\n#--> 17\nB = convert_to_float_array(A)\ntypeof(B)\n#--> Array{Float64,2}\n```\n\nUse `Base.writedlm` to write XYZ (`lon`,`lat`,`pga/pgv/pgd/psa`) data to text\nfile:\n\n```julia\nwritedlm(\"Downloads/xyz.txt\", B) # where B is N×3 Array{Float64}\n```\n\nUse `convert_to_point_vs30` to convert Array{Float64,2} array to\nArray{GroundMotion.Point_vs30,1}\n\n## Earthquake location data\n\nLets define `lat`,`lon`,`depth`,`Ml`,`Mw`:\n\n```julia\neq = Earthquake(143.04,51.92,13,6,5.8)\n# OR\neq = Earthquake(143.04,51.92,13,6)\n```\n\nLatitude and longitude assumes degrees for WGS84 ellipsoid. Depth in km. `Mw`\nusually not ready right after earthquake. `Mw=0` in case of moment magnitude is\nnot specified. All gmpe models uses `Mw` if it is or `Ml` otherwise.\n\n## GMPE Models\n\n### Abrahamson and Silva 2008 GMPE Model (AS2008)\n\nThis is a very simple implementation of the AS2008 model. Please see\nsimidorikawa1999 and mf2013 models for more accurate modeling.\n\nReference: `Abrahamson, Norman, and Walter Silva. \"Summary of the Abrahamson &\nSilva NGA ground-motion relations.\" Earthquake spectra 24.1 (2008): 67-97.`\n\n```julia\n## ON GRID\ngmpe_as2008(\n    eq::Earthquake,\n    config_as2008::Params_as2008,\n    grid::Array{Point_vs30};\n    min_val::Number\n)\n## Without grid\ngmpe_as2008(\n    eq::Earthquake,\n    config::Params_as2008;\n    VS30::Number=350,\n    distance::Int64=1000\n)\n\n```\n\nKeyword arguments: `min_val`,`VS30`,`distance`.\n\nFor model Parameters see `examples/as2008.conf` (`PGA` only).\n\n**The variables that always zero for current version:**\n\n`a12*Frv`, `a13*Fnm`, `a15*Fas`, `Fhw*f4(Rjb,Rrup,Rx,W,S,Ztor,Mw)`, `f6(Ztor)`,\n`f10(Z1.0, Vs30)`.\n\nActually they are not presented at code. **R_rup** is a distance to\nhypocenter.\n\n### Si and Midorikawa 1999 GMPE Model (simidorikawa1999)\n\n![si-midorikawa-1999](https://user-images.githubusercontent.com/3518847/35567902-c89220ac-061a-11e8-98f1-0deb520f1be2.jpg)\n\nReferences:\n\n1. `Si, Hongjun, and Saburoh Midorikawa. \"New attenuation relations for peak\n   ground acceleration and velocity considering effects of fault type and site\n   condition.\" Proceedings of twelfth world conference on earthquake\n   engineering. 2000.`\n2. `Si H., Midorikawa S. New Attenuation Relationships for Peak Ground\n   Acceleration and Velocity Considering Effects of Fault Type and Site\n   Condition // Journal of Structural and Construction Engineering, A.I.J.\n   1999. V. 523. P. 63-70, (in Japanese with English abstract).`\n\n```julia\n## ON GRID\ngmpe_simidorikawa1999(\n    eq::Earthquake,\n    config::Params_simidorikawa1999,\n    grid::Array{Point_vs30};\n    min_val::Number\n)\n## Without grid\ngmpe_simidorikawa1999(\n    eq::Earthquake,\n    config::Params_simidorikawa1999;\n    VS30::Number=350,\n    distance::Int64=1000\n)\n```\n\nKeyword arguments: `min_val`,`VS30`,`distance`.\n\nFor model parameters see `examples/si-midorikawa-1999.conf` (`PGA` only). **X**\nis a distance to hypocenter.\n\n### Morikawa and Fujiwara 2013 GMPE Model (gmpe_mf2013)\n\n![mf2013](https://user-images.githubusercontent.com/3518847/35567875-ad83ebba-061a-11e8-8023-7bb372176042.jpg)\n\nReference `Morikawa N., Fujiwara H. A New Ground Motion Prediction Equation for\nJapan Applicable up to M9 Mega-Earthquake // Journal of Disaster Research.\n2013. Vol. 5 (8). P. 878–888.`\n\n```julia\n## On grid whithout Dl data\ngmpe_mf2013(\n    eq::Earthquake,\n    config::Params_mf2013,\n    grid::Array{Point_vs30};\n    min_val::Number=0,\n    Dl::Number=250,\n    Xvf::Number=0\n)\n## On grid with Dl data\ngmpe_mf2013(\n    eq::Earthquake,\n    config::Params_mf2013,g\n    grid::Array{Point_vs30_dl};\n    min_val::Number=0,\n    Xvf::Number=0\n)\n## without any grid\ngmpe_as2008(\n    eq::Earthquake,\n    config::Params_mf2013;\n    VS30::Number=350,\n    distance::Int64=1000,\n    Dl::Number=250,\n    Xvf::Number=0\n)\n```\n\n`min_val=0`, `Xvf=0` [km] by default. `Dl=250` [km] by default in case of grid\npass without Dl data.\n\nNOTE that `gmpe_mf2013` has next keyword arguments: `min_val`, `min_val`, `Dl`,\n`VS30`, `distance`. The keyword arguments should be pass with name. Example:\n`gmpe_mf2013(eq,config,VS30=500,Xvf=40)`.\n\nFor model parameters see `examples/morikawa-fujiwara-2013.conf` (`PGA`, `PGV`,\n`PSA`). **X** is a distance to hypocenter.\n\n#### About Dl variable\n\nThe `Dl` is the top depth to the layer whose S-wave velocity is `l` (in\n`[m/s]`) at the site. Actually it should be another one grid with `Dl` depths\non each grid point (`Point_vs30_dl` type). If you pass grid without `Dl`, then\n`Dl` variable pass to GMPE functions as a constant.\n\n## Appendix 1. Seismic scale intensity (Russian GOST R 57546-2017).\n\n| SSI | PGA %g |\n| --- | --- |\n| 1.0 | 0.044 |\n| 1.5 | 0.07 |\n| 2.0 | 0.11 |\n| 2.5 | 0.175 |\n| 3.0 | 0.28 |\n| 3.5 | 0.44 |\n| 4.0 | 0.7 |\n| 4.5 | 1.11 |\n| 5.0 | 1.75 |\n| 5.5 | 2.8 |\n| 6.0 | 4.4 |\n| 6.5 | 7.0 |\n| 7.0 | 11.0 |\n| 7.5 | 18.0 |\n| 8.0 | 28.0 |\n| 8.5 | 44.0 |\n| 9.0 | 70.0 |\n| 9.5+ | 110.0+ |\n\n## Citation\n\n```\n@article{konovalov2022new,\n  title={New Tools for Rapid Assessment of Felt Reports and a Case Study on Sakhalin Island},\n  author={Konovalov, AV and Stepnov, AA and Bogdanov, ES and Dmitrienko, R Yu and Orlin, ID and Sychev, AS and Gavrilov, AV and Manaychev, KA and Tsoy, AT and Stepnova, Yu A},\n  journal={Seismic Instruments},\n  volume={58},\n  number={6},\n  pages={676--693},\n  year={2022},\n  publisher={Springer}\n}\n```\n\n## LICENSE\n\n   Copyright (c) 2018-2023 GEOPHYSTECH LLC and Andrey Stepnov\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n",
        "createdAt": "2017-06-20T07:09:16.000Z",
        "updatedAt": "2025-06-23T20:29:17.000Z",
        "language": "Julia",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/geophystech/GroundMotion.jl/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "nfsi-canada/MSW2023",
        "url": "https://github.com/nfsi-canada/MSW2023",
        "description": "Instructions and demo data for the OBStools tutorial at the 2023 Marine Seismology Workshop",
        "stars": 2,
        "forks": 0,
        "readme": "## 2023 Marine Seismology Workshop - OBS data processing\n\n\n**Instructors**: [Pascal Audet](https://www.uogeophysics.com/authors/admin/)\n\n**When**: Wednesday/Thursday, May 23/24, 2023 at 8:30 AM (ADT). \n\n**Where**: Room LSC2012 in Life Sciences Centre, Dalhousie University\n\n**What**: This tutorial will provide hands-on experience to process broadband ocean-bottom seismic (BBOBS) data using the Automatic Tilt and Compliance Removal (ATaCR, pronounced \"attacker\") software. This software is designed to automate, as best as possible, the process of characterizing and removing tilt and compliance noise from vertical component BBOBS data. The tutorial will use the Python implementation of ATaCR. \n\n---\n\n### Installing the Python version\n\nThe software has been pre-installed on the computers in Room LSC2012, therefore there is no need to follow these steps. The following steps provide instructions to install the software on your personal computer.\n\nATaCR is implemented as a separate module in the open-source Python package OBStools:\n\n- Git repository: [OBStools](https://github.com/nfsi-canada/OBStools)\n\n- Documentation can be found [here](https://nfsi-canada.github.io/OBStools/)\n\nTo install `obstools`, we strongly recommend installing and creating a `conda` environment (either from the [Anaconda](https://anaconda.org) distribution or [mini-conda](https://docs.conda.io/en/latest/miniconda.html)) where the code can be installed alongside its dependencies. This **significantly reduces** the potential conflicts in package versions. In a bash (or zsh) terminal, follow these steps:\n\n- Create a conda environment (here we call it `mss` for the name of the symposium) and install `python=3.8` and `obspy`:\n\n```bash\nconda create -n msw python=3.8 obspy -c conda-forge\n```\n\n- Activate the environment:\n\n```bash\nconda activate msw\n```\n\n- Install the required [`stdb`](https://github.com/schaefferaj/StDb) package using `pip`:\n\n```bash\npip install stdb\n```\n\nNow you're ready to install `obstools`. You might consider one of two options: 1) you want to look at the source code and are considering contributing (awesome!!); 2) you are only interested in using the software and are not interested in the source code.\n\n##### 1) Developer mode: Installing from source\n\n- Navigate on the command line to a path where the software will be installed\n\n- Clone the OBStools repository ([fork](https://docs.github.com/en/github/getting-started-with-github/fork-a-repo) it first, if you are serious about contributing):\n\n```bash\ngit clone https://github.com/paudetseis/OBStools.git\ncd OBStools\n```\n\n- Install using `pip`:\n\n```bash\npip install -e .\n```\n\n##### 2) User mode: Installing from the Python Package Index (PyPI):\n\n```bash\npip install obstools\n```\n\n### Getting the demo data\n\nFinally, download the demo data provided on this github repository by navigating to some work folder (where the data and results of the processing will be located) and typing:\n\n```bash\ngit clone https://github.com/nfsi-canada/MSW2023.git\ncd MSW2023\n```\n\nThe `DATA` and `EVENTS` folders should now be on your computer and you are ready to start the tutorial.\n\n### Testing your installation\n\nIf you want to make sure everything is installed properly, make sure your conda environment has been activated and open a python window by typing in a terminal:\n\n```bash\npython\n```\n\nwhich will produce something like:\n\n```bash\nPython 3.8.16 (default, Feb  1 2023, 16:05:36) \n[Clang 14.0.6 ] :: Anaconda, Inc. on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> \n```\n\nThen type:\n\n```bash\n>>> import stdb\n>>> import obstools\n```\n\nIf nothing happens, you're good to go! If you run into a problem, let us know by [raising an issue](https://github.com/nfsi-canada/MSW2023/issues). \n\n",
        "createdAt": "2023-05-16T18:44:33.000Z",
        "updatedAt": "2025-10-25T07:37:55.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/nfsi-canada/MSW2023/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "marcelobianchi/fetchtool",
        "url": "https://github.com/marcelobianchi/fetchtool",
        "description": "A package for building and fetching waveform data from seismological archives in a FLEXIBLE uniform way.",
        "stars": 0,
        "forks": 3,
        "readme": "# FetchTool\n\nFetchTool is a multi modular mass downloader tool for seismological data. It is aware of FDSN and ArcLink services for metadata and data download and saves data as SAC, Qfiles or MSeed today.\n\n## Installing\n\nTo install it we recomend you to use pip along with a checkout of this repository since this is still in an alpha stage. It is being constant updated, to easy the constant upgrade you can install a link to your repo using 'pip install . -e', something like:\n\n```\n$ git clone https://github.com/marcelobianchi/fetchtool.git\n$ cd fetchtool/\n$ pip install . -e --user\n```\n\n> Don't forget to change pip to pip3 if needed. This code is not expected to work with python 2 anymore.\n\nAlso, if you want to fetch an update, just move to were you cloned it the first time and run git pull again. Something like:\n\n```\n$ cd fetchetool/\n$ git pull\n```\n\n# Overview\n\nThis is a modular package. You have 3 different types of modules to build your program. The first module is the request builder, that can build request lines combining event and station in two different ways: by station and by events. The second module is the Downloader that reads the request data from the request builder and download waveform data from server. Finally, you have the savers that are given to the downloaders. Each saver is responsible to save the files in a different format and place. You have the SAC saver, a Qfile saver and in the future, a MS saver.\n\nAlso, it should be easy in the future to complement already mass downloaded data with new acquired data.\n\n## Builders\n\nThere are two different builders, the FDSNBuilder that is a complete FDSN builder and an ArcLinkFDSNBuilder, that reads events from FDSN and metadata from ArcLink server.\n\n## Downloader\n\nThe Downloader main module just download the data, but it needs the fetchers. There are available two different fetchers, a FDSN fetcher and an SCc3ArclinkFetcher that get data from an ArcLink server.\n\nThe Downloader module can be instructed to save the RAW data obtained from the server. Once data is downloaded from server it is checked and passed to each of the supplied saver module.\n\n## Savers\n\nSaver modules are modules that enforces constrains in the data and save it to certain pre-estabilished directory structure. Main savers implemented are the SacSaver, QSaver and a MSSaver. When saving the data they first fill in the headers with the maximum amount of available data. Also QSaver, prepares a SeismiHandler station file to easy the import of data into the program. MSSaver don't save any station or event information associated with the data.\n\n# About the Author\n\nThe whole package was written by Marcelo Bianchi while working at the University of São Paulo. It uses the ObsPy package and ArcLink clients supplied by the SeisComP3 system. It is tested with ObsPy 1.0.\n\nFor comments and question please address directly Marcelo Bianchi (<m.bianchi@iag.usp.br>).\n\nThis is free software, under the GNU-GPL 3 license.\n\n_Last Updated 24/March/2016_\n\n",
        "createdAt": "2022-06-21T03:50:54.000Z",
        "updatedAt": "2025-02-27T17:50:12.000Z",
        "language": "Python",
        "homepage": "http://www.moho.iag.usp.br/fetchtool/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/marcelobianchi/fetchtool/master/readme.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "lmizrahi/etas",
        "url": "https://github.com/lmizrahi/etas",
        "description": "calibrate ETAS, simulate using ETAS, estimate completeness magnitude & magnitude frequency distribution",
        "stars": 92,
        "forks": 28,
        "readme": "# ETAS: Epidemic-Type Aftershock Sequence\n\n[![DOI](https://zenodo.org/badge/341629005.svg)](https://zenodo.org/badge/latestdoi/341629005)\n\n### This code was written for the following articles:\n\nLeila Mizrahi, Shyam Nandan, Stefan Wiemer 2021;<br/>The Effect of Declustering on the Size Distribution of Mainshocks.<br/>\n_Seismological Research Letters_; doi: https://doi.org/10.1785/0220200231<br/>\n<br/>\n\n### The option for (space-time-)varying completeness magnitude in the parameter inversion is described in:\n\nLeila Mizrahi, Shyam Nandan, Stefan Wiemer 2021;<br/> Embracing Data Incompleteness for Better Earthquake Forecasting. (Section 3.1)<br/>\n_Journal of Geophysical Research: Solid Earth_; doi: https://doi.org/10.1029/2021JB022379<br/>\n<br/>\n<br/>\n\nTo cite the code, plase use its [DOI](https://zenodo.org/badge/latestdoi/341629005), and cite the relevant article(s).<br/>\nFor more documentation on the code, see the (electronic supplement of the) articles.<br/>\nFor Probabilistic, Epidemic-Type Aftershock Incomplenteness, see [PETAI](https://github.com/lmizrahi/petai).<br/>\nIn case of questions or comments, contact me: leila.mizrahi@sed.ethz.ch.\n<br/>\n<br/>\n\nTo install, run\n<code>pip install git+https://github.com/lmizrahi/etas</code>\n<br/>\n<br/>\n\n### Contents:\n\n-   <code>runnable_code/</code> scripts to be run for parameter inversion or catalog simulation\n    -   <code>ch_forecast.py</code> estimates ETAS parameters and creates 100 simulations using the Swiss catalog\n    -   <code>estimate_mc.py</code> estimates constant completeness magnitude for a set of magnitudes\n    -   <code>invert_etas.py</code> calibrates ETAS parameters based on an input catalog (option for varying mc, and option to fix certain parameters available)\n    -   <code>simulate_catalog.py</code> simulates a synthetic catalog\n    -   <code>simulate_catalog_continuation.py</code> simulates a continuation of a catalog, after the parameters have been inverted. if you run this _many times_, you get a forecast. **this only works if you run <code>invert_etas.py</code> beforehand.**\n    -   <code>visualize_fit.py</code> makes plots which visualize the model fit to the data. **this only works if you run <code>invert_etas.py</code> beforehand, and set <code>store_pij = True</code>.**\n    -   <code>predict_etas.py</code> evaluates the model using the event-based log-likelihood on the test window\n-   <code>config/</code> configuration files for running the scripts in <code>runnable_code/</code>\n    -   names should be self-explanatory.\n-   <code>input_data/</code> input data to run example inversions and simulations\n    -   <code>california_shape.npy</code> shape of polygon around California\n    -   <code>ch_catalog.csv</code> Swiss catalog 1972 - 2021, used by <code>ch_forecast.py</code>    \n    -   <code>ch_rect.npy</code> shape of rectangle around Switzerland\n    -   <code>example_catalog.csv</code> to be inverted by <code>invert_etas.py</code>\n    -   <code>example_catalog_mc_var.csv</code> to be inverted by <code>invert_etas.py</code> when varying mc mode is used\n    -   <code>magnitudes.npy</code> example magnitudes for mc estimation\n-   <code>output_data/</code> does not contain anything.\n    -   your output goes here\n-   <code>etas/ </code>\n    -   here is where all the important functions algorithms are defined\n",
        "createdAt": "2021-02-23T17:08:44.000Z",
        "updatedAt": "2025-11-26T09:00:38.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/lmizrahi/etas/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "smousavi05/EPS55",
        "url": "https://github.com/smousavi05/EPS55",
        "description": "This repository includes supplimentary materials for Harvard's introductory course on earthquake seismology (EPS55)",
        "stars": 5,
        "forks": 0,
        "readme": "# Harvard EPS55\n\n## Interactive Visualizations\n\n### Click on each image to see the live interactive version:\n\n---------------------------------------------------------\n## Lecture 1: Eearthquake Impacts\n\n<table>\n  <tr align=\"center\">\n    <th>Earth's Timeline with Human Population Explosion</th>\n    <th>Earthquake Timeline: Magnitude vs. Human Impact</th>\n  </tr>\n  \n  <tr align=\"center\">\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l01_earth_human_timeline.html\">\n        <img src=\"interactive_visualizations/viz__l01_earth_human_timeline.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l01_earthquake-death.html\">\n        <img src=\"interactive_visualizations/viz__l01_earthquake-death.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n  </tr>\n</table>\n\n---------------------------------------------------------\n## Lecture 2: Geological Faults\n\n<table>\n  <tr align=\"center\">\n    <th>Fault types!</th>\n    <th>Fault orientations (Strike, Dip, and Rake angles)!</th>\n    <th>Mohr-Coulomb Failure!</th>\n  </tr>\n  \n  <tr align=\"center\">\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l02_fault_types.html\">\n        <img src=\"interactive_visualizations/viz__l02_fault_types.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l02_fault_strik_dip_rake.html\">\n        <img src=\"interactive_visualizations/viz__l02_fault_strik_dip_rake.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l02_coulomb_criterion.html\">\n        <img src=\"interactive_visualizations/viz__l02_coulomb_criterion.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n  </tr>\n</table>\n\n<br> Increase principal stresses - try adjusting the sliders so the blue Mohr circle clearly crosses the red failure envelope. Watch the Mohr circle grow until it touches the failure line. You’ll see:\n\nChange material properties - see how stronger materials (higher c, φ) resist failure\n\nCompare different scenarios - weak vs. strong materials under various stress conditions\n\nUnderstand the physics - why normal stress increases shear resistance\n\n---------------------------------------------------------\n\n## Lecture 6: Earthquake Cycle\n\n<table>\n  <tr align=\"center\">\n    <th>Interseismic/Coseismic Ground Displacement!</th>\n    <th>Interseismic-Coseismic-Postseismic Phases!</th>\n    <th>Interactive Spring-Block Earthquake Model!</th>\n    <th>Interactive Recurrence Models with Rate-and-State Friction!</th>\n    <th>Rate-and-State Friction Explorer!</th>\n  </tr>\n  \n  <tr align=\"center\">\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l06_coseismic_calculator.html\">\n        <img src=\"interactive_visualizations/viz__l06_coseismic_calculator.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l06_earthquake_cycle.html\">\n        <img src=\"interactive_visualizations/viz__l06_earthquake_cycle.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l06_spring_block_model.html\">\n        <img src=\"interactive_visualizations/viz__l06_spring_block_model.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l06_timepredictable_slippredictable.html\">\n        <img src=\"interactive_visualizations/viz__l06_timepredictable_slippredictable.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l06_rate_and_state.html\">\n        <img src=\"interactive_visualizations/viz__l06_rate_and_state.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n  </tr>\n</table>\n\n---------------------------------------------------------\n\n## Lecture 7: Fault Rupture\n\n<table>\n  <tr align=\"center\">\n    <th>Far-Field Apparent Rupture!</th>\n    <th>Directivity Explorer!</th>\n    <th>Earthquake Directivity Effects!</th>\n    <th>Earthquake Nucleation Hypotheses!</th>\n    <th>Interactive Rupture Dynamics Explorer!</th>\n  </tr>\n  \n  <tr align=\"center\">\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l07_apparent_rupture.html\">\n        <img src=\"interactive_visualizations/viz__l07_apparent_rupture.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l07_directivity_1.html\">\n        <img src=\"interactive_visualizations/viz__l07_directivity_1.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l07_directivity_2.html\">\n        <img src=\"interactive_visualizations/viz__l07_directivity_2.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l07_nucleation.html\">\n        <img src=\"interactive_visualizations/viz__l07_nucleation.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l07_rupture_dynamics.html\">\n        <img src=\"interactive_visualizations/viz__l07_rupture_dynamics.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n  </tr>\n</table>\n\n<table>\n  <tr align=\"center\">\n    <th>The Haskell Source Model!</th>\n    <th>Interactive Source Time Function (STF) Explorer!</th>\n    <th>Earthquake Source Time Function Calculator!</th>\n  </tr>\n  \n  <tr align=\"center\">\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l07_haskell_source_model.html\">\n        <img src=\"interactive_visualizations/viz__l07_haskell_source_model.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l07_source_time_function_1.html\">\n        <img src=\"interactive_visualizations/viz__l07_source_time_function_1.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l07_source_time_function_2.html\">\n        <img src=\"interactive_visualizations/viz__l07_source_time_function_2.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n  </tr>\n</table>\n\n---------------------------------------------------------\n\n## Lecture 8: Seismic Waves\n\n<table>\n  <tr align=\"center\">\n    <th>Seismic Waves Speed!</th>\n    <th>Seismic Waves Simulator!</th>\n    <th>Seismic Wave Propagation!</th>\n  </tr>\n  \n  <tr align=\"center\">\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l08_seismic_phase_racing.html\">\n        <img src=\"interactive_visualizations/viz__l08_seismic_phase_racing.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l08_seismic_wave_simulator.html\">\n        <img src=\"interactive_visualizations/viz__l08_seismic_wave_simulator.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l08_waves_propagation.html\">\n        <img src=\"interactive_visualizations/viz__l08_waves_propagation.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n  </tr>\n</table>\n\n---------------------------------------------------------\n\n## Lecture 9: Sensors\n\n<table>\n  <tr align=\"center\">\n    <th>Das Basics!</th>\n    <th>DAS Technology: Fiber Optic Seismic Detection!</th>\n    <th>GPS and InSAR!</th>\n  </tr>\n  \n  <tr align=\"center\">\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l09_das.html\">\n        <img src=\"interactive_visualizations/viz__l09_das.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l09_das_simulation.html\">\n        <img src=\"interactive_visualizations/viz__l09_das_simulation.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l09_gps_vs_insar.html\">\n        <img src=\"interactive_visualizations/viz__l09_gps_vs_insar.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n  </tr>\n</table>\n\n---------------------------------------------------------\n\n## Lecture 11: Reading Seismograms\n\n<table>\n  <tr align=\"center\">\n    <th>Seismic Cross-over Distance!</th>\n    <th>Direct Waves vs Head Waves!</th>\n    <th>Lg Wave - Crustally Guided Seismic Wave!</th>\n  </tr>\n  \n  <tr align=\"center\">\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/viz__l11_crossover_distance.html\">\n        <img src=\"interactive_visualizations/viz__l11_crossover_distance.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l11_direct-vs-head-waves.html\">\n        <img src=\"interactive_visualizations/viz__l11_direct-vs-head-waves.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l11_lg_wave.html\">\n        <img src=\"interactive_visualizations/viz__l11_lg_wave.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n  </tr>\n</table>\n\n<table>\n  <tr align=\"center\">\n    <th>Rayleigh Wave Creation and Propagation!</th>\n    <th>Interactive Seismogram Viewer!</th>\n    <th>Travel Time Curves & Seismic Phase Identification!</th>\n  </tr>\n  \n  <tr align=\"center\">\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l11_rayleigh-wave.html\">\n        <img src=\"interactive_visualizations/viz__l11_rayleigh-wave.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l11_seismogram_viewer.html\">\n        <img src=\"interactive_visualizations/viz__l11_seismogram_viewer.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l11_travel-time-curves.html\">\n        <img src=\"interactive_visualizations/viz__l11_travel-time-curves.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n  </tr>\n</table>\n\n---------------------------------------------------------\n\n## Lecture 12: Earthquake Signal Detection\n\n<table>\n  <tr align=\"center\">\n    <th>Amplitude Thresholding Method!</th>\n    <th>STA/LTA Method!</th>\n    <th>Cross-Correlation Basics!</th>\n  </tr>\n  \n  <tr align=\"center\">\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l12_detection_amplitude-threshold.html\">\n        <img src=\"interactive_visualizations/viz__l12_detection_amplitude-threshold.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l12_detection_sta-lta.html\">\n        <img src=\"interactive_visualizations/viz__l12_detection_sta-lta.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l12_detection_cross-correlation.html\">\n        <img src=\"interactive_visualizations/viz__l12_detection_cross-correlation.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n  </tr>\n</table>\n\n<table>\n  <tr align=\"center\">\n    <th>Template Matching Method!</th>\n    <th>Template Matching vs Similarity Search!</th>\n    <th>FAST Method!</th>\n  </tr>\n  \n  <tr align=\"center\">\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l12_detection_template_matched.html\">\n        <img src=\"interactive_visualizations/viz__l12_detection_template_matched.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l12_templateMatching_vs_similaritySearch.html\">\n        <img src=\"interactive_visualizations/viz__l12_templateMatching_vs_similaritySearch.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l12_detection_fast.html\">\n        <img src=\"interactive_visualizations/viz__l12_detection_fast.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n  </tr>\n</table>\n\n<table>\n  <tr align=\"center\">\n    <th>Neural Network Detectors - Basics!</th>\n    <th>Neural Network Detectors - !</th>\n    <th>Method Comparison!</th>\n  </tr>\n  \n  <tr align=\"center\">\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l12_detection_ANN_a.html\">\n        <img src=\"interactive_visualizations/viz__l12_detection_ANN_a.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l12_detection_ANN_b.html\">\n        <img src=\"interactive_visualizations/viz__l12_detection_ANN_b.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n  </tr>\n</table>\n\n---------------------------------------------------------\n\n## Lecture 13: Earthquake Location\n\n<table>\n  <tr align=\"center\">\n    <th>Triangulation Method!</th>\n    <th>Basics of Inverse Problems!</th>\n    <th>Gieger Method!</th>\n    <th>Compare Location Methods!</th>\n    <th>Relative Relocation!</th>\n  </tr>\n  \n  <tr align=\"center\">\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l13_location_triangulation.html\">\n        <img src=\"interactive_visualizations/viz__l13_location_triangulation.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l13_location_inverse-problems.html\">\n        <img src=\"interactive_visualizations/viz__l13_location_inverse-problems.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l13_Location_gieger.html\">\n        <img src=\"interactive_visualizations/viz__l13_Location_gieger.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l13_earthquake_location_methods.html\">\n        <img src=\"interactive_visualizations/viz__l13_earthquake_location_methods.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l13_location_relative_relocation.html\">\n        <img src=\"interactive_visualizations/viz__l13_location_relative_relocation.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n  </tr>\n</table>\n\n---------------------------------------------------------\n\n## Lecture 14: Magnitude Estimation\n\n<table>\n  <tr align=\"center\">\n    <th>Cumulative Seismic Moment!</th>\n    <th>Magnitude Saturation!</th>\n    <th>Interactive Magnitude Scale Comparison!</th>\n    <th>Earthquake Magnitude Scales Comparison!</th>\n    <th>Seismic Moment Calculator!</th>\n  </tr>\n  \n  <tr align=\"center\">\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l14_general_cumulative-seismic-moment.html\">\n        <img src=\"interactive_visualizations/viz__l14_general_cumulative-seismic-moment.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l14_magnitude_saturation.html\">\n        <img src=\"interactive_visualizations/viz__l14_magnitude_saturation.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l14_magnitude_scale_comparison.html\">\n        <img src=\"interactive_visualizations/viz__l14_magnitude_scale_comparison.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l14_magnitude_scales-comparison2.html\">\n        <img src=\"interactive_visualizations/viz__l14_magnitude_scales-comparison2.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l14_seismic_moment_calculator.html\">\n        <img src=\"interactive_visualizations/viz__l14_seismic_moment_calculator.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n  </tr>\n</table>\n\n<table>\n  <tr align=\"center\">\n    <th>Interactive Magnitude Scale Simulator!</th>\n  </tr>\n  \n  <tr align=\"center\">\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l14_magnitude_scale_simulator.html\">\n        <img src=\"interactive_visualizations/viz__l14_magnitude_scale_simulator.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n  </tr>\n</table>\n\n---------------------------------------------------------\n\n## Lecture 15: Quake’s Focal Point\n\n<table>\n  <tr align=\"center\">\n    <th>Focal Mechanism</th>\n    <th>Interactive 3D Fault & Beachball Simulator</th>\n    <th>Interactive Moment Tensor Decomposer</th>\n  </tr>\n  \n  <tr align=\"center\">\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l15_seismology_focal_sphere.html\">\n        <img src=\"interactive_visualizations/viz__l15_seismology_focal_sphere.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l15_3d_fault_beachball_simulator.html\">\n        <img src=\"interactive_visualizations/viz__l15_3d_fault_beachball_simulator.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l15_moment_tensor.html\">\n        <img src=\"interactive_visualizations/viz__l15_moment_tensor.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n  </tr>\n</table>\n\n---------------------------------------------------------\n\n## Lecture 16: Seismic Attenuation\n\n<table>\n  <tr align=\"center\">\n    <th>Interactive Coda-Q Simulator!</th>\n    <th>East-Coast vs West-Coast Attenuation!</th>\n    <th>Seismic Wave Attenuation Simulator!</th>\n  </tr>\n  \n  <tr align=\"center\">\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l16_coda_q_simulator.html\">\n        <img src=\"interactive_visualizations/viz__l16_coda_q_simulator.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l16_east_vest_attenuation.html\">\n        <img src=\"interactive_visualizations/viz__l16_east_vest_attenuation.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l16_eismic_attenuation_simulator.html\">\n        <img src=\"interactive_visualizations/viz__l16_eismic_attenuation_simulator.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n  </tr>\n</table>\n\n---------------------------------------------------------\n\n## Lecture 17: Site Effects and Strong Ground Motion\n\n<table>\n  <tr align=\"center\">\n    <th>Site Amplification: Two Physical Mechanisms!</th>\n    <th>Seismic Wave Effects in Sedimentary Layers!</th>\n    <th>Building Failure Encyclopedia!</th>\n    <th>Interactive ShakeMap Simulator!</th>\n    <th>Site Amplification & Resonance Simulator!</th>\n  </tr>\n  \n  <tr align=\"center\">\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l17__site_amplification.html\">\n        <img src=\"interactive_visualizations/viz__l17__site_amplification.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l17_site_amplification_2.html\">\n        <img src=\"interactive_visualizations/viz__l17_site_amplification_2.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l17_building_failure_encyclopedia.html\">\n        <img src=\"interactive_visualizations/viz__l17_building_failure_encyclopedia.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l17_shakemap_simulator.html\">\n        <img src=\"interactive_visualizations/viz__l17_shakemap_simulator.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l17_amplificaiton_and_building_resonance.html\">\n        <img src=\"interactive_visualizations/viz__l17_amplificaiton_and_building_resonance.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n  </tr>\n</table>\n\n---------------------------------------------------------\n\n## Lecture 18: Earthquake Interactions\n\n<table>\n  <tr align=\"center\">\n    <th>Gutenberg-Richter Law Interactive Tool!</th>\n    <th>Interactive Gutenberg-Richter Law Simulator!</th>\n    <th>Interactive Modified Omori Law Simulator!</th>\n    <th>Modified Omori Law Simulator!</th>\n    <th>Earthquake Stress Triggering Simulator!</th>\n  </tr>\n  \n  <tr align=\"center\">\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l18_gutenberg_richter_tool.html\">\n        <img src=\"interactive_visualizations/viz__l18_gutenberg_richter_tool.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l18_gr_simulator.html\">\n        <img src=\"interactive_visualizations/viz__l18_gr_simulator.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l18__omori_law.html\">\n        <img src=\"interactive_visualizations/viz__l18__omori_law.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l18_omori_law_simulator.html\">\n        <img src=\"interactive_visualizations/viz__l18_omori_law_simulator.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n    <td>\n      <a href=\"https://smousavi05.github.io/EPS55/interactive_visualizations/viz__l18_stress_triggering_simulator.html\">\n        <img src=\"interactive_visualizations/viz__l18_stress_triggering_simulator.png\" alt=\"Interactive Figure Preview\" width=\"200\">\n      </a>\n    </td>\n  </tr>\n</table>\n\n---------------------------------------------------------\n",
        "createdAt": "2025-05-25T01:49:21.000Z",
        "updatedAt": "2025-11-24T15:16:24.000Z",
        "language": "HTML",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/smousavi05/EPS55/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "GilbertoAquino/seismoTK",
        "url": "https://github.com/GilbertoAquino/seismoTK",
        "description": "Seismological Tool Kit build in python for quick pre-processing and processing seismic signals. Convert from ASA2.0 (plain text) format to SAC. Use and implement an easy F-K and Polarization analysis in your studies.",
        "stars": 0,
        "forks": 0,
        "readme": "# seismoTK\nSeismological Tool Kit build in python for quick pre-processing and processing seismic signals. Convert from ASA2.0 (plain text) format to SAC. Quick and easy to use F-K and Polarization analysis.\n",
        "createdAt": "2021-05-28T05:14:12.000Z",
        "updatedAt": "2022-03-03T18:37:27.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/GilbertoAquino/seismoTK/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Akashkharita/Strong_Motion_seismology_assignments",
        "url": "https://github.com/Akashkharita/Strong_Motion_seismology_assignments",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2020-09-18T05:57:56.000Z",
        "updatedAt": "2020-09-18T06:06:39.000Z",
        "language": "MATLAB",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Daniil-Shulgin/ETL_pipeline_Seismology",
        "url": "https://github.com/Daniil-Shulgin/ETL_pipeline_Seismology",
        "description": "ETL-конвейер (Airflow, Python, ClickHouse): API → S3/MinIO → PostgreSQL DWH → Power BI",
        "stars": 0,
        "forks": 0,
        "readme": "# 🌋 ETL-конвейер: API -> MinIO S3 -> PostgreSQL\n\n\n## 📋 Обзор проекта\n\nДанный проект представляет собой ETL-конвейер для ежедневного сбора, обработки и агрегации глобальных сейсмических данных.\n\nОсновная цель — преобразование сырых данных о землетрясениях в структурированные метрики, пригодные для анализа и визуализации. Конвейер построен с акцентом на надежность и идемпотентность.  \n\n## 🛠️ Стек\n\nКонвейер построен на базе Apache Airflow с использованием подхода Data Lake (MinIO) и DWH (PostgreSQL). \n| **Категория** | **Технология** | **Версия** |\n| ----- | ----- | ----- |\n| Оркестрация | Apache Airflow | 2.10.5 |\n| Хранилище RAW | MinIO S3 | RELEASE.2025-02-18T16-25-55Z |\n| Хранилище DWH | PostgreSQL | 13 |\n| Data Mover | ClickHouse | 24.3.6 |\n| Визуализация | Power BI | desktop |\n\n## ⚙️ Инженерные практики\n \nПроект охватывает полный цикл дата-инжиниринга: от настройки инфраструктуры до витрин данных.\n \n1. **Инфраструктура и DevOps:**\n \n    * Явное версионирование.\n        \n    * Воспроизводимость среды(Docker-сompose для настройки Airflow, PostgreSQL, MinIO и ClickHouse).\n     \n    * Конфигурация отделена от кода.\n  \n    * Логирование критически важных операций.\n   \n    * Стабильность и контролируемость:\n    ```bash\n\n    concurrency=1,\n    \n    max_active_tasks=1,\n    \n    max_active_runs=1,\n    \n    ```\n    * Подключения Airflow Connections: необходимо создать подключения postgres_dwh и clickhouse_default.\n    \n    * Ключи Airflow: необходимо добавить ключи от Minio\n \n2. **RAW -> ODS:**\n \n    * Использование ClickHouseOperator для быстрого перемещения данных.\n    \n    * Сохранение сырых данных в Parquet-формате с использованием LZ4-сжатия.\n \n    * Строгая типизация и стандартизация (ODS-слой).\n```bash\nCREATE_TABLE_SQL = f\"\"\"\nCREATE TABLE IF NOT EXISTS {SCHEMA}.{TABLE_NAME} (\n    time date,\n    depth float8,\n    mag_type varchar,\n    nst smallint\n)\n\"\"\"\n```\n3. **Идемпотентность и зависимости:**\n \n    * Использование макросов Airflow ({{ ds }}) в операторах SQL для обработки только одного конкретного дня, изолируя его от других партиций.\n \n    * Использование логики ON CONFLICT (PRIMARY KEY) DO UPDATE SET для предотвращения дублирования записей при повторных запусках DAG.\n    \n    * И конечно управление зависимостями DAG-ов с помощью ExternalTaskSensor.\n \n4. **Агрегация и витрины данных**\n \n    * Формирование отдельных витрин с агрегированными данными для быстрой работы в BI-инструментах.\n",
        "createdAt": "2025-10-31T08:23:22.000Z",
        "updatedAt": "2025-11-29T16:27:11.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Daniil-Shulgin/ETL_pipeline_Seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Dzianddafi/earthquake-viz",
        "url": "https://github.com/Dzianddafi/earthquake-viz",
        "description": "This project is an application of machine learning and data visualization on seismological approach",
        "stars": 0,
        "forks": 0,
        "readme": "# Visualizing Earthquake Location with Python\n\nThe devastating earthquake that struck Turkey on February 6, 2023, had a magnitude of 7.8 and was centered in Nurdağı, near Gaziantep. The tremors caused massive destruction \nin Turkey and Syria, claiming the lives of more than 50,000 people and injuring tens of thousands of others.\n\nMany buildings were destroyed, leaving residents in emergency conditions amidst the winter season. Relief efforts included rescue operations, distribution of food, water, and \npsychosocial support, carried out by various local and international organizations to help survivors cope with the trauma and challenges of life after the disaster.\n\n![turkey eq drawio (1)](https://github.com/user-attachments/assets/26da24e8-a926-478a-9858-d993476e8ef1)\n\nIn this project, the dataset used came from USGS where we can find it easily on their website [https://www.usgs.gov/programs/earthquake-hazards]. The data that I used here\nis earthquake data from January 1st 2020 until January 1st 2024 which located near Turkey and have a minimum magnitude of 5.0. The Anatolian Fault data that I used here is \ngiven by a friend of mine but it also available in Kaggle as of November 2024.\n\n![Screenshot 2025-02-03 at 16 49 09](https://github.com/user-attachments/assets/79e2a862-e7fe-4417-819c-e5151aef44bc)\n\n## Exploratory Data Analysis\nIn this section, a brief EDA is implemented to the data to see an introduction of the data that I used in this project\n\n![Screenshot 2025-02-03 at 17 38 53](https://github.com/user-attachments/assets/b0712a98-f87b-4b24-a4f5-5f3cbf77e553)\n\n### Key takeaways\nThe amount of earthquakes happened during February 6th 2023 to January 1st 2024 is 50 which presents 51.55% of the data being used in this project. Although the span of time is \nmuch shorter, the amount of earthquake that occured during this span of time ( under 1 year ) has already overcome the amount of earthquakes during Janurary 1st 2020 to \nFebruary 6th 2023 ( over 3 years )\n\n## Traditional Machine Learning Model\n\n![Screenshot 2025-02-03 at 16 50 22](https://github.com/user-attachments/assets/5d864c2a-07dd-4e7a-b1c6-fdeda9eb0789)\n\nI used traditional machine learning model for this project where my ML goal is to cluster the earthquake data that are located near the East Anatolian Fault. On February 6th 2023,\na giant earthquake occured and it should have produced a plenty much data on its region. Thus, a high density data should have been made after the earthquake. I Used 3 different\nalgos at first and I figure out that DBSCAN worked better than the others to cluster a such high density kind of data.\n\n![Screenshot 2025-02-03 at 17 02 23](https://github.com/user-attachments/assets/4e8afc48-7cfd-40bc-99f3-7abb71da4787)\n![Screenshot 2025-02-03 at 17 02 49](https://github.com/user-attachments/assets/c2657d7f-2789-4077-b565-72046c2bba83)\n![Screenshot 2025-02-03 at 17 05 47](https://github.com/user-attachments/assets/7a5290af-adaa-4328-9781-d8617226a10d)\n\nAfter finding out that DBSCAN works bettter, I test the epsilon number to see the maximum range that can be clustered as a high density data. I tested 1.3, 1.7, 1.9 for the epsilon\nand it works the best for 1.7 as shown in these figures below.\n\n![Screenshot 2025-02-03 at 17 08 54](https://github.com/user-attachments/assets/223ba3ca-c792-4aa8-97af-f88ba7566c9a)\n![Screenshot 2025-02-03 at 17 09 13](https://github.com/user-attachments/assets/630d5506-f6f8-44ac-898e-4c6ff225df6e)\n\nFrom figures above, we can see that epsilon 1.7 can involved the data inside the green marker (near the East Anatolian Fault) and left out the data inside the red marker \n(far from the East Anatolian Fault). Thus, epsilon 1.7 perform better that 1.3 and 1.9 that underfitted and overfitted respectively.\n\n## Data Visualization\nThe video below is the demo of how this data can be visualized interactively\n\nhttps://drive.google.com/file/d/11Y-bUP5_AcIDhYLfll7CEJu-1jMcK0oP/view?usp=sharing\n\n## Analysis\nThe analysis objective for this project is to find the moving trend of the earthquake\n\n![Screenshot 2025-02-03 at 17 41 44](https://github.com/user-attachments/assets/eb823d17-50f7-470c-9d10-d03c9442a6de)\n\nThe figure above shows that from each data that is labeled by the ML, only 22.22% that occured before the earthquake on February 6th 2023. This conclude that 77.77% data that occured inside the high densed data could be produced by the earthquake since a main earthquake can have a following and still related to it.\n\n![Screenshot 2025-02-03 at 17 42 48](https://github.com/user-attachments/assets/29cfa40a-c59e-4fd3-85a8-813d89bebf11)\n\nThe Figure above described the different location between before and after February 6th 2024. On the left side, we can see that almost every data produced located on the north-east side of the image and the right side described that earthquakes on and anfter February 6th located on the south-west side of the image. This trend can conclude that most likely, the earthquake in this area is moving from north-east to south-west of the image which is located near the East Anatolian Fault.\n\nFigure below shows if the data that occured before and after the giant earthquake shown at the same time. This picture defined that not every earthquake  on the south-west side was labeled as the earthquake that happened on or after the giant earthquake on February 6th 2023.\n\n![Screenshot 2025-02-03 at 17 52 41](https://github.com/user-attachments/assets/ae101036-3a4e-4f86-833c-2eff81830df1)\n\nThis marks as the end of the project, Thanks for looking at this project. Feel free to contact me regarding of this project in order to discuss or provide any further advice.\n",
        "createdAt": "2025-01-28T03:44:10.000Z",
        "updatedAt": "2025-09-08T04:59:36.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Dzianddafi/earthquake-viz/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "IESDMC/fetchTAPS",
        "url": "https://github.com/IESDMC/fetchTAPS",
        "description": "A command line interface for fetch data from Taiwan Archive Platform for Seismology(TAPS)",
        "stars": 2,
        "forks": 0,
        "readme": "# fetchTAPS\n\n# For installing fetchTAPS, just clone this repo and run this command in your shell\n```\npip install -r requirements.txt\n```\n\n# Features\n## fetchWaveform\n```\nusage: fetchWaveform.py [-h] -format format [format ...] -tb time begin\n                        [time begin ...] -te time end [time end ...] -proj\n                        project [project ...] -net network [network ...] -sta\n                        station [station ...] -loc location [location ...]\n                        -cha channel [channel ...] -label label [label ...] -u\n                        email [email ...] -p password [password ...]\n```\n## fetchResponse\n```\nfetchResponse.py [-h] -format format [format ...] -tb time begin\n                        [time begin ...] -te time end [time end ...] -proj\n                        project [project ...] -net network [network ...] -sta\n                        station [station ...] -loc location [location ...]\n                        -cha channel [channel ...] -label label [label ...] -u\n                        email [email ...] -p password [password ...]\n```",
        "createdAt": "2021-03-16T03:35:28.000Z",
        "updatedAt": "2025-09-11T01:14:22.000Z",
        "language": "Python",
        "homepage": "https://taps.earth.sinica.edu.tw",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/IESDMC/fetchTAPS/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "arjundatta23/disp_codes",
        "url": "https://github.com/arjundatta23/disp_codes",
        "description": "A collection of seismological codes imlpementing three array-based techniques for measuring seismic surface wave phase velocity dispersion (multi-mode)",
        "stars": 20,
        "forks": 14,
        "readme": "<<<<<<< HEAD\n# dispersion_codes\nCode accompanying Datta (2019):\n\nDatta, A (2019). On the application of the fk-MUSIC method to measurement of multimode surface wave phase velocity dispersion from receiver arrays. Journal of Seismology, 23(2), 243-260.\n\nRefer to the above for any abbreviations/acronyms used in this README.\n\nThis package consists of three main programs, implementing the three methods discussed in Datta (2019):\n\n1. disp_ParkMethod/implement_park.py: implements the frequency-domain slant stack technique.\n2. disp_fkMUSIC/implement_MUSIC.py: implements the fk-MUSIC technique.\n3. disp_ucd/implement_ucd.py: implements the UC-diagram technique.\n\nThe programs rely on some extraneous modules, which are available as separate repositories:\nhttps://github.com/arjundatta23/array_proc\nhttps://github.com/arjundatta23/signal_proc\nhttps://github.com/arjundatta23/SW1D_earthsr\n\nApart from the three main programs, this package consists of a few supplementary programs or scripts which perform ancillary functions related to storage or presentation of results. These supplementary programs are described in sepaate README files within the \"disp_ucd\" and \"disp_fkMUSIC\" directories.\n\nThis parent README discusses only the three main programs.\n\n**********************************************************************************************\nI. BASIC CODE USAGE\n\nThis package is based on the ObsPy Python framework (https://github.com/obspy/obspy/wiki) and works with SAC-format input seismograms. All three main programs have the same usage format:\n\npython <name of program> <input dir> <file ext>\n\nHere <input dir> is (the path to) a directory containing all the input seismograms (in SAC format) and <file ext> is the extension (along with the leading dot) of the file names for the seismograms. All input seismograms must have the same <file ext>.\n\nHence for instance, to run the frequency-domain slant-stack code on the provided example directory, do\n\npython disp_ParkMethod/implement_park.py example_2010181072232 .LHZ\n\nNote that the input directory \"example_2010181072232\" contains other files apart from seismograms, but the code considers only those files in the directory which end, in this case, with \".LHZ\".\n\nThe fk-MUSIC and UC-diagram codes are run in exactly the same way.\n\n**********************************************************************************************\nII. INTERACTIVE USER PROMPTS\n\nAll three programs take a number of user inputs before/after running their respective algorithms. Here I first describe those that are common to all three and then those of each program individually.\n\nCommon user inputs/prompts:\n(not necessarily consecutive, may be separated by other prompts specific to a particular method)\n\n1. \"Enter frequency range of interest (Hz): \" <f1 f2> f1 and f2 are lower and upper bounds of frequency in Hz. This defines the frequency range in which calculations are performed.\n2. \"Use all stations (y/n): \" This gives the user the option of using all stations/seismograms in <input dir>, or only a subset of them. Enter 'y' if all are to be used. If you enter 'n', you will face the following prompt:\n\t2a. \"Start and end station numbers: \" <x1 x2>. x1 and x2 are integers denoting station numbers, when stations are ordered according to epicentral distance. If number of stations is N, \"1\" denotes the nearest (to source) station in the profile and \"N\" denotes the farthest. Each of x1 and x2 can be any integer in the range 1-N, with x1 less than x2. The ordering of stations is done internally by the code but the user needs to know the numbers of stations (in the ordered profile) he/she wishes to include in the analysis. This can be known using the \"modules_common/seisarray_data.py\" module.\n\nNB: this code is meant to be used with data from roughly linear arrays of stations so that an ordering based on epicentral distance leads to a record section type ordering.\n\n**********************************************************************************************\nIII. USER CONTROLS IN THE SOURCE CODE\n\nwindowing etc.\n",
        "createdAt": "2017-07-28T12:14:11.000Z",
        "updatedAt": "2025-11-25T08:43:57.000Z",
        "language": "PostScript",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/arjundatta23/disp_codes/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "anowacki/Beamforming.jl",
        "url": "https://github.com/anowacki/Beamforming.jl",
        "description": "Global seismic array analysis in Julia",
        "stars": 13,
        "forks": 2,
        "readme": "# Beamforming\n\nBeamforming is a Julia module for the array analysis of seismic\ndata, using [Seis.jl](https://github.com/anowacki/Seis.jl).\n\n## Build status\n[![Build Status](https://github.com/anowacki/Beamforming.jl/workflows/CI/badge.svg)](https://github.com/anowacki/Beamforming.jl/actions)\n[![codecov](https://codecov.io/gh/anowacki/Beamforming.jl/branch/master/graph/badge.svg?token=d0ePcA1m54)](https://codecov.io/gh/anowacki/Beamforming.jl)\n\n\n\n## Installation\n\n```julia\njulia> ] # press ']' to enter pkg mode\n\n(v1.7) pkg> add https://github.com/anowacki/Geodesics.jl https://github.com/anowacki/Seis.jl https://github.com/anowacki/Beamforming.jl\n```\n\nBeamforming.jl requires Julia v1.6 or above.\n\n## Usage overview\n\nThe main functions exported are:\n\n- For traditional beamforming:\n  - `array_response`: Compute the array response function for a set\n    of stations\n  - `beamform`: Compute the beam power across a grid of slowness points\n    (as in f–k analysis)\n  - `vespagram`: Compute a slowness vespagram\n- For cross-correlation beamforming:\n  - `crosscorrelation_array_response`\n  - `crosscorrelation_beamform`\n\nDocstrings are exhaustive, and can be consulted for usage of these\nfunctions.  To bring up docstrings in the REPL, type `?` followed\nby the name of the function and press return.\n\n### Array response\nThe array response for a set of stations can be computed like so:\n```julia\nusing Beamforming, Seis\nt = sample_data(:array); # 60 UK stations\n# Array response parameters\narf = array_response(\n    t,   # Set of traces\n    5,   # Maximum slowness in s/°\n    0.1, # Slowness grid spacing\n    0.1, # Minimum frequency in Hz\n    1,   # Maximum frequency\n    0.1, # Frequency spacing\n    true # `true` for s/° slowness; `false` for s/km\n)\nusing CairoMakie\nplot(arf, powscale=:dB)   # If a Makie backend is installed\n```\n<img src=\"doc/images/array_response.png\" width=\"600\">\n\n### Beamforming\nBeamforming can be performed like so:\n```julia\nbf = beamform(\n    t,\n    1100, # Start time in s\n    1160, # End time in s\n    -1,   # Minimum east slowness in s/°\n    0.5,  # Maximum east slowness\n    0,    # Minimum north slowness\n    4,    # Maximum north slowness\n    0.01  # Slowness spacing\n)\nusing SeisTau       # See note below\nplot(bf, phases=[\"PKiKP\", \"PKIKP\"])\n```\n\n\n<img src=\"doc/images/beamforming.png\" width=\"300\">\n\n### Vespagrams\nCompute a vespagram like so:\n```julia\nvesp = vespagram(\n    t,\n    1110, # Start time in s\n    1150, # End time\n    1,    # Minimum slowness in s/°\n    4,    # Maximum slowness\n    0.01, # Slowness spacing\n)\nplot(vesp, phases=[\"PKiKP\", \"PKIKP\"])\n```\n<img src=\"doc/images/vespagram.png\" width=\"600\">\n\n## Optional extras\n\n### Plotting\n#### Makie.jl\nPlots can be created using the [Makie.jl](https://docs.makie.org/stable/)\necosystem.  To enable this, load a [Makie backend](https://docs.makie.org/stable/explanations/backends/backends) module by doing e.g. `using GLMakie` in\nyour REPL or module.  You will likely need to add this backend as a package\nto your environment, e.g. by doing `import Pkg; Pkg.add(\"GLMakie\").`\n\nThe following plotting functions are provided through Makie:\n- `plot_array_response`\n- `plot_array_response!`\n- `plot_beamforming`\n- `plot_beamforming!`\n- `plot_vespagram`\n- `plot_vespagram!`\n\n`Makie.plot` and `Makie.plot!` are overloaded such that\n`plot(arf_or_bf_or_vesp)` and `plot!(axis, arf_or_bf_or_vesp)` will produce\nthe right plot, whether the result of `array_response`, `beamform` or\n`vespagram` is passed in, as seen above.\n\n#### Plots.jl\nAs an alternative to Makie plotting, Beamforming.jl supports using\n[Plots.jl](https://docs.juliaplots.org/stable/) as a legacy implementation.\nIf you have installed Plots to your environment, then you can easily\ncreate plots as shown above.\n\nFor more help on the Plots.jl plotting commands provided in this package, call up\nthe online help for `Beamforming.plot`:\n\n```julia\nhelp?> Beamforming.plot\n```\n\n### Travel times\nTo plot the predicted location in slowness and time of arrivals,\nyou can install [SeisTau.jl](https://github.com/anowacki/SeisTau.jl).\n(See the [installation notes](https://github.com/anowacki/SeisTau.jl#installation) for more.)\n\nSimply do `using SeisTau`, then the `phases` option to `plot` will\nenable easy predicted phase arrival plotting for `BeamformGrid`s\nand `VespaGrid`s, produced respectively by `beamform` and\n`vespagram`.\n",
        "createdAt": "2020-03-20T21:36:56.000Z",
        "updatedAt": "2025-11-25T12:18:44.000Z",
        "language": "Julia",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/anowacki/Beamforming.jl/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "RyanJamesSchultz/Seismology-Tools",
        "url": "https://github.com/RyanJamesSchultz/Seismology-Tools",
        "description": "A set of helpful Matlab functions in handling earthquake data.",
        "stars": 5,
        "forks": 0,
        "readme": "",
        "createdAt": "2019-08-23T19:22:40.000Z",
        "updatedAt": "2025-06-12T18:56:04.000Z",
        "language": "MATLAB",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "liyche/moment",
        "url": "https://github.com/liyche/moment",
        "description": "seismology",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2016-08-14T11:50:22.000Z",
        "updatedAt": "2016-08-14T11:50:22.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "dmelgarm/pykal",
        "url": "https://github.com/dmelgarm/pykal",
        "description": "Kalman filters for seismology",
        "stars": 1,
        "forks": 0,
        "readme": "",
        "createdAt": "2013-11-05T23:52:23.000Z",
        "updatedAt": "2023-09-20T19:26:52.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/dmelgarm/pykal/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "iGNSS/seis_realtime",
        "url": "https://github.com/iGNSS/seis_realtime",
        "description": "Seismology realtime workflow",
        "stars": 0,
        "forks": 0,
        "readme": "# seis_realtime\n\n[![Actions Status](https://github.com/shakeflow/seis_realtime/actions/workflows/workflow.yml/badge.svg)](https://github.com/shakeflow/seis_realtime/actions)\n[![coverage](https://codecov.io/gh/shakeflow/seis_realtime/branch/main/graph/badge.svg)](https://codecov.io/gh/shakeflow/seis_realtime)\n[![docs](https://img.shields.io/badge/docs-stable-blue.svg)](https://shakeflow.github.io/seis_realtime/)\n[![supported versions](https://img.shields.io/pypi/pyversions/seis_realtime.svg?label=python_versions)](https://pypi.python.org/pypi/seis_realtime)\n[![docs](https://badge.fury.io/py/seis_realtime.svg)](https://badge.fury.io/py/seis_realtime)\n\n\nSeismology realtime workflow\n\n## License\n",
        "createdAt": "2023-07-22T10:00:19.000Z",
        "updatedAt": "2023-07-22T10:00:19.000Z",
        "language": null,
        "homepage": "https://shakeflow.github.io/seis_realtime/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/iGNSS/seis_realtime/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "moonsaint99/PyShot_hiawatha",
        "url": "https://github.com/moonsaint99/PyShot_hiawatha",
        "description": "A bundle of python tools to process active source seismic shot records",
        "stars": 1,
        "forks": 0,
        "readme": "# Active Source Seismic Processing Toolbox\n\nThis toolbox contains a number of python scripts designed to let you take active source shot records stored in Seismic\nUnix format and do amplitude analysis on them in various ways.\n\n## Summary of Contents\n\n* load_segy.py: This can turn a folder of SU files into a dictionary containing ObsPy stream objects, each containing\ntraces corresponding to each trace in the shot record. It includes SU headers. ",
        "createdAt": "2023-10-03T22:42:28.000Z",
        "updatedAt": "2024-08-22T16:27:00.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/moonsaint99/PyShot_hiawatha/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "sergiventosa/ts-PWS",
        "url": "https://github.com/sergiventosa/ts-PWS",
        "description": "Time-scale phase-weighted stack software for seismic signal denoising",
        "stars": 44,
        "forks": 21,
        "readme": "Time-scale phase-weighted stack\n===============================\n\n[![DOI](https://www.zenodo.org/badge/DOI/10.5281/zenodo.1154588.svg)](https://doi.org/10.5281/zenodo.1154588)\n[![License: LGPL v3](https://img.shields.io/badge/License-LGPL%20v3-blue.svg)](https://www.gnu.org/licenses/lgpl-3.0)\n\nSoftware to compute the time-scale phase-weighted stack (ts-PWS), including the \ntwo-stage stack and the unbiased phase coherence strategies ([Ventosa et al., GJI 2017](https://doi.org/10.1093/gji/ggx284)).\n\nThe software packages of [fast phase cross-correlation](https://github.com/sergiventosa/FastPCC) \nand ts-PWS stacking are basic building blocks in the design of efficient \nsignal extraction methods from interstation correlations.\n\nMain features\n-------------\n * Fast: The time-frequency expansion used in the ts-PWS is implemented using frames\n   of wavelets (Morlet and Mexican hat wavelets are available). The most demanding \n   operation is conventionally reading the data.\n * Better filtering: The two-stage stacking and the unbiased phase coherence help\n   to reduce signal attenuation and increase noise attenation.\n * Simple setup: All parameters are automatically set, e.g., when w0, Q or number of \n   cycles is changed the parameters describing the frame are adapted accordingly.\n * Resampling techniques: The bootstrap and jackknife techniques can be applied to \n   build a pull of stacked sequences for a subsequent error analysis.\n\nCompilation\n-----------\nTo compile execute \"make\" in the src directory. Use \"make clean\" to remove \nany previouly compiled code.\n\n * The [Seismic Analysis Code](http://ds.iris.edu/ds/nodes/dmc/software/downloads/sac/) (SAC) is used to read and write sac files.\n * The SACHOME enviorment variable should provide the path to directory where sac is\n   installed. For example, in bash this can be defined as:  \n   export SACHOME=/opt/sac  \n   Change \"/opt/sac\" to your current sac directory if necessary.\n * OpenMP is used to speed up computations. When OpenMP is not available, use \n   make -f makefile_NoOpenMP\".\n\nCompile SAC\n-----------\nThe precompiled sac libraries may not work in some systems/compilers. If you use the gcc \ncompiler on, e.g., Ubuntu you may have errors similar to:\n>  /usr/bin/ld: /opt/sac/lib/sacio.a(getfhv.o): relocation R_X86_64_32 against undefined symbol 'kmlhf' can not be used when making a PIE object; recompile with -fPIC\n\nThis can be solved by compiling the source version of SAC. From the source directory of sac do:\n>  ./configure --enable-optim=2 --prefix=/opt/sac CFLAGS='-march=native -fPIC' \\\n>  make \\\n>  make install\n\nThe key detail here is the -fPIC flag, you can adapt the other options to your needs.\nThe flag --prefix=/opt/sac sets the directory where sac will be installed when doing \n\"make install\". You can change this directory, e.g., to keep using your current \nversion of SAC for other purposes.\n\nWarming up\n----------\n 1. Read ./examples/example.sh\n 2. Execute it, e.g., bash example.sh\n 3. Do ts-pws for the parameters usage.\n \nOrigin of PWS\n-------------\nSchimmel M. & J. Gallart, 2007. Frequency-dependent phase coherence for noise \nsuppression in seismic array data, Journal of Geophysical Research, 112, B04303, \ndoi: [10.1029/2006JB004680](https://doi.org/10.1029/2006JB004680)\n\nSchimmel M. & H. Paulssen, 1997. Noise reduction and detection of weak, coherent \nsignals through phase weighted stacks, Geophysical Journal International, 130, \n497-505, doi: [10.1111/j.1365-246X.1997.tb05664.x](https://doi.org/10.1111/j.1365-246X.1997.tb05664.x)\n\nPaper to be cited\n-----------------\nVentosa S., Schimmel M. & E. Stutzmann, 2017. Extracting surface waves, hum and \nnormal modes: Time-scale phase-weighted stack and beyond, Geophysical Journal \nInternational, 211(1), 30-44, doi: [10.1093/gji/ggx284](https://doi.org/10.1093/gji/ggx284)\n\n2024/06/26 Sergi Ventosa Rahuet (sergiventosa(at)hotmail.com)\n",
        "createdAt": "2017-10-03T10:27:02.000Z",
        "updatedAt": "2025-12-05T01:03:46.000Z",
        "language": "C",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.1154588",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.1154588",
            "dataCite": "10.5281/zenodo.1154588",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/sergiventosa/ts-PWS/master/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.1154588",
            "title": "Time-scale phase-weighted stack",
            "journal": "Zenodo",
            "dateReleased": "2018-01-18T00:00:00.000Z",
            "abstract": "Software to compute the time-scale phase-weighted stack (ts-PWS) using frames of continuous wavelets, including the two-stage stack and the unbiased phase coherence strategies. This stacking method is introduced in Ventosa et al. (GJI, 2017) to extract weak signals from seismic ambient noise.",
            "citationsArray": []
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "petroseskinder/born-bazel",
        "url": "https://github.com/petroseskinder/born-bazel",
        "description": "Born Reverse Time Migration using Bazel build",
        "stars": 0,
        "forks": 1,
        "readme": "\n# Born RTM using Bazel build\n\nThis is my attempt at migrating the Born RTM application to Bazel build. \nInitiated by my frustrations building and working with the Born application. \n",
        "createdAt": "2017-10-07T14:56:37.000Z",
        "updatedAt": "2024-03-04T03:30:43.000Z",
        "language": "HTML",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/petroseskinder/born-bazel/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "geophystech/GroundMotion.rs",
        "url": "https://github.com/geophystech/GroundMotion.rs",
        "description": "A performant, modular Rust library for computing and analyzing ground motion predictions using GMPE models",
        "stars": 0,
        "forks": 0,
        "readme": "# GroundMotion.rs 📈🌏\n\nA fast, parallelized Rust implementation of seismic ground motion prediction\nmodels (GMPEs) with a clean separation between a reusable library\n(`ground-motion-lib`) and a command-line application (`ground-motion-bin`) for\nbatch site-specific predictions.\n\nThis project is built around a modern, scalable Rust workspace structure\ndesigned for both research and operational hazard modeling workflows.\n\n<!-- vim-markdown-toc GFM -->\n\n* [📦 Project Structure](#-project-structure)\n* [Features](#features)\n* [Implemented models](#implemented-models)\n  * [Morikawa & Fujiwara (2013)](#morikawa--fujiwara-2013)\n* [ground-motion-lib](#ground-motion-lib)\n* [ground-motion-bin](#ground-motion-bin)\n* [Full example](#full-example)\n  * [Precompiled Release](#precompiled-release)\n  * [Compile the Binary](#compile-the-binary)\n  * [Get VS30 Input Data](#get-vs30-input-data)\n  * [Run a Test Prediction — Neftegorsk Earthquake (1995)](#run-a-test-prediction--neftegorsk-earthquake-1995)\n  * [Check Output Files](#check-output-files)\n\n<!-- vim-markdown-toc -->\n\n---\n\n## 📦 Project Structure\n\n```text\nGroundMotion.rs/\n├── Cargo.toml              # Workspace manifest\n├── README.md               # This readme\n├── ground-motion-lib/      # Core GMPE library crate\n└── ground-motion-bin/      # Command-line application crate\n```\n\n## Features\n\n⚡ Rayon-powered parallelized ground motion calculations.\n\n📄 CSV-based data loaders and writers for site grids and GMPE outputs.\n\n📊 Statistical summaries of predicted ground motions.\n\n🛠️ Configurable GMPE scenarios via built-in or user-provided configurations.\n\n🔌 Modular, workspace-based Rust project structure.\n\n## Implemented models\n\n### Morikawa & Fujiwara (2013)\n\n* Crustal events: PGA, PGV, PSA at multiple periods.\n* Interplate and intraplate events: Fully supported.\n* ASID (site classification) adjustment support.\n* Predefined GMPE configurations included.\n\n## ground-motion-lib\n\n[![crates.io](https://img.shields.io/crates/v/ground-motion-lib.svg)](https://crates.io/crates/ground-motion-lib)  \n[![docs.rs](https://docs.rs/ground-motion-lib/badge.svg)](https://docs.rs/ground-motion-lib)  \n[![Apache-2.0 License](https://img.shields.io/badge/license-Apache--2.0-blue)](LICENSE)\n\nThe `ground-motion-lib` crate is the core Rust library powering\n`GroundMotion.rs`. It provides a modular, high-performance framework for\nseismic ground motion prediction using GMPE models, featuring a clean,\nextensible design.  Please follow the links above to access the full API\ndocumentation or to browse the source code of this project.\n\n## ground-motion-bin\n\nA command-line interface (CLI) tool for seismic ground motion prediction using\nthe `ground-motion-lib` Rust library.\n\n`ground-motion-bin` allows users to perform ground motion predictions on\nsite-specific VS30 points by leveraging configured GMPE models and earthquake\nscenarios directly from the terminal.\n\nWorkflow:\n\n* Load VS30 site points from CSV files\n* Select from preconfigured GMPE models or specify custom configuration files (future)\n* Define earthquake parameters (location, depth, magnitude) via CLI\n* Perform parallelized ground motion predictions\n* Export prediction results as CSV files\n* List available GMPE models or display their configuration details\n\nCLI Arguments:\n\n```bash\nground-motion-bin -h\nInput command line arguments\n\nUsage: ground-motion-bin [OPTIONS] <--in-file <IN_FILE>|--list-configs|--show-config <SHOW_CONFIG>>\n\nOptions:\n  -i, --in-file <IN_FILE>\n          Input VS30 CSV file containing site data\n  -u, --use-config <USE_CONFIG>\n          Use a predefined GMPE configuration by name\n  -c, --custom-config <CUSTOM_CONFIG>\n          Provide a custom GMPE configuration TOML file\n  -e, --earthquake <lon> <lat> <depth> <magnitude>\n          Earthquake parameters e.g. --earthquake 141.1 50.2 10.0 4.5 (Mw assumed)\n  -o, --out-file <OUT_FILE>\n          Output CSV file to write computed GMPE values [default: out_gmpe_grid.txt]\n  -d, --delimeter <DELIMETER>\n          Delimiter character for input and output CSV files [default: \"\\t\"]\n  -l, --list-configs\n          List all available GMPE configurations\n  -s, --show-config <SHOW_CONFIG>\n          Show details of a specific GMPE configuration by name\n  -h, --help\n          Print help (see more with '--help')\n  -V, --version\n          Print version\n```\n\n## Full example\n\n### Precompiled Release\n\nYou can download a precompiled binary for your platform from the\n[Releases](https://github.com/geophystech/GroundMotion.rs/releases) page.\n\n| Platform              | Binary                                             |\n|:---------------------|:---------------------------------------------------|\n| Linux (x86_64, musl)  | `ground-motion-bin-x86_64-unknown-linux-musl`      |\n| macOS (x86_64)        | `ground-motion-bin-x86_64-apple-darwin`            |\n| Windows (x86_64)      | `ground-motion-bin-x86_64-pc-windows-msvc.exe`     |\n\nIf you prefer, you can also compile the binary from source — see the\ninstructions in the next section.\n\n### Compile the Binary\n\nClone the repo:\n\n```bash\ngit clone https://github.com/geophystech/GroundMotion.rs.git .\ncd GroundMotion.rs\n```\n\nInstall Rust for your OS if you haven’t already, then build the project:\n\n```bash\ncargo build --release\n```\n\nCheck that the CLI is working:\n\n```bash\ntarget/release/ground-motion-bin -h\n```\n\n### Get VS30 Input Data\n\nDownload GMT `grd` file from the [USGS Vs30 Models and Data\npage](https://earthquake.usgs.gov/data/vs30/).\n\n```bash\ncurl -O https://apps.usgs.gov/shakemap_geodata/vs30/global_vs30.grd\nls -lh global_vs30.grd\n# Example output:\n# -rw-r--r-- 1 user user 582M May 19 23:52 global_vs30.grd\n```\n\nConvert the grd file into a tab-delimited XYZ text file using\n[GMT2XYZ](https://www.soest.hawaii.edu/gmt/gmt/html/man/grd2xyz.html) from\n[GMT](https://www.soest.hawaii.edu/gmt/):\n\n```bash\n# Example: extract Sakhalin region\ngmt grd2xyz global_vs30.grd -R139.0/146.0/39.0/56.0 > test_sakh_vs30.txt\n\n# Check number of rows\ncat test_sakh_vs30.txt |wc -l\n# 1716481\n\n# Preview file content\nhead test_sakh_vs30.txt\n```\n\nThe output file will be tab-delimited by default.\n\n### Run a Test Prediction — Neftegorsk Earthquake (1995)\n\nUse the [1995 Neftegorsk\nearthquake](https://en.wikipedia.org/wiki/1995_Neftegorsk_earthquake) as a test\ncase.\n\nFirst, list available GMPE configurations and inspect one:\n\n```bash\n# List configs\ntarget/release/ground-motion-bin -l\n\n# Show details of a specific config\ntarget/release/ground-motion-bin -s config_mf2013_crustal_pga\n```\n\nNow, run a PGA prediction using the `config_mf2013_crustal_pga` config:\n\n```bash\ntarget/release/ground-motion-bin -i test_sakh_vs30.txt \\ \n  -u config_mf2013_crustal_pga --earthquake 142.83 52.63 11.0 7.1 \\\n  -o neftegorsk_pga.txt\n```\n\nExample stdout:\n\n```text\nUse test_sakh_vs30.txt as input grid...\nUse config MF2013 {\n    mw0: 8.1,\n    a: 0.5507,\n    b: -0.004531,\n    c: 0.4631,\n    d: 0.006875,\n    e: 0.5,\n    sigma: 0.377556,\n    pd: 0.0663,\n    dl_min: 100.0,\n    d0: 250.0,\n    ps: -0.3709,\n    vs_max: 1950.0,\n    v0: 350.0,\n    gamma: 7.602e-5,\n    asid: false,\n    motion_kind: Pga,\n}\nUse Earthquake with parameters Earthquake {\n    lon: 142.83,\n    lat: 52.63,\n    depth: 11.0,\n    magnitude: 7.1,\n    magnitude_kind: Mw,\n}\nStats for out grid:\nStats {\n    mean: 0.9098975207980508,\n    std_dev: 3.5239501645854148,\n    min: 1.270494568926633e-7,\n    max: 68.434148866177,\n    median: 0.006957938777551972,\n}\nWrite gmpe points to neftegorsk_pga.txt...\nDone\n```\n\nThe operation should complete in less than a second on modern CPUs, including\nIO.\n\n### Check Output Files\n\nConfirm the output content and number of points::\n\n```bash\nhead test_sakh_vs30.txt\n\ncat test_sakh_vs30.txt |wc -l # Should match input grid size\n```\n\nTip:\n\nYou can also extend the base `vs30` file with additional `dl` (distance) and\n`xvf` (optional site variable factor) columns. The tool will automatically\nhandle those extra columns during predictions.\n",
        "createdAt": "2025-05-25T13:03:14.000Z",
        "updatedAt": "2025-05-27T09:53:58.000Z",
        "language": "Rust",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/geophystech/GroundMotion.rs/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "AI4EPS/EPS207_Observational_Seismology",
        "url": "https://github.com/AI4EPS/EPS207_Observational_Seismology",
        "description": null,
        "stars": 19,
        "forks": 8,
        "readme": "",
        "createdAt": "2023-08-16T05:39:42.000Z",
        "updatedAt": "2025-09-23T09:44:08.000Z",
        "language": "Shell",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "moshebeutel/summer_2022_Seismology",
        "url": "https://github.com/moshebeutel/summer_2022_Seismology",
        "description": "This repo descibes my developments and stuff I wrote during a summer internship at the GFZ.",
        "stars": 1,
        "forks": 1,
        "readme": "# summer_2022_Seismology\nThis repo descibes my developments and stuff I wrote during a summer internship at the GFZ.\n\n## Notebooks \nThe following notebooks form a pipeline to create databases that help measure model adaptation to noise\n1.  Create high SNR traces - <a target=\"_blank\" href=\"https://colab.research.google.com/github/moshebeutel/summer_2022_Seismology/blob/main/notebooks/create_high_snr_traces_dataset.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n2.  Create a noisy dataset - <a target=\"_blank\" href=\"https://colab.research.google.com/github/moshebeutel/summer_2022_Seismology/blob/main/notebooks/create_noisy_dataset.ipynb\">\n  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n3.  Model Evaluation -  <a target=\"_blank\" href=\"https://colab.research.google.com/github/moshebeutel/summer_2022_Seismology/blob/main/notebooks/model_evaluation.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n",
        "createdAt": "2022-09-06T06:45:37.000Z",
        "updatedAt": "2023-01-19T08:21:15.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/moshebeutel/summer_2022_Seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "enjuichang/PSD-lockdown",
        "url": "https://github.com/enjuichang/PSD-lockdown",
        "description": "Calculation of PSD of seismic waves in order to measure the impact of culture noise during the COVID-19 pandemic.",
        "stars": 6,
        "forks": 0,
        "readme": "# PSD lockdown\n\nPower spectral density (PSD) is a common methodology to measure the energy of waves. This project focuses on the calculation of PSD of seismic waves in order to measure the impact of culture noise during the COVID-19 pandemic according to \"*Global quieting of high-frequency seismic noise due to COVID-19 pandemic lockdown measures*\" from Lecocq et al. (2020).\n\nInputs to the project include daily miniseed waveform file, instrument response file, station list, and duration of the project, which can be downloaded easily via various sites. In this project, GDMS database from the Central Weather Bureau (CWB) was used to measure the impact of soft lockdown measures starting from mid-May.\n\nThe outputs include a CSV file including all PSD values and a daily average along with the corresponding `matplotlib` datetime values and a PNG file plotted using matplotlib.pyplot.\n\n\n# Setup\n\nTo clone the repository:\n```sh\ngit clone https://github.com/enjuichang/PSD-lockdown.git\n```\n\nAfter cloning the repository, there are some CSV files that contain calculated PSD of each station to work with if your interest is in visualization. If the goal is to understand the complete data pipeline, please follow the next steps.\n\n## Data Pipeline Overview\n\n<img src=\"./Readme_figs/data_pipeline_viz.png\" alt=\"Data pipeline visualization\" width=\"800\" title=\"Data Pipeline\">\n\nThe figure above shows the data pipeline contained in this repository. The goal of the repository is to use Python tools to process the seismic waveforms into useful information for the utilization of subsequent data visualization tools (e.g., Tableau). However, there is one additional step required that is not included in this image -- the retrieval of the ms_DATA files. Please follow the next section to obtain the files.\n\n## Retrieve Raw Files\n\nHere is the list of raw files that are required to execute seismic noise analysis:\n\n1. **miniSEED**:MiniSEED files contain the time series of the seismic waveform, which is then used to calculate the PSD and further analysis.\n\n2. **RESP**: RESP files contain the instrument responses that are used to adjust the waveforms that are highly susceptible to instrumental bias.\n\nThe easiest way to collect this information is through [Taiwan Geophysical Database Management System (GDMS)](https://gdmsn.cwb.gov.tw/index.php) website from the Central Weather Bureau of Taiwan. The following steps are required to download the miniSEED files listed above:\n\n1. **Login/Sign up**: Please log into the GDMS website to access raw files.\n2. **Select Data**: After logging in, go to the \"*Data*\" tab and choose \"*Continous Waveform Data*\". You should see the following page once you have successfully entered the correct system.\n\n<img src=\"./Readme_figs/gdms_interface.png\" alt=\"GDMS data download page\" width=\"800\">\n\n3. **Select Configuration**: This interface shows the data format available, select the appropriate data and submit.\n    - *Output format*: Select miniSEED. Sometimes SEED files are available, but SEED files contain additional meta-information of the data that is unnecessary in this application.\n    - *Network*: Select CWBSN. Please find the network of the seismic stations associated with your station of interest.\n    - *Station*: Input the station name\n    - *Location*: Select 00 or 10. If the station has seismometers at different depths, it is possible to select different seismometers individually.\n    - *Channel*: Select HHZ. Select the appropriate channel of interest on the seismometers.\n    - *Start Time/End Time*: Select the appropriate start and end time of the data.\n    - *Label*: Please enter Station_startYearMonthDay_endYearMonthDay (e.g., BAC_20200601_20200605) for easier processing in the later procedure.\n\n4. **Submit**: Submit the form and wait for the file.\n\nFor the instrument response files, please follow these steps below:\n\n1. **Login/Sign up**: Please log into the GDMS website to access raw files.\n2. **Select Data**: After logging in, go to the \"*Data*\" tab and choose \"*Instrument Response*\". You should see a similar interface as the miniSEED but with different fields.\n\n3. **Select Configuration**: This interface shows the data format available, select the appropriate data and submit.\n    - *Output format*: Select RESP file.\n    - *Network*: Select CWBSN. Please find the network of the seismic stations associated with your station of interest.\n    - *Station*: Input the station name\n    - *Location*: Select 00 or 10. If the station has seismometers at different depths, it is possible to select different seismometers individually.\n    - *Channel*: Select HHZ. Select the appropriate channel of interest on the seismometers.\n    - *Start Time/End Time*: Select the appropriate start and end time of the data.\n    - *Label*: No need to input.\n\n4. **Submit**: Submit the form and wait for the file.\n\nNow you have successfully retrieved the seismic waveform data from an open-source database.\n\n### One last step\n\nOne additional step is needed before inputting the miniSEED data into the files. As ObsPy package cannot handle data larger than 1 month, the unit for our miniSEED files is \"1 day\". Therefore, it is necessary to run '`cut_ms.py`' in the `src` folder with your miniSEED file as an input. Here is a simple way to do it using command-line tools:\n\n```sh\npython3 ./src/cut_ms.py [miniSEED Filename (e.g., BAC_20200601_20200605.miniSEED)]\n```\n\nThis should automatically generate the appropriate files in the `ms_DATA` folder as the format of Network_Station_YearMonthDay.ms (e.g., CWB24_HSN_20210401.ms), which is required for running all other code.\n\n\n# Implementation\n\n## Data processing\nFollowing the data pipeline visualization, two files are used to conduct the steps.\n\n1. For the **calculation of the from the waveforms**, this is conducted using `src/Caculate_PSD.py` file. To be exact, the following command line script runs this file:\n\n```sh\npython3 ./src/Caculate_PSD.py TAP 1/9,1/11\n```\n\nThe first argument after the file names refers to the station name. In this case, the station of interest is the \"TAP\" station. The second argument is the selected period list. In this code, it is possible to calculate the PSD values of any frequency range. One would only need to specify the periods of interest and separate them by commas (,).\n\nThe output of this file is a PNG figure with the raw PSD values plotted out and a CSV file generated for the specific configurations given.\n\n2. For **averaging and normalizing time series**, this is conducted using `src/draw_avg.py` file. To be exact, the following command line script runs this file:\n\n```sh\npython3 ./src/draw_avg.py TAP_full.csv\n```\n\nThe argument after the filename is the CSV file that contains all the raw PSD values from the previous file. This Python code outputs the 1-day, 3-day, and 7-day averages.\n\n## Data visualization\n\nFor the visualizations, the `discontinuity.py`, `heatmap.py`, `mobility_corr.py`, and `multi_station.py` can create appropriate visualizations using the generated averaged PSD values from `draw_avg.py`.\n\n## Data transformation\n\nFor data transformations, `merge_csv_tableau.py` creates two CSV files that join the CSVs of different stations into one large table. This is necessary to avoid connecting 40+ CSVs in the connection procedure when working in Tableau.\n\n# Notes\n\n## Script folder\n\nThe script folder in the repository refers to running the repository using shell scripts. This was included as I ran the code mostly using a remote server. Especially when calculating the PSD values for 2 years, the computation time needed is on the basis of hours; therefore, enabling offline running on a remote server was implemented using the `nohup` command on a Linux server. All of the scripts can be conducted using only Python code described in the \"Implementation\" section above.\n\n# Structure\n\n```\n│\n├── README.md          <- The top-level README for developers using this project\n├── Readme_figs        <- Figures in the README files\n│\n├── EVTLST             <- Event list files that contain all dates of interest.\n│\n├── STALST             <- Station list files that contain all stations of interest.\n│\n├── ms_DATA            <- Preprocessed miniSEED files that contain 1 day of waveform each\n│\n├── CSV                <- All generated CSV files, including PSD values and transformed data for Tableau\n│\n├── PNG                <- Generate visualization from the repository\n│\n├── RESP               <- Instrument response files for each station\n│\n├── SCRIPT             <- Serialized shell scripts that were used for remote computing\n│   ├── M00_run.sh              <- Run the data pipeline from retrieval to visualization\n│   ├── M01_Calculate_psd.sh    <- Run only the calculation of PSD from miniSEED files\n│   ├── M02_Calculate_avg.sh    <- Run only the averaging of PSD from PSD CSVs\n│   ├── PRE_...                 <- Additional preprocessing scripts from generating events or RESPs\n│   └── POST_...                <- Additional post-processing visualization scripts from CSVs\n│\n├── src                <- Python codes for calculating the PSD, generating visualization, and inference\n│   ├── Calculate_PSD.py        <- Calculate the PSD from miniSEED files\n│   ├── cut_ms.py               <- Cut downloaded miniSEED files to appropriate date formats\n│   ├── discontinuity.py        <- Run discontinuity analysis on CSVs\n│   ├── draw_avg.py             <- Calculate the rolling averages for raw PSD values\n│   ├── draw_avg_spect.py       <- Calculate the rolling averages for raw PSD values over different frequencies\n│   ├── heatmap.py              <- Generate heatmaps from selected stations\n│   ├── merge_csv_tableau.py    <- Transform data to format suitable for Tableau\n│   ├── mobility_corr.py        <- Preprocess the Apple/Google mobility index and find correlations\n│   └── multi_station.py        <- Generate line graphs with multiple stations\n│\n└─── requirements.txt  <- The requirements file for reproducing the analysis environment.\n\n```\n",
        "createdAt": "2021-08-07T07:24:45.000Z",
        "updatedAt": "2024-11-03T04:52:45.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/enjuichang/PSD-lockdown/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "jmsaurel/phaseworm",
        "url": "https://github.com/jmsaurel/phaseworm",
        "description": "A Python wrap-up of PhaseNet for use with EarthWorm",
        "stars": 20,
        "forks": 3,
        "readme": "# PhaseWorm\n\nThis python script wraps-up [PhaseNet](https://github.com/wayneweiqiang/PhaseNet)\nfor use as a picker within an [Earthworm](http://www.earthwormcentral.org/)\ninstallation. For manual validation of the results, we recommend to use [SeisComP](http://www.seiscomp.de) or [SeisComP3](http://www.seiscomp.de/seiscomp3) software to build a catalog of the events with magnitudes.\n\n<!-- Saurel, J.-M., Retailleau, L., Zhu, W., Issartel, S., Satriano, C., and Beroza, G. C.: Implementation of a new real time seismicity detector for the Mayotte crisis, EGU General Assembly 2021, online, 19–30 Apr 2021, EGU21-10646, [10.5194/egusphere-egu21-10646](https://doi.org/10.5194/egusphere-egu21-10646), 2021. -->\n\nRetailleau, L., Saurel, J.-M., Zhu, W., Satriano, C., Beroza, G. C, Issartel, S., Boissier, P., OVPF Team and OVSM Team (2022). A wrapper to use a machine-learning-based algorithm for earthquake monitoring, Seismological Research Letter, **XX**, 1-9, [10.1785/0220210279](https://doi.org/10.1785/0220210279).\n\n![flow-chart](/doc/flow-chart.png)\n_\nPhaseWorm can access data from 4 differents data sources, using [ObsPy](https://www.obspy.org) clients:\n\n* SeedLink\n* FDSN webservice\n* Earthworm WaveServerV\n* SDS disk file archive\n\nPicks can be written as Earthworm **TYPE\\_PICK\\_SCNL** messages files or simply\nwritten to the standard output.\n\nPhaseWorm can run in an infinite loop in an almost real-time manner\nand can also process (replay) old data.\n\n**PhaseNet** identifies P and S arrivals and **PhaseWorm** binds them to,\nrespectively, vertical and horizontal channels.\n\n---\n## Installation and configuration\n\nThe _PhaseWorm_ code has been developed with **Python 3.8**,\n**ObsPy 1.2.2** and **Earthworm v7.10**.\nThe _PhaseNet_ included version uses **TensorFlow 2**.\n\n*Note that TensorFlow 2 is currently not compatible with Python 3.9.*\n\n\n### PhaseWorm\n\nDownload PhaseWorm by cloning the repository into your local installation\ndirectory.\n\n\tgit clone https://github.com/jmsaurel/phaseworm.git\n\n\n#### Preparing your environment\n\nA python environment with proper libraries and modules is necessary to run\nPhaseWorm and the underlying PhaseNet.\n\n##### Using Anaconda (recommended)\n```\nconda create --name phaseworm python=3.8\nconda activate phaseworm\nconda install tensorflow=2.3\nconda install matplotlib scipy numpy\nconda install obspy -c conda-forge\n```\n##### Using virtualenv\n```\npip install virtualenv\nvirtualenv .phaseworm\nsource .phaseworm/bin/activate\n```\n\n#### Installing PhaseWorm\n\nYou can install _PhaseWorm_ using `pip` from the main directory:\n\n\tpip install .\n\nOr, in \"developer mode\":\n\n\tpip install -e .\n\n\n\n### Earthworm\n_PhaseWorm_ is designed to output **TYPE\\_PICK\\_SCNL** Earthworm messages files\nthat will be read by the _file2ew_ module and injected into a RingBuffer.\n\nBecause _PhaseNet_ is designed to use overlapping time sequences,\nit is recommended to use a _pkfilter_ module to remove duplicated picks.\n\n#### file2ew\n\nPhaseWorm pick file should be looked for regularly by the _file2ew_ module to be\ninjected in the correct RingBuffer.\nWe recommend a scan every 0.5 second and no logging extensive logging.\nPick files are written by PhaseWorm with the file extension **.pick**.\n\nBecause pick files are written, and then a hard symlink made to the _file2ew_\n deposit directory, it's recommended to not activate the _SaveDataFiles_\n option.\n\nThe _SuffixType_ option should be configure with the InsitutionID used in the\n Earthworm setup.\n\n```\nCheckPeriod     0.5                # sleep this many seconds between looks\nOpenTries       2                  # How many times we'll try to open a file\nOpenWait        100                # Milliseconds to wait between open tries\nSaveDataFiles   0                  # 0 = remove files after processing\nLogOutgoingMsg  0\nSuffixType  .pick     TYPE_PICK_SCNL       INST_xxx\n```\n\n#### pkfilter\n\nPhaseNet picks predictions are equally accurate when working on overlapping\ndata windows. We recommend a value of **0.05** seconds for _PickTolerance_.\n\nWhen working with 30s timewindows, a pick could be detected only on the next\noverlapping time window. We then allow **60s** _OlderPickLimit_ to be checked.\n\n```\nPickTolerance  0.05\nOlderPickLimit 60\n```\n\n#### binder_ew\n\n_Binder_ew_ module should be setup as usual, according to the local area\nsettings. However, to take full advantage of the **P** and **S** phase\nidentifications from _PhaseNet_, two parameters should be activated:\n* no_S_on_Z (only _PhaseNet_ **P** readings are sent on vertical channels)\n* no_P_on_Horiz (only _PhaseNet_ **S** readings are sent on horizontal channels)\n\nSince **S** phases are sent to horizontal channel, binder_ew shouldn't be\nallowed to initiate the stack on horizontal channels.\n\nCurrently, PhaseNet doesn't output any amplitude pick measurements. It's\nthen recommended not to use the **S** to **P** amplitude ratio setting.\n\n```\n# stack_horizontals\n# s_to_p_amp_ratio 2.0\nno_S_on_Z\nno_P_on_Horiz\n```\n\n#### eqassemble\n\nBecause _PhaseNet_ picks **S** readings, it's important to activate them in _eqassemble_ module.\n\n```\nReportS 1\n```\n\n---\n## Usage\nBy default, PhaseWorm will use the configuration from the **config.cfg** file.\n\n\tphaseworm\n\nAlternative configuration file can be passed on the command line.\n\n\tphaseworm --config-file /my/path/to/my_config.cfg\n\nThe configuration file is divided into 3 different sections.\n\n### Earthworm section\nThis section defines variables linked to the Earthworm setup that will be used\nto process the pick messages (`module_id`, `inst_id`, `message_id`).\nThis section also defines the directory in which **TYPE\\_PICK\\_SCNL** messages\nwill be written to.\n\n### PhaseNet section\nThis section defines PhaseNet related variables.\nCurrently there is only one available variable: the neural network training\nfile to use.\nThe recommended default is to use the file shipped with PhaseNet and\nincluded in this bundle.\n\nIf you have trained PhaseNet on your own data, you might want to use\nyour training file.\n\n### General section\nThe general section contains various variables to set-up _PhaseWorm_:\n* data source\n* time window lenght to process\n* network.stations to look for\n* channels priority list to look for each station\n* **replay** or **real-time** mode\n* whether to activate or not the **TYPE\\_PICK\\_SCNL** message writing.\n\n---\n## Examples\nTwo configuration examples are shipped: `config_replay.cfg` and `config_rt.cfg`.\n\n### Replay mode\nThe file `config_replay.cfg` contains a configuration for replay use.\n\nPhaseWorm reads data from FDSN webservice, starting from xxx until xxx.\n\n### Real-time mode\nThe file `config_rt.cfg` contains a configuration for near real-time use.\n\nPhaseWorm reads data from an Earthworm WaveServerV (recommended) or SeedLink.\n\nPhaseWorm write picks in the **TYPE\\_PICK\\_SCNL** directory.\n\n---\n## Acknowledgements\n_PhaseNet_ code was simplified on purpose by Weiqiang Zhu.\n\nData preprocessing was designed and implemented by Lise Retailleau.\n\nOverall wrapping was done by Jean-Marie Saurel.\n\nClaudio Satriano provided extensive code review.\n\nTesting was performed on Mayotte seismo-volcano crisis for the\nRéseau de surveillance volcanologique et sismologique de Mayotte: [REVOSIMA](\nhttp://www.ipgp.fr/revosima).\n",
        "createdAt": "2021-01-20T16:54:45.000Z",
        "updatedAt": "2025-03-04T14:23:14.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "10.5194/egusphere-egu21-10646",
            "openCitations": "10.5194/egusphere-egu21-10646",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/jmsaurel/phaseworm/master/README.md",
        "mainPaper": {
            "doi": "10.5194/egusphere-egu21-10646",
            "title": "PhaseWorm",
            "dateReleased": "2021-08-25T00:00:00.000Z",
            "abstract": ""
        },
        "repoDoi": "10.5194/egusphere-egu21-10646",
        "publications": [
            {
                "doi": "10.5194/egusphere-egu21-10646",
                "name": "PhaseWorm",
                "source": "",
                "authorNames": [],
                "abstract": "",
                "publicationDate": "2021-08-25T00:00:00.000Z"
            }
        ]
    },
    {
        "source": "GitHub",
        "name": "ROBelgium/VLPGreenland",
        "url": "https://github.com/ROBelgium/VLPGreenland",
        "description": "Accompanying repository for \"A rockslide-generated tsunami in a Greenland fjord rang Earth for 9 days\" by Svennevig et al. 2024",
        "stars": 4,
        "forks": 0,
        "readme": "# A rockslide-generated tsunami in a Greenland fjord rang Earth for 9 days\n\n[![DOI](https://zenodo.org/badge/724008094.svg)](https://zenodo.org/doi/10.5281/zenodo.10449632)\n\nAbstract: Climate change is increasingly predisposing polar regions to large landslides. Tsunamigenic landslides have occurred recently in Greenland, but none have been reported from the eastern fjords. In  September 2023, we detected the start of an unprecedented up to 9-day-long global 10.88mHz (92s) monochromatic very-long period (VLP) seismic signal, originating from East Greenland. We demonstrate how this event started with a 25 M m3 glacial thinning-induced rockslide plunging into Dickson Fjord, triggering a 200 m high tsunami. Simulations show the tsunami stabilized into a 7 m-high long-duration seiche with a near-identical frequency (11.45mHz) and slow amplitude decay as the seismic signal. An oscillating, fjord-transverse single-force with a maximum amplitude of 5×1011 N reproduces the seismic amplitudes and their radiation pattern relative to the fjord, demonstrating how a seiche directly caused the 9-day long seismic signal. Our findings highlight how climate change is causing cascading, hazardous feedbacks between the cryosphere, hydrosphere and lithosphere.\n\nThese results are published in Science ([doi:10.1126/science.adm9247](https://doi.org/10.1126/science.adm9247)).\n\n## This repository\n\nThis repository contains codes used in our analysis and information required to prepare the figures in the manuscript and supplementary material.\n\nThe repository follows the structure of the manuscript, where codes and/or instructions and data sources are provided for each figure and its subplots:\n\n`fig1\\`: Seismic signal, position, and local setting\n\n`fig2\\`: Landslide observations.\n\n`fig3\\`: Landslide seismic signal and modelling.\n\n`fig4\\`: Tsunami observations and modeling\n\n`fig5\\`: VLP seismic signal and comparison with simulated seiche\n\n`supplementary\\`: Figures in the supplementary materials.\n\n`bathymetry\\`: Detailed bathymetry of the Kempe Fjord area.\n\n`seiche\\`: input parameters and output time series for the HySEA tsunami simulation that shows a fjord seiche\n\n## Local tide gauge data\n\nCTD data are available from an associated repository: [https://www.vliz.be/nl/imis?dasid=8425&doiid=921](https://www.vliz.be/nl/imis?dasid=8425&doiid=921).\n",
        "createdAt": "2023-11-27T07:54:33.000Z",
        "updatedAt": "2025-02-24T06:41:55.000Z",
        "language": "Jupyter Notebook",
        "homepage": "https://doi.org/10.1126/science.adm9247",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.10449632",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.10449632",
            "dataCite": "10.5281/zenodo.10449632",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ROBelgium/VLPGreenland/main/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.10449632",
            "title": "ROBelgium/VLPGreenland: Accepted version",
            "journal": "Zenodo",
            "dateReleased": "2024-08-05T00:00:00.000Z",
            "abstract": "Accompanying repository for \"A rockslide-generated tsunami in a Greenland fjord rang the Earth for 9 days\" by Svennevig et al.",
            "citationsArray": []
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "TomSear01/PINNs",
        "url": "https://github.com/TomSear01/PINNs",
        "description": "All codes used for the research project 'Exploring Physics Informed Neural Networks in Seismology'",
        "stars": 0,
        "forks": 0,
        "readme": "# PINNs\nAll codes used for the research project 'Exploring Physics Informed Neural Networks in Seismology'\n",
        "createdAt": "2023-04-15T15:46:24.000Z",
        "updatedAt": "2023-04-15T15:51:37.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/TomSear01/PINNs/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "giladoo/sd_seismology",
        "url": "https://github.com/giladoo/sd_seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2025-07-03T22:39:02.000Z",
        "updatedAt": "2025-07-07T19:58:50.000Z",
        "language": "JavaScript",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "imitelis/seismology-with-py",
        "url": "https://github.com/imitelis/seismology-with-py",
        "description": "Selected codes from that Seismology course, Spring 2021",
        "stars": 2,
        "forks": 0,
        "readme": "",
        "createdAt": "2022-09-23T04:00:47.000Z",
        "updatedAt": "2024-12-02T03:29:19.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "lbrotherson/PhD",
        "url": "https://github.com/lbrotherson/PhD",
        "description": "Codes written for PhD work in seismology",
        "stars": 0,
        "forks": 0,
        "readme": "# PhD\n*Codes written for PhD work in seismology*\n*Louisa Brotherson - 03/01/23.*\n*This file lists the Matlab2021b codes used to process seismic and mechanical data for my PhD at the University of Liverpool.*\n1. find_stickslip_v4.m: function which uses short-term average, long-term average (STA-LTA) algorithm to find large stress drops, which indicate the start of stick-slip (lab-generated earthquake). \n2. stick-slip_process.m: code to process mechanical Labview data (force, displacement, time, normal stress etc.) to calculate stress drops, coefficient of friction and slip distance. Reads from and writes to files, makes data visualisations of choice. Calculates stastical measures for data analytics\n3. read_atf_acoustic.m: function to read in multiple Excel files from an inputted filepath into cell called \"celldata\" and return filepath, name of file andextension separately. Used to mine seismic data .atf files, which are large, complex data structures.\n",
        "createdAt": "2023-01-03T10:45:08.000Z",
        "updatedAt": "2023-01-03T10:52:36.000Z",
        "language": "MATLAB",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/lbrotherson/PhD/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "omid-b/ans",
        "url": "https://github.com/omid-b/ans",
        "description": "Ambient Noise Seismology",
        "stars": 1,
        "forks": 0,
        "readme": "# ANS: Ambient Noise Seismology\n\n**IMPORTANT NOTE:** *ans* is added to *gdp* as a seismic module and the main updates/developments will be done in *gdp* (https://github.com/omid-b/gdp).\n\nans is a python wrapper for ambient noise seismology tasks and it has a GUI for easier configuration of ambient-noise seismology projects. In its backend, this package depends on Perl interpreter, GMT (Generic Mapping Tools), and SAC (Seismic Analysis Code) as well as python modules including ObsPy etc. ans is successfully tested on Python versions 3.6-3.11.\n\n## Pre-requisites \n\n### To run the gui (Debian Linux)\n\n```bash\nsudo apt install libxcb-icccm4 libxcb-image0 libxcb-keysyms1 libxcb-render-util0\nsudo apt-get install '^libxcb.*-dev' libx11-xcb-dev libglu1-mesa-dev libxrender-dev libxi-dev libxkbcommon-dev libxkbcommon-x11-dev\n```\n\n### Convert to pdf file after plotting (Debian Linux)\n\n```bash\nsudo apt-get update\nsudo apt-get -y install ghostscript\nsudo apt install texlive-font-utils\n```\n\n\n## Version 0.0.1\n\nThis version include all the necessary commands and tools required for generating Rayleigh wave (ZZ and RR cross-correlations) and Love wave (TT cross-correlation component) Empirical Green's Functions (EGFs). A brief description of most useful CLI commands is given below:\n\n## Workflow / Procedure\n\n```bash\nans init <maindir>\n```\n> **Description:** initialize ans project at project main directory i.e. \\<maindir\\>\n\n```bash\nans config\nans config --maindir <maindir>\n```\n> **Description:** open the program GUI to configure the ans project. Note: default \\<maindir\\> is current working directory\n\n```bash\nans download stations\n```\n> **Description:** download list of available stations at the given region boundary and dates that was previously set using the GUI. Datacenters, desired station components etc should also be set using '$ ans config'.\n\n```bash\nans download metadata\n```\n> **Description:** download station metadata files (xml file format) that will be used for instrument response removal and updating sac headers.\n\n```bash\nans download mseeds\n```\n> **Description:** main data acquisition module; download seismograms in mseed format\n\n```bash\nans mseed2sac <mseeds_dir> <sacs_dir>\n```\n> **Description:** convert mseed to sac files while applying the listed processing steps in project configuration mseed2sac tab (i.e., '$ ans config').\n\t  \\<mseeds_dir\\>: input mseed dataset directory \\<sacs_dir\\>: output sac files dataset directory\n\t\n```bash\nans sac2ncf <sacs_dir> <ncfs_dir>\n```\n> **Description:** process the input sacfiles in <sacs_dir> and output NCFs (noise cross-correlation functions) while applying the list of sac2ncf processes defined in project configuration file. Note: At this step it is necessary that all sac headers are updated and this can be done by either performing instrument response removal or adding \"write headers\" process to the list of processes.\n\n```bash\nans ncf2egf <ncfs> <egfs_dir>\n```\n> **Description:** \\<ncfs\\>: either path to the \\<ncfs_dir\\> (full stack EGFs) or an ASCII datalist containing a one-column data format list of paths to event directories (seasonal EGFs; e.g., \"14001000000\" i.e. 2014/01/01) \\<egfs_dir\\>: path to output stacked EGF\n\t\t  \n\n",
        "createdAt": "2022-01-12T23:17:54.000Z",
        "updatedAt": "2025-03-07T13:59:20.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/omid-b/ans/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "kasra-hosseini/obspyDMT",
        "url": "https://github.com/kasra-hosseini/obspyDMT",
        "description": "A Python toolbox for the query, retrieval, processing and management of seismological data sets, including very large, heterogeneous and/or dynamically growing ones.",
        "stars": 94,
        "forks": 57,
        "readme": "<div align=\"center\">\n    <p align=\"center\">\n    <img src=\"./figures/logo.jpg\" \n         alt=\"obspyDMT logo\" width=\"50%\" align=\"center\">\n    </p>\n    <h2>A Python Toolbox for Retrieving, Processing and Management of Seismological Datasets</h2>\n</div>\n \n<p align=\"center\">\n    <a href=\"https://pypi.org/project/obspyDMT/\">\n        <img alt=\"PyPI\" src=\"https://img.shields.io/pypi/v/obspyDMT\">\n    </a>\n    <a href=\"https://github.com/kasra-hosseini/obspyDMT/blob/master/LICENSE.txt\">\n        <img alt=\"License\" src=\"https://img.shields.io/badge/License-LGPL_v3-yellow.svg\">\n    </a>\n    <a href=\"https://github.com/kasra-hosseini/obspyDMT/actions/workflows/dmt_ci.yml/badge.svg\">\n        <img alt=\"Integration Tests badge\" src=\"https://github.com/kasra-hosseini/obspyDMT/actions/workflows/dmt_ci.yml/badge.svg\">\n    </a>\n    <br>\n</p>\n\n[obspyDMT][dmt] (obspy Data Management Tool) is a tool for retrieving, processing and management of seismological datasets in a fully automatic way. Written in the Python programming language (Python 2 and 3 compatible), it can be used as a stand-alone command-line tool (requiring no knowledge of Python) or can be integrated as a module with other Python codes.\n\nTable of contents\n-----------------\n\n- [Gallery](#gallery)\n   *  [Quick tour](#quick-tour): run a quick tour.\n   *  [Earthquake meta-data](#earthquake-meta-data): request event information without downloading any waveforms.\n   *  [Seismicity map](#seismicity-map)\n   *  [Event-based mode](#event-based-mode): retrieve waveform data, stationXML/response files and meta-data from several different data centers.\n   *  [Update an existing data set](#update-an-existing-data-set): request the same data again (in case that part of the earlier request failed), or expand the number of earthquakes, stations, or seismograms.\n   *  [Time-continuous mode](#time-continuous-mode): retrieve waveforms, stationXML/response files and meta-data of waveforms that are not relative to or centered on specific earthquake occurances.\n   *  [Processing and instrument correction](#processing-and-instrument-correction): process the data automatically after the data retrieval and/or on an existing data-set.\n   *  [Synthetic seismograms](#synthetic-seismograms)\n   *  [Explore station meta-data (StationXML files, filterstages)](#explore-station-meta-data-stationxml-files-filterstages):\n   *  [Speeding up data retrieval and processing by parallelization](#speeding-up-data-retrieval-and-processing-by-parallelization): this section introduces some options (*bulk* and *parallel retrieving and processing*) to speed-up the data retrieval and processing.\n   *  [KML format](#kml): create a KML file for event/station/ray. KML format is readable by Google-Earth.\n   *  [VTK format](#vtk): create a VTK file for event(s). VTK format is readable by Paraview.\n-  [Supported event catalogs and data centers](#supported-event-catalogs-and-data-centers): supported data centers and earthquake catalogs.\n-  [Directory structure](#directory-structure): standardized directory structure where obspyDMT organizes retrieved seismograms and metadata.\n-  [How to cite obspyDMT](#how-to-cite-obspydmt)\n-  [Installation](#installation): installation and system requirements.\n\n## Gallery\n\n\n| **Quick tour**                                                 <a href=\"#quick-tour\">![](figures/quick_tour_ray.png)                                                 | **Earthquake meta-data**                                           <a href=\"#earthquake-meta-data\">![](figures/neic_event_no_focal_2014_2015.png)                                   |\n| -------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| **Seismicity map**                                             <a href=\"#seismicity-map\">![](figures/japan_seismicity.png)                                           | **Event-based mode**                                               <a href=\"#event-based-mode\">![](figures/iris_ev_based_mode.png)                                               |\n| **Update of an existing waveform data set**                    <a href=\"#update-an-existing-data-set\">![](figures/iris_gfz_ipgp_ev_based.png)                        | **Time-continuous mode**                                           <a href=\"#time-continuous-mode\">![](figures/continuous_example.png)                                           |\n| **Processing and instrument correction**                       <a href=\"#processing-and-instrument-correction\">![](figures/fiji_processed.png)                       | **Synthetic seismograms**                                          <a href=\"#synthetic-seismograms\">![](figures/fiji_iasp91_2s.png)                                              |\n| **Explore station meta-data (StationXML files, filterstages)** <a href=\"#explore-station-meta-data-stationxml-files-filterstages\">![](figures/ic_LBTB_gallery.png)   | **Speeding up data retrieval and processing by parallelization**   <a href=\"#speeding-up-data-retrieval-and-processing-by-parallelization\">![](figures/gallery_parallel.png)     |\n| **KML format (e.g., Google-Earth)**                            <a href=\"#kml\">![](figures/KML_event_based_example.png)                                               | **VTK format (e.g., Paraview)**                                    <a href=\"#vtk\">![](figures/events_neic_vtk.png)                                                               |\n\n\n\n\n## Quick tour\n\nRun a quick tour:\n\n```bash\nobspyDMT --tour\n```\n\n``dmt_tour_dir`` directory will be created in the current path, and retrieved/processed waveforms as well as meta-data will be organized there (refer to [Directory structure](#directory-structure) section for more information).\n\nThe (raw) retrieved waveforms can be plotted by:\n\n```bash\nobspyDMT --datapath dmt_tour_dir --local --plot_waveform\n```\n\n<p align=\"center\">\n<img src=\"figures/quick_tour_raw.png\" width=\"70%\" align=\"middle\">\n</p>\n\nTo plot the processed (e.g., instrument corrected) waveforms, ``--plot_dir_name processed`` can be added to the previous command line:\n\n```bash\nobspyDMT --datapath dmt_tour_dir --local --plot_waveform --plot_dir_name processed\n```\n\n<p align=\"center\">\n<img src=\"figures/quick_tour_corrected.png\" width=\"70%\" align=\"middle\">\n</p>\n\nobspyDMT has many tools to plot the contents of a data set. As an example, the following command line plots the ray coverage (ray path between each source-receiver pair) of ``dmt_tour_dir`` directory:\n\n```bash\nobspyDMT --datapath dmt_tour_dir --local --plot_ev --plot_sta --plot_ray\n```\n\n<p align=\"center\">\n<img src=\"figures/quick_tour_ray.png\" width=\"70%\" align=\"middle\">\n</p>\n\n## Earthquake meta-data\n\nFirst, we request event information from one of several supported seismicity catalogs, without downloading any waveforms yet:\n\n```bash\nobspyDMT --datapath neic_event_metadata --min_mag 5.5 --min_date 2014-01-01 --max_date 2015-01-01 --event_catalog NEIC_USGS --event_info\n```\n\n`neic_event_metadata/` directory - created with the above command - can be updated for events that occured in 2015 of magnitude more than 5.5: (no waveform retrieval)\n\n```bash\nobspyDMT --datapath neic_event_metadata --min_mag 5.5 --min_date 2015-01-01 --max_date 2016-01-01 --event_catalog NEIC_USGS --event_info\n```\n\nTo plot the content of local data set (`neic_event_metadata/`):\n\n```bash\nobspyDMT --datapath neic_event_metadata --local --plot_ev\n```\n\n<p align=\"center\">\n<img src=\"figures/neic_event_no_focal_2014_2015.png\" width=\"70%\" align=\"middle\">\n</p>\n\n## Seismicity map\n\nSeismicity map (``--plot_seismicity`` option flag) of Japan region based on earthquakes of magnitude more than 5.0 that occured from 2000-01-01 until 2017-01-01 from NEIC event catalog.\nNote ``--event_rect`` option flag to define a region around Japan:\n\n```bash\nobspyDMT --datapath japan_seismicity --min_mag 5.0 --min_date 2000-01-01 --max_date 2017-01-01 --event_catalog NEIC_USGS --event_rect 110./175./15/60 --plot_seismicity --event_info\n```\n\n<p align=\"center\">\n<img src=\"figures/japan_seismicity.png\" width=\"70%\" align=\"middle\">\n</p>\n\nGlobal seismicity map of archived earthquakes in NEIC catalogue with magnitude more than 5.0 that occurred between 1990 and 2016.\nOne command queried the NEIC catalogue, stored and organised the retrieved information and generated the seismicity map.\n(No actual waveform data were queried in this example):\n\n```bash\nobspyDMT --datapath neic_event_dir --min_date 1990-01-01 --max_date 2017-01-01 --min_mag 5.0 --event_catalog NEIC_USGS --event_info --plot_seismicity\n```\n\n<p align=\"center\">\n<img src=\"figures/neic_catalog_1990.png\" width=\"100%\" align=\"middle\">\n</p>\n\nThe results of some basic statistics (magnitude and depth histograms) are also generated and plotted automatically (top-left panel).\nNote the rendering of coloured beach balls in the map inset (deepest seismicity in the foreground).\nThe global map also contains beach balls rather than just simple black dots, but they do not become apparent at this zoom level.\n\n**ISC catalog**\n\nThe International Seismological Centre (ISC) provides two catalogs:\n \n * **COMPREHENSIVE bulletin** contains all events collected by the ISC, \n   including most recent events, which are awaiting review.\n * **REVIEWED bulletin** includes all events that have been \n   reviewed and relocated by an ISC analyst.\n\n``--isc_catalog`` option specifies the ISC bulletin type (default: COMPREHENSIVE). Example:\n\n```bash\nobspyDMT --datapath test_isc_comprehensive --min_date 2013-01-01 --max_date 2015-01-01 --min_mag 6.5 --event_catalog ISC --isc_catalog COMPREHENSIVE --event_info --plot_seismicity\n```\n\n<p align=\"center\">\n<img src=\"figures/isc_catalog_comprehensive.png\" width=\"100%\" align=\"middle\">\n</p>\n\nRetrieval result for the same request using ``--isc_catalog REVIEWED``:\n\n<p align=\"center\">\n<img src=\"figures/isc_catalog_reviewed.png\" width=\"100%\" align=\"middle\">\n</p>\n\n## Event-based mode\n\nHere, we retrieve actual BHZ seismograms from `II` network that recorded earthquakes of magnitude more than 7.5 that occured from 2014-01-01 until\n2015-01-01 (NEIC catalog). For this example, only stations with network code ``II``, location code ``00`` and channel codes ``BHZ`` are retrieved:\n\n```bash\nobspyDMT --datapath event_based_dir --min_date 2014-01-01 --max_date 2015-01-01 --min_mag 7.5 --event_catalog NEIC_USGS --data_source IRIS --net \"II\" --loc \"00\" --cha \"BHZ\" --preset 100 --offset 1800\n```\n\n``--data_source`` specifies that the waveform data center of IRIS should be contacted for seismograms.\nOmitting this flag would trigger the default ``--data_source IRIS``.\n``--preset 100`` and ``--offset 1800`` specify the retrieval of waveform time windows of 100 s before to 1800 s after the reference time.\nSince we are downloading in event-based mode, i.e., centered around earthquake occurrences, the reference time defaults to the event origin time.\nThis could be changed to the time of P-wave arrival by invoking ``--cut_time_phase``,\nin which case each seismogram would have a different absolute start time.\n\nTo plot the stations/events/rays:\n\n```bash\nobspyDMT --datapath event_based_dir --local --plot_ev --plot_sta --plot_ray\n```\n\n<p align=\"center\">\n<img src=\"figures/iris_ev_based_mode.png\" width=\"70%\" align=\"middle\">\n</p>\n\n## Update an existing data set\n\nIn the course of working with a waveform data set, it often becomes necessary to update.\nThis could mean requesting the same data again (because part of the earlier request failed for some reason), \nor expanding the number of earthquakes, stations, or seismograms. \nThe following command updates the data-set that we created in the previous [Event-based mode section](#event-based-mode) with ``BHZ`` channels of ``AW and E*`` networks (`E*`: all stations that their network codes start with E)\nfrom the ``GFZ`` data center:\n\n```bash\nobspyDMT --datapath event_based_dir --data_source \"GFZ\" --net \"AW,E*\" --cha \"BHZ\" --preset 100 --offset 1800\n```\n\nAdditionally, we can update the data set with ``BHZ`` channels of ``G*`` networks (i.e., all stations that their network codes start with G)\nfrom the ``IPGP`` data center:\n\n```bash\nobspyDMT --datapath event_based_dir --data_source \"IPGP\" --net \"G*\" --cha \"BHZ\" --preset 100 --offset 1800\n```\n\nTo plot the stations/events/rays:\n\n```bash\nobspyDMT --datapath event_based_dir --local --plot_ev --plot_sta --plot_ray\n```\n\n<p align=\"center\">\n<img src=\"figures/iris_gfz_ipgp_ev_based.png\" width=\"70%\" align=\"middle\">\n</p>\n\n## Time-continuous mode\n\nIn contrast to the examples of [Event-based mode section](#event-based-mode) and [Update of an existing waveform data set](#update-an-existing-data-set),\nsome usage cases require waveforms that are not relative to or centered on specific earthquake occurances.\nWe refer to this usage mode as \"time-continuous\" `--continuous`.\n\nFor example, the following command retrieves one-month long time series (from 2011-03-03 until 2011-04-03) recorded by two stations (`--sta \"BFO,RER\" --loc \"00\" --cha \"BHZ\"`) from the IRIS data center:\n \n```bash\nobspyDMT --continuous --datapath continuous_example --min_date 2011-03-03 --max_date 2011-04-03 --sta \"BFO,RER\" --loc '00' --cha \"BHZ\" --data_source IRIS\n```\n\n<p align=\"center\">\n<img src=\"figures/continuous_example.png\" width=\"100%\" align=\"middle\">\n</p>\n\n## Processing and instrument correction\n\nobspyDMT can process the waveforms directly after retrieving the data, or it can process an existing data set in a separate step (local mode).\nBy default, obspyDMT follows processing instructions described in the ``process_unit.py`` located at ``/path/to/my/obspyDMT/obspyDMT`` directory.\nAlthough this file is fully customizable, several common processing steps can be done via options flags (without changing/writing new processing instructions).\n\nThe following command retrieves all BHZ channels from the IRIS data center that:\n\n- 50 <= Azimuth <= 55 (specified by ``--min_azi`` and ``--max_azi``)\n- 94 <= Distance <= 100 (specified by ``--min_epi`` and ``max_epi``)\n- recorded events of magnitude more than 6.8 that occured on ``2014-07-21``.\n\n```bash\nobspyDMT --datapath data_fiji_island --min_mag 6.8 --min_date 2014-07-21 --max_date 2014-07-22 --event_catalog NEIC_USGS --data_source IRIS --min_azi 50 --max_azi 55 --min_epi 94 --max_epi 100 --cha BHZ --instrument_correction\n```\n\nTo plot the processed/corrected waveforms (Note ``--plot_dir_name processed``, omitting this option would result in plotting raw counts, i.e., ``--plot_dir_name raw``):\n\n```bash\nobspyDMT --datapath data_fiji_island --local --plot_waveform --plot_dir processed\n```\n\n<p align=\"center\">\n<img src=\"figures/fiji_processed.png\" width=\"70%\" align=\"middle\">\n</p>\n\n## Synthetic seismograms\n\nobspyDMT can retrieve synthetic waveforms matching the real data using [Syngine](http://ds.iris.edu/ds/products/syngine/) webservice.\nThe following example command retrieves not only observed waveforms but also their synthetic counterparts, computed on an IASP91 background model (note `--syngine --syngine_bg_model iasp91_2s`):\n\n```bash\nobspyDMT --datapath data_fiji_island --min_mag 6.8 --min_date 2014-07-21 --max_date 2014-07-22 --event_catalog NEIC_USGS --data_source IRIS --min_azi 50 --max_azi 55 --min_epi 94 --max_epi 100 --cha BHZ --instrument_correction --syngine --syngine_bg_model iasp91_2s\n```\n\nTo plot the synthetic waveforms (note ``--plot_dir_name syngine_iasp91_2s``):\n\n```bash\nobspyDMT --datapath data_fiji_island --local --plot_waveform --plot_dir_name syngine_iasp91_2s\n```\n\n<p align=\"center\">\n<img src=\"figures/fiji_iasp91_2s.png\" width=\"70%\" align=\"middle\">\n</p>\n\n\n## Explore station meta-data (StationXML files, filterstages)\n\nobspyDMT implements several plotting options to explore station meta-data.\nFor example, the following command generates a visual representation of transfer function spectra (amplitude and phase) of `IC.XAN` station in China.\nBlue lines show transfer function components computed for all filter stages in the StationXML file;\nred lines are for the analogue part.\n\n```bash\nobspyDMT --datapath /path/to/STXML.IC.XAN.00.BHZ --plot_stationxml --plotxml_paz --plotxml_min_freq 0.0001\n```\n\n<p align=\"center\">\n<img src=\"figures/ic_XAN.png\" width=\"50%\" align=\"middle\">\n</p>\n\nTo plot transfer function spectra (amplitude and phase) of `GT.LBTB` station in Botswana:\n\n```bash\nobspyDMT --datapath /path/to/STXML.GT.LBTB.00.BHZ --plot_stationxml --plotxml_paz --plotxml_min_freq 0.0001\n```\n\n<p align=\"center\">\n<img src=\"figures/ic_LBTB.png\" width=\"50%\" align=\"middle\">\n</p>\n\nTransfer function spectra (amplitude and phase) of each stage in the StationXML file can be also plotted by (note `--plotxml_allstages`): \n\n```bash\nobspyDMT --datapath /path/to/STXML.GT.LBTB.00.BHZ --plot_stationxml --plotxml_min_freq 0.0001 --plotxml_allstages\n```\n\n<p align=\"center\">\n<img src=\"figures/ic_LBTB_stages.png\" width=\"100%\" align=\"middle\">\n</p>\n\nIn the phase response, two stages (1 and 5) have non-zero values.\n\n## Speeding up data retrieval and processing by parallelization\n\nTo increase the efficiency in retrieving waveform data, a functionality for parallelized data retrieval can be enabled as follows:\n\n```bash\n--req_parallel --req_np 4\n```\n\nThe second flag (`--req_np 4`) specifies the number of parallel requests.\n\nA further speed-up can be achieved by specifying a bulk request.\nInstead of requesting individual items, this will send a list of items (time series or meta data) \nto the data center:\n\n```bash\n--bulk\n```\n\nTo enable parallel processing with, for example, 10 threads:\n\n```bash\n--parallel_process --process_np 10\n```\n\n## KML\n\nTake the example of [Event-based mode](#event-based-mode) section. To create a KML file (readable by Google-Earth) based on that data set:\n\n```bash\nobspyDMT --datapath event_based_dir --local --plot_ev --plot_sta --plot_focal --plot_ray --create_kml\n```\n\n<p align=\"center\">\n<img src=\"figures/KML_event_based_example.png\" width=\"70%\" align=\"middle\">\n</p>\n\nor to plot events of magnitude more than 7.0 in the global example of [Seismicity map](#seismicity-map) section:\n\n```bash\nobspyDMT --datapath neic_event_dir --local --plot_ev --plot_focal --min_mag 7.0 --create_kml\n```\n\n<p align=\"center\">\n<img src=\"figures/KML_neic_event_catalog_more_7.png\" width=\"70%\" align=\"middle\">\n</p>\n\n## VTK\n\nTake the global example of [Seismicity map](#seismicity-map) section. To create a VTK file (readable by Paraview) for all events in that data set:\n\n```bash\nobspyDMT --datapath neic_event_dir --local --create_event_vtk\n```\n\n<p align=\"center\">\n<img src=\"figures/events_neic_vtk.png\" width=\"70%\" align=\"middle\">\n</p>\n\n## Supported event catalogs and data centers\n\nPrint supported data centers that can be passed as arguments to ``--data_source``:\n\n```bash\nobspyDMT --print_data_sources\n```\n\nPrint supported earthquake catalogs that can be passed as arguments to ``--event_catalog``:\n\n```bash\nobspyDMT --print_event_catalogs\n```\n\n**Read an existing local event catalog**\n \n``--read_catalog <PATH>`` option flag reads in an existing event catalog located at ``<PATH>`` and proceeds. \nCurrently supported catalog metadata formats: \"CSV\", \"QUAKEML\", \"NDK\", \"ZMAP\" (Refer to obspy documentation for details on QuakeML, NDK and ZMAP formats).\n \n**CSV format:** obspyDMT can read a CSV file as an event catalog. \nThis must be a list of comma-separated values containing some or all of the fields below, \none event per line:\n\n```bash\nevent_number,event_id,datetime,latitude,longitude,depth,magnitude,magnitude_type,author,flynn_region,mrr,mtt,mpp,mrt,mrp,mtp,stf_func,stf_duration\n```\n \nFile ``catalog.txt``, generated by obspyDMT in ``EVENTS-INFO`` subdirectory provides an example of such a file.\n \nExample:\n \n```bash\n#number,event_id,datetime,latitude,longitude,depth,magnitude,magnitude_type,author,flynn_region,mrr,mtt,mpp,mrt,mrp,mtp,stf_func,stf_duration\n1,20110311_054623.a,2011-03-11T05:46:23.200000Z,38.2963,142.498,19.7,9.1,MW,None,NAN,None,None,None,None,None,None,triangle,164.914\n```\n \ndatetime, latitude, longitude, depth and magnitude are mandatory. Optional fields may be set to ```None```, as in the following example where only datetime, latitude, longitude, depth and magnitude are set:\n \n```bash\n#number,event_id,datetime,latitude,longitude,depth,magnitude,magnitude_type,author,flynn_region,mrr,mtt,mpp,mrt,mrp,mtp,stf_func,stf_duration\n1,None,2011-03-11T05:46:23.200000Z,38.2963,142.498,19.7,9.1,None,None,None,None,None,None,None,None,None,None,None\n```\n\n## Directory structure\n\nFor each request, obspyDMT creates the depicted directory tree inside the user-specified directory `datapath/`, and arranges the retrieved data either in different event directories (for event-based requests) or in chronologically named directories (for continuous requests). It also creates a directory in which a catalog of all requested events/time spans is stored. Raw waveforms, StationXML/response files and corrected waveforms are collected in sub-directories. While retrieving the data, obspyDMT creates metadata files such as station/event location files, stored in the `info/` directory of each event.\n\n<p align=\"center\">\n<img src=\"figures/dmt_dir_structure.png\" width=\"100%\" align=\"middle\">\n</p>\n\n## How to cite obspyDMT\n\nPlease consider acknowledging obspyDMT if it helps you to obtain results and figures for publication or presentation, by citing:\n\n    Hosseini, K. and Sigloch, K.: ObspyDMT: a Python toolbox for retrieving and processing large seismological data sets, Solid Earth, 8, 1047-1070, https://doi.org/10.5194/se-8-1047-2017, 2017.\n\n## Installation\n\n:warning: Python versions >= 3.7 are recommended.\n\nWe strongly recommend installation via Anaconda (refer to [Anaconda website and follow the instructions](https://docs.anaconda.com/anaconda/install/)).\n\n* Create a new environment for obspyDMT\n\n```bash\nconda create -n py38dmt python=3.8\n```\n\n* Activate the environment:\n\n```bash\nconda activate py38dmt\n```\n\n* 🗺️ For plotting data on maps, we use [Cartopy](https://scitools.org.uk/cartopy/docs/v0.14/index.html). If you don't want to plot data on maps, you can skip this step. \n\n```bash\nconda install -c scitools cartopy\n```\n\n* 🌏 To create KML files (readable by Google-Earth), we use [PyKML](https://pythonhosted.org/pykml/). If you don't want to create KML files, you can skip this step. \n\n```bash\npip install pykml\n```\n\n* obspyDMT can be installed in different ways:\n\n  1. **Install obspyDMT via [PyPi](https://pypi.org/project/obspyDMT/)** (which tends to be the most user-friendly option):\n      \n      * Install obspyDMT:\n\n      ```bash\n      pip install obspyDMT\n      ```\n\n  2. **Install obspyDMT from the source code**:\n\n      * Clone obspyDMT source code:\n\n      ```bash\n      git clone https://github.com/kasra-hosseini/obspyDMT\n      ```\n\n      * Install obspyDMT dependencies:\n\n      ```\n      cd /path/to/my/obspyDMT\n      pip install -v -e .\n      ```\n\nobspyDMT can be used from a system shell without explicitly calling the Python interpreter. The following command checks the dependencies required for running the code properly:\n\n```bash\nobspyDMT --check\n```\n\nobspyDMT contains various option flags for customizing the request. Each option has a reasonable default value, which the user can change to adjust obspyDMT option flags to a specific request. The following command displays all available options with their default values:\n\n```bash\nobspyDMT --help\n```\n\nThe options are grouped by topics. To display only a list of these topic headings, use\n\n```bash\nobspyDMT --options\n```\n\nand to see the full help text for only one topic (e.g., group 2), use\n\n```bash\nobspyDMT --list_option 2\n```\n\n[dmt]: https://github.com/kasra-hosseini/obspyDMT\n",
        "createdAt": "2012-11-02T12:41:43.000Z",
        "updatedAt": "2025-12-04T00:56:01.000Z",
        "language": "QML",
        "homepage": "http://kasra-hosseini.github.io/obspyDMT/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/kasra-hosseini/obspyDMT/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "anowacki/CIJ.jl",
        "url": "https://github.com/anowacki/CIJ.jl",
        "description": "CIJ.jl is a Julia package for dealing with linear elastic constants, particularly for geophysics problems",
        "stars": 4,
        "forks": 0,
        "readme": "# CIJ.jl\n\n[![Build Status](https://github.com/anowacki/CIJ.jl/workflows/CI/badge.svg)](https://github.com/anowacki/CIJ.jl/actions)\n[![codecov](https://codecov.io/gh/anowacki/CIJ.jl/branch/master/graph/badge.svg?token=9jiYUCivdO)](https://codecov.io/gh/anowacki/CIJ.jl)\n\n## What is CIJ.jl?\nA [Julia](http://julialang.org) package for dealing with linear elastic\nconstants, with particular applicability to geophysics problems.\n\n\n## How to install\nAlthough not registered as an official package, CIJ.jl can be added to your\nJulia install like so:\n\n```julia\njulia> ] # Type ']' to enter pkg mode\n\n(v1.6) pkg> add https://github.com/anowacki/CIJ.jl\n```\n\nYou then need only do\n\n```julia\njulia> import CIJ\n```\n\nand if that works, you're ready to go.\n\n\n## How to use\nCIJ.jl, to make your life as easy as possible, does not insist on your using\na special type for elastic constants, but relies on them being `AbstractArray`s\nof an elastic tensor's Voigt matrix.\nThe Voigt matrix is a 6 &times; 6 array representing the full\n3 &times; 3 &times; 3 &times; 3 &times; tensor, subject to symmetries present\nfor linear elasticity.\n\nThroughout CIJ.jl, we make no assumptions about the *units* of a matrix `C`,\nbut when it does matter (for instance, calculating phase velocities with\n`phase_vels()`), then it is assumed you are dealing with *density-normalised*\nconstants (i.e., the units are m<sup>2</sup>&nbsp;s<sup>-2</sup>), sometimes called\n*A<sub>ij</sub>* instead.  (This is the same as Pa/(kg&nbsp;m<sup>-3</sup>)),\n\nLet's try out a simple example of what we can do:\n\n```julia\njulia> C = zeros(6, 6);\n\njulia> CIJ.is_stable(C)\nfalse\n```\n\nHere, the `is_stable()` function tells one whether a set of elastic constants\n`C` is dynamically possible.  It turns out, a material where all constants are\nzero is not.  Olivine, however, should be, so\n\n```julia\njulia> C, rho = CIJ.ol(); # Return density-normalised constants, and density, for olivine\n\njulia> is_stable(C)\ntrue\n```\n\nis not a surprise.\n\n## `EC` type\n\nWhilst you can deal with plain `Array`s, CIJ exports the `EC` type which is a\nwrapper around a `StaticArrays.MMatrix`.\n\nCalculations using this type are quicker.  For instance, finding the compliance\nmatrix *S* from the stiffness matrix *C* is found by matrix inversion, and this\nis about twice as fast when using `EC`s rather than plain `Array`s.\n\n`EC`s are mutable and can be treated just like any other 6 &times; 6 matrix,\nincluding accessing and setting elements like `C[i,j]`.\n\nConstruct an `EC` object by calling the `EC()` constructor:\n\n```julia\njulia> EC([10i+j for i in 1:6, j in 1:6])\n6×6 EC{Float64}:\n 11.0  12.0  13.0  14.0  15.0  16.0\n 12.0  22.0  23.0  24.0  25.0  26.0\n 13.0  23.0  33.0  34.0  35.0  36.0\n 14.0  24.0  34.0  44.0  45.0  46.0\n 15.0  25.0  35.0  45.0  55.0  56.0\n 16.0  26.0  36.0  46.0  56.0  66.0\n```\n\nNote that `EC`s are enforced to be symmetric, and will copy the upper half\nof the input matrix into the bottom half.  Subsequent modification of any\nelement will be reflected in both upper and lower halves automatically,\nso `EC`s cannot be non-symmetric.\n\nTo specify the type of the elements `T`, use the parametric constructor,\n`EC{T}()`.\n\n### Calculating phase velocities\nOne of the common uses for the package is to compute the *phase velocities* in\na given direction through some elastic constants.  This is done using the\n`phase_vels()` function, which returns a `NamedTuple`, like so:\n\n```julia\njulia> C, rho = CIJ.ol();\n\njulia> az, inc = 20, 45; # Directions\n\njulia> vp, vs1, vs2, pol, avs = CIJ.phase_vels(C, az, inc)\n(vp = 8590.639816324323, vs1 = 5422.968116295959, vs2 = 4602.70348828534, pol = -20.682503753509465, avs = 16.363285381017125)\n```\n\n`vp`, `vs1` and `vs2` and so on, are velocities in m&nbsp;s<sup>-1</sup>,\nassuming `C` is in m<sup>2</sup>&nbsp;s<sup>-2</sup>,\n`pol` is the orientation of the fast shear wave in degrees, and `avs` is the\npercentage shear wave anisotropy along this direction.\n\n### Converting to Voigt notation\nIf you have an 81-component tensor `c`, how do you get the Voigt matrix `C`?\n\n```julia\nC = CIJ.cij(c)\n```\n\nAnd the other way?\n\n```julia\nc = CIJ.cijkl(C)\n```\n\n## Plotting\nIf you are using Julia v1.9 or later, and you have also loaded a\nbackend from the [Makie.jl](https://docs.makie.org/stable/) ecosystem (e.g.,\nby doing `import GLMakie`, `import CairoMakie`, etc.), then you\ncan create plots of phase velocity surfaces by calling one of the\nfollowing functions:\n\n- `CIJ.plot_hemisphere` for a set of upper-hemisphere phase velocity\n  surfaces;\n- `CIJ.plot_hemisphere!` for a single surface into an existing `Makie.PolarAxis`;\n- `CIJ.plot_sphere` for a 3D view of the phase velocity surface; and\n- `CIJ.plot_sphere!` for a 3D view into an existing `Makie.Axis3`.\n\nFor example:\n\n```julia\njulia> import GLMakie\n\njulia> CIJ.plot_hemisphere(CIJ.ol()[1])\n```\n\n![Upper hemisphere phase velocities of olivine](docs/images/olivine_upper_hemisphere.png)\n\n```julia\njulia> CIJ.plot_sphere(CIJ.ol()[1], :avs)\n```\n\n![3D spherical view of phase velocities of olivine](docs/images/olivine_sphere.jpg)\n\n## Getting help\nFunctions are documented, so at the REPL type `?` to get a `help?>` prompt,\nand type the name of the function:\n\n```julia\nhelp?> CIJ.phase_vels\n\n  phase_vels(C, az, inc) -> vp, vs1, vs2, pol, avs\n  \n  Calculate the phase velocities for the 6x6 elasticity matrix C\n  along the direction (az, inc), in degrees, and return P-wave\n  velocity vp, the fast and slow shear wave velocities, vs1 and\n  vs2, the polarisation of the fast shear wave pol, and the shear\n  wave velocity anisotropy, avs. Velocities are in m/s if the\n  tensor C is in m^2/s^2 (i.e., is a density-normalised tensor,\n  sometimes called A).\n  \n  az is the azimuth in degrees measured from the x1 towards to\n  -x2 axis.\n  \n  inc is the inclination in degrees from the x1-x2 plane towards\n  the x3 axis.\n```\n\n## Other software\n\n* If you use MATLAB, then you should use [MSAT](https://github.com/andreww/MSAT/).\n* If you use Fortran, then you should investigate the module\n  `anisotropy_ajn` which is in the\n  [seismo-fortran](https://github.com/anowacki/seismo-fortran) repo.\n\n\n## Why the name?\nLinear elastic constants are a fourth-rank tensor, relating the stress\n_&sigma;_ in a material to the strain _&epsilon;_ with the relationship\n\n*&sigma;<sub>ij</sub> = c<sub>ijkl</sub> &epsilon;<sub>kl</sub>*,\n\nwhere _i_, _j_, _k_ and _l_ are indices taking values 1 to 3 and\nrepresenting the three cartesian directions in space.  This leads to there\nbeing 3 &times; 3&times; 3 &times; 3 = 81 numbers in the tensor *c*, which\nis somewhat unwieldy.\n\nHowever, certain symmetries mean one can reduce this to 21, and represent\nthe 4-tensor with a symmetric 2-tensor or matrix instead; the so-called 'Voigt\nnotation'.  Typically, the lowercase 4-tensor *c<sub>ijkl</sub>* becomes the\nuppercase matrix *C<sub>ij</sub>*, and thus the package is born.\n\n\n## Acknowledgments\nCredit goes to [James Wookey](http://www1.gly.bris.ac.uk/~wookey) and\n[Mike Kendall](http://www1.gly.bris.ac.uk/~jmk/) for the original set of Fortran\nroutines on which the code is based, and which lives on in the\n[seismo-fortran](https://github.com/anowacki/seismo-fortran) repo.\n\n",
        "createdAt": "2016-05-18T14:28:15.000Z",
        "updatedAt": "2025-11-25T12:23:50.000Z",
        "language": "Julia",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/anowacki/CIJ.jl/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "NoiseCIEI/SensKernel",
        "url": "https://github.com/NoiseCIEI/SensKernel",
        "description": "1-D Seismic Surface Wave Isotropic Group/Phase Speed Dispersion/Eigenfunction/Sensitivity Kernel",
        "stars": 23,
        "forks": 9,
        "readme": "",
        "createdAt": "2018-11-12T01:18:01.000Z",
        "updatedAt": "2025-07-14T17:56:10.000Z",
        "language": "Rebol",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ds-modules/EPS256-FA20",
        "url": "https://github.com/ds-modules/EPS256-FA20",
        "description": "UC Berkeley EPS 256 Fall 2020, Spring 2022",
        "stars": 8,
        "forks": 9,
        "readme": "# Earthquake of the Week - EPS256\n\nBerkeley Seismology Lab - Graduate Seminar on Earthquakes - Prof Richard Allen\n\n### Using these notebooks in class\n\nFor use in class: open this repository in UC Berkeley's DataHub.  Click on the following link:\n\n [![Datahub](https://img.shields.io/badge/Launch-UCB%20Datahub-blue.svg)](https://datahub.berkeley.edu/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fds-modules%2FEPS256-FA20)\n \n You can also open the notebooks in the Binder application online...\n\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/ds-modules/EPS256-FA20/master)\n\n\n### Instructions to upload your notebook for presentation in class\n\n1. You will need permission to add to this public repository.  Email your github id to Richard and he will add you.\n2. Create a folder for your notebook/data/everything.  Folder naming convention is: Wk0#-DescriptiveTitle-YourName.  To do this click on the \"Add file\" button.  Then type the name of your directory as if it is a file name, then add \"/\" after the file name.  You will then need a file, just use \".gitkeep\".  Then click on the green \"Commit\" button to creat the folder.\n3. Add the juypter notebook file and any associated data etc to the directory.  To do this, click on the folder you just created to enter the folder.  Then click on the Add file buton and select the upload option.  You can then drag and drop your files to add them to the directory.\n",
        "createdAt": "2020-08-28T20:28:25.000Z",
        "updatedAt": "2025-05-29T18:45:39.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ds-modules/EPS256-FA20/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "PotatoXi/gsl",
        "url": "https://github.com/PotatoXi/gsl",
        "description": "This is a study on the combination of geodesy, seismology, and the deep Earth.",
        "stars": 1,
        "forks": 1,
        "readme": "# gsl\nThis is a study on the combination of geodesy, seismology, and the deep Earth.\n\nContinuously updated！\n\n主页地址：\n [![github](https://img.shields.io/badge/github-Jamie-brightgreen.svg)](https://github.com/PotatoXi)\n# Visitor <a href=\"https://visitorbadge.io/status?path=https%3A%2F%2Fgithub.com%2FPotatoXi%2FJamie\"><img src=\"https://api.visitorbadge.io/api/daily?path=https%3A%2F%2Fgithub.com%2FPotatoXi%2FJamie&label=Visitor&labelColor=%232ccce4&countColor=%23555555&style=plastic\" /></a>\n",
        "createdAt": "2022-11-07T23:58:08.000Z",
        "updatedAt": "2025-05-24T23:43:44.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/PotatoXi/gsl/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "DanieleMolinari/Formal-statistical-analysis-in-SAS",
        "url": "https://github.com/DanieleMolinari/Formal-statistical-analysis-in-SAS",
        "description": "Models to analyse seismological data of earthquakes.",
        "stars": 2,
        "forks": 0,
        "readme": "# Formal-statistical-analysis-in-SAS\nModels to analyse seismological data of earthquakes.\n",
        "createdAt": "2021-09-11T08:50:02.000Z",
        "updatedAt": "2022-08-12T14:35:41.000Z",
        "language": "SAS",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/DanieleMolinari/Formal-statistical-analysis-in-SAS/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "spacefan/seismanpy",
        "url": "https://github.com/spacefan/seismanpy",
        "description": "seisman's Python package for seismological researches.",
        "stars": 0,
        "forks": 0,
        "readme": "seismanpy\n=========\n\n.. image:: https://travis-ci.org/seisman/seismanpy.svg?branch=master\n    :target: https://travis-ci.org/seisman/seismanpy\n\n.. image:: https://codecov.io/gh/seisman/seismanpy/branch/master/graph/badge.svg\n    :target: https://codecov.io/gh/seisman/seismanpy\n\n.. image:: https://img.shields.io/github/license/seisman/HinetPy.svg\n    :target: https://github.com/seisman/HinetPy/blob/master/LICENSE\n\nseismanpy is a collection of SEISmic MANipulation tools in Python.\n\nIt's only for personal use.\n",
        "createdAt": "2017-07-06T15:20:51.000Z",
        "updatedAt": "2018-01-15T02:55:16.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/spacefan/seismanpy/master/README.rst",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "agilescientific/bruges",
        "url": "https://github.com/agilescientific/bruges",
        "description": "Bag of really useful geoscience equations and stuff",
        "stars": 296,
        "forks": 117,
        "readme": "# bruges is a\n\n![Bruges](http://agile.geosci.ai/bruges.png)\n\nIn other words, it's just a load of functions that implement important equations in (mostly seismic) geophysics, from Aki-Richards to Zoeppritz.\n\n[![Run tests](https://github.com/agilescientific/bruges/actions/workflows/run-tests.yml/badge.svg)](https://github.com/agilescientific/bruges/actions/workflows/run-tests.yml)\n[![Build docs](https://github.com/agilescientific/bruges/actions/workflows/build-docs.yml/badge.svg)](https://github.com/agilescientific/bruges/actions/workflows/build-docs.yml)\n[![PyPI version](https://img.shields.io/pypi/v/bruges.svg)](https://pypi.python.org/pypi/bruges/)\n[![PyPI versions](https://img.shields.io/pypi/pyversions/bruges.svg)](https://pypi.org/project/bruges//)\n[![PyPI license](https://img.shields.io/pypi/l/bruges.svg)](https://pypi.org/project/bruges/)\n\n\n## Quick start\n\nInstall with:\n\n```shell\npip install bruges\n```\n\nMake a trapezoidal wavelet like:\n\n```python\nimport bruges as bg\nw, t = bg.filters.ormsby(duration=0.256, dt=0.002, f=[5, 10, 40, 80])\n```\n\nThis produces two arrays: amplitude `w` and time `t`.\n\n\n## Links\n\n- [Documentation](https://code.agilescientific.com/bruges)\n- [Issue Tracker](https://github.com/agilescientific/bruges/issues/)\n- [PyPi](http://pypi.python.org/pypi/bruges/)\n- [Agile's website](http://www.agilescientific.com)\n\n![Bruges rooves](https://www.dropbox.com/s/tzvi22ujq6rozdb/bruges_long_rooves.png?raw=1)\n",
        "createdAt": "2013-09-06T16:12:04.000Z",
        "updatedAt": "2025-12-04T04:23:10.000Z",
        "language": "Python",
        "homepage": "https://code.agilescientific.com/bruges",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/agilescientific/bruges/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "gc364/SeisSurv2D",
        "url": "https://github.com/gc364/SeisSurv2D",
        "description": "2D Finite difference solver and processing library for modelling marine seismic refraction surveys.",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2024-12-14T16:34:14.000Z",
        "updatedAt": "2025-01-27T17:34:21.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "JunhaoSong/seismology_tutorials_in_notebook",
        "url": "https://github.com/JunhaoSong/seismology_tutorials_in_notebook",
        "description": null,
        "stars": 1,
        "forks": 0,
        "readme": "# seismology_tutorials_in_notebook",
        "createdAt": "2022-03-02T18:25:31.000Z",
        "updatedAt": "2023-01-07T10:18:54.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/JunhaoSong/seismology_tutorials_in_notebook/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "SCECcode/UCVMC",
        "url": "https://github.com/SCECcode/UCVMC",
        "description": "SCEC Unified Community Velocity Model C-language (UCVMC) software framework",
        "stars": 20,
        "forks": 10,
        "readme": "# UCVM\n\nThe current UCVM software version is UCVM 21.10.0 which was released in October 2021. We recommend that users work with this most recent version.\nhttps://github.com/sceccode/ucvm\n\n\n## Archived Version of UCVMC Software Release\n\nThe version of UCVM in this repository (UCVMC) is now archived. It is available for review, but we do not recommend researchers use this old version of the software. \n\nUCVM is distributed as open-source scientific software. It can be installed compiled and run on most Linux-based computer systems if the system includes software development tools including Python, C, and Fortran compilers, and other software tools. The UCVM v19.4.0 source code is distributed using a github repository. On Github, users can find the source code, installation directions for Linux, and a wiki that provide examples and the expected results from UCVM.\n\n## Background\n\nThe SCEC Unified Community Velocity Model (UCVM) software is used to access information, including estimated Vp and Vs seismic wave velocities and density, about the earth's crust. \n\nUCVM provides seismic velocity information for several California regions. Seismic velocities of rocks and sediments at shallow depths determine how strongly an area will shake. By assigning velocities to rock types in the 3-D geologic model, geologists can gain an understanding of the extent of areas of low shear velocity that are most likely to experience localized strong shaking and future earthquake damage. An important application for the models accessible through UCVM is for use in computer simulations of California earthquakes.\n\nThe Unified Community Velocity Model C-language (UCVMC) software framework is a collection of software tools designed to provide a standard interface to multiple, alternative, California velocity models. UCVMC development is an interdisciplinary research collaboration involving geoscientists, computer scientists, and software developers. UCVMC is used in high resolution 3D wave propagation simulations for California. \n\nThe map below shows the coverage regions for currently supported California velocity models that are accessible through UCVMC. Each of the models shown is considered a regional velocity model. Typically the models return values down to about 50km or 100km, but most models are undefined below 100km. For earth material properties below 100km, global seismic velocity models, such as the Preliminary Earth Reference Model (PREM), are possible alternative models.\n\n\n<img src=\"documentation/coverage.png\" width=\"80%\">\n\nMap shows coverage region for California CVMs registered into UCVMC.\nCoverage region for UCVM 2D maps (yellow) overlayed upon regions of various California 3D velocity models (CVM-S4: red, CVM-S4 geotechnical regions: red polygons, CVM-H high resolution: small light blue square, CVM-H low resolution: larger light blue square, USGS High Resolution Bay Area: small white rectangle, USGS Low Resolution Bay Area: large white rectangle, CVM-S4.26, CVM-S4.26M01: green, CCA 06: small yellow, CS17.3, CS17.3-H: large orange rectangle, Havard San Joaquin Basin Model: small orange rectangle, Havard Santa Maria Basin Model: orange square, CS18.5 Cypershake Study's Tiled Velocity Model: blue) : [Coverage.kml](documentation/coverage.kml)\n\nUCVMC software repository contains a software codebase developed by Philip Maechling, Mei-Hui Su, David Gill, Patrick Small, and others at SCEC. UCVMC is released as open-source scientific software under an Apache 2 software license.\n\nUCVM was developed with support from National Science Foundation (NSF), US Geological Survey (USGS), and other sources.\n\n\n## System and Software Requirements\n\nTesting UCVMC on all possible combinations of operating sysetms and software stacks requires more software developer resources than currently available. So, we have defined a UCVMC reference software stack that we use to develop and test the software. This UCVMC distrbution has been shown to work on the following reference software stack. It may work on other software stacks, also, but this is the supported software environment.\n\n*  Linux operating system (e.g. CentOS 7 Linux) x86_64-linux \n*  GNU gcc/gfortran compilers version 4.8.5\n*  Python 2.7.11 (Anaconda 4.0.0 (64-bit))\n*  Autotools build software for Linux\n*  Automake, Autoconf build tools for Linux\n*  Git client\n\nExternal Libraries installed by UCVMC\n\n*  Euclid Etree library: http://www.cs.cmu.edu/~euclid/ (provided during installation)\n*  Proj.5 projection library: http://trac.osgeo.org/proj/ (provided during installation)\n\nOptional Software for building MPI binaries:\n\n*  openmpi 1.8.8\n\n## Installation\nOnce the target computer has the required software tools installed, the basic install of UCVMC is:\n*  git clone https://github.com/SCECcode/UCVMC.git\n*  cd UCVMC/largefiles\n*  ./get_large_files.py\n*  ./check_largefiles_md5.py\n*  ./stage_large_files.py\n*  cd ..\n*  ./ucvm_setup.py\n\nThe get_large_files.py and ucvm_setup.py scripts run in a terminal window and print text questions to the user.  The user types answers to the questions in the terminal window. The retrieval script asks the user which velocity model they would like to stage into the local system and the install script asks the user which velocity models they would like to install from retrieved model list: ( CVM-H v15.1, CVM-S4, CVM-S4.26, CVM-S4.26.M01, CCA06, CS17.3, CS17.3-H, and USGS CenCal). Several models are very large. CCA06, CS17.3 and CS17.3-H are 9.2G, 72G and 72G respectively. We recommend that the user only retrieve needed models and install all retrieved models.\n\nThe script will then automatically compile, build, and install the selected models.\n\n## MPI Compilers and UCVM Programs\n\nIf a GNU-based MPI compiler is detected, the MPI version of several utilities are created, including ucvm2mesh_mpi, ucvm2mesh_mpi_layer, ucvm2etree_mpi, and basin_query_mpi are built. Otherwise, only the serial versions for these programs are built.\n\n## Configuration\nThe main UCVMC configuration file is ${UCVM_INSTALL_DIR}/conf/ucvm.conf. \nThis file defines the paths to all configured models and maps, and it defines selected model flags, such as CVM-H USE_GTL.\nThe UCVM installer sets up this ucvm.conf file automatically.\n\nIn most cases, the user does not need to edit the UCVMC/conf/ucvm.conf. However, in some circumstances, such as if the user wants to move the UCVMC installation directory, or configure the behavior of the CVM-H model, the user  might want to edit the ucvm.conf file. Please see the User Guide for more details on how to edit the UCVMC/conf/ucvm.conf configuration file.\n\n## Standard Models and Maps\nThe following California velocity models packages are included as part of a standard UCVMC installation.  Each model is assigned an abbreviation, and these abbreviations are used to specify the models when making UCVM queries. The model abbreviations used by UCVM are defined in following tables:\n\nModel Name | Description | UCVM Abbreviation | Size\n-----------|-------------|-------------------|------\nCVM-H v15.1     | Southern California Velocity Model developed by Harvard Structural Geology Group with optional geotechnical layer | cvmh | 1.6G \nCVM-S4     | Southern California Velocity Model developed by SCEC, Caltech, USGS Group with geotechnical layer | cvms | 326M\nCVM-S4.26  | Tomography improved version of CVM-S4 with optional geotechnical layer(Ely-Jordan GTL, default is off)| cvms5 | 1.2G\nCVM-S4.26.M01 | CVM-S4.26 with added geotechnical layer | cvmsi | 1.6G\nCCA06 | Central California Velocity Model with optional geotechnical layer (Ely-Jordan GTL, default is off) | cca | 9.2G\nCS17.3 | Cypershake study 17.3 Central California Velocity Model and optional geotechincal layer (Ely-Jordan GTL, default is off) | cs173 | 72G\nCS17.3-H | Cypershake study 17.3 Central California Velocity Model with San Joaquin and Santa Maria Basins data from Havard's group and optional geoptechnical layer (Ely-Jordan GTL, default is off) | cs173h |72G\nUSGS Bay Area Velocity Model 0.8.3| USGS developed San Francisco and Central California velocity model | cencal | 17G\nSouthern California 1D  | Modified Hadley Kanamori 1D model based on Hadley-Kanamori model | 1d | 8k\nNorthridge Region 1D | Los Angeles Region 1D model used in SCEC Broadband Platform | bbp1d | -\n\nA state-wide California standard topography map is distribued with UCVMC. This is a statewide\ntopography map, that also includes statewide Vs30 values, combined into an etree structure.\n\nToopgrahy and Vs30 Map Name | Description | UCVM Abbreviation\n----------------------------|-------------|------------------\nUSGS NED DEM and Wills-Wald Vs30 | California elevation and Vs30 data in etree format | ucvm\n\n## Documentation\nOnline UCVMC documentation is available at:\n*  https://github.com/SCECcode/UCVMC/wiki\n\nAdditional documentation advanced features and previous versions of UCVM are posted at:\n*  http://scec.usc.edu/scecpedia/UCVMC\n \nMore Installation instruction:\n\n* [Additional guidelines for building UCVMC](documentation/Installation.md)\n\n## Support\n\nIssue Tracking:\n\n* GitHub: https://github.com/SCECcode/UCVMC/issues\n\nEmail:\n\n* Contact: software@scec.usc.edu\n\n## Preferred Reference\nIf you use the UCVM software in your research, please include a reference to the following publication in your research publications. References help us obtain continued financial support for the development of the software. The preferred reference for the UCVM software is:\n\nSmall, P., Gill, D., Maechling, P. J., Taborda, R., Callaghan, S., Jordan, T. H., Ely, G. P., Olsen, K. B., & Goulet, C. A. (2017). The SCEC Unified Community Velocity Model Software Framework. Seismological Research Letters, 88(5). doi:10.1785/0220170082.\n\n## License\nUCVMC is released under the Apache 2.0 license. Please see the LICENSE file for distribution license and disclaimers.\n",
        "createdAt": "2016-12-12T21:09:04.000Z",
        "updatedAt": "2025-05-23T11:47:48.000Z",
        "language": "C",
        "homepage": "https://github.com/SCECcode/UCVMC/wiki",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/SCECcode/UCVMC/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "longxin1991/seis",
        "url": "https://github.com/longxin1991/seis",
        "description": "code about seismology.",
        "stars": 2,
        "forks": 1,
        "readme": "",
        "createdAt": "2014-04-26T01:47:46.000Z",
        "updatedAt": "2021-04-14T00:35:03.000Z",
        "language": "C",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "garrettsickles/Stereonet",
        "url": "https://github.com/garrettsickles/Stereonet",
        "description": "Introduction to Seismology",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2016-04-03T17:26:41.000Z",
        "updatedAt": "2016-11-27T19:43:12.000Z",
        "language": "C",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "sstaehler/marsquake_teaching",
        "url": "https://github.com/sstaehler/marsquake_teaching",
        "description": "Interactive tools to learn about Martian seismology",
        "stars": 2,
        "forks": 1,
        "readme": "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/sstaehler/marsquake_teaching/main?urlpath=git-pull%3Frepo%3Dhttps%253A%252F%252Fgithub.com%252Fsstaehler%252Flocate_marsquakes%26urlpath%3Dtree%252Flocate_marsquakes%252Flocate_marsquakes.ipynb%26branch%3Dmaster)\n# marsquake_teaching\nInteractive tools to learn about Martian seismology\n",
        "createdAt": "2022-10-23T16:13:11.000Z",
        "updatedAt": "2025-07-16T22:09:25.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/sstaehler/marsquake_teaching/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "geophystech/ground-motion-modeling",
        "url": "https://github.com/geophystech/ground-motion-modeling",
        "description": "The ground motion evaluation module",
        "stars": 2,
        "forks": 0,
        "readme": "# ground-motion-modeling\nThe ground motion evaluation module (WORK IN PROGRESS)\n\n## Libs\n- GeographicLib v.1.48+\n- Glib v.2.50+\n\n## Dependencies\n- GNU Make (tested on 4.2.1)\n- GCC (testd on 5.4.0)\n\n## Command-line interface\n**Using:**\n```\ngmm <path_to_config_file> [path_to_s_file]\n```\n**Example with s-file**\n```\nbin/gmm examples/configs/gmm.conf examples/seisan/19-1248-22L.S201402\n```\n**Example without s-file:**\n```\necho \"51.92 143.04 13.0 6.0 0\" | bin/gmm examples/configs/gmm.conf\n```\nThe PGA xyz data `lon lat %g` will be saved to `gmm_pga_out.xyz` file in the same directory.\n\n## AS2008 GMPE Model\n`Abrahamson, Norman, and Walter Silva. \"Summary of the Abrahamson & Silva NGA ground-motion relations.\" Earthquake spectra 24.1 (2008): 67-97.`\n\n**The meaning of some variables in this implementation:**\n\n`a12 * Frv = 0`, `a13 * Fnm = 0`, `a15 * Fas = 0`, `Fhw * f4(Rjb, Rrup, Rx, W, S, Ztor, Mw) = 0`, `f6(Ztor) = 0`, `f10(Z1.0, Vs30) = 0`.\n",
        "createdAt": "2017-05-12T05:32:52.000Z",
        "updatedAt": "2022-01-05T14:08:43.000Z",
        "language": "C",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/geophystech/ground-motion-modeling/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Arvindm16/iit_roorkee_computional_seismology",
        "url": "https://github.com/Arvindm16/iit_roorkee_computional_seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2025-08-11T18:37:54.000Z",
        "updatedAt": "2025-08-11T18:37:55.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "larsgeb/convert-SEGY",
        "url": "https://github.com/larsgeb/convert-SEGY",
        "description": "Very small code snippet to convert SEGY velocity models to plain text files",
        "stars": 1,
        "forks": 0,
        "readme": "# convert-SEGY\nVery small code snippet to convert SEGY velocity models to plain text files\n",
        "createdAt": "2018-03-25T16:02:33.000Z",
        "updatedAt": "2023-09-24T08:19:26.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/larsgeb/convert-SEGY/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "pysmo/aimbat",
        "url": "https://github.com/pysmo/aimbat",
        "description": "AIMBAT: Automated and Interactive Measurement of Body wave Arrival Times",
        "stars": 44,
        "forks": 19,
        "readme": "<h1 align=\"center\">AIMBAT</h1>\n\n<p align=\"center\">\n<em>Automated and Interactive Measurement of Body wave Arrival Times</em>\n</p>\n\n<div align=\"center\">\n<a href=\"https://github.com/pysmo/aimbat/actions/workflows/run-tests.yml\" target=\"_blank\">\n<img src=\"https://github.com/pysmo/aimbat/actions/workflows/run-tests.yml/badge.svg\" alt=\"Test Status\">\n</img></a>\n<a href=\"https://github.com/pysmo/aimbat/actions/workflows/build.yml\" target=\"_bank\">\n<img src= \"https://github.com/pysmo/aimbat/actions/workflows/build.yml/badge.svg\" alt=\"Build Status\">\n</img></a>\n<a href=\"https://aimbat.readthedocs.io/en/latest/?badge=latest\" target=\"_blank\">\n<img src=\"https://readthedocs.org/projects/aimbat/badge/?version=latest\" alt=\"Documentation Status\">\n</img></a>\n<a href=\"https://codecov.io/gh/pysmo/aimbat\" target=\"_blank\">\n<img src=\"https://codecov.io/gh/pysmo/aimbat/branch/master/graph/badge.svg?token=ZsHTBN4rxF\" alt=\"codecov\">\n</img></a>\n<a href=\"https://pypi.org/project/aimbat/\" target=\"_blank\">\n<img src=\"https://img.shields.io/pypi/v/aimbat\" alt=\"PyPI\">\n</img></a></div>\n\n<p align=\"center\">\n<em>Documentation:</em> <a href=\"https://aimbat.readthedocs.io\" target=\"_blank\">https://aimbat.readthedocs.io</a>\n</p>\n<p align=\"center\">\n<em>Source Code:</em> <a href=\"https://github.com/pysmo/aimbat\" target=\"_blank\">https://github.com/pysmo/aimbat</a>\n</p>\n\n\n---\n\nAIMBAT (Automated and Interactive Measurement of Body wave Arrival Times) is an\nopen-source software package for efficiently measuring teleseismic body wave arrival\ntimes for large seismic arrays [[1]](#1). It is based on a widely used method called\nMCCC (Multi-Channel Cross-Correlation) [[2]](#2). The package is automated in the sense\nof initially aligning seismograms for MCCC, which is achieved by an ICCS (Iterative Cross\nCorrelation and Stack) algorithm. Meanwhile, a GUI (graphical user interface) is built to\nperform seismogram quality control interactively. Therefore, user processing time is\nreduced while valuable input from a user's expertise is retained. As a byproduct, SAC\n[[3]](#3) plotting and phase picking functionalities are replicated and enhanced.\n\nModules and scripts included in the AIMBAT package were developed using\n[Python](http://www.python.org/) and its open-source modules on the Mac OS X platform\nsince 2009. The original MCCC [[2]](#2) code was transcribed into Python.\nThe GUI of AIMBAT was inspired and initiated at the\n[2009 EarthScope USArray Data Processing and Analysis Short Course](https://www.iris.edu/hq/es_course/content/2009.html).\nAIMBAT runs on Mac OS X, Linux/Unix and Windows thanks to the platform-independent\nfeature of Python.\n\nFor more information visit the\n[project website](http://www.earth.northwestern.edu/~xlou/aimbat.html) or the\n[pysmo repositories](https://github.com/pysmo).\n\n\n## Authors' Contacts\n\n* [Xiaoting Lou](http://geophysics.earth.northwestern.edu/people/xlou/aimbat.html) Email: xlou at u.northwestern.edu\n\n* [Suzan van der Lee](http://geophysics.earth.northwestern.edu/seismology/suzan/) Email: suzan at northwestern.edu\n\n* [Simon Lloyd](https://www.slloyd.net/) Email: simon at pysmo.org\n\n## References\n\n<a id=\"1\">[1]</a>\nXiaoting Lou, Suzan van der Lee, and Simon Lloyd (2013),\nAIMBAT: A Python/Matplotlib Tool for Measuring Teleseismic Arrival Times.\n*Seismol. Res. Lett.*, 84(1), 85-93, doi:10.1785/0220120033.\n\n<a id=\"2\">[2]</a>\nVanDecar, J. C., and R. S. Crosson (1990),\nDetermination of teleseismic relative phase arrival times using multi-channel\ncross-correlation and\nleast squares.\n*Bulletin of the Seismological Society of America*, 80(1), 150–169.\n\n<a id=\"3\">[3]</a>\nGoldstein, P., D. Dodge, M. Firpo, and L. Minner (2003),\nSAC2000: Signal processing and analysis tools for seismologists and engineers,\n*International Geophysics*, 81, 1613–1614.\n",
        "createdAt": "2012-09-27T16:55:04.000Z",
        "updatedAt": "2025-09-29T23:29:43.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/pysmo/aimbat/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "NoiseCIEI/ANCC",
        "url": "https://github.com/NoiseCIEI/ANCC",
        "description": "Seismic Ambient Noise Two-Station Interferometry",
        "stars": 9,
        "forks": 2,
        "readme": "",
        "createdAt": "2018-11-12T00:16:51.000Z",
        "updatedAt": "2025-01-18T15:59:54.000Z",
        "language": "C",
        "homepage": "https://ds.iris.edu/ds/products/ancc-ciei/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "yvonnefroehlich/gmt-pygmt-plotting",
        "url": "https://github.com/yvonnefroehlich/gmt-pygmt-plotting",
        "description": "Python Scripts and Jupyter Notebooks to Prepare Geophysical Figures and Geographic Maps via PyGMT (Pythonic Interface for GMT)",
        "stars": 52,
        "forks": 6,
        "readme": "# PyGMT Plotting\n\nCollection of Python scripts or Jupyter notebooks (supported by JupyterLab) to reproduce some of the geographic and\ngeophysical maps shown in the publications I am involved in. To prepare these maps, [_PyGMT_](https://www.pygmt.org)\nthe Python wrapper for the [_Generic Mapping Tools_ (_GMT_)](https://www.generic-mapping-tools.org) is used.\n\n_Please note_: Scripts or notebooks are available up on acceptance of the related publication.\n\n<img src=\"https://github.com/yvonnefroehlich/gmt-pygmt-plotting/raw/main/_images/github_maps_readme_main.png\" width=\"800\">\n\n\n## Content\n\n_Please note_: The required versions are given in the single folders as well as notebooks and scripts.\n\n| Folder | Overview | Requirements |\n| --- | --- | --- |\n| •&nbsp;**[000_general_stuff](https://github.com/yvonnefroehlich/gmt-pygmt-plotting/tree/main/000_general_stuff#000_general_stuff)** <br> •&nbsp;**[001_paper_RFSG_2022](https://github.com/yvonnefroehlich/gmt-pygmt-plotting/tree/main/001_paper_RFSG_2022#001_paper_RFSG_2022)** <br> •&nbsp;**[002_paper_FGR_2024](https://github.com/yvonnefroehlich/gmt-pygmt-plotting/tree/main/002_paper_FGR_2024#002_paper_FGR_2024)** <br> •&nbsp;**[003_taup](https://github.com/yvonnefroehlich/gmt-pygmt-plotting/tree/main/003_taup#003_taup)** <br> •&nbsp;**[004_earthquakes_eruptions](https://github.com/yvonnefroehlich/gmt-pygmt-plotting/tree/main/004_earthquakes_eruptions#004_earthquakes_eruptions)** <br> •&nbsp;**[005_global_seismicity](https://github.com/yvonnefroehlich/gmt-pygmt-plotting/tree/main/005_global_seismicity#005_global_seismicity)** <br> •&nbsp;**[006_tomographies_databases](https://github.com/yvonnefroehlich/gmt-pygmt-plotting/tree/main/006_tomographies_databases#006_tomographies_databases)** <br> •&nbsp;**[007_dissertation_F_2025](https://github.com/yvonnefroehlich/gmt-pygmt-plotting/tree/main/007_dissertation_F_2025#007_dissertation_F_2025)** <br> •&nbsp;**[008_urg_vs_norsa](https://github.com/yvonnefroehlich/gmt-pygmt-plotting/tree/main/008_urg_vs_norsa#008_urg_vs_norsa)** <br> •&nbsp;**[009_deepdyn](https://github.com/yvonnefroehlich/gmt-pygmt-plotting/tree/main/009_deepdyn#009_deepdyn)** <br> •&nbsp;**[010_axisem](https://github.com/yvonnefroehlich/gmt-pygmt-plotting/tree/main/010_axisem#010_axisem)** <br> •&nbsp;**[011_agu_FTLJG_2024](https://github.com/yvonnefroehlich/gmt-pygmt-plotting/tree/main/011_agu_FTLJG_2024#011_agu_FTLJG_2024)** <br> •&nbsp;**[012_uefa_euro25](https://github.com/yvonnefroehlich/gmt-pygmt-plotting/tree/main/012_uefa_euro25#012_uefa_euro25)** <br> •&nbsp;**[013_general_maps](https://github.com/yvonnefroehlich/gmt-pygmt-plotting/tree/main/013_general_maps#013_general_maps)** <br> •&nbsp;**[014_general_stats](https://github.com/yvonnefroehlich/gmt-pygmt-plotting/tree/main/014_general_stats#014_general_stats)** | <img src=\"https://github.com/yvonnefroehlich/gmt-pygmt-plotting/raw/main/_images/github_maps_readme_main.gif\" width=\"400\"> | •&nbsp;[PyGMT](https://www.pygmt.org) <br> •&nbsp;[GMT](https://www.generic-mapping-tools.org) <br> •&nbsp;[Python](https://www.python.org) <br> •&nbsp;[Jupyter](https://jupyter.org) <br> •&nbsp;[NumPy](https://numpy.org) <br> •&nbsp;[pandas](https://pandas.pydata.org) <br> •&nbsp;[ObsPy](https://docs.obspy.org) <br> •&nbsp;[pymagglobal](https://sec23.git-pages.gfz-potsdam.de/korte/pymagglobal) <br> •&nbsp;[GeoPandas](https://geopandas.org) <br> •&nbsp;[geodatasets](https://geodatasets.readthedocs.io)|\n\n\n## Citation\n\nIf you make use of this material, please acknowledge the relating publications in which framework these scripts and notebooks were written:\n\n**Peer-reviewed journal articles**\n- [**_Fröhlich Y, Grund M, Ritter J R R (2024)_**](https://doi.org/10.1093/gji/ggae245).\n  Lateral and vertical variations of seismic anisotropy in the lithosphere-asthenosphere system underneath Central Europe from long-term splitting measurements.\n  *Geophysical Journal International*, 239(1):112-135.\n  https://doi.org/10.1093/gji/ggae245.\n- [**_Ritter J R R, Fröhlich Y, Sanz Alonso Y, Grund M (2022)_**](https://doi.org/10.1007/s10950-022-10112-w).\n  Short-scale laterally varying SK(K)S shear wave splitting at BFO, Germany – implications for the determination of anisotropic structures.\n  *Journal of Seismology*, 26:1137-1156.\n  https://doi.org/10.1007/s10950-022-10112-w, correction https://doi.org/10.1007/s10950-023-10136-w.\n\n**Doctoral studies**\n- [**_Fröhlich Y (2025a)_**](https://doi.org/10.5445/IR/1000183786).\n  Shear wave splitting analysis of long-term data: Anisotropy studies in the Upper Rhine Graben area, Central Europe.\n  Dissertation, *Karlsruhe Institute of Technology, Geophysical Institute*.\n  https://doi.org/10.5445/IR/1000183786.\n- [**_Fröhlich Y (2025b)_**](https://doi.org/10.5281/zenodo.15982581).\n  Shear wave splitting analysis of long-term data: Anisotropy studies in the Upper Rhine Graben area, Central Europe.\n  Disputation, *Karlsruhe Institute of Technology, Geophysical Institute*.\n  https://doi.org/10.5281/zenodo.15982581.\n\n<details><summary>Please click for details on <b>Presentations</b> and <b>Posters.</b> </summary>\n<p>\n\n**Presentations**\n- [**_Fröhlich Y, Ritter J R R (2024)_**](https://dx.doi.org/10.5281/zenodo.14510993).\n  Vertical and Small-scale Lateral Varying Seismic Anisotropy in the Upper Mantle Underneath the Upper Rhine Graben, Central Europe.\n  *Annual Meeting of the American Geophysical Union*, Washington D.C..\n  Division Session Exploring Innovations and New Directions in Seismic Anisotropy and Attenuation: Observations, Models, and Experiments I Oral, DI21A-02.\n  [Abstract ID 1578275](https://agu.confex.com/agu/agu24/meetingapp.cgi/Paper/1578275).\n  https://dx.doi.org/10.5281/zenodo.14510993.\n- [**_Fröhlich Y, Tian D, Leong W J, Jones M, Grund M (2024)_**](https://doi.org/10.6084/m9.figshare.28049495).\n  PyGMT – Accessing and Integrating GMT with Python and the Scientific Python Ecosystem.\n  *Annual Meeting of the American Geophysical Union*, Washington D.C..\n  Union Session The impact of GMT in the Earth, Ocean and Space sciences: What's next? I Oral, U12B-05 (invited).\n  [Abstract ID 1578856](https://agu.confex.com/agu/agu24/meetingapp.cgi/Paper/1578856).\n  https://doi.org/10.6084/m9.figshare.28049495.\n\n**Posters**\n- [**_Fröhlich Y, Dorn F, Dillah M I F, Ritter J R R (2024)_**](https://doi.org/10.5281/zenodo.14801004).\n  Investigation of seismic anisotropy in the D'' layer using *X*KS pairs.\n  *2th DeepDyn annual meeting*, Rügen.\n  https://doi.org/10.5281/zenodo.14801004.\n- [**_Fröhlich Y, Dillah M I F, Dorn F, Ritter J R R (2024)_**](https://doi.org/10.5281/zenodo.12658821).\n  Investigation of seismic anisotropy in the D'' layer and at the CMB regarding intense magnetic flux regions.\n  *18th Symposium of Study of the Earth's Deep Interior*, Great Barrington.\n  https://doi.org/10.5281/zenodo.12658821.\n- [**_Fröhlich Y, Thiyagarajan H, Tölle L S, Ritter J R R, Thomas C (2024)_**](https://doi.org/10.5281/zenodo.10927349).\n  Understanding the influence of seismic mantle structures at the core-mantle boundary on intense magnetic flux regions.\n  *84th Annual Meeting of the German Geophysical Society*, Jena.\n  https://doi.org/10.5281/zenodo.10927349.\n\n</p>\n</details>\n\n\n## Contributing\n\nFor bug reports, suggestions, or recommendations feel free to [open an issue](https://github.com/yvonnefroehlich/gmt-pygmt-plotting/issues)\nor [submit a pull request](https://github.com/yvonnefroehlich/gmt-pygmt-plotting/pulls) directly here on\n[GitHub](https://github.com/yvonnefroehlich/gmt-pygmt-plotting).\n\n\n## References\n\n_Please note_: Specific references are given in the single notebooks and scripts.\n\n- [**_Crameri F (2023)_**](https://doi.org/10.5281/zenodo.1243862).\n  Scientific colour maps.\n  https://www.fabiocrameri.ch/colourmaps.php.\n  *Zenodo*. https://doi.org/10.5281/zenodo.1243862.\n- [**_Thyng K M, Greene C A, Hetland R D, Zimmerle H M, DiMarco S F (2016)_**](https://dx.doi.org/10.5670/oceanog.2016.66).\n  True colors of oceanography: Guidelines for effective and accurate colormap selection.\n  *Oceanography*, 29(3):9-13.\n  https://dx.doi.org/10.5670/oceanog.2016.66.\n- [**_Tian D, Leong W J, Fröhlich Y, Grund M, Schlitzer W, Jones M, Toney L, Yao J, Tong J-H, Mage Y, Materna K, Belem A, Newton T, Anant A, Ziebarth M, Quinn J, Uieda L, Wessel P (2024)_**](https://doi.org/10.5281/zenodo.17156962).\n  PyGMT: A Python interface for the Generic Mapping Tools, version v0.17.0.\n  *Zenodo*. https://doi.org/10.5281/zenodo.17156962 (v0.17.0), https://doi.org/10.5281/zenodo.3781524 (all versions / latest version).\n- [**_Wessel P, Smith W H F, Scharroo R, Luis J F, Wobbe F (2013)_**](https://doi.org/10.1002/2013EO450001).\n  Generic mapping tools: improved version released.\n  *Eos, Transactions American Geophysical Union*, 94(45):409-410.\n  https://doi.org/10.1002/2013EO450001.\n- [**_Wessel P, Luis J F, Uieda L, Scharroo R, Wobbe F, Smith W H F, Tian D (2019)_**](https://doi.org/10.1029/2019GC008515).\n  The Generic Mapping Tools version 6.\n  *Geochemistry, Geophysics, Geosystems*, 20(11):5556-5564.\n  https://doi.org/10.1029/2019GC008515.\n- [**_Wessel P, Luis J F, Uieda L, Scharroo R, Wobbe F, Smith W H F, Tian D, Jones M, Esteban F, Fröhlich Y (2025)_**](https://doi.org/10.5281/zenodo.16448627).\n  The Generic Mapping Tools, version 6.6.0.\n  *Zenodo*. https://doi.org/10.5281/zenodo.16448627 (6.6.0), https://doi.org/10.5281/zenodo.3407865 (all versions / latest version).\n\n\n## Funding\n\nThe presented research and YF received support from various sources:\n\n- [Graduate Funding from the German States](https://www.khys.kit.edu/english/graduate_funding.php) (scholarship)\n- [NSF grant EAR-1948602](https://www.nsf.gov/awardsearch/showAward?AWD_ID=1948602) (travel support for AGU24)\n- [DFG project 521545943](https://gepris.dfg.de/gepris/projekt/521545943?language=en) within the\n  [DFG Priority Program DeepDyn SPP 2404 – 500707704](https://www.geo.lmu.de/deepdyn/en/) (research assistant)\n",
        "createdAt": "2022-04-27T06:19:30.000Z",
        "updatedAt": "2025-12-04T14:03:00.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.15982581",
            "dataCite": "10.5281/zenodo.15982581",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/yvonnefroehlich/gmt-pygmt-plotting/main/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.15982581",
            "title": "Shear wave splitting analysis of long-term data: Anisotropy studies in the Upper Rhine Graben area, Central Europe [Disputation]",
            "journal": "Zenodo",
            "dateReleased": "2025-07-04T00:00:00.000Z",
            "abstract": "Presentation slides of the disputation to the dissertation\n\n      Shear wave splitting analysis of long-term data: Anisotropy studies in the Upper Rhine Graben area, Central Europe\n\n      Yvonne Fröhlich, Karlsruhe Institute of Technology, Geophysical Institute, 2025-07-04\n\n      https://doi.org/10.5445/IR/1000183786\n\n \n\nRelated peer-reviewed journal articles\n\n\n\nFröhlich Y., Grund M. & Ritter J. R. R. (2024). Lateral and vertical variations of seismic anisotropy in the lithosphere-asthenosphere system underneath Central Europe from long-term splitting measurements. Geophysical Journal International, 239(1):112-135. https://doi.org/10.1093/gji/ggae245.\n\nRitter J. R. R., Fröhlich Y., Sanz Alonso Y. & Grund M. (2022). Short-scale laterally varying SK(K)S shear wave splitting at BFO, Germany – implications for the determination of anisotropic structures. Journal of Seismology, 26:1137-1156. https://doi.org/10.1007/s10950-022-10112-w, correction https://doi.org/10.1007/s10950-023-10136-w.\n\nFröhlich Y., Grund M. & Ritter J. R. R. (2022). On the effects of wrongly aligned seismogram components for shear wave splitting analysis. Annals of Geophysics, 66(2). https://doi.org/10.4401/ag-8781.",
            "citationsArray": []
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "paudetseis/SplitPy",
        "url": "https://github.com/paudetseis/SplitPy",
        "description": "Teleseismic shear-wave splitting analysis",
        "stars": 24,
        "forks": 10,
        "readme": "\n![](./splitpy/examples/figures/SplitPy_logo.png)\n\n## Software for teleseismic shear-wave splitting analysis\n\nSeismic anisotropy refers to the property of seismic waves to propagate\nat different wavespeeds depending on the direction of propagation. This\nproperty can be related to the coherent alignment of rock-forming minerals,\nwhich is thought to reflect the current dynamics or fossilized structure of Earth\nmaterials due to tectonic deformation. In the upper mantle, seismic anisotropy \nis most easily determined using the distortion of teleseismic body waves with a \nknown initial polarity, typically core-refracted shear-waves (SKS, SKKS). \n\nSplitPy is a teleseismic shear-wave Splitting Toolbox based on the \nMatlab Tool [`SplitLab`](http://splitting.gm.univ-montp2.fr), \nbut with modifications from [Wustefeld et al (2008)](#references). \nAdditional error surface implementation has been added, however these error \nsurfaces have not been fully tested. The code produces output identical to\nthose in [Audet et al. (2016)](#references)\n\n[![DOI](https://zenodo.org/badge/211722700.svg)](https://zenodo.org/badge/latestdoi/211722700)\n[![Build Status](https://travis-ci.org/paudetseis/SplitPy.svg?branch=master)](https://travis-ci.org/paudetseis/SplitPy)\n\nInstallation, Usage, API documentation and tutorials are described at \nhttps://paudetseis.github.io/SplitPy/.\n\nAuthors: [`Pascal Audet`](https://www.uogeophysics.com/authors/admin/) (Developer and Maintainer) & [`Andrew Schaeffer`](https://schaeffer.ca) (Contributor)\n\n#### Citing\n\nIf you use `SplitPy` in your work, please cite the \n[`Zenodo DOI`](https://zenodo.org/badge/latestdoi/211722700).\n\n#### Contributing\n\nAll constructive contributions are welcome, e.g. bug reports, discussions or suggestions for new features. You can either [open an issue on GitHub](https://github.com/paudetseis/SplitPy/issues) or make a pull request with your proposed changes. Before making a pull request, check if there is a corresponding issue opened and reference it in the pull request. If there isn't one, it is recommended to open one with your rationale for the change. New functionality or significant changes to the code that alter its behavior should come with corresponding tests and documentation. If you are new to contributing, you can open a work-in-progress pull request and have it iteratively reviewed.\n\nExamples of straightforward contributions include notebooks that describe published examples of teleseismic shear-wave splitting. Suggestions for improvements (speed, accuracy, etc.) are also welcome.\n\n#### References\n\n- Audet, P., Sole, C., and Schaeffer, A.J. (2016). Control of lithospheric\n  inheritance on neotectonic activity in northwestern Canada? Geology,\n  44, 807-810, https://doi.org/10.1130/G38118.1\n\n- Wustefeld, A., and Bokelmann, G. (2007). Null detection in shear-wave splitting \n  measurements. Bulletin of the Seismological Society of America, 97, 1204-1211,\n  https://doi.org/10.1785/0120060190\n\n- Wustefeld, A., Bokelmann, G., Zeroli, C., and Barruol, G. (2008). SplitLab: \n  A shear-wave splitting environment in Matlab. Computers & Geoscience, 34, \n  515-528, https://doi.org/10.1016/j.cageo.2007.08.002\n\n#### Use Cases\n\n- Esteve, C., Audet, P., Schaeffer, A.J., Schutt, D.L., Aster, R.A., and Cubley, J. (2020). Seismic evidence for craton chiseling and displacement of lithospheric mantle by the Tintina Fault in the Northern Canadian Cordilleras, Geology, 48, 1120–1125, https://doi.org/10.1130/G47688.1",
        "createdAt": "2019-09-29T20:34:21.000Z",
        "updatedAt": "2025-10-28T01:49:04.000Z",
        "language": "Python",
        "homepage": "https://paudetseis.github.io/SplitPy/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/paudetseis/SplitPy/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Mallcock1/ARTICLE-Asymmetric-slab-SMS",
        "url": "https://github.com/Mallcock1/ARTICLE-Asymmetric-slab-SMS",
        "description": "Research article applying the linear theory of asymmetric magnetic slab modes to solar magneto-seismology",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2017-05-22T13:05:00.000Z",
        "updatedAt": "2017-05-22T13:21:41.000Z",
        "language": "TeX",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ajay6763/LitMod2D_2.0_package_dist_users",
        "url": "https://github.com/ajay6763/LitMod2D_2.0_package_dist_users",
        "description": "LitMod2D_2.0 is a tool  to unravel the thermal, compositional, density, and seismological structure of the crust and upper mantle (~400 km depth) along cross-sections by integrating geophysical and geochemical dataset in a self-consistence thermodynamic manner. ",
        "stars": 10,
        "forks": 3,
        "readme": "# LitMod2D_2.0_package_dist_users\nLitMod2D_2.0 is a tool  to unravel the thermal, compositional, density, and seismological structure of the crust and upper mantle (~400 km depth) along cross-sections by integrating geophysical and geochemical dataset in a self-consistence thermodynamic manner. It is a finite-element forward modeling approach code based on the architecture of the CAGES code (Zeyen & Fernàndez 1994). For a detailed description of LitMod2D the reader is referred to Afonso et al. (2008,https://doi.org/10.1029/2007GC001834  and Kumar et al. (2020,https://doi.org/10.1029/2019GC008777).\n\n---\n## Working flow-chart\n![Earthquake back-projection method](https://github.com/ajay6763/LitMod2D_2.0_package_dist_users/blob/master/GUI/Images/LitMod_scheme_new.png)\n\n\n#### Dependency\n* Python2.7 \n* gfortran\n* wine [https://www.winehq.org/](https://www.winehq.org/)\n",
        "createdAt": "2019-08-30T16:26:39.000Z",
        "updatedAt": "2025-04-25T05:52:39.000Z",
        "language": "PostScript",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ajay6763/LitMod2D_2.0_package_dist_users/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "bakerb845/temblor",
        "url": "https://github.com/bakerb845/temblor",
        "description": "screwing around with ui's and ml for seismology",
        "stars": 0,
        "forks": 0,
        "readme": "# Temblor\n\nTemblor is a user interface for processing seismic waveform data.  It is geared towards processing data collected by local and regional seismic networks.  In particular, I am interested in asking the question of how to incorporate machine learning into an analyst' workflow.  And I refuse to attempt to answer that question with AQMS's Jiggle because some things belong in the past.\n\n*Warning* I'm still screwing around with the idea of making a UI.  I have no deadlines or timelines so this project may stall indefinitely.\n\n## Directory Structure\n\nIn an attempt to decouple Temblor's functionality from its user interface the major code segregration is split between \n\n   1. lib - handles the business logic, i.e., here is where the Temblor actually performs operations on the data models.\n   2. gui - provides a view of the business logic, i.e., here is where you interact with the data models on the screen.\n\nThe reason for distinction is two-fold.  First, you can take the temblor core business logic and potentially tackle other interesting problems.  Second, we can, and do, write unit tests for the core business logic.\n\nSince the GUI is very specific it is doubtful whether it will ever be suitable for a library.  Consequently, the include splits\n\n   1. include/temblor/library - the includes for the core business logic.\n   2. include/temblor/ui - the includes for the user interfaces.\n\nThe namespacing then\n\n   - Temblor::Library::Utilities\n   - Temblor::Library::DataReaders\n\nTemblorUI::Applications\nTemblorUI::\n",
        "createdAt": "2019-05-08T21:14:20.000Z",
        "updatedAt": "2023-01-28T14:40:21.000Z",
        "language": "C++",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/bakerb845/temblor/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ReetMhaske/IITB-SSP-GLEE-Reet-Mhaske-",
        "url": "https://github.com/ReetMhaske/IITB-SSP-GLEE-Reet-Mhaske-",
        "description": "GLEE SEISMOLOGY PAYLOAD CODES",
        "stars": 0,
        "forks": 0,
        "readme": "# IITB-SSP-GLEE-Reet\nCodes Under GLEE\n",
        "createdAt": "2021-07-07T17:01:22.000Z",
        "updatedAt": "2021-10-30T11:57:00.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ReetMhaske/IITB-SSP-GLEE-Reet-Mhaske-/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "chrismentrek/OhioSeis",
        "url": "https://github.com/chrismentrek/OhioSeis",
        "description": "Eduation resources for Ohio earthquake / seismology lessons",
        "stars": 0,
        "forks": 0,
        "readme": "# OhioSeis\nEduation resources for Ohio earthquake / seismology lessons\n",
        "createdAt": "2015-08-17T15:30:54.000Z",
        "updatedAt": "2015-08-17T15:30:54.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/chrismentrek/OhioSeis/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "anyshake/nexus",
        "url": "https://github.com/anyshake/nexus",
        "description": "⛩️ A plugin that can seamlessly link AnyShake products to SeisComP.",
        "stars": 3,
        "forks": 3,
        "readme": "# nexus\n\n⛩️ A plugin that can seamlessly link AnyShake products to SeisComP.\n",
        "createdAt": "2025-04-27T19:09:13.000Z",
        "updatedAt": "2025-08-29T20:10:34.000Z",
        "language": "Go",
        "homepage": "https://anyshake.org",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/anyshake/nexus/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "shuleyu/seismic-tomography-models",
        "url": "https://github.com/shuleyu/seismic-tomography-models",
        "description": "whole mantle tomography, seismology, S and P seismic wave speed models",
        "stars": 42,
        "forks": 20,
        "readme": "# seismic-tomography-models\n\n\n![Summary](https://github.com/shuleyu/seismic-tomography-models/blob/master/Summary.txt)\n![alt text](https://github.com/shuleyu/seismic-tomography-models/blob/master/src/utils/largePlot.png)\n\nIn this database, I collected several whole mantle tomography models.\nI converted them into nc files for conformity and easy-access.\nEach nc files contain 4 variables: depth, longitude, latitude and \"v\".\nThe \"v\"alues could represent:\n- S wave or P wave speed (in km/s)\n- S wave or P velocity purturbation (%)\n- density (in kg/m^3)\n- attenuation factor Q\n- anisotropic parameters\n- etc.\n\nPlease refer to the file name and the comments within each nc files for the\nmeaning of \"v\"alue.\n\nThe c++ tools to read the nc files and to get value at\ncertain location are provided in `src`\n\nDependencies are \n- netcdf-4.6.1\n- (optional, for plotting) GMT-5.4.4\n\nHere are details about data sources and preprocessings to make these nc files:\n\n1. GyPSuM\\_vs.nc\\\nPaper        : [Simmons et al., 2010] https://doi.org/10.1029/2010JB007631 \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10131119 (netCDF binary of S velocity expressed as km/s)\\\nChanges      : rename variable \"vs\" to \"v\".\n\n2. GyPSuM\\_vp.nc\\\nPaper        : [Simmons et al., 2010] https://doi.org/10.1029/2010JB007631 \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10131119 (netCDF binary of P velocity expressed as km/s)\\\nChanges      : rename variable \"vp\" to \"v\".\n\n3. SEMum\\_vs.nc\\\nPaper        : [Lekic and Romanowicz, 2011] https://doi.org/10.1111/j.1365-246X.2011.04969.x \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10131263 (the netCDF binary for the above model expressed as absolute velocity)\\\nChanges      : lose variable \"xi\", rename variable \"vs\" to \"v\".\n\n4. SEMum\\_xi.nc\\\nPaper        : [Lekic and Romanowicz, 2011] https://doi.org/10.1111/j.1365-246X.2011.04969.x \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10131263 (the netCDF binary for the above model expressed as absolute velocity)\\\nChanges      : lose variable \"vs\", rename variable \"xi\" to \"v\". xi= (vsh^2/vsv^2).\n\n5. MITP08\\_dvp.nc\\\nPaper        : [Li et al., 2008] https://doi.org/10.1029/2007GC001806 \\\nDownload link: https://agupubs.onlinelibrary.wiley.com/action/downloadSupplement?doi=10.1029%2F2007GC001806&file=ggge1202-sup-0002-ds01.txt.gz \\\nChanges      : convert the ascii file into nc file, rename variable \"dvp\" to \"v\", see https://github.com/shuleyu/seismic-tomography-models/blob/master/Processing/Create_MITP08_dvp.cpp\n\n6. MITP08\\_vp.nc\\\nPaper        : [Li et al., 2008] https://doi.org/10.1029/2007GC001806 \\\nDownload link: https://agupubs.onlinelibrary.wiley.com/action/downloadSupplement?doi=10.1029%2F2007GC001806&file=ggge1202-sup-0002-ds01.txt.gz \\\nChanges      : convert dvp into vp using ak135 reference model and create this nc file in which \"v\" representing \"vp, km/s\", see https://github.com/shuleyu/seismic-tomography-models/blob/master/Processing/Create_MITP08_vp.cpp\n\n7. SAW642AN\\_vs.nc\\\nPaper        : [Panning and Romanowicz, 2006] https://doi.org/10.1111/j.1365-246X.2006.03100.x \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10131202 (the netCDF binary for the above model expressed in km/s)\\\nChanges      : lose variables \"vp\", \"rho\", \"Qs\", rename variable \"vs\" to \"v\".\n\n8. SAW642AN\\_vp.nc\\\nPaper        : [Panning and Romanowicz, 2006] https://doi.org/10.1111/j.1365-246X.2006.03100.x \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10131202 (the netCDF binary for the above model expressed in km/s)\\\nChanges      : lose variables \"vs\", \"rho\", \"Qs\", rename variable \"vp\" to \"v\".\n\n9. SAW642AN\\_rho.nc\\\nPaper        : [Panning and Romanowicz, 2006] https://doi.org/10.1111/j.1365-246X.2006.03100.x \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10131202 (the netCDF binary for the above model expressed in km/s)\\\nChanges      : lose variables \"vs\", \"vp\", \"Qs\", rename variable \"rho\" to \"v\".\n\n10. SAW642AN\\_qs.nc\\\nPaper        : [Panning and Romanowicz, 2006] https://doi.org/10.1111/j.1365-246X.2006.03100.x \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10131202 (the netCDF binary for the above model expressed in km/s)\\\nChanges      : lose variables \"vs\", \"vp\", \"rho\", rename variable \"Qs\" to \"v\".\n\n11. SAW642ANb\\_vs.nc\\\nPaper        : [Panning and Romanowicz, 2010] https://doi.org/10.1029/2010JB007520 \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10131193 (the netCDF binary for the above model expressed in km/s)\\\nChanges      : lose variables \"vp\", \"rho\", \"Qs\", rename variable \"vs\" to \"v\".\n\n12. SAW642ANb\\_vp.nc\\\nPaper        : [Panning and Romanowicz, 2010] https://doi.org/10.1029/2010JB007520 \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10131193 (the netCDF binary for the above model expressed in km/s)\\\nChanges      : lose variables \"vs\", \"rho\", \"Qs\", rename variable \"vp\" to \"v\".\n\n13. SAW642ANb\\_rho.nc\\\nPaper        : [Panning and Romanowicz, 2010] https://doi.org/10.1029/2010JB007520 \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10131193 (the netCDF binary for the above model expressed in km/s)\\\nChanges      : lose variables \"vs\", \"vp\", \"Qs\", rename variable \"rho\" to \"v\".\n\n14. SAW642ANb\\_qs.nc\\\nPaper        : [Panning and Romanowicz, 2010] https://doi.org/10.1029/2010JB007520 \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10131193 (the netCDF binary for the above model expressed in km/s)\\\nChanges      : lose variables \"vs\", \"vp\", \"rho\", rename variable \"Qs\" to \"v\".\n\n15. SAW24B16\\_vs.nc\\\nPaper        : [Megnin and Romanowicz, 2000] https://doi.org/10.1046/j.1365-246X.2000.00298.x \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10131184 (The netCDF binary for the above 2°x2° model expressed as S-velocity in km/s)\\\nChanges      : rename variable \"vsh\" to \"v\".\n\n16. S362ANI\\_vs.nc\\\nPaper        : [Kustowski et al., 2008] https://doi.org/10.1029/2007JB005169 \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10131231 (the netCDF binary for the above model expressed in km/s)\\\nChanges      : lose variables \"vsv\", \"vsh\", rename variable \"vs\" to \"v\".\n\n17. S362ANI\\_vsv.nc\\\nPaper        : [Kustowski et al., 2008] https://doi.org/10.1029/2007JB005169 \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10131231 (the netCDF binary for the above model expressed in km/s)\\\nChanges      : lose variables \"vs\", \"vsh\", rename variable \"vsv\" to \"v\".\n\n18. S362ANI\\_vsh.nc\\\nPaper        : [Kustowski et al., 2008] https://doi.org/10.1029/2007JB005169 \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10131231 (the netCDF binary for the above model expressed in km/s)\\\nChanges      : lose variables \"vs\", \"vsv\", rename variable \"vsh\" to \"v\".\n\n19. S362ANI+M\\_vs.nc\\\nPaper        : [Moulik and Ekstrom] https://doi.org/10.1093/gji/ggu356 \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10131294 (the netCDF binary for the above model expressed in km/s)\\\nChanges      : lose variables \"vsv\", \"vsh\", rename variable \"vs\" to \"v\".\n\n20. S362ANI+M\\_vsv.nc\\\nPaper        : [Moulik and Ekstrom] https://doi.org/10.1093/gji/ggu356 \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10131294 (the netCDF binary for the above model expressed in km/s)\\\nChanges      : lose variables \"vs\", \"vsh\", rename variable \"vsv\" to \"v\".\n\n21. S362ANI+M\\_vsh.nc\\\nPaper        : [Moulik and Ekstrom] https://doi.org/10.1093/gji/ggu356 \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10131294 (the netCDF binary for the above model expressed in km/s)\\\nChanges      : lose variables \"vs\", \"vsv\", rename variable \"vsh\" to \"v\".\n\n22. S362WMANI\\_vs.nc\\\nPaper        : [Kustowski et al., 2008] https://doi.org/10.1029/2007JB005169 \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10131240 (the netCDF binary for the above model expressed in km/s)\\\nChanges      : lose variables \"vsv\", \"vsh\", rename variable \"vs\" to \"v\".\n\n23. S362WMANI\\_vsv.nc\\\nPaper        : [Kustowski et al., 2008] https://doi.org/10.1029/2007JB005169 \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10131240 (the netCDF binary for the above model expressed in km/s)\\\nChanges      : lose variables \"vs\", \"vsh\", rename variable \"vsv\" to \"v\".\n\n24. S362WMANI\\_vsh.nc\\\nPaper        : [Kustowski et al., 2008] https://doi.org/10.1029/2007JB005169 \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10131240 (the netCDF binary for the above model expressed in km/s)\\\nChanges      : lose variables \"vs\", \"vsv\", rename variable \"vsh\" to \"v\".\n\n25. HMSL-S06\\_dvs.nc\\\nPaper        : [Houser et al., 2008] https://doi.org/10.1111/j.1365-246X.2008.03763.x \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10131133 (The netCDF binary of the model)\\\nChanges      : rename variable \"dvs\" to \"v\".\n\n26. HMSL-S06\\_vs.nc\\\nPaper        : [Houser et al., 2008] https://doi.org/10.1111/j.1365-246X.2008.03763.x \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10131133 (The netCDF binary of the model)\\\nChanges      : convert dvs into vs using ak135 reference model. However, this doesn't have much meaning. In the model's online discription \"While ak135 is the 1D model used for ray tracing in the inversion, the travel time measurements for each phase have the mean removed, so the 3D model is not built with respect to any particular 1D model.\", therefore use with care, in this nc file, \"v\" represents \"vs\", see https://github.com/shuleyu/seismic-tomography-models/blob/master/Processing/Create_HMSL-S06_vs.cpp\n\n27. HMSL-P06\\_dvp.nc\\\nPaper        : [Houser et al., 2008] https://doi.org/10.1111/j.1365-246X.2008.03763.x \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10131126 (The netCDF binary of the model)\\\nChanges      : rename variable \"dvp\" to \"v\".\n\n28. HMSL-P06\\_vp.nc\\\nPaper        : [Houser et al., 2008] https://doi.org/10.1111/j.1365-246X.2008.03763.x \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10131126 (The netCDF binary of the model)\\\nChanges      : convert dvp into vp using ak135 reference model. However, this doesn't have much meaning. In the model's online discription \"While ak135 is the 1D model used for ray tracing in the inversion, the travel time measurements for each phase have the mean removed, so the 3D model is not built with respect to any particular 1D model.\", therefore use with care, in this nc file, \"v\" represents \"vp\", see https://github.com/shuleyu/seismic-tomography-models/blob/master/Processing/Create_HMSL-P06_vp.cpp\n\n29. LLNL-G3Dv3\\_vp.nc\\\nPaper        : [Simmons et al., 2012] https://doi.org/10.1029/2012JB009525 \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10131310 (Complete model package contains LLNL\\_G3Dv3\\_README.pdf file detailing the contents. Velocity values are expressed as absolute velocity and percentage variations relative to the layer average)\\\nChanges      : Use the depth (column 3) in \"LLNL\\_G3Dv3.LayerAverages.txt\", latitude ranging from -90 to 90, longitude ranging from -180 to 180, values (column 2) in \"LLNL\\_G3Dv3.Interpolated.Layer??\\_\\*km.txt\" to creat the vp grid. This ignore a lot details. Use with care. in this nc file, \"v\" represents \"vp\". see https://github.com/shuleyu/seismic-tomography-models/blob/master/Processing/Create_LLNL-G3Dv3_vp.cpp\n\n30. LLNL-G3Dv3\\_dvp.nc\\\nPaper        : [Simmons et al., 2012] https://doi.org/10.1029/2012JB009525 \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10131310 (Complete model package contains LLNL\\_G3Dv3\\_README.pdf file detailing the contents. Velocity values are expressed as absolute velocity and percentage variations relative to the layer average)\\\nChanges      : Use the depth (column 3) in \"LLNL\\_G3Dv3.LayerAverages.txt\", latitude ranging from -90 to 90, longitude ranging from -180 to 180, values (column 3) in \"LLNL\\_G3Dv3.Interpolated.Layer??\\_\\*km.txt\" to creat the dvp grid. This ignore a lot details. Use with care. in this nc file, \"v\" represents \"dvp\". see https://github.com/shuleyu/seismic-tomography-models/blob/master/Processing/Create_LLNL-G3Dv3_dvp.cpp\n\n31. TX2000\\_dvs.nc\\\nPaper        : [Davies et al., 2002] https://doi.org/10.1098/rsta.2002.1077 \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10131209 (Model on a 4°x4° grid for depths from 60 to 2800 km at non-uniform intervals in netCDF format.)\\\nChanges      : rename variable \"dvs\" to \"v\". The desicription in the dvs reads \"(% deviation from layer mean)\".\n\n32. TX2011\\_dvs.nc\\\nPaper        : [Davies et al., 2002] https://doi.org/10.1098/rsta.2002.1077 \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10131216 (Model on a 2°x2° grid for depths from 0 to 2890 km in netCDF format)\\\nChanges      : rename variable \"dvs\" to \"v\".\n\n33. TX2011\\_vs.nc\\\nPaper        : [Davies et al., 2002] https://doi.org/10.1098/rsta.2002.1077 \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10131216 (Model on a 2°x2° grid for depths from 0 to 2890 km in netCDF format)\\\nChanges      : convert dvs into vs using TX2011\\_ref downloaded from http://ds.iris.edu/files/products/emc/data/TX2011/TX2011_ref. in this nc file, \"v\" represents \"vs\". see https://github.com/shuleyu/seismic-tomography-models/blob/master/Processing/Create_TX2011_vs.cpp\n\n34. SEMUCB-WM1\\_dvs.nc\\\nPaper        : [French and Romanowicz, 2014] https://doi.org/10.1093/gji/ggu334 \\\nDownload link: http://www.seismo.berkeley.edu/~barbara/REPRINTS/UCB_a3d_dist.SEMUCB-WM1.r20151019.tar.gz \\\nChanges      : Use the program provided in the tar file, a 10km depth interval (from 60km to 2890km to avoid \"nan\"), 1 deg x 1 deg grid is created for dvs. This dvs is \"the relative Voigt-average shear velocity perturbation relative to the reference 1D model data/model.ref\". in this nc file, \"v\" represents \"dvs\". see https://github.com/shuleyu/seismic-tomography-models/blob/master/Processing/Create_SEMUCB-WM1_dvs.cpp\n\n35. SEMUCB-WM1\\_vsh.nc\\\nPaper        : [French and Romanowicz, 2014] https://doi.org/10.1093/gji/ggu334 \\\nDownload link: http://www.seismo.berkeley.edu/~barbara/REPRINTS/UCB_a3d_dist.SEMUCB-WM1.r20151019.tar.gz \\\nChanges      : Use the program provided in the tar file, a 10km depth interval (from 60km to 2890km to avoid \"nan\"), 1 deg x 1 deg grid is created for vsh. in this nc file, \"v\" represents \"vsh\". see https://github.com/shuleyu/seismic-tomography-models/blob/master/Processing/Create_SEMUCB-WM1_vsh.cpp\n\n36. SEMUCB-WM1\\_vsv.nc\\\nPaper        : [French and Romanowicz, 2014] https://doi.org/10.1093/gji/ggu334 \\\nDownload link: http://www.seismo.berkeley.edu/~barbara/REPRINTS/UCB_a3d_dist.SEMUCB-WM1.r20151019.tar.gz \\\nChanges      : Use the program provided in the tar file, a 10km depth interval (from 60km to 2890km to avoid \"nan\"), 1 deg x 1 deg grid is created for vsv. in this nc file, \"v\" represents \"vsv\". see https://github.com/shuleyu/seismic-tomography-models/blob/master/Processing/Create_SEMUCB-WM1_vsv.cpp\n\n37. SEMUCB-WM1\\_vs.nc\\\nPaper        : [French and Romanowicz, 2014] https://doi.org/10.1093/gji/ggu334 \\\nDownload link: http://www.seismo.berkeley.edu/~barbara/REPRINTS/UCB_a3d_dist.SEMUCB-WM1.r20151019.tar.gz \\\nChanges      : This vs is the Voigt-average shear velocity (sqrt((vsh^2+2vsv^2)/3)) in the same grid. in this nc file, \"v\" represents \"vs\". see https://github.com/shuleyu/seismic-tomography-models/blob/master/Processing/Create_SEMUCB-WM1_vs.cpp\n\n38. S20RTS\\_dvs.nc\\\nPaper        : [Ritsema et al., 1999] https://doi.org/10.1126/science.286.5446.1925 \\\nDownload link: https://jritsema.earth.lsa.umich.edu/S20RTS_plotting.tar.gz \\\nChanges      : Because S40RTS has PREM as reference 1D model, I assume S20RTS is also using PREM as 1D reference model. Use the program provided in the tar fille, a 10km depth interval (24.4km, 25km to 2885km, 2891km), 1 deg x 1 deg grid is created for dvs. in this nc file, \"v\" represents \"dvs\". see https://github.com/shuleyu/seismic-tomography-models/blob/master/Processing/Create_S20RTS_dvs.cpp\n\n39. S40RTS\\_dvs.nc\\\nP9per        : [Ritsema et al., 2011] https://doi.org/10.1111/j.1365-246X.2010.04884.x \\\nDownload link: https://jritsema.earth.lsa.umich.edu/S20RTS_plotting.tar.gz \\\nChanges      : In paper it reads \"... S40RTS is a model of 3‐D perturbations of isotropic shear velocity with respect to the PREM model...\". The reference 1D model is PREM. Use the program provided in the tar fille, a 10km depth interval (24.4km, 25km to 2885km, 2891km), 1 deg x 1 deg grid is created for dvs. in this nc file, \"v\" represents \"dvs\". see https://github.com/shuleyu/seismic-tomography-models/blob/master/Processing/Create_S40RTS_dvs.cpp\n\n40. SP12RTS\\_dvs.nc\\\nPaper        : [Koelemeijer et al., 2015] https://doi.org/10.1093/gji/ggv481 \\\nDownload link: https://www.earth.ox.ac.uk/~univ4152/downloads_sp12rts.html (\"SP12RTS\\_plotting.tar.gz\")\\\nChanges      : In \"SP12RTS.1x1.zip\" it reads \"... in % from anisotropic PREM ...\". The reference 1D model should be PREM. Grid depths are taken from \"SP12RTS.1x1.zip\", 1 deg x 1 deg. in this nc file, \"v\" represents \"dvs\". see https://github.com/shuleyu/seismic-tomography-models/blob/master/Processing/Create_SP12RTS_dvs.cpp\n\n41. SP12RTS\\_dvp.nc\\\nPaper        : [Koelemeijer et al., 2015] https://doi.org/10.1093/gji/ggv481 \\\nDownload link: https://www.earth.ox.ac.uk/~univ4152/downloads_sp12rts.html (\"SP12RTS\\_plotting.tar.gz\")\\\nChanges      : In \"SP12RTS.1x1.zip\" it reads \"... in % from anisotropic PREM ...\". The reference 1D model should be PREM. Grid depths are taken from \"SP12RTS.1x1.zip\", 1 deg x 1 deg. in this nc file, \"v\" represents \"dvp\". see https://github.com/shuleyu/seismic-tomography-models/blob/master/Processing/Create_SP12RTS_dvp.cpp\n\n42. SEISGLOB2\\_dvs.nc\\\nPaper        : [Durand et al., 2017] https://doi.org/10.1093/gji/ggx405 \\\nDownload link: http://ds.iris.edu/spud/earthmodel/16588566 (The netCDF binary for the above model expressed as shear velocity (%))\\\nChanges      : rename \"dvs\" to \"v\". Reference model is PREM.\n\n43. SGLOBE-rani\\_dvs.nc\\\nPaper        : [Chang et al., 2015] https://doi.org/10.1002/2014JB011824 \\\nDownload link: http://ds.iris.edu/spud/earthmodel/13619038 (netCDF binary files for the above model expressed as Voigt isotropy)\\\nChanges      : rename \"dvs\" to \"v\". Reference model is PREM.\n\n44. SGLOBE-rani\\_vsv.nc\\\nPaper        : [Chang et al., 2015] https://doi.org/10.1002/2014JB011824 \\\nDownload link: http://ds.iris.edu/spud/earthmodel/13619038 (netCDF binary files for the above model expressed as Voigt isotropy)\\\nChanges      : convert dvs in previous nc file to vs (Voigt isotropy) using PREM. Then use the nc file downloaded from (netCDF binary files for the above model expressed as radial anisotropy in (SH/SV)^2 dimensionless scale) to calculate the vsv. in this nc file, \"v\" represents \"vsv\". see https://github.com/shuleyu/seismic-tomography-models/blob/master/Processing/Create_SGLOBE-rani_vsv_vsh.cpp\n\n45. SGLOBE-rani\\_vsh.nc\\\nPaper        : [Chang et al., 2015] https://doi.org/10.1002/2014JB011824 \\\nDownload link: http://ds.iris.edu/spud/earthmodel/13619038 (netCDF binary files for the above model expressed as Voigt isotropy)\\\nChanges      : convert dvs in previous nc file to vs (Voigt isotropy) using PREM. Then use the nc file downloaded from (netCDF binary files for the above model expressed as radial anisotropy in (SH/SV)^2 dimensionless scale) to calculate the vsh. in this nc file, \"v\" represents \"vsh\". see https://github.com/shuleyu/seismic-tomography-models/blob/master/Processing/Create_SGLOBE-rani_vsv_vsh.cpp\n\n46. SPani\\_dvp.nc\\\nPaper        : [Tesoniero et al., 2015] https://doi.org/10.1002/2015JB012026 \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10162642 (the netCDF binary for the above model)\\\nChanges      : lose variables \"dlnvs\", \"vp\", \"vs\", \"xi\", \"phi\", rename variable \"dlnvp\" to \"v\". Relative to PREM.\n\n47. SPani\\_dvs.nc\\\nPaper        : [Tesoniero et al., 2015] https://doi.org/10.1002/2015JB012026 \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10162642 (the netCDF binary for the above model)\\\nChanges      : lose variables \"dlnvp\", \"vp\", \"vs\", \"xi\", \"phi\", rename variable \"dlnvs\" to \"v\". Relative to PREM.\n\n48. SPani\\_vp.nc\\\nPaper        : [Tesoniero et al., 2015] https://doi.org/10.1002/2015JB012026 \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10162642 (the netCDF binary for the above model)\\\nChanges      : lose variables \"dlnvp\", \"dlnvs\", \"vs\", \"xi\", \"phi\", rename variable \"vp\" to \"v\". vp=(vpv^2+4vph^2)/5.\n\n49. SPani\\_vs.nc\\\nPaper        : [Tesoniero et al., 2015] https://doi.org/10.1002/2015JB012026 \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10162642 (the netCDF binary for the above model)\\\nChanges      : lose variables \"dlnvp\", \"dlnvs\", \"vp\", \"xi\", \"phi\", rename variable \"vs\" to \"v\". vs=(vph^2+2vpv^2)/3;\n\n50. SPani\\_xi.nc\\\nPaper        : [Tesoniero et al., 2015] https://doi.org/10.1002/2015JB012026 \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10162642 (the netCDF binary for the above model)\\\nChanges      : lose variables \"dlnvp\", \"dlnvs\", \"vp\", \"vs\", \"phi\", rename variable \"xi\" to \"v\". In paper it reads xi = (vsh^2/vsv^2). In this nc file, the comment reads xi=(vsv^2/vsh^2), which could be a typo.\n\n51. SPani\\_phi.nc\\\nPaper        : [Tesoniero et al., 2015] https://doi.org/10.1002/2015JB012026 \\\nDownload link: http://ds.iris.edu/spud/earthmodel/10162642 (the netCDF binary for the above model)\\\nChanges      : lose variables \"dlnvp\", \"dlnvs\", \"vp\", \"vs\", \"xi\", rename variable \"phi\" to \"v\". In paper it reads phi = (vpv^2/vph^2).\n\n52. GAP\\_P4\\_dvp.nc\\\nPaper        : [Obayashi et al., 2013] https://doi.org/10.1002/2013GL057401 [Fukao and Obayashi, 2013] https://doi.org/10.1002/2013JB010466 \\\nDownload link: http://www.godac.jamstec.go.jp/catalog/data_catalog/metadataDisp/GAP_P4?lang=en \\\nChanges      : created GAP_P4.01 ~ GAP_P4.29, then doubled the depth layer. For example: 29 and 50.999 km get values from GAP_P4.01\n",
        "createdAt": "2018-09-12T21:39:04.000Z",
        "updatedAt": "2025-12-03T03:09:28.000Z",
        "language": "C++",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/shuleyu/seismic-tomography-models/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "velgueta/Theoretical-seismology-homeworks",
        "url": "https://github.com/velgueta/Theoretical-seismology-homeworks",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2024-05-08T07:22:07.000Z",
        "updatedAt": "2024-06-06T04:04:23.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "xemerald/TankConvertor",
        "url": "https://github.com/xemerald/TankConvertor",
        "description": "Convert the tank format to other kinds of seismological using format, like SAC or miniSEED.",
        "stars": 0,
        "forks": 1,
        "readme": "# TankConvertor\nConvert the tank format to other kinds of seismological using format, like SAC or miniSEED.\n\n## Dependencies\nNothing special, it can be compiled under standard C library & pre-attached miniSEED library.\n\nNOTE: miniSEED library version should newer than **3.0** if you want to use your own libmseed.\n\n## Supported Platforms\n- Linux\n- MacOS (**Not pass testing yet**)\n\n## Building & Installation\n- Linux/MacOS\n\t- Simply run `make normal`\n\t- Or run `make centos` under redhat-like distribution\n\t- Then you can run `sudo make install` to install this program to `/usr/local/bin`\n\n## Usage\n- `./tbconvert -h` show some helping tips.\n- `./tbconvert -v` show version information.\n- `./tbconvert -f SAC input.tnk` or `./tbconvert input.tnk` convert the input.tnk into SAC format file(s).\n- `./tbconvert -f MSEED input.tnk` convert the input.tnk into **MSEED(version 2)** format file.\n- `./tbconvert -f MSEED3 input.tnk` convert the input.tnk into **MSEED(version 3)** format file.\n- `./tbconvert -f SAC -o output input.tnk` convert the input.tnk into SAC format file(s) & store in specified **output** directory.\n- `./tbconvert -f MSEED -o output.mseed input.tnk` convert the input.tnk into **MSEED(version 2)** format file with specified file name, **output.mseed**.\n\n## Something else\n- When **NOT** specified the output path with **-o**, the result file(s) will be output to the current directory.\n- Under miniSEED output, the earthworm null location code **\"--\"** will be omitted. However it will be kept under SAC output.\n\n## License\n\nCopyright&copy; 2019-2022 Benjamin Ming Yang\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n\n[Apache License 2.0](http://www.apache.org/licenses/LICENSE-2.0)\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n\nThis Software contains [miniSEED Library v3.0.11](https://github.com/iris-edu/libmseed) under Apache License 2.0. For the licensing rule of miniSEED Library v3.0.11, please see the license file inside libmseed directory.\n",
        "createdAt": "2019-04-18T07:07:46.000Z",
        "updatedAt": "2022-09-16T12:18:41.000Z",
        "language": "C",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/xemerald/TankConvertor/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "eejwa/Array_Seis_Circle",
        "url": "https://github.com/eejwa/Array_Seis_Circle",
        "description": "(work in progress) python package for array seismology methods correcting for a curved wavefront.",
        "stars": 10,
        "forks": 1,
        "readme": "# Array_Seis_Circle\nPython package for array seismology methods correcting for a curved wavefront.\n\nThe code is currently being improved and functions added regularly, therefore there may still be some bugs in the code. This is mainly a result of me using this for specific purposes. There will be additional utilities added to this package as time goes on.\n\nI would suggest cloning this onto your machine and add the path to circ_array to your $PYTHON_PATH for use.\n\n## Brief Descriptions of the directories and what is in them:\n\n### ./circ_array/\n\n  - arf.py: holds function to calculate array response function.\n  - array_info.py: holds 'array' class which then can get info from obspy stream.\n  - array_plotting.py: class with plotting functions.\n  - beamforming_polar.py: holds functions to perform grid search over slowness vectors in polar format.\n  - beamforming_xy.py: functions to perform beamforming over grid of slowness vectors in x/y format.\n  - cluster_utilities.py: class with functions to calculate the\n                          properties of clusters found from\n                          DBSCAN.\n  - extract_peaks.py: find peaks from 2-D array.\n  - geo_sphere_calcs.py: functions to calculate distances and relocate points on a sphere.\n  - make_sub_array.py: functions to break up sup arrays.\n  - manual_pick.py: allows user to pick time window on a record section.\n  - output_writing.py: functions to write results to file.\n  - rl_decon.py: performs richardson-lucy deconvolution.\n  - shift_stack.py: functions to calculate time shifts and shift seismograms.\n  - slow_vec_calcs.py: calculates locus and converts from polar to cartesian representations.\n  - utilities.py: to round and clip traces in stream.\n  - vespagram.py: calculates vespagrams in backazimuth and horizontal slowness.\n\n### ./docs/\n  - doc_Array_Circ.py: prints the doc string for the utilities module.\n  - doc_Array_Plotting.py: prints the doc string for the plotting module.\n  - doc_Circ_Beam.py: prints the doc string for the beamforming module.\n  - doc_Cluster_Utilities.py: prints the doc string for the cluster utilities class.\n\n\n### ./scripts/\n  Several python codes which utilise functions in the module to perform analysis.\n  These act as out of the box tools for users to use and edit to their own purposes.\n  The current tools include:\n\n  - Bootstrap_Peak_Recover_XY.py\n    - Bootstrap samples the waveforms and recovers potential slowness vectors via beamforming\n      and estimating a noise value.\n      - Takes parameters from Parameters_Bootstrap.py.\n      - Can be run over multiple cores with `$ mpirun -np N Bootstrap_Peak_Recover_XY.py`,\n        where N is the number of cores.\n      - These are stored in a numpy array in a results directory defined by the user.\n\n  - break_sub_arrays.py\n    - Give arguments for radius of sub arrays, min number of stations, distance between sub array centres,\n      output file name and string describing files you want to use.\n    - Will output plots of sub arrays and stations as well as a file with the sub array information.\n    - A separate script will need to be written to make/use sub array information.\n    - output file format:\n      - lon_centre lat_centre number_of_stations station_names\n\n  - Clustering.py\n    - Using the output of Bootstrap_Peak_Recover_XY.py, perform DBSCAN with MinPts\n      and $\\epsilon$ defined in Parameters_Bootstrap.py.\n      - Will plot the resulting clusters and noise with the mean of all the $\\theta-p$\n        plots found from each bootstrap sample.\n\n  - pick_tw.py\n    - Given a stream of SAC files, plot a record section and allow user to manually pick a time window.\n    - writes to 'tw.txt'\n\n  - Plot_Record_Section.py\n    - Takes in file_path, target phase, time before target, time after target, whether you want to filter or not, minimum frequency and maximum frequency from the command line.\n    - Plots two record sections one aligning the traces on the phase and one not aligning the traces.\n\n  - Plot_stations.py\n    - Takes path to data files from the command line.\n    - Plots the stations and great circle path from event to stations.\n    - Does not save to pdf currently.\n\n\n  - TP_XY.py\n    - Performs a beamforming grid search correcting for a curved wavefront describing slowness vectors in cartesian\n      coordinates.\n      - Input parameters are taken from \"Parameters_TP_XY.py\".\n      - Can stack either linearly or phase weighted stacking.\n      - Currently, the code targets a particular phase such as SKS. The predicted\n        arrival time needs to be in one of the sac headers tn and the phase name\n        in the associated ktn.\n      - Results of backazimuth and horizontal slowness deviations are stored in the\n        results directory defined in the parameter file.\n      - Can manually pick the time window or give times relative to predicted arrival\n        time of target phase.\n      - Can perform a relative beamforming process by aligning on the horizontal slowness\n        of the target phase.\n\n  - TP_Pol.py\n    - Performs a beamforming grid search correcting for a curved wavefront describing slowness vectors in polar coordinates.\n      - Input parameters are taken from \"Parameters_TP_Pol.py\".\n      - Can stack either linearly or phase weighted stacking.\n      - Currently, the code targets a particular phase such as SKS. The predicted\n        arrival time needs to be in one of the sac headers tn and the phase name\n        in the associated ktn.\n      - Results of backazimuth and horizontal slowness deviations are stored in the\n        results directory defined in the parameter file.\n      - Can manually pick the time window or give times relative to predicted arrival\n        time of target phase.\n      - Can perform a relative beamforming process by aligning on the horizontal slowness\n        of the target phase.\n  - Vesp.py\n    - Creates a slowness or backazimuth vespagram with either linear or phase weighted stacking.\n      - The arrival time for the target phase needs to be stored in the SAC headers tn and the phase name in ktn.  \n\n### ./examples/\n  - Several testing scripts which also serve as example python codes using the package.\n  - Example_uses.ipynb: Jupyter notebook showing uses of the package and functions.\n\n### Installation\n  - Install python using [anaconda](https://www.anaconda.com/products/individual) or [miniconda](https://docs.conda.io/en/latest/miniconda.html) to manage the python packages.\n  - Once you have either anaconda or miniconda, make or move to a directory you want to install the code in.\n  - Move to the directory you want to store the code and download the git repository:\n    - ```git clone https://github.com/eejwa/Array_Seis_Circle.git```\n\n  - This should have created a directory called \"Array_Seis_Circle\". Enter this directory:\n    - ```$ cd ./Array_Seis_Circle/```\n\n  - You can install all the python packages you need via the conda environment yml file in the directory. To create a separate conda environment with the packages, run the following in the terminal.  \n    - ```$ conda env create -f Bootstrap_Cluster.yml ```\n    - This contains some hefty packages like sklearn so may take a while on the 'configure environment stage'. Don't worry! It will happen eventually.\n  - Then this environment can be activated by:\n    - ```$ conda activate boots_cluster ```\n    - You will have to activate this environment every time you need to use the code.\n\n  - To use the codes, add the path to Array_Seis_Circle to your $PYTHONPATH by:\n    - ```export $PYTHONPATH=\"${PYTHONPATH}:/path/to/Array_Seis_Circle/circ_array\"```\n    - You can find the path to the directory by typing \"pwd\" while in the Array_Seis_Circle directory.\n\n  - Ok! now time to check if things work!\n    - Run a few scripts in the 'test' and 'scripts' directory.\n\n### ./setup.py\n  - Currently only says to add the path to Circ_Array to your python path.\n",
        "createdAt": "2020-05-29T10:43:20.000Z",
        "updatedAt": "2024-10-04T20:58:06.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.7973945",
            "openAlex": "10.5281/zenodo.7973945",
            "openCitations": "10.5281/zenodo.7973945",
            "dataCite": "10.5281/zenodo.7973945",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/eejwa/Array_Seis_Circle/master/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.7973945",
            "title": "eejwa/Array_Seis_Circle: Alpha release",
            "journal": "Zenodo",
            "dateReleased": "2023-05-26T00:00:00.000Z",
            "abstract": "",
            "citationsArray": []
        },
        "repoDoi": "10.5281/zenodo.7973945",
        "publications": [
            {
                "doi": "10.5281/zenodo.7973945",
                "name": "Array_Seis_Circle: Alpha release",
                "source": "",
                "authorNames": [],
                "abstract": "",
                "publicationDate": "2021-05-19T00:00:00.000Z"
            }
        ]
    },
    {
        "source": "GitHub",
        "name": "umeshkhatiwada13/NASA-Planetary-Seismology-Signal-Detection",
        "url": "https://github.com/umeshkhatiwada13/NASA-Planetary-Seismology-Signal-Detection",
        "description": "This project aims to analyze seismic data from the Apollo missions and the Mars InSight Lander to identify and classify seismic events. Given the power constraints in planetary missions for transmitting high-resolution seismic data back to Earth, the goal is to develop algorithms that distinguish scientifically valuable signals from noise.",
        "stars": 0,
        "forks": 0,
        "readme": "# Planetary Seismology Signal Detection\n\n## Overview\nThis project aims to analyze seismic data from the Apollo missions and the Mars InSight Lander to identify and classify seismic events. Given the constraints of planetary missions, where high-resolution seismic data transmission to Earth requires significant power, the goal is to develop algorithms that can distinguish scientifically valuable signals from noise.\n\n## Objectives\n- Develop a system to process and analyze seismic data from planetary missions.\n- Implement the Short-Term Average/Long-Term Average (STA/LTA) algorithm to detect seismic events.\n- Utilize machine learning techniques, particularly Random Forest classifiers, to separate noise from seismic signals.\n- Visualize the results, including filtered seismic data, detection performance, and classification metrics.\n\n## Data Sources\n- Seismic data from the Apollo missions.\n- Data from the Mars Interior Exploration using Seismic Investigations, Geodesy, and Heat Transport (InSight) Lander.\n- Example datasets provided for training and testing purposes.\n\n## Methodology\n\n### Data Preprocessing\n- Load seismic data and perform necessary cleaning.\n- Apply a bandpass filter to isolate relevant seismic frequencies.\n\n### Event Detection\n- Implement the STA/LTA algorithm to identify potential seismic events in the filtered data.\n- Mark detected events for further analysis.\n\n### Machine Learning Classification\n- Extract features from the data, such as amplitude and energy.\n- Train a Random Forest classifier to distinguish between noise and signal.\n- Evaluate model performance using metrics like precision, recall, and F1-score.\n\n### Visualization\n- Plot the filtered seismic data with detected events.\n- Generate confusion matrices and precision-recall curves to assess model performance.\n\n## Results\n- The model successfully identifies seismic events within the noisy data.\n- Visualizations provide insight into the model's performance and the characteristics of the seismic signals.\n",
        "createdAt": "2024-10-06T22:01:36.000Z",
        "updatedAt": "2024-10-06T22:06:27.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/umeshkhatiwada13/NASA-Planetary-Seismology-Signal-Detection/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "csmcgs/eqseis_data_processing",
        "url": "https://github.com/csmcgs/eqseis_data_processing",
        "description": "Seismic Data Processing Exercises for GPGN455/555 EARTHQUAKE SEISMOLOGY",
        "stars": 0,
        "forks": 0,
        "readme": "# Seismic Data Processing Notebooks for Earthquake Seismology Class\n\n## Running the notebooks\n\nYou can run the notebooks using [Docker](#using-docker), [Binder](#using-binder) or [Anaconda](#using-anaconda).\n\n### Using Docker\n\n#### Installing Docker\n\nFollow the guides for your operating system:\n\n- [Windows](https://docs.docker.com/desktop/install/windows-install/)\n- [Mac](https://docs.docker.com/desktop/install/mac-install/)\n- [Linux](https://docs.docker.com/desktop/linux/)\n\n#### Running the Docker container\n\nIn the terminal window, first create and change into a working directory (e.g. `gpn455_eqseis_notebooks`)\n\n```console\n$ mkdir gpn455_eqseis_notebooks\n$ cd gpn455_eqseis_notebooks\n```\n\nthen run the following `docker run` command.\n\n```console\n$ docker run -v ${PWD}:/notebooks --rm -p 8888:8888 csmcgs/eqseis:2022\n```\n\nCommand Explanation:\n- `docker run` docker run command\n- `-v ${PWD}:/notebooks` binds current working directory (host) to container's `/notebooks` directory.\n- `--rm` removes container after shutdown (optional)\n- `-p 8888:8888` bridges port 8888 between host and container. If your computer has some application using `8888` port it can be changed to another number (`8889`, `9000` e.g.).\n- `csmcgs/eqseis:2022` docker image name that denotes username, image name and the version.\n\nCopy the last url (it should look like: `http://127.0.0.1:8888/lab?token=...`) printed to the terminal to your web browser window. Open the `Welcome` page which can be seen in the left pane. Execute the notebook to get the latest exercises into your computer. Following the link should take you to the `Index` page.\n\n![Welcome notebook in the file manager](img/fm_welcome.png)\n\n### Using Binder\n\nYou can run the notebooks on the Binder cloud service by clicking the following link:\n\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/csmcgs/eqseis_data_processing/HEAD)\n\n### Using Anaconda\n\nYou can also download the repository and create a conda environment to run the notebooks.\n\n```console\n$ git clone https://github.com/csmcgs/eqseis_data_processing.git\n$ cd eqseis_data_processing\n$ conda env create -f environment.yml\n$ conda activate eqseis_data_processing\n$ jupyter lab\n```\n\n\n\n",
        "createdAt": "2022-11-27T12:02:05.000Z",
        "updatedAt": "2022-11-27T16:04:43.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/csmcgs/eqseis_data_processing/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "zhaozhiyuan1989/seisnote",
        "url": "https://github.com/zhaozhiyuan1989/seisnote",
        "description": "学习笔记",
        "stars": 12,
        "forks": 0,
        "readme": "# 地震学学习笔记\r\n\r\n![CI](https://github.com/zhaozhiyuan1989/seisnote/workflows/CI/badge.svg)\r\n[![License: CC BY-NC 4.0](https://img.shields.io/badge/License-CC%20BY--NC%204.0-blue.svg)](https://creativecommons.org/licenses/by-nc/4.0/deed.zh)\r\n\r\n\r\n本项目主要用来记录学习过程中遇到的一些问题。\r\n\r\n- 项目主页： https://github.com/zhaozhiyuan1989/seisnote\r\n- GitHub Pages： https://zhaozhiyuan1989.github.io/seisnote/\r\n- Gitee Pages： https://seismology.gitee.io/seisnote\r\n\r\n因为 Gitee Pages 目前是手动同步 GitHub Pages，所以会有延迟更新。\r\n\r\n## 构建项目\r\n\r\n本项目使用 [Sphinx](http://www.sphinx-doc.org/) 构建得到。Sphinx 是基于 Python 的\r\n文档生成工具。\r\n\r\n1.  下载文档源码\r\n\r\n        $ git clone git@github.com:zhaozhiyuan1989/seisnote.git\r\n\r\n2.  安装所需依赖\r\n\r\n- 方法一：利用 pip 安装依赖，不能执行 python 脚本\r\n\r\n        $ cd seisnote\r\n        $ pip install -r requirements.txt\r\n\r\n- 方法二：利用 conda 安装依赖，**强烈建议**\r\n\r\n        $ cd seisnote\r\n        # 利用 environment.yml 构建运行环境\r\n        $ conda env create  # 也许你需要为 conda 添加国内镜像源\r\n        # 激活运行环境\r\n        $ conda activate seisnote\r\n        # 查看所有环境\r\n        $ conda info -e\r\n        # 你可以随时删除创建的 seisnote 环境\r\n        $ conda activate base\r\n        $ conda remove -n seisnote --all\r\n\r\n\r\n3.  编译生成 HTML 格式的文档。生成的文档位于 `build/html/` 目录下\r\n\r\n        # 执行时 obspy 手册中的 python 脚本会自动运行\r\n        # 网络不好可能会卡死\r\n        $ make html\r\n\r\n        # 可以执行如下命令只运行 pygmt 手册中的脚本\r\n        $ make pyg\r\n\r\n        # 执行如下命令不自动运行所有 python 脚本\r\n        $ make nofig\r\n\r\n        # 删除临时文件\r\n        $ make clean \r\n\r\n\r\n4.  维护\r\n\r\n        # 创建并切换到开发分支\r\n        $ git checkout -b mydev\r\n        \r\n        # 合并主分支的更新\r\n        $ git checkout main\r\n        $ git pull\r\n        $ git checkout mydev\r\n        $ git merge main\r\n\r\n        # 修改并提交\r\n        $ git add --all\r\n        $ git commit -m \"\"\r\n\r\n        # 将开发分支推送到远程仓库\r\n        $ git push -u origin mydev\r\n        $ git push\r\n        \r\n        # 切换回主分支\r\n        $ git checkout main\r\n        # 删除本地开发分支\r\n        $ git branch -D mydenv\r\n        # 删除远程开发分支\r\n        $ git push origin :mydev\r\n\r\n## 项目部署\r\n\r\n本项目采用 GiHub Actions 自动部署到 gh-pages 分支。\r\n\r\n## 许可协议\r\n\r\n本作品采用 [知识共享署名-非商业性使用 4.0 国际许可协议 (CC BY-NC 4.0)](https://creativecommons.org/licenses/by-nc/4.0/deed.zh) 。\r\n任何人都可以自由地分享、修改本作品，但必须遵循如下条件：\r\n\r\n- 署名：必须提到原作者，提供指向此许可协议的链接，表明是否有做修改\r\n- 非商业性使用：不能对本作品进行任何形式的商业性使用\r\n",
        "createdAt": "2021-01-24T12:12:16.000Z",
        "updatedAt": "2025-05-25T23:29:57.000Z",
        "language": "Python",
        "homepage": " https://zhaozhiyuan1989.github.io/seisnote/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/zhaozhiyuan1989/seisnote/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "aradfarahani/Seismology",
        "url": "https://github.com/aradfarahani/Seismology",
        "description": "This collection of scripts is designed to assist seismologists and geophysicists in their research and data analysis. Whether you’re processing seismic data, visualizing waveforms, or performing complex analyses, these tools will help streamline your workflow.",
        "stars": 26,
        "forks": 3,
        "readme": "# Seismology\n\n[![CodeFactor](https://www.codefactor.io/repository/github/aradfarahani/seismology/badge)](https://www.codefactor.io/repository/github/aradfarahani/seismology)\n\n![Seismic Trace Animation](seismic_trace_deep_zoom_4.gif)\n\n**Coming Soon! Stay tuned for exciting updates!**\n\nWelcome to the **Seismology** repository! This project explores the intersection of machine learning and seismology, providing tools and resources to analyze seismic data, enhance earthquake detection, and improve event characterization.\n\n![Seismic Image](https://github.com/user-attachments/assets/cb18aaf0-494d-4b17-a0e1-1f7302dcac6d)\n\n## Table of Contents\n\n- [Introduction](#introduction)\n- [Features](#features)\n- [Data Sources](#data-sources)\n- [Installation](#installation)\n- [Usage](#usage)\n- [Contributing](#contributing)\n- [License](#license)\n- [Acknowledgements](#acknowledgements)\n\n## Introduction\n\nThe **Seismology** project harnesses the power of machine learning to revolutionize seismic data analysis. Our mission is to develop robust algorithms and tools that improve the accuracy, efficiency, and scalability of earthquake detection and seismic event characterization, benefiting researchers, scientists, and disaster preparedness efforts worldwide.\n\n## Features\n\n- **Data Preprocessing**: Streamlined tools for cleaning, formatting, and preparing seismic datasets.\n- **Machine Learning Models**: State-of-the-art ML implementations for detecting and classifying seismic events.\n- **Visualization**: Interactive and insightful visualizations of seismic data and model predictions.\n- **Benchmarking**: Standardized evaluation metrics and benchmarks to assess model performance.\n\n## Data Sources\n\nThis project leverages the following publicly available datasets:\n\n- **[ObsPy Seismic Data](https://examples.obspy.org/RJOB_061005_072159.ehz.new)**: Sample seismic waveforms for testing and development.\n- **[USGS Earthquake Data](https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/2.5_month.csv)**: Real-time and historical earthquake records (magnitude 2.5+ over the past month).\n\nAdditional datasets may be integrated as the project evolves—stay tuned!\n\n## Installation\n\nTo set up the **Seismology** project locally, follow these steps:\n\n1. Clone the repository:\n   ```bash\n   git clone https://github.com/aradfarahani/Seismology.git\n   cd Seismology\n   ```\n\n2. Install dependencies (requirements will be added soon—check back for updates!):\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. Verify the setup (instructions forthcoming as the project develops).\n\n## Usage\n\nDetailed usage instructions will be provided once the core functionality is released. For now, explore the repository structure and sample data to get familiar with the project. Example workflows and scripts will be added in upcoming updates.\n\n## Contributing\n\nWe warmly welcome contributions from the community! To contribute:\n\n1. Fork the repository.\n2. Create a feature branch:\n   ```bash\n   git checkout -b feature/your-feature-name\n   ```\n3. Implement your changes.\n4. Commit your work:\n   ```bash\n   git commit -m \"Add your concise commit message\"\n   ```\n5. Push to your branch:\n   ```bash\n   git push origin feature/your-feature-name\n   ```\n6. Submit a pull request via GitHub.\n\nPlease adhere to the [Contributor Covenant Code of Conduct](https://www.contributor-covenant.org/version/2/0/code_of_conduct/) to ensure a collaborative and inclusive environment.\n\n## License\n\nThis project is licensed under the [MIT License](LICENSE). See the [LICENSE](LICENSE) file for details.\n\n## Acknowledgements\n\nA heartfelt thank you to:\n- The open-source community for their invaluable tools and inspiration.\n- Contributors who are helping shape this project.\n- Organizations like [ObsPy](https://obspy.org/) and the [USGS](https://www.usgs.gov/) for providing accessible seismic data.\n\nStay tuned for more updates as we continue to build and refine **Seismology**!\n",
        "createdAt": "2024-08-05T20:42:11.000Z",
        "updatedAt": "2025-11-08T10:31:05.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/aradfarahani/Seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "singhsatyampratap/IITR_seismo",
        "url": "https://github.com/singhsatyampratap/IITR_seismo",
        "description": "Basic Seismological Tools ",
        "stars": 0,
        "forks": 5,
        "readme": "",
        "createdAt": "2019-07-18T09:19:01.000Z",
        "updatedAt": "2020-07-27T16:39:02.000Z",
        "language": "TeX",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "anyshake/observer",
        "url": "https://github.com/anyshake/observer",
        "description": "🔭 Read, parse seismic data from AnyShake Explorer, stream via SeedLink, WebSocket, TCP and archive to database, miniSEED.",
        "stars": 69,
        "forks": 10,
        "readme": "<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/anyshake/observer/master/images/header.png\" width=\"500\" alt=\"banner\" />\n</p>\n\n[![Codacy Badge](https://app.codacy.com/project/badge/Grade/7b75168a5b03403987122835d74bb448)](https://app.codacy.com/gh/anyshake/observer/dashboard)\n[![Downloads](https://img.shields.io/github/downloads/anyshake/observer/total.svg)](https://github.com/anyshake/observer/releases/latest)\n[![Go Report Card](https://goreportcard.com/badge/github.com/anyshake/observer)](https://goreportcard.com/report/github.com/anyshake/observer)\n[![Build Status](https://github.com/anyshake/observer/actions/workflows/release.yml/badge.svg)](https://github.com/anyshake/observer/actions/workflows/release.yml)\n[![Latest Release](https://img.shields.io/github/release/anyshake/observer.svg)](https://github.com/anyshake/observer/releases/latest)\n\n## 🚀 **Join the Open Science Movement!** 🚀\n\nAnyShake Explorer is now **live on Crowd Supply**! This open-source, next-generation seismic monitoring system is officially available for crowdfunding.\n\n👉 **[Order now on Crowd Supply](https://www.crowdsupply.com/senseplex/anyshake-explorer)** and be among the first to experience real-time seismic data visualization, analysis, and export with professional-grade performance.\n\n📣 **Help us spread the word and grow the open science community: [www.crowdsupply.com/senseplex/anyshake-explorer](https://www.crowdsupply.com/senseplex/anyshake-explorer)**\n\n---\n\n## Overview\n\n**AnyShake Observer** is the companion software for [AnyShake Explorer](https://github.com/anyshake/explorer), the world’s first fully open-source, high-precision seismic monitoring system. It is a cross-platform, web-based application designed to visualize, archive, and export seismic data in real time.\n\nAnyShake Observer is written in **Go** and **TypeScript**, supporting a wide range of OS and CPU architectures, including embedded Linux. It offers real-time waveform display, event analysis, and database archiving, with a strong focus on professional usability and extensibility.\n\nIt works seamlessly with **AnyShake Explorer** over a serial connection and supports exporting data in standard seismic formats such as **SAC** and **MiniSEED**, as well as **SeedLink** streaming for networked data systems.\n\n## Features\n\n- 📦 **Single-binary deployment** – fast, simple, and cross-platform\n- 🖥️ **Web-based user interface** – no client installation required\n- 📱 **Responsive design** – optimized for desktop, tablet, and mobile\n- 🌍 **Multi-language support** – auto-detects browser language\n- ⏱️ **Real-time waveform display** – view live seismic data from your device\n- 🎛️ **Multi-channel support** – visualize multiple sensors (e.g. geophone and accelerometer)\n- 📊 **Historical waveform queries** – search by time or global seismic events\n- 🌐 **Sharable waveform links** – share analysis results with a single URL\n- 📸 **Daily helicorder generation** – auto-generated visual timeline of activity\n- 🚨 **QuakeSense service** – built-in earthquake detection engine (STA/LTA and Z-Detect methods)\n- 📁 **Data export** – save data as **MiniSEED**, **SAC**, **TXT**, or **WAV**\n- 🔁 **Streaming & forwarding** – supports **SeedLink** and **TCP** protocols\n- 🧩 **Flexible storage** – compatible with PostgreSQL, MariaDB/MySQL, SQL Server, and SQLite\n- 🔗 **Seamless SeisComP integration** – easily connect to professional seismic networks\n- 🚀 **... and more!** – with active development and community-driven features\n\n## Documentation\n\nStart here 👉 [anyshake.org/docs/software-overview](https://anyshake.org/docs/software-overview/)\n\n## Preview\n\nHere are some screenshots showcasing the key features of **AnyShake Observer**.\n\n### Home Dashboard\n\nThe Home Dashboard provides a concise overview of the station and device status, including current location, service module health, link connectivity, and real-time system statistics.\n\n<img src=\"https://raw.githubusercontent.com/anyshake/observer/master/images/key-features/home.webp\" width=\"600\" alt=\"Home Dashboard\" />\n\n### Realtime Waveform\n\nThe real-time waveform view displays seismic data and the current sample rate from your AnyShake Explorer device in a web-based interface. It supports zooming, panning, and customizable channel layouts. Layout configurations are persistent and can be locked to prevent accidental changes.\n\n<img src=\"https://raw.githubusercontent.com/anyshake/observer/master/images/key-features/realtime.webp\" width=\"600\" alt=\"Realtime View\" />\n\n### SeedLink Streaming\n\nThe AnyShake team independently developed a SeedLink protocol implementation in pure Go ([github.com/bclswl0827/slgo](https://github.com/bclswl0827/slgo)), enabling native SeedLink services without relying on RingServer or SeisComP. This allows seamless integration with tools like Swarm and ObsPy.\n\n<img src=\"https://raw.githubusercontent.com/anyshake/observer/master/images/key-features/seedlink.webp\" width=\"600\" alt=\"SeedLink View\" />\n\n### Historical Data Query\n\nThe historical query feature lets users retrieve waveforms from the database by specifying a time range (up to 1 hour). It also integrates global seismic agency data for reverse earthquake lookup. Retrieved waveform parts can be exported in multiple formats, including MiniSEED, SAC, TXT, and WAV audio.\n\n<img src=\"https://raw.githubusercontent.com/anyshake/observer/master/images/key-features/history.webp\" width=\"600\" alt=\"History Query\" />\n\nExported data formats, such as MiniSEED, are fully compatible with third-party analysis tools like Swarm. The image below shows a seismic event in Myanmar on Mar 28th, 2025, detected from over 2,400 kilometers away using AnyShake Explorer — demonstrating its remarkable sensitivity and real-world performance, on par with many proprietary, closed-source systems.\n\n<img src=\"https://raw.githubusercontent.com/anyshake/observer/master/images/key-features/miniseed.webp\" width=\"600\" alt=\"MiniSEED View\" />\n\n### Data Download\n\nThe Data Download page allows users to access and download daily archived MiniSEED files and helicorder images directly from disk for extended archiving or offline analysis.\n\n<img src=\"https://raw.githubusercontent.com/anyshake/observer/master/images/key-features/download.webp\" width=\"600\" alt=\"Data Download\" />\n\n### Station Metadata\n\nStation metadata files for AnyShake Explorer devices — including instrument response (poles and zeros) are available in both SeisComP XML and FDSNWS StationXML formats. Users can easily copy the content to the clipboard or download it as a file.\n\n<img src=\"https://raw.githubusercontent.com/anyshake/observer/master/images/key-features/metadata.webp\" width=\"600\" alt=\"Station Metadata\" />\n\n### Service Control\n\nA rich set of modular service modules are integrated, each running independently to ensure stability and flexibility. Users can enable, disable, or configure these modules directly through the web interface. The system also includes earthquake detection services using STA/LTA and Z-Detect algorithms (QuakeSense Service). Upon detecting seismic activity, it can push alerts in real time via MQTT protocol.\n\n<img src=\"https://raw.githubusercontent.com/anyshake/observer/master/images/key-features/service.webp\" width=\"600\" alt=\"Service Control\" />\n\n### User Management\n\nThe system supports both administrator and general user roles, making it adaptable for scenarios where multiple individuals manage and operate the site.\n\n<img src=\"https://raw.githubusercontent.com/anyshake/observer/master/images/key-features/users.webp\" width=\"600\" alt=\"User Management\" />\n\n## Credits\n\nThis project is maintained by **SensePlex Limited**, a UK-based company dedicated to developing open-source hardware and software.\n\n## License\n\nThis project is dual-licensed:\n\n1. **Open Source License (AGPLv3):**  \n   You may use, modify, and redistribute this project under the terms of the GNU Affero General Public License version 3.0. This license requires that any derivative works also be released under the same license.\n\n2. **Commercial License:**  \n   If you intend to use this project in closed-source, commercial, or proprietary applications, please contact us at [anyshake@senseplex.net](mailto:anyshake@senseplex.net) to obtain a commercial license.\n\n---\n\n![Star History Chart](https://api.star-history.com/svg?repos=anyshake/observer&type=Date)\n",
        "createdAt": "2023-07-18T14:31:41.000Z",
        "updatedAt": "2025-12-04T01:26:18.000Z",
        "language": "Go",
        "homepage": "https://anyshake.org",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/anyshake/observer/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "DariaMalik/Research-project-M1",
        "url": "https://github.com/DariaMalik/Research-project-M1",
        "description": "Some results from our team project on the image processing for the seismological lab tests.",
        "stars": 0,
        "forks": 0,
        "readme": "# Research-project-M1\nSome results from our team project on the image processing for the seismological lab tests.\n\nThe aim was to write an algorithm which could detect the points of interest on images and extract their coordinates for the following studies. You'll find our project report and code in the repository. The link to the ZIP with images (.mat) is below :  \n\nhttps://www.dropbox.com/s/tqdqeutkv7ds6vx/IRP_images.zip?dl=0\n\n:)\n",
        "createdAt": "2020-11-11T14:02:33.000Z",
        "updatedAt": "2020-11-11T15:09:11.000Z",
        "language": "MATLAB",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/DariaMalik/Research-project-M1/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "alexmcbride/seismologyapp",
        "url": "https://github.com/alexmcbride/seismologyapp",
        "description": "Seismology app created for Mobile Platform Development fourth year coursework",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2019-03-08T17:21:26.000Z",
        "updatedAt": "2019-05-02T20:33:47.000Z",
        "language": "Java",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "TrynaThinkOf1/TerraMetrics",
        "url": "https://github.com/TrynaThinkOf1/TerraMetrics",
        "description": "Global Analysis of Seismology And Weather. ",
        "stars": 3,
        "forks": 0,
        "readme": "# Global Analysis for Weather And Seismic Activity\n\nThis is a Python-based web application that displays **global weather data** (temperature and precipitation) alongside **seismic activity** (earthquakes) on a dynamic, real-time map. \n\nThe application uses a **Flask backend** to process data and generate interactive maps using **Matplotlib** and **Cartopy**, which are then served to the frontend for visualization.\n\n---\n\n## Features\n\n### Weather Data\n- Displays global **temperature** and **precipitation** data.\n- Automatically updates the map at regular intervals to reflect recent weather conditions.\n- Fetches weather data using APIs or libraries like **Meteostat**.\n\n### Seismic Activity\n- Displays earthquake data, including:\n  - **Magnitude** of seismic events.\n  - **Coordinates** (latitude, longitude) of the epicenters.\n- Automatically refreshes to show the latest seismic activity.\n\n### Dynamic Visualization\n- Maps are generated dynamically and displayed on the frontend.\n- Real-time auto-refresh every 30 seconds.\n- Fully responsive map that fits the screen size.\n\n---\n\n## Technologies Used\n\n### Backend\n- **Python 3**: Core language for the backend.\n- **Flask**: Web framework to serve data and generate map images.\n- **Matplotlib**: For creating the map visualizations.\n- **Cartopy**: For rendering global maps and handling geographic projections.\n- **ObsPy**: For fetching seismic data.\n- **Meteostat**: For retrieving weather data (temperature and precipitation).\n\n### Frontend\n- **HTML5**: Structure of the web page.\n- **CSS3**: Styling the webpage for a responsive design.\n- **JavaScript**: For auto-refreshing the map image every 30 seconds.\n\n---\n\n## Installation\n1. Enter Virtual Environment:\n   ```bash\n   python3 -m venv gasawa\n   source gasawa/bin/activate\n   ```\n2. Clone the repository:\n   ```bash\n   git clone https://github.com/TrynaThinkOf1/TerraMetrics.git\n   cd TerraMetrics\n   ```\n3. Install dependancies:\n    ```bash\n   pip install -r requirements.txt\n    ```\n4. Run the server:\n    ```bash\n   python3 BACKEND/server.py\n   ```\n5. Navigate to `http://127.0.0.1:5000/`\n",
        "createdAt": "2024-11-25T18:01:59.000Z",
        "updatedAt": "2025-01-03T03:33:01.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/TrynaThinkOf1/TerraMetrics/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "EIDA/eidaws",
        "url": "https://github.com/EIDA/eidaws",
        "description": "EIDA NG webservice implementations",
        "stars": 2,
        "forks": 3,
        "readme": "================================================\nWebservice implementations being part of EIDA NG\n================================================\n\n.. image:: https://github.com/EIDA/eidaws/workflows/continuous-integration/badge.svg\n   :target: https://github.com/EIDA/eidaws/actions\n.. image:: https://img.shields.io/badge/code%20style-black-000000.svg\n    :target: https://github.com/psf/black\n\n|\n\nThis repository hosts implementations of:\n\n- `eidaws-federator <eidaws.federator/README.rst>`_\n- `eidaws-stationlite <eidaws.stationlite/README.rst>`_\n- `eidaws-endpoint-proxy <eidaws.endpoint_proxy/README.rst>`_\n\n\nInstallation\n============\n\nPackages are deployed as `Python namespace distributions\n<https://packaging.python.org/guides/packaging-namespace-packages/>`_. Detailed\ninstallation instructions come along with the corresponding distributions.\n\nFor production most of the services provide container files ready to be build\nand deployed. For detailed information please refer to the\n`eidaws-federator-deployment\n<https://github.com/EIDA/eidaws-federator-deployment>`_ repository.\n\nIssues\n======\n\nPlease report bugs, issues, feature requests, etc. on `GitHub\n<https://github.com/EIDA/eidaws/issues>`_.\n\n\nLicense\n=======\n\nLicensed under the the `GPLv3 <https://www.gnu.org/licenses/gpl-3.0.en.html>`_.\nFor more information see the `COPYING\n<https://github.com/EIDA/eidaws/tree/master/COPYING>`_ file.\n\n\nContributions\n=============\n\nContributions are very welcome. Made with :two_hearts:.\n",
        "createdAt": "2020-02-26T10:25:17.000Z",
        "updatedAt": "2025-10-10T10:50:07.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/EIDA/eidaws/master/README.rst",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "scheingraber/obspyload",
        "url": "https://github.com/scheingraber/obspyload",
        "description": "ObsPyLoad: a tool for automated seismic data retrieval and quality control - as published at Seismological Research letters.",
        "stars": 3,
        "forks": 1,
        "readme": "",
        "createdAt": "2016-09-29T13:42:56.000Z",
        "updatedAt": "2020-01-16T06:51:57.000Z",
        "language": "Python",
        "homepage": "http://www.seismosoc.org/Publications/SRL/SRL_84/srl_84-3_es/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ghfbsd/SAC",
        "url": "https://github.com/ghfbsd/SAC",
        "description": "Distribution point for SAC/BRIS binary packages ",
        "stars": 1,
        "forks": 0,
        "readme": "# SAC[^rev]\n\n[^rev]: Revision 14\n\nThis is a repository where you can download binary distributions of the\nSAC/BRIS (also known as Mac SAC) for computers running MacOS.  If you are\nnot running MacOS, you need to compile SAC yourself by installing it from\nsource.  An automated script to do this can be found\n[here](https://raw.githubusercontent.com/ghfbsd/SAC/main/dobuild);\nthis is a shell script that downloads, patches and compiles the source code\nbased on educated guesses about your system.\n\nSAC/BRIS is based on the 10.6d source code, updated to equivalent functionality\nas 10.6f, and extended from there.  It is compatible with IRIS's SAC (except\nfor any bugs) but does not contain IRIS's recent extensions to query metadata\nvia SAC and download data.  (These features will be added in a future release.)\nSAC/BRIS is documented in the book, *The Seismic Analysis Code* (Cambridge\nUniversity Press).\n\n# Downloading and Installing\n\nDownloads are in the form of a MacOS disk image (folder) that you can mount\nand look inside of.  Generally, you will find an MacOS package to install and\na separate uninstall app.  The `pkgutil --pkgs` command on MacOS will list the \npackages that you have installed.  For packaging purposes, SAC is called\n`uk.ac.bris.gly.sac`.  The uninstall app will uninstall SAC and, optionally,\nerase all knowledge of having ever\nhad it installed.  After you install, you can close and throw away the disk\nimage.  You can save and cherish it, or download it again later to get the\nuninstall app if you decide to get rid of SAC.\n\nIf your Mac won't let you install from unknown .dmg (disk image) files,\ntemporarily turn off enforcement of this rule from the command line:\n`sudo spctl --master-disable`\nwhich will allow you to see and check \"Anywhere\" from the\nSystem Preferences -> Security & Privacy \"General\" menu.\nAfter installing, you might want to revert to the original protection level\nby re-enabling checks: `sudo spctl --master-enable`\n\nMacOS has evolved through the years through different releases (Panther,\nTiger, Leopard, ..., Big Sur, Monterey) and on different hardware\n(Motorola, PowerPC 32 and 64 bit, Intel 32 and 64 bit, and Apple (ARM)).\nYour download choice depends on the OS release and hardware you're running\non, which, if you're unfamiliar with the details, you can get from the\n`About this Mac` Apple menu item, and from the `uname -a` command.\n\n## Versions and Options\n\nThere are also some features that you might or might not want.  The main choice\nyou have is whether you prefer to use X graphics or native Mac graphics.  If\nyou want to have an pure Mac system, you can opt out of needing to install X and\ndownload the *no X* variant.\n\nSAC implicitly has parallel processing built into it.  For example, if you\nwant to filter data, then the filtering command acts on all traces in SAC\nmemory simultaneously.  Internally, SAC will take advantage of any multiple \nCPUs on your machine to handle the processing for each trace, speeding up the\nprocessing.  If you *don't* want to have SAC monopolize all the processing\npower of your system, don't download a version marked *parallel*.\n\nNot all choices are available for all OS/hardware combinations.  If you\ncrave one, we can build one to suit *provided* we have a system with that\nOS/hardware combination.  Otherwise, build it yourself from source.\n\n## Contemporary versions\n\nInformation about all of the releases may be found [here](https://members.elsi.jp/~george/sac-bugs.html#rel).\n\n| SAC/BRIS Release | 11.6 | 10.9-10.6 | 10.5-10.4 |\n| ---------------- | -----| ---------- | --------- |\n| [grh-116](https://members.elsi.jp/~george/sac-bugs.html#grh116) | [M1 X+mac](https://raw.githubusercontent.com/ghfbsd/SAC/main/rels/MacSAC-grh116-11.6a.dmg) | [Intel X+mac](https://raw.githubusercontent.com/ghfbsd/SAC/main/rels/MacSAC-grh116-10.9j.dmg)  | |\n| | | | |\n| [grh-115](https://members.elsi.jp/~george/sac-bugs.html#grh115) | | [Intel X+mac](https://members.elsi.jp/~george/MacSAC-grh115-10.9i.dmg) | |\n| | | | |\n| [grh-114](https://members.elsi.jp/~george/sac-bugs.html#grh114) | | [Intel X+mac](https://members.elsi.jp/~george/MacSAC-grh114-10.9i.dmg) | |\n| | | | |\n| [grh-113](https://members.elsi.jp/~george/sac-bugs.html#grh113) | | [Intel X+mac](http://www1.gly.bris.ac.uk/MacSAC/MacSAC-grh113-10.9i.dmg) | |\n| | | | |\n| [grh-112](https://members.elsi.jp/~george/sac-bugs.html#grh112) | | [Intel X+mac](http://www1.gly.bris.ac.uk/MacSAC/MacSAC-grh112-10.9i.dmg) | |\n| | | | |\n| [grh-111](https://members.elsi.jp/~george/sac-bugs.html#grh111) | | [Intel X+mac](http://www1.gly.bris.ac.uk/MacSAC/MacSAC-grh111-10.9j.dmg) | |\n| | | | |\n| [grh-110](https://members.elsi.jp/~george/sac-bugs.html#grh110) | | [Intel X+mac parallel](http://www1.gly.bris.ac.uk/MacSAC/MacSAC-grh110-10.6jP.dmg) [Intel X+mac](http://www1.gly.bris.ac.uk/MacSAC/MacSAC-grh110-10.6i.dmg) | [Intel X+mac](http://www1.gly.bris.ac.uk/MacSAC/MacSAC-grh110-10.4i.dmg) [PPC   X+mac](http://www1.gly.bris.ac.uk/MacSAC/MacSAC-grh110-10.4p.dmg) |\n\n\n### Glossary\n* M1 - *Apple silicon* (ARM 64 bit) hardware.\n* Intel - Intel hardware.\n* PPC - PowerPC hardware.\n* X+mac - Support for both X11 graphics and native Mac graphics.\n* mac - Support for native Mac graphics ONLY. This means that you do NOT have to have X11 installed on your Mac in order to run SAC.\n* X - Support for X11 graphics only.\n* parallel - Support for parallel trace operations with OpenMP; sequential\noperations on traces, otherwise.  OpenMP support provided within the SAC\nlibraries and neither gfortran nor OpenMP need be installed on your computer --\nat least that's the idea, so if OpenMP features don't work, report it as a bug!\n",
        "createdAt": "2022-04-07T16:51:57.000Z",
        "updatedAt": "2024-08-08T12:19:37.000Z",
        "language": "Shell",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ghfbsd/SAC/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "smousavi05/dl_seismology",
        "url": "https://github.com/smousavi05/dl_seismology",
        "description": "This repo contains the database and supporting materials for Deep-Learning Seismology",
        "stars": 44,
        "forks": 3,
        "readme": "## This repository contains an _updating database of machine-learning papers in Seismology_ used in [Deep Learning Seismology](https://smousavi05.github.io/dl_seismology/) paper. _You can add or edit your paper info_ to the database if it is missing or the description is not accurate. \n\nSeismic waves from earthquakes and other sources are used to infer the structure and properties of Earth’s interior. The availability of large-scale seismic datasets and the suitability of deep-learning techniques for seismic data processing have pushed deep learning to the forefront of fundamental, long-standing research investigations in seismology. However, some aspects of applying deep learning to seismology are likely to prove instructive for the geosciences, and perhaps other research areas more broadly. Deep learning is a powerful approach, but there are subtleties and nuances in its application. We present a systematic overview of trends, challenges, and opportunities in applications of deep-learning methods in seismology. The large amount and availability of datasets in seismology creates a great opportunity to apply machine learning and artificial intelligence to data processing. Mousavi and Beroza provide a comprehensive review of the deep-learning techniques being applied to seismic datasets, covering approaches, limitations, and opportunities. The trends in data processing and analysis can be instructive for geoscience and other research areas more broadly. —BG The ways in which deep learning can help process and analyze large seismological datasets are reviewed.\n\n![Deep-Learning Seismology](dl-seismology.png)\n### Free-Access Link to the Paper: https://shorturl.at/yNY06\n\n\n    article{\n    doi:10.1126/science.abm4470,\n    author = {S. Mostafa Mousavi  and Gregory C. Beroza },\n    title = {Deep-learning seismology},\n    journal = {Science},\n    volume = {377},\n    number = {6607},\n    pages = {eabm4470},\n    year = {2022},\n    doi = {10.1126/science.abm4470},\n    URL = {https://www.science.org/doi/abs/10.1126/science.abm4470},\n    eprint = {https://www.science.org/doi/pdf/10.1126/science.abm4470},\n    abstract = {}}\n",
        "createdAt": "2022-02-04T00:37:34.000Z",
        "updatedAt": "2025-10-22T12:22:26.000Z",
        "language": null,
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/smousavi05/dl_seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "meqash/leeds_seismology_programs",
        "url": "https://github.com/meqash/leeds_seismology_programs",
        "description": "A repository for scripts, programs and functions created at the University of Leeds for the processing of seismic data. May include software written in Python (obspy library) or SAC.",
        "stars": 1,
        "forks": 0,
        "readme": "# Leeds-Seismology-Programs\nA repository for scripts, programs and functions created at the University of Leeds for the processing of seismic data\n",
        "createdAt": "2019-04-17T05:20:47.000Z",
        "updatedAt": "2020-05-18T15:31:51.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/meqash/leeds_seismology_programs/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "nfsi-canada/OBStools",
        "url": "https://github.com/nfsi-canada/OBStools",
        "description": "Tools for processing broadband ocean-bottom seismic data",
        "stars": 56,
        "forks": 37,
        "readme": "\n![](./obstools/examples/picture/obstools_logo.png)\n\n## Software for processing broadband ocean-bottom seismic data\n\nOBStools is a package containing Python tools for processing broadband\nocean-bottom seismic (OBS) data. Current functionalities include removing\nvertical component noise from tilt and compliance effects, and calculating\nseafloor compliance. The code uses \nthe ``StDb`` package for querying and building a station database and\ncan be used through command-line scripts.\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4281480.svg)](https://doi.org/10.5281/zenodo.4281480)\n[![build](https://github.com/nfsi-canada/OBStools/workflows/Build/badge.svg)](https://github.com/nfsi-canada/OBStools/actions)\n<!-- [![codecov](https://codecov.io/gh/nfsi-canada/OBStools/branch/master/graph/badge.svg)](https://codecov.io/gh/nfsi-canada/OBStools) -->\n\nInstallation, API documentation, scripts and tutorials are described at https://nfsi-canada.github.io/OBStools/\n\nAuthors: [`Pascal Audet`](https://www.uogeophysics.com/authors/admin/) (Developer and Maintainer) & [`Helen Janiszewski`](https://helenjaniszewski.squarespace.com) & (Developer of original Matlab version)\n\n<!-- #### Citing\n\nIf you use `OBStools` in your work, please cite the \n[`Zenodo DOI`](https://zenodo.org/badge/latestdoi/211722700).\n -->\n#### Contributing\n\nAll constructive contributions are welcome, e.g. bug reports, discussions or suggestions for new features. You can either [open an issue on GitHub](https://github.com/nfsi-canada/OBStools/issues) or make a pull request with your proposed changes. Before making a pull request, check if there is a corresponding issue opened and reference it in the pull request. If there isn't one, it is recommended to open one with your rationale for the change. New functionality or significant changes to the code that alter its behavior should come with corresponding tests and documentation. If you are new to contributing, you can open a work-in-progress pull request and have it iteratively reviewed. \n\nExamples of contributions include notebooks that describe published examples of OBS data\nprocessing. Suggestions for improvements (speed, accuracy, etc.) are also welcome.\n\n#### References\n\n- Bell, S. W., D. W. Forsyth, and Y. Ruan (2014), Removing noise from the vertical component records of ocean-bottom seismometers: Results from year one of the Cascadia Initiative, Bull. Seismol. Soc. Am., 105, 300-313, https://doi.org/10.1785/0120140054\n\n- Crawford, W.C., Webb, S.C., (2000). Identifying and removing tilt noise from low-frequency (0.1 Hz) seafloor vertical seismic data, Bull. seism. Soc. Am., 90, 952-963, https://doi.org/10.1785/0119990121\n\n- Janiszewski, H A, J B Gaherty, G A Abers, H Gao, Z C Eilon, Amphibious surface-wave phase-velocity measurements of the Cascadia subduction zone, Geophysical Journal International, Volume 217, Issue 3, June 2019, Pages 1929-1948, https://doi.org/10.1093/gji/ggz051\n",
        "createdAt": "2019-10-15T21:14:35.000Z",
        "updatedAt": "2025-11-03T07:59:51.000Z",
        "language": "Python",
        "homepage": "https://nfsi-canada.github.io/OBStools/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/nfsi-canada/OBStools/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "seismologyfolder/seismologyfolder",
        "url": "https://github.com/seismologyfolder/seismologyfolder",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2020-05-14T13:02:13.000Z",
        "updatedAt": "2020-05-14T13:02:13.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "anowacki/StationXML.jl",
        "url": "https://github.com/anowacki/StationXML.jl",
        "description": "Parse seismic station information in the FDSN StationXML format, in Julia",
        "stars": 2,
        "forks": 2,
        "readme": "# StationXML\n\nRead and write [FDSN StationXML-format](https://www.fdsn.org/xml/station)\nfiles describing seismic stations.\n\n[![Build Status](https://github.com/anowacki/StationXML.jl/workflows/CI/badge.svg)](https://github.com/anowacki/StationXML.jl/actions)\n[![codecov](https://codecov.io/gh/anowacki/StationXML.jl/branch/master/graph/badge.svg?token=M8H386OZCH)](https://codecov.io/gh/anowacki/StationXML.jl)\n\nThe package follows the [FDSN schema](https://www.fdsn.org/xml/station/fdsn-station-1.1.xsd).\n\n## Installation\n\n```julia\nimport Pkg; Pkg.pkg\"add https://github.com/anowacki/StationXML.jl\"\n```\n\n\n## Use\n\n`StationXML` is mainly designed to be used by other modules such\nas [Seis](https://github.com/anowacki/Seis.jl) to process\nstation information.  Therefore, relatively few convenience functions\nare defined for working with `FDSNStationXML` objects.  However,\nthe package aims to comprehensively document all objects and functions\nused.\n\nThe basic type exported by StationXML.jl is `FDSNStationXML`.\n\n\n### Reading FDSN StationXML data\n\nTwo unexported functions are available for use in creating `FDSNStationXML` objects:\n\n- `StationXML.read(filename)`: Read from a file on disk.\n- `StationXML.readstring(string)`: Parse from a `String`.\n\nFor instance (using an example StationXML file supplied with this module):\n\n```julia\njulia> using StationXML\n\njulia> sxml = StationXML.read(joinpath(dirname(pathof(StationXML)), \"..\", \"test\", data\", \"JSA.xml\"))\nStationXML.FDSNStationXML\n  source: String \"IRIS-DMC\"\n  sender: String \"IRIS-DMC\"\n  module_name: String \"IRIS WEB SERVICE: fdsnws-station | version: 1.1.36\"\n  module_uri: String \"http://service.iris.edu/fdsnws/station/1/query?network=GB&station=JSA&level=response&format=xml&nodata=204\"\n  created: Dates.DateTime\n  network: Array{StationXML.Network}((1,))\n  schema_version: String \"1.0\"\n\n```\n\n### Writing data\n\nSimply call `write` on an `FDSNStationXML` object to write it to disk or other\n`IO` stream:\n\n```julia\njulia> write(\"output_file.xml\", sxml)\n```\n\nStationXML.jl **always** writes files according to the v1.1 schema.\n\nNote that v1.1 removed a small number of fields from the specification.  Therefore, if\nyou are writing an `FDSNStationXML` object read from a v1.0 file, there is the potential\nthat information may be lost.  If you are worried, pass the `warn=true` keyword argument\nto `write` to enable warnings for the presence of any fields which will not be written.\n\n### Accessing fields\n\nYou should access the fields of `FDSNStationXML` objects directly.  These match the\nStationXML specification directly, and are listed in each type's docstrings.\nThese are accessible via the REPL by typing `?` and then the name of the\ntype.  For example:\n\n```julia\njulia> ? # REPL prompt becomes help?>\nhelp> StationXML.Channel\n  Channel\n\n  A channel is a time series recording of a component of some observable, often\n  colocated with other channels at the same location of a station.\n\n  Equivalent to SEED blockette 52 and parent element for the related the response\n  blockettes.\n\n  │ Note\n  │\n  │  The presence of a sample_rate_ratio without a sample_rate field is not\n  │  allowed in the standard, but it permitted by StationXML.jl.\n\n  List of fields\n  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\n\n    •    description::Union{Missing, String}\n      \n        Default: missing\n...\n```\n\nTo find out how many stations are in each of the networks returned in your request\nXML, and what the network code is, you can do:\n\n```julia\njulia> for net in sxml.network\n           println(net.code, \": \", net.total_number_stations, \" stations\")\n       end\nBG: 29 stations\n```\n\nTo get a vector of the station codes in the one network (GB) returned in our request:\n\n```julia\njulia> gb = sxml.network[1];\n\njulia> stas = [sta.code for sta in gb.station]\n1-element Array{String,1}:\n \"JSA\"\n\n```\n\nYou can also access individual networks, stations and channels using functions with\nthese names.  For example, using the [SeisRequests](https://github.com/anowacki/SeisRequests.jl)\npackage to get all the broadband, high-gain channels stations in the GB network from 2012 to now:\n\n```julia\njulia> using StationXML, SeisRequests, Dates\n\njulia> sxml = get_request(\n                 FDSNStation(starttime=DateTime(2012), network=\"GB\",\n                             location=\"--\", channel=\"BH?\",\n                             level=\"channel\")).body |> String |>\n                 StationXML.readstring;\n\njulia> [s.code for s in stations(sxml)]\n28-element Array{String,1}:\n \"BIGH\"\n \"CCA1\"\n \"CLGH\"\n \"CWF\"\n \"DRUM\"\n \"DYA\"\n \"EDI\"\n \"EDMD\"\n \"ELSH\"\n \"ESK\"\n \"FOEL\"\n \"GAL1\"\n \"HMNX\"\n \"HPK\"\n \"HTL\"\n \"IOMK\"\n \"JSA\"\n \"KESW\"\n \"KPL\"\n \"LBWR\"\n \"LMK\"\n \"LRW\"\n \"MCH1\"\n \"SOFL\"\n \"STNC\"\n \"SWN1\"\n \"WACR\"\n \"WLF1\"\n\n```\n\n\n### Accessor functions\n\nYou can easily construct vectors of all the networks, stations and channels in the StationXML\nusing the following accessor functions:\n\n- `networks(stationxml)`\n- `stations(stationxml_or_network)`\n- `channels(stationxml_or_network_or_station)`\n\nNote that `station`, for instance, accepts either a `Network` or a whole `FDSNStationXML`\nobject, whilst either of those or a `Station` can be given to `channels`.\n\n```julia\njulia> stations(gb)\n28-element Array{StationXML.Station,1}:\n StationXML.Station(missing, StationXML.Comment[], \"BIGH\", 2009-12-15T00:00:00, 2599-12-31T23:59:59, StationXML.RestrictedStatus\n  value: String \"open\"\n...\n```\n\nThe `channel_codes` function returns a list of all of the channel codes within\na `FDSNStationXML` document or a `Network`.\n\n\n### Dot-access to arrays of objects\n\n\n**Note: `getproperty` access for arrays of these objects is deprecated and will be\nremoved in a future version.**\n\nThe module defines `getproperty` methods for conveniently accessing the fields of each member\nof arrays of `Network`s, `Station`s and `Channel`s.  So our previous example of finding all\nthe station codes could actually have been done like this:\n\n```julia\njulia> stations(sxml).code\n28-element Array{String,1}:\n \"BIGH\"\n \"CCA1\"\n \"CLGH\"\n \"CWF\"\n...\n```\n\nWe can equally access any other field of the items this way:\n\n```julia\njulia> channels(sxml).longitude\n84-element Array{Float64,1}:\n StationXML.Longitude(-3.9087, missing, missing, missing, \"WGS84\")\n StationXML.Longitude(-3.9087, missing, missing, missing, \"WGS84\")\n StationXML.Longitude(-3.9087, missing, missing, missing, \"WGS84\")\n StationXML.Longitude(-5.227299, missing, missing, missing, \"WGS84\")\n StationXML.Longitude(-5.227299, missing, missing, missing, \"WGS84\")\n StationXML.Longitude(-5.227299, missing, missing, missing, \"WGS84\")\n StationXML.Longitude(-6.110599, missing, missing, missing, \"WGS84\")\n StationXML.Longitude(-6.110599, missing, missing, missing, \"WGS84\")\n StationXML.Longitude(-6.110599, missing, missing, missing, \"WGS84\")\n ⋮\n```\n\n\n### Merging multiple sets of metadata\n#### `merge[!]`\nYou can merge together mulitiple `FDSNStationXML` objects with `merge` (returining\na copy and not modifying the originals), or `merge!`, which updates the first\nobject given.  In situations where it is obvious that two networks, stations\nor channels are the same, these will not be duplicated in the final merged\nobject.  Cases which are ambiguous are not merged and the user is warned by\ndefault.\n\n#### `append!`\nAs an alternative to `merge!`, one can simply `append!` two objects together.\nThis simply has the effect of copying everything in one `FDSNStationXML`\nobject into another, and duplication is not avoided.\n\n\n### Structure of objects\n\n`StationXML` represents the XML as laid out in the StationXML schema.\nTherefore, it contains all the information in a StationXML file which\nis part of the StationXML standard.  Elements and attributes of the XML\nare fields within structures nested several layers deep.\n\n\n## Contributing\n\n### Bugs and omissions\nStationXML.jl should read any schema-compatible StationXML\nfile without error, therefore any examples of documents failing are very\nwarmly welcomed as are all bug reports.\n\nPlease [open an issue](https://github.com/anowacki/StationXML.jl/issues/new)\nwith a link to the document which is causing problems and as much\ninformation as possible about how to reproduce your error.\n\n### Features\nIf you would like to add a feature to StationXML, this will be seriously\nconsidered.  The package aims to be fairly minimal so please think very\ncarefully before adding large dependencies.\n\nNew code for the repo should come via a pull request.\n",
        "createdAt": "2019-01-09T14:21:25.000Z",
        "updatedAt": "2025-11-25T12:11:04.000Z",
        "language": "Julia",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/anowacki/StationXML.jl/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "jakewalter/seis_tools",
        "url": "https://github.com/jakewalter/seis_tools",
        "description": "Common seismological utilities",
        "stars": 1,
        "forks": 2,
        "readme": "",
        "createdAt": "2019-01-25T17:24:07.000Z",
        "updatedAt": "2020-06-09T01:32:24.000Z",
        "language": "Perl",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "zigdim/M2_UFAZ_seismology_modeling",
        "url": "https://github.com/zigdim/M2_UFAZ_seismology_modeling",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# M2_UFAZ_seismology_modeling\n\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/zigdim/M2_UFAZ_seismology_modeling/HEAD)\n",
        "createdAt": "2021-11-03T16:06:13.000Z",
        "updatedAt": "2021-11-03T16:18:07.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/zigdim/M2_UFAZ_seismology_modeling/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "EIDA/find-net-board",
        "url": "https://github.com/EIDA/find-net-board",
        "description": "Dashboard exposing findability score of seismological data",
        "stars": 0,
        "forks": 0,
        "readme": "# find-net-board\n\nThis is an application that tests and presents the consistency of networks metadata of various sources.\n\n## Installation\n\nClone current GitHub repository and enter project folder:\n```bash\ngit clone https://github.com/EIDA/find-net-board.git\ncd find-net-board\n```\n\nFor production see the recommended option using [Docker](https://www.docker.com/) below. For development purposes see the step by step installation.\n\n### With Docker containers (recommended)\n\n- Connect to your PostgreSQL server to create the database and a role:\n  ```bash\n  psql -U postgres -h localhost\n  ```\n  Execute the below commands in psql shell:\n  ```sql\n  CREATE ROLE netstests WITH LOGIN PASSWORD 'netstests' CREATEDB;\n  CREATE DATABASE networks_tests OWNER netstests;\n  exit\n  ```\n  **Note:** You can use your own choices for role name, database name, password, host and port. Be sure to pass them through environment variables when running the docker container.\n\n- From within the project folder, either build the Docker image:\n  ```bash\n  buildah build --network host -t networkstests .\n  ```\n  or pull from [https://ghcr.io/eida/eida/find-net-board:main](https://ghcr.io/eida/eida/find-net-board:main).\n\n- Run the Docker container:\n  ```bash\n  podman run --network host -p 8000:8000 networkstests\n  ```\n  **Note:** You might need to use you own names for database variables. The supported environment variables are:\n   - NETSTESTS_DBNAME\n   - NETSTESTS_DBUSER\n   - NETSTESTS_DBPASSWORD\n   - NETSTESTS_DBHOST\n   - NETSTESTS_DBPORT\n\n   Example:\n   ```\n   NETSTESTS_DBNAME=networks_tests NETSTESTS_DBUSER=netstests docker run --network host -p 8000:8000 networkstests\n   ```\n\n### Step by step (ideal for development purposes)\n\nFollow the steps below from within the project folder to locally install the application:\n\n- Install dependencies (recommended way using [uv](https://docs.astral.sh/uv/):\n  ```bash\n  # create a virtual environment and activate it\n  uv sync\n  ```\n\n- Run a docker instance of postgresql:\n\n     podman run -p 5432:5432 -e POSTGRES_HOST_AUTH_METHOD=trust -e POSTGRES_DB=networks_tests -e POSTGRES_USER=netstests docker.io/postgres\n\n- Connect to your PostgreSQL server to create the database and a role:\n  Execute the below commands in postgresql shell:\n\n- Connect to your Postgresql server to create the database and a user:\n  ```sql\n  CREATE USER 'netstests';\n  CREATE DATABASE networks_tests OWNER netstests;\n  ```\n  \n- Setup the connection for the project, by editing en .env file ([example\n  provided](.env-sample))\n\n- Go back to project folder with the virtual environment activated and build the database schema:\n  ```bash\n  uv run manage.py migrate\n  ```\n\n- Create an admin user for the application:\n  ```bash\n  uv run manage.py createsuperuser\n  # enter desired username, email and password in the corresponding prompts\n  # be sure to remember the username and password you are going to use\n  ```\n\n- Start the development server:\n  ```bash\n  uv run manage.py runserver\n  ```\n\n  **Note that deploying in a production web server might require more steps.**\n\n- Schedule periodic tasks with [Celery](https://docs.celeryq.dev/en/stable/):\n  ```bash\n  uv run celery -A netstests worker -B --loglevel=info\n  ```\n\n## Use\n\nProvided that the application is up and running:\n\n- Visit http://localhost:8000/admin/ to view the admin page.\n  While at it you can click `Update DB from Sources` or `Run Tests` buttons to perform the corresponding tasks.\n  Update might take around 30 minutes and running the tests around 40 minutes.\n\n- Visit http://localhost:8000/board/ to view the board that presents the results of the tests.\n  When the page loads, the last set of tests is shown. You can use the search form to view specific tests.\n\n- Visit http://localhost:8000/board/tests/ to view the available test runs.\n\n- Visit http://localhost:8000/board/datacenter/datacenter_name/ to view results of tests for networks of a specific datacenter (replace *datacenter_name* with the name of your datacenter as it appears in https://www.fdsn.org/datacenters/). You can use the search form to view tests within a specific time frame.\n\n## Testing (for development purposes)\n\nTo run the tests and get `coverage` information, run the below command from within the project folder:\n```bash\npytest --cov-report html\n```\n\nThe report will be available in `htmlcov/index.html` file.\n\n\n## Things pending\n\n- A few `ruff` errors are still not addressed. Most of them could probably be ignored though.\n\n- Test suite is quite poor and needs more meaningful tests to increase coverage percentage.\n\n- Maybe a good idea could be to use class based views.\n",
        "createdAt": "2024-03-22T14:10:14.000Z",
        "updatedAt": "2024-11-14T15:51:34.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/EIDA/find-net-board/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "SCECcode/bbp",
        "url": "https://github.com/SCECcode/bbp",
        "description": "SCEC Broadband Platform",
        "stars": 48,
        "forks": 34,
        "readme": "# The SCEC Broadband Platform (BBP) Software\r\n\r\n<a href=\"https://github.com/sceccode/bbp.git\"><img src=\"https://github.com/sceccode/bbp/wiki/images/SRL_Cover_v8.png\"></a>\r\n\r\n[![Python](https://img.shields.io/badge/python-%3E%3D3.7-blue)](https://www.python.org)\r\n[![License](https://img.shields.io/badge/License-BSD_3--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause)\r\n![GitHub repo size](https://img.shields.io/github/repo-size/sceccode/bbp)\r\n[![bbp-ci Actions Status](https://github.com/SCECcode/bbp/workflows/bbp-ci/badge.svg)](https://github.com/SCECcode/bbp/actions)\r\n\r\n## Description\r\nThe Southern California Earthquake Center (SCEC) Broadband Platform (BBP) is a software system that can generate 0-20+ Hz seismograms for historical and scenario earthquakes in California, Eastern North America, and Japan using several alternative computational methods.\r\n\r\n## Table of Contents\r\n1. [Software Documentation](https://github.com/SCECcode/bbp/wiki)\r\n2. [Installation](#installation)\r\n3. [Usage](#usage)\r\n4. [Support](#support)\r\n5. [Citation](#citation)\r\n6. [Contributing](#contributing)\r\n7. [Credits](#credits)\r\n8. [License](#license)\r\n\r\n## Installation\r\n\r\nBBP was developed to support ground simulations run on Linux servers and high-performance computing systems,\r\nso it is designed to compile and run on Linux-based computers. Before installing BBP, they should be aware that it is\r\npossible to run some versions of the BBP software without installing the software on a Linux computer.\r\nBelow we outline the two options for running the BBP software:\r\n\r\n1. [BBP Docker Images](https://github.com/sceccode/bbp_docker) Users can run a version of BBP using Docker on their\r\nlocal computers including laptops. The BBP Docker version contains an LA-Basin region velocity model, and three validation events in that regeion.\r\nUsers can install free Docker software on most computers (e.g. Linux, MacOS, Windows) then run a BBP Docker image in a terminal window on their computer.\r\n2. [Installation Instructions for Linux Systems](https://github.com/SCECcode/bbp/wiki/Installation)\r\nAdvanced users that want to install many or all of the BBP simulation regions models, or that want to run large\r\nparallel queries of the CVM models, should install the BBP software on a Linux system. BBP software is developed\r\non USC Center for Advanced Research Computing (CARC) Linux cluster.\r\n\r\n## Usage\r\n\r\nTo get a list of the current available options, run run_bbp.py with the -h flag.\r\n\r\n```\r\n $ ./run_bbp.py --help\r\n Usage: run_bbp.py [options]\r\n\r\n Options:\r\n  -h, --help                                show this help message and exit\r\n  -x XML_FILE, --xml-file=XML_FILE          Run using XML description of workflow\r\n  -s SIM_ID, --sim-id=SIM_ID                Force a sim id\r\n  -o OPTFILE, --option-file=OPTFILE         File containing responses to interactive platform prompts\r\n  -v, --version                             Broadband platform version\r\n  -g, --generate-only                       Generates the XML description but does not run the platform\r\n  -l LOG_FILE, --log=LOG_FILE               Directs output to a file, use to run BBP in background\r\n  -m, --no-xml                              Do not generate xml\r\n  -r RESUME_MODULE, --resume=RESUME_MODULE  Resume workflow from a certain module\r\n  -e END_MODULE, --end=END_MODULE           End workflow after a certain module\r\n  --expert                                  Turns on expert mode\r\n```\r\n\r\n### Validation Simulations\r\n\r\nTo run a validation simulation, go to the data/run directory and run run_bbp.py. The platform will ask you a series of questions. Answer 'y' to \"Do you want to perform a validation run?\" (the exact list of validation events users see when running the Broadband Platform depends on what validation events are installed on their computers):\r\n\r\n```\r\n $ run_bbp.py\r\n Welcome to the SCEC Broadband Platform version 22.4.0.\r\n ================================================================================\r\n\r\n Please select the Broadband Platform mode of operation:\r\n    * Validation - Simulates a historical event\r\n    * Scenario   - Runs a user-defined hypothetical event\r\n\r\n Do you want to perform a validation simulation (y/n)? y\r\n ================================================================================\r\n\r\n Please select a validation event from the list below:\r\n\r\n (1) Alum Rock\r\n (2) Chino Hills\r\n (3) LOMAP\r\n (4) NR\r\n (5) Whittier\r\n ?\r\n ...\r\n```\r\n\r\nNo input files are required by the user. However, you may wish to customize the validation simulation by selecting an alternate source description (src file) or a reduced station list to speed up the computations. You can put your own source description and/or station list into the run directory (the format is described in [File Formats](./File-Format-Guide)) or you can tell the platform where each file is located by using an absolute path. Note that any stations which do not have observed seismograms will not be included in the automatically generated goodness-of-fit comparison. To supply alternative source description and/or station list files, please run the Broadband Platform in 'expert' mode using the '--expert' command-line flag.\r\n\r\n### User-defined Simulations\r\n\r\nTo run a user-defined simulation, two input files are required, a source description (src file) and a station list (stl file). A simple source description (src file) is always required, but, for certain methods, a source description in SRF format (the format is described in [File Formats](./File-Format-Guide) can be supplied as well and will be used for the seismogram computation modules in the Broadband Platform. To run a user-defined simulation, run run_bbp.py:\r\n\r\n```\r\n $ run_bbp.py\r\n Welcome to the SCEC Broadband Platform version 22.4.0.\r\n ================================================================================\r\n\r\n Please select the Broadband Platform mode of operation:\r\n    * Validation - Simulates a historical event\r\n    * Scenario   - Runs a user-defined hypothetical event\r\n\r\n Do you want to perform a validation simulation (y/n)? n\r\n ================================================================================\r\n\r\n The Broadband Platform provides the following velocity models, which also include\r\n several method-specific and region-specific parameters.\r\n\r\n Please select a velocity model (either number or name is ok):\r\n\r\n (1) CentralJapan500\r\n (2) CentralCal500\r\n (3) NOCAL500\r\n (4) LABasin500\r\n (5) WesternJapan500\r\n (6) Mojave500\r\n ?\r\n ...\r\n```\r\n\r\nYou may then choose the method you would like to run:\r\n\r\n```\r\n The Broadband Platform includes several scientific methods that can be\r\n used to calculate synthetic seismograms.\r\n\r\n Choose a Method to use in this Broadband scenario simulation:\r\n (1) GP (Graves & Pitarka)\r\n (2) UCSB\r\n (3) SDSU\r\n (4) EXSIM\r\n (5) Song\r\n (6) Irikura Recipe Method 1 (Irikura1)\r\n (7) Irikura Recipe Method 2 (Irikura2)\r\n ?\r\n```\r\n\r\n## Support\r\nSupport for BBP is provided by that Southern California Earthquake Center (SCEC) Research Computing Group. This group supports several research software distributions including BBP. Users can report issues and feature requests using BBP's github-based issue tracking link below. Developers will also respond to emails sent to the SCEC software contact listed below.\r\n1. [BBP Github Issue Tracker](https://github.com/SCECcode/bbp/issues)\r\n2. Email Contact: software@scec.org\r\n\r\n## Citation\r\nReferences, citations, and acknowledgements help us obtain continued support for the development of the BBP software. If you use the BBP software in your research, please include the citation of the BBP paper in the references/bibliography section of your publication. This is more effective than you providing in-text acknowledgements.\r\n\r\n* Preferred Reference: Maechling, P. J., F. Silva, S. Callaghan, and T. H. Jordan (2015). SCEC Broadband Platform: System Architecture and Software Implementation, Seismol. Res. Lett., 86, no. 1, doi: 10.1785/0220140125.\r\n\r\n* Example Acknowlegement: We would like to acknowledge the use of the SCEC Broadband Platform Software (Maechling 2015) in this research.\r\n\r\nAlong with citing the BBP software, researchers should also cite the appropriate publication for any of the ground motion models they use in their research. Citations for individual ground motion methods are included in the [Credits.md](CREDITS.md) file in this repository.\r\n\r\n## Contributing\r\nWe welcome contributions to the BBP software framework.\r\nGeoscientists can register their ground motion models into BBP and software developers can\r\nimprove and extend the BBP software. An overview of the process for contributing seismic models or\r\nsoftware updates to the BBP Project is provided in the BBP [contribution guidelines](CONTRIBUTING.md).\r\nBBP contributors agree to abide by the code of conduct found in our [Code of Conduct](CODE_OF_CONDUCT.md) guidelines.\r\n\r\n## Credits\r\nDevelopment of BBP is a group effort. A list of developers that have contributed to the BBP Software framework\r\nare listed in the [Credits.md](CREDITS.md) file in this repository.\r\n\r\n## License\r\nThe SCEC-developed portions of the Broadband platform software is distributed under the BSD 3-Clause open-source license.\r\nPlease see the [LICENSE.txt](LICENSE.txt) file for more information. Individual models codes may be offered under their own open-source software licenses.\r\n",
        "createdAt": "2016-06-21T18:22:56.000Z",
        "updatedAt": "2025-10-28T03:13:03.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/SCECcode/bbp/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "jbeale1/seismo",
        "url": "https://github.com/jbeale1/seismo",
        "description": "seismology signal processing",
        "stars": 0,
        "forks": 1,
        "readme": "# seismo\nPrograms useful for signal processing in amateur seismology.\n\nFor example, \"Hammer\" vertical seismometer with \"BBShark\" digitizer board and Earthworm server.\n",
        "createdAt": "2021-01-25T05:53:30.000Z",
        "updatedAt": "2025-06-12T06:34:06.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/jbeale1/seismo/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ghfbsd/makelog",
        "url": "https://github.com/ghfbsd/makelog",
        "description": "Utility for use with cdseis",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2015-11-20T03:50:34.000Z",
        "updatedAt": "2023-02-27T15:05:22.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "shuleyu/seismic-raytracing",
        "url": "https://github.com/shuleyu/seismic-raytracing",
        "description": "seismology body wave ray tracing (simple 2D)",
        "stars": 2,
        "forks": 0,
        "readme": "# Download and Install\n```\n$ git clone --recursive https://github.com/shuleyu/seismic-raytracing.git\n```\n\nIf `--recursive` is not added, the dependencies will not be downloaded. In such case, do:\n\n```\n$ cd ./seismic-raytracing\n$ git submodule update --init --recursive\n```\n\n\n# Parameter file (INFILE)\nThe WORKDIR parameter specify the output folder of this program. The program will create this folder if it doesn't exist.\n\n```\n$ vim ./INFILE\n```\n\n# Task file (LIST.sh)\nSpecify which task(s) to execute. Use `#` to comment out unwanted tasks.\n\n```\n$ vim ./LIST.sh\n```\n\nFor now, there's two tasks `a01` (calculation) and `b01` (plotting using GMT4).\n\n# Execution file (Run.sh)\nWhen INFILE and LIST.sh are properly set, run this script:\n\n```\n$ bash ./Run.sh\n```\n\nThis scipt will do several things:\n\n1. create folder WORKDIR\n2. create subfolders within WORKDIR:\n   - `bin` complied binary files\n   - `INPUT` history INFILE(s)\n   - `LIST` history LIST.sh(s)\n   - `PLOTS` generated figures\n   - a bunch of temporary files `tmpfile_XXXX` (they get automatically deleted when the job is finished)\n3. try compling the code (if complie failed, program will exit directly. Error message will be printed to the screen)\n4. read parameters from INFILE\n5. execute tasks specified in LIST.sh\n\n# Example\n\nWith a proper WORKDIR, the program will calculate the following example case.\n\nIf gmt4 is installed, it will also produce the following figure:\n\n![alt text](https://github.com/shuleyu/raytracing/blob/master/SRC/example1.png)\n\nThis is an ScS reflection on an ellipse-shaped low velocity structure on the core mantle boundary.\n\n  - Ray is coming from left hand side.\n\n  - Green lines for polarity change.\n\n  - Line width are comparable to displacement amplitude.\n\nAnother example showing multiple ScS reflections on similar structure (varying takeoff angle each time):\n\n![alt text](https://github.com/shuleyu/raytracing/blob/master/SRC/example2.png)\n\n",
        "createdAt": "2018-05-01T00:37:10.000Z",
        "updatedAt": "2022-05-25T22:27:49.000Z",
        "language": "C++",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/shuleyu/seismic-raytracing/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "jpampuero/sem2dpack",
        "url": "https://github.com/jpampuero/sem2dpack",
        "description": "SEM2DPACK - A spectral element method for 2D wave propagation and fracture dynamics, with emphasis on computational seismology and earthquake source dynamics.",
        "stars": 41,
        "forks": 11,
        "readme": "# SEM2PACK\n\n## A spectral element method for 2D wave propagation and fracture dynamics, with emphasis on computational seismology and earthquake source dynamics.\n\n--------------------------------\n## Features\n\n  * Elastic and inelastic wave propagation in heterogeneous media\n  * Rupture dynamics on non-planar faults, with friction and off-fault damage or plasticity\n  * High order (spectral) elements on unstructured meshes\n\n## Documentation\n\nUser's manual:\n\nJ. P. Ampuero (2012), SEM2DPACK, a spectral element software for 2D seismic wave propagation and earthquake source dynamics, v2.3.8. Zenodo, doi:10.5281/zenodo.230363  \n [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.230363.svg)](https://doi.org/10.5281/zenodo.230363)\n\nA more recent description of the input file is in `doc/doc.txt`. \nTo update it: in directory `doc`, run `./MakeDoc.csh > doc.txt`. \n\n## Cite as\n\nAmpuero, J. P., T. W. Currie, M. T. Herrera, Y. Huang, H. Lestrelin, C. Liang, F. Llorens, E. Oral and H. Weng (2024). jpampuero/sem2dpack: SEM2DPACK v2.3.9 (SEM2DPACK_2.3.9). Zenodo, \\[software\\], doi:10.5281/zenodo.13821494  \n [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.13821494.svg)](https://doi.org/10.5281/zenodo.13821494)\n",
        "createdAt": "2018-12-11T23:02:01.000Z",
        "updatedAt": "2025-12-03T13:51:24.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.230363",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.230363",
            "dataCite": "10.5281/zenodo.230363",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/jpampuero/sem2dpack/master/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.230363",
            "title": "SEM2DPACK, a spectral element software for 2D seismic wave propagation and earthquake source dynamics, v2.3.8",
            "journal": "Zenodo",
            "dateReleased": "2012-08-07T00:00:00.000Z",
            "abstract": "<strong>SEM2DPACK </strong>is a software for 2D wave propagation and fracture dynamics based on the spectral element method, with emphasis on computational seismology and earthquake source dynamics. <strong>Version 2.3.8</strong> has the following main new features: compatibility with the mesh generation programs CUBIT and EZ4U kinematic finite fault sources heterogeneous coefficients for anisotropic media absorbing boundaries with arbitrary orientation Gaussian source time function pre-processing utilities to compute critical time steps revised time-weakening friction revised handling of fault tip nodes of single-sided faults",
            "citationsArray": []
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "hazardgoat/Focal_Mechanisms_Demo",
        "url": "https://github.com/hazardgoat/Focal_Mechanisms_Demo",
        "description": "A comprehensive tutorial for plotting focal mechanism \"beach-balls\" using the PyGMT package for Python.",
        "stars": 25,
        "forks": 1,
        "readme": "# Focal Mechanisms Demo\nA comprehensive tutorial for plotting focal mechanism \"beach-balls\" using the PyGMT package for Python.\n\n\n<p align=\"center\">\n  <img alt=\"\" style=\" margin-left: 50px;margin-right: 50px;\" src=\"https://user-images.githubusercontent.com/74040471/140022974-1ee3211b-3399-4436-83ea-b57db9d45912.png\"/>\n</p>\n<p align=\"center\">\n  (Resulting plot of this demo)\n</p>\n\n# Background\nTo better understand the faulting motions that create earthquakes, it can be useful to graphically display focal mechanisms. A [focal mechanism](https://www.usgs.gov/natural-hazards/earthquake-hazards/science/focal-mechanisms-or-beachballs?qt-science_center_objects=0#qt-science_center_objects) is the orientation and type of slip of a fault, and it is represented as a “beach-ball” symbol.\n\n\n<p align=\"center\">\n  <img alt=\"\" style=\" margin-left: 50px;margin-right: 50px;\" src=\"https://user-images.githubusercontent.com/74040471/140017707-e316e092-9508-4c7a-b2c0-0e28d55c93da.png\"/>\n</p>\n<p align=\"center\">\n  (Image is sourced from the United States Geological Survey earthquake hazards webpage on focal mechanisms)\n</p>\n\nWithin a focal mechanism beach-ball, the dark quarters represent compressional forces and the white quarters represent tensional forces. A beach-ball has two possible orientations of fault movement, each along with one of the planes dividing the beach-ball into fourths, and determining the correct orientation requires additional information. When beach-balls present four equally sized quadrants in a “top-down” view like the one in the above diagram, strike-slip faulting is represented. Rotated beach-balls with a white quarter in the center represent normal faulting, while beach-balls with a dark quarter in the center represent thrust faulting.\n\n# The Demonstration\nIn this demo, focal mechanism data will be retrieved from the [Southern California Earthquake Data Center (SCEDC)](https://scedc.caltech.edu/index.html) and then plotted as focal mechanism beach-balls on a map with the geospatial Python package [PyGMT](https://www.pygmt.org/latest/index.html).\n\nPlease keep in mind that with the exception of the folder creation script, all code blocks shown in this demo are intended to be run as part of a complete script, which is included at the bottom of the page.\n\n# Getting Started\n### Create Project Folders\nTo follow this demo as written, project folders need to be created on the Desktop. A diagram of the desired directory tree for Windows 10 is shown below:\n\n<p align=\"center\">\n  <img alt=\"\" style=\" margin-left: 50px;margin-right: 50px;\" src=\"https://user-images.githubusercontent.com/74040471/140018003-6a7d41ee-61ca-4ad6-90d7-0aed64a324f3.png\"/>\n</p>\n<p align=\"center\">\n  (Image created with Lucidchart)\n</p>\n\nThese folders can be created either by running the script provided below from anywhere after changing the `main_dir` file path to reflect your username or by manually creating a *Focal_Mechanism_Demo* folder and within it *Data*, *Methods*, and *Results* folders.\n\n```\n'''\nDecription:\nThis script creates folders for the focal_mechanism_demo.py script if they don't already exist\n'''\n\nimport os\n\n# main directory path\nmain_dir = r'C:\\Users\\USER\\Desktop\\Focal_Mechanism_Demo'\n\n# path to the directory holding the project data files\ndata_folder = os.path.join(main_dir, 'Data')\n\n# path to the directory holding the project Python scripts\nmethods_folder = os.path.join(main_dir, 'Methods')\n\n# path to the directory holding the map generated by the scripts\nresults_folder = os.path.join(main_dir, 'Results')\n\ndirectories = [main_dir, data_folder, methods_folder, results_folder]\n\n# Iterates through the list of directories and creates them if they don't already exist\nfor directory in directories:\n    os.makedirs(directory, exist_ok = True)\n```\n\nData files used and created by the demo script will be located in the *Data* folder, and the focal mechanism map will save to the *Results* folder. The *Methods* folder should contain the demo scripts.\n\n### Retrieve Focal Mechanism Data\nFocal mechanism data will be acquired by downloading it from the [SCEDC focal mechanism catalog](https://service.scedc.caltech.edu/eq-catalogs/FMsearch.php). This demo uses the following search parameters:\n\n**Date range:** 2019/01/01 to 2020/01/01\n\n**Magnitude range:** 5 to 10\n\n**Latitude range:** 35.5 to 36.1\n\n**Longitude range:** -118 to -117.15\n\n**Depth range:** -5 to 100\n\n<p align=\"center\">\n  <img alt=\"\" style=\" margin-left: 50px;margin-right: 50px;\" src=\"https://user-images.githubusercontent.com/74040471/140018516-129a42d3-cc39-4c8f-bd9e-307ab87b47ed.png\"/>\n</p>\n<p align=\"center\">\n  (Example of entering the focal mechanism parameters into the SCEDC focal mechanism catalog search)\n</p>\n\nAfter clicking the search button, the results are presented as a text page. Select the data and column headers as shown below, and copy them into a text file. Then save the text file to the *Data* folder and name it **focal_mechanism_data**.\n\n<p align=\"center\">\n  <img alt=\"\" style=\" margin-left: 50px;margin-right: 50px;\" src=\"https://user-images.githubusercontent.com/74040471/140018737-98fdabd6-1e43-40f0-b605-32546607ed38.png\"/>\n</p>\n<p align=\"center\">\n  (Example of selecting and copying the focal mechanism data and column headers)\n</p>\n\n# Creating The Project Files\n### Conditioning The Focal Mechanism Data\nThe focal mechanism data as retrieved from the SCEDC does not have a uniform delimiter separating the columns and has an empty line between the column header and the column values. In order to read the data into a data frame, it needs to be conditioned so that the aforementioned issues are resolved.\n\nThe following demo script function removes the empty line and standardizes the delimiter to be tabs, then saves the conditioned data as a CSV file:\n```\n    # Conditions focal mechanism files downloaded from the Southern California Earthquake Data Center so that they can be used\n    def Condition_Focal_Mechanism_Data(self):\n        import os\n        import re\n\n        print('Conditioning focal mechanisms...')\n\n\n        input_file = os.path.join(self.main_dir, 'Data', 'focal_mechanism_data.txt')\n        # reads in the file by line rather than the default of by character\n        with open(input_file, 'r') as f:\n            lines = f.readlines()\n\n        # holds the conditioned lines, namely the removal of empty lines\n        conditioned_lines = []\n        # iterates over each line and adds them to a list as long as they are not empty\n        for line in lines:\n            if line == '\\n':\n                continue\n            conditioned_lines.append(line)\n        \n        # empty placeholder for the conditioned lines list to be rejoined into a string\n        conditioned_data = ''\n        # rejoins the conditioned lines into a string so that it can be further conditioned\n        conditioned_data = conditioned_data.join(conditioned_lines)\n\n        # regular expression pattern for finding 1 or more white spaces (key) and the replacment value for matches (value)\n        replacements = {' +':'\\t'}\n        # subsitutes text using the patterns in \"replacements\"\n        for key, value in replacements.items():\n            data = re.sub(key, value, conditioned_data)\n\n        output_file = os.path.join(self.main_dir, 'Data', 'focal_mechanism_data.csv')\n        with open(output_file, 'w') as f:\n            f.write(data)\n```\n\n### Filtering Focal Mechanism Data Into The “Aki” Format\nAfter conditioning the focal mechanism data, it needs to be filtered so that the resulting file can be read by the PyGMT [meca function](https://www.pygmt.org/dev/api/generated/pygmt.Figure.meca.html), which plots focal mechanism beach-balls. As the data provided by the SCEDC is in terms of strike, drip, rake, and magnitude, the focal mechanism beach-balls will be determined by way of the Aki and Richards (1980) convention. In this demo, earthquakes greater than or equal to M7.0 will be offset to showcase how offsetting is accomplished. The `meca` function requires a file to be passed directly to the `spec` parameter when offsetting, so a CSV file with specific columns and column order pertaining to the convention needs to be created. Additionally, the column headers must be removed, so that header names don’t plot as text.\n\nThe following demo script function splits the focal mechanism data into two data frames at M7.0 earthquakes, then converts the magnitudes so that they are exponentially scaled; this is how the beach-balls will be sized when they are plotted. Next, it adds offset coordinates and text label columns to the data frame of M7.0 and greater earthquakes so they will be offset from their point of origin when plotted. Finally, the two data frames are saved as CSV files:\n\n```\n    # Creates a focal mechanism dataframe in the \"aki\" format from the focal mechanism data\n    def Filter_AKI_Format_Focal_Mechanism_Data(self):\n        import os\n        import pandas as pd\n\n        print('Writing focal mechanisms to CSV...')\n\n\n        focal_mechanisms =  os.path.join(self.main_dir, 'Data', 'focal_mechanism_data.csv')\n        df_focal_mechanisms = pd.read_csv(focal_mechanisms , sep='\\t')\n        \n        # saves the unified depth column as a pandas series so that it can be recalled later for coloring the focal mechanisms by depth\n        fm_depths = df_focal_mechanisms['DEPTH']\n        # saves the unified depth magnitude column as a pandas series so that it can be recalled later for creating the legend\n        fm_mags = df_focal_mechanisms['MAG']\n\n        # magnitude of earthquakes used to filter out the focal mechanisms that will be offset\n        magnitude_filter = 7\n\n        # focal mechanisms with earthquake magnitude less than the magnitude filter\n        df_mag_filtered_focal_mecha = df_focal_mechanisms[(df_focal_mechanisms.MAG < magnitude_filter)]\n        # focal mechanisms with earthquake magnitude greater than or equal to the magnitude filter\n        df_offset_mag_filtered_focal_mecha =df_focal_mechanisms[(df_focal_mechanisms.MAG >= magnitude_filter)]\n\n        # reads the unmodifed magnitude of the focal mechanisms that will be offset into a list so it can be later used to label them\n        fm_label = []\n        for i in df_offset_mag_filtered_focal_mecha.MAG:\n            fm_label.append(i)\n\n        fm_dataframes = [df_mag_filtered_focal_mecha, df_offset_mag_filtered_focal_mecha]\n\n        # iterates through each focal mechanism dataframe and scales them expoentially so that they will be plotted as exponentially larger sizes, and resets the index in place while not including the old index as a column which is done for tidyness \n        for dataframe in fm_dataframes:\n            dataframe.reset_index(inplace=True, drop=True)\n            dataframe['MAG'] = 0.1 * (2 ** dataframe.MAG)\n\n        # new dataframes to hold only the columns used for the \"aki\" focal mechanism format\n        fm_aki_format = pd.DataFrame()\n        fm_aki_format_offset = pd.DataFrame()\n        aki_dataframes = [fm_aki_format, fm_aki_format_offset]\n        # column names of the aki format, with keys being columns from the focal mechanism dataframes and values being the new column names to be used\n        aki_format_attributes = {'LON':'longitude', 'LAT':'latitude', 'DEPTH':'depth', 'STRIKE':'strike', 'DIP':'dip', 'RAKE':'rake', 'MAG':'magnitude'}\n\n        # add the desired columns for the 'aki' focal mechanism format to the new dataframes in the desired order\n        for aki_data, fm_data in zip(aki_dataframes, fm_dataframes):\n            for key, value in aki_format_attributes.items():\n                aki_data[value] = fm_data[key]\n\n        # offset coordinates (lon:lat) used for offsetting the selected focal mechanisms\n        offset_coordinates = {-117.70:35.78}\n        fm_aki_format_offset['offset_longitude'] = offset_coordinates.keys()\n        fm_aki_format_offset['offset_latitude'] = offset_coordinates.values()\n\n        # adds label data for plotting above each focal mechanism beachball\n        fm_aki_format_offset['label_mag'] = fm_label\n\n        # path to the \"small\" EQ focal mechanisms \n        focal_mechanisms_lesser_output =  os.path.join(self.main_dir, 'Data', 'focal_mechanism_data_less_than_M{}.csv').format(magnitude_filter)\n        # path to the \"large\" EQ focal mechanisms \n        focal_mechanisms_greater_output =  os.path.join(self.main_dir, 'Data', 'focal_mechanism_data_greater_than_or_equal_to_M{}.csv').format(magnitude_filter)\n        focal_mechanism_outputs = [focal_mechanisms_lesser_output, focal_mechanisms_greater_output]\n\n        # writes the aki format dataframes to csv files, leaving out the header. This is important because if the header is included, fig.meca will plot the header text\n        for dataframe, path in zip(aki_dataframes, focal_mechanism_outputs):\n            dataframe.to_csv(path, sep='\\t', index=False, header=None)\n\n        return fm_depths, fm_mags, magnitude_filter\n```\n\n### Creating The Legend\nWhen the focal mechanisms beach-balls are eventually plotted on a map, the size differences will need to be given context with a legend. For this kind of legend, the PyGMT [legend function](https://www.pygmt.org/dev/api/generated/pygmt.Figure.legend.html) needs to be passed a postscript file.\n\nThe following demo script function creates a postscript file for the legend:\n\n```\n# Creates a postscript legend file\ndef Create_Legend(self, fm_mags):\n    import os\n    import math\n\n    # File path to postscript legend file to be created\n    focal_mechanism_legend = os.path.join(self.main_dir, 'Data', 'focal_mechanism_legend.txt')\n\n    # Creates a postscript file and writes some explainer, legend header text, and the column format\n    with open(focal_mechanism_legend, 'w') as f:\n        f.write(\n            '# G is vertical gap, V is vertical line, N sets # of columns,\\n'\n            '# D draws horizontal line, H is header, L is column header,\\n'\n            '# S is symbol <symbol size><symbol color><symbol border thickness><text position><text>\\n'\n            '\\n'\n            'H 12p,Helvetica Magnitude\\n'\n            'D 0.1i 1p\\n'\n            'G 0.05i\\n'\n            'N 1\\n'\n            '\\n'\n        )\n\n    # sets the index to the minimum magnitude of the unified dataset. This is used to set the minimum size for the circles used to represent magnitude within the legend, as well as label them\n    i = math.floor(fm_mags.min())\n    # sets the vertical space between each magnitude circle/text line\n    v_spacer = 0.08\n\n    # creteas a legend entry for each integer magnitude within the dataset and appends it to the legend file\n    with open(focal_mechanism_legend, 'a') as f:\n        while i <= round(fm_mags.max(), 0):\\\n            # replicates the exponential scaling of the magnitide and linear scaling of the fig.meca function so that the magnitude circles in the legend are the same size as the plotted focal mechanisms\n            size = 0.1*(0.1*(2**i))\n            size = round(size, 2)\n            \n            # creates a legend of transparent circles that are labeld with the associated magnitude\n            f.write(\n                'S 0.20i c {} white@100 0.5p 0.6i M{}\\n'\n                'G {}i\\n'\n                .format(size, i, v_spacer)\n            )\n\n            # exponentially scales the vertical spacing between each legend entry so that they are evenly spaced in this particular demo\n            v_spacer = v_spacer**0.5\n            i += 1\n```\n\n### Building The Map\nNow that the focal mechanism data and legend have been properly prepared, it’s time to plot everything on a map.\n\nThe following demo script function plots focal mechanism beach-balls over an [automatically downloaded 3 arc second Digital Elevation Model from the Shuttle Radar Topography Mission](https://gmt.soest.hawaii.edu/doc/latest/datasets.html). The beach-balls are scaled exponentially in size by the magnitude and colored by depth, with magnitude, and depth legends plotted along the map axes. An inset map displaying the study region relative to the greater region around it is plotted in the upper right corner of the map:\n\n```\n    # Plots the map    \n    def Plot_Map(self, fm_depths, magnitude_filter):\n        import os\n        import pygmt\n\n        print('Building map...')\n\n        ### BASEMAP -----------------------------------------------------------------------------\n        # sets the region to be displayed in the map <min lon><max lon><min lat><max lat>\n        region = [-118.0, -117.15, 35.5, 36.1]  \n        # map projection (Mercator): <type><size><units>\n        projection = 'M6i' \n        # frame  annotations: [<frame sides with/without ticks>, <x-axis tick rate>, <y-axis tick rate>]\n        frame = ['SWne', 'xa', 'ya']\n        # map scale - displays a fancy (f) distance scale (lScale) in kilometers (u). Format of <type><longitude><latitude>/<latitude of km symbol>/<scale length in km>+<unit>+<label><label text>\n        map_scale = 'f-118.0/32.15/20/100+u+lScale:'\n        \n\n\n        ### GRID -----------------------------------------------------------------------------\n        # sets the color palette table for the digital elevation map\n        grid_cmap = 'grayC'\n\n\n\n        ### MECA -----------------------------------------------------------------------------\n        # focal mechanism data of earthquakes less than the magnitude filter\n        focal_mechanisms_less_than_magnitude_filter = os.path.join(self.main_dir, 'Data', 'focal_mechanism_data_less_than_M{}.csv').format(magnitude_filter)\n        # focal mechanism data of earthquakes greater than or equal to the magnitude filter\n        focal_mechanisms_greater_than_magnitude_filter = os.path.join(self.main_dir, 'Data', 'focal_mechanism_data_greater_than_or_equal_to_M{}.csv').format(magnitude_filter)\n        focal_mechanisms = [focal_mechanisms_less_than_magnitude_filter, focal_mechanisms_greater_than_magnitude_filter]\n        # column format: <longitiude><latitude><strike><dip><rake><magnitude>. Optionally add <offset longitude><offset latitude><label> to the end\n        convention = 'aki'\n        # scale defines the size for magnitude = 5 (0.5). Also optionally plots labels above the focal mechanisms (+f15p,Helvetica,black). \n        fm_scale = '0.5+f15p,Helvetica,black'\n        # thickness of the lines drawn from the offset focal mechanisms back to their point of origin\n        fm_offset = '1p'\n        # colors by which to color focal mechanisms\n        depth_color = 'yellow,red,purple'\n\n\n\n        ### COLORBAR -----------------------------------------------------------------------------\n        # automaticly determined fancy frame with label. Format of: <type>+<label><label text>\n        cbar_frame = 'af+l\"Depth (km)\"'\n\n\n\n        ### LEGENDS -----------------------------------------------------------------------------\n        # path to the legend file\n        legend = os.path.join(self.main_dir, 'Data', 'focal_mechanism_legend.txt')\n        # <position>+<width>+<offset x-dir>/<offset y-dir>\n        legend_position = 'JMR+w1.05i+o0.15i/-0.02i'\n        # <background color>+<line thickness>\n        #legend_box = '+gwhite+p1p'\n\n\n\n        ### INSET MAP -----------------------------------------------------------------------------\n        # extent of the inset map\n        i_region = [-130, -105, 27, 45]\n        i_projection = 'M1.75i'\n        # sets the border types, line thickness, and color <border type><line thickness><color>. In this case 1 = national boundaries, 2 = state boundaries within America\n        i_borders = ['1/0.8p,gray40', '2/0.8p,gray40']\n        # sets the color of the land\n        i_land = 'gray'\n        # sets the shorline type, line thickness, and color <shoreline type><line thickness><line color>. In this case, 1 = coastline\n        i_shorelines = '1/0.1p,gray40'\n        i_pen = '1p,black'\n        # sets the water color\n        i_water = 'azure2'\n        # shifts the inset map in the x-direction. Positive numbers shift up, negative numbers shifts down. <amount><units>\n        i_xshift = '12.5c'\n        # shifts the inset map in the y-direction. Positive numbers shift up, negative numbers shifts down. <amount><units>\n        i_yshift ='11.0c'\n        # first two list elements of 'rectangle' are the longitude and latitude of the bottom left corner of the rectangle, and the last two elements are the longitude and latitude of the uppper right corner. \n        rectangle = [[region[0], region[2], region[1], region[3]]]\n        # sets style of plot as rectagle (r) with first two list items as the longitude and latitude of the bottom left corner of the rectangle, and the last two list items as the longitude and latitude of the uppper right corner (+s)\n        i_style = 'r+s'\n        # sets an auto-detrmined frame with ticks in intervals of 10 (a10), then displays the frame with ticks on the top and right sides of the map (NE), but doesn't show ticks for the left and bottoms sides (sw)\n        i_frame = ['a10', 'swNE']\n        \n\n\n        ### Save File -----------------------------------------------------------------------------\n        # map save name\n        save_name = os.path.join(self.main_dir, 'Results', 'Focal_Mechanism_Demo.png')\n        # sets the transparency of the white space around the map    \n        save_transparency = False\n\n\n\n        # Establishes a figure to hold map layers\n        fig = pygmt.Figure()\n\n        # Forces Lat/Lon to display as degree decimal\n        pygmt.config(FORMAT_GEO_MAP = 'D')\n        # Forces Lat/Lon to display with two decimal places <f>, as opposed the default that limits by significan digits <g>\n        pygmt.config(FORMAT_FLOAT_OUT = '%.2f')\n        # Forces the map frame annotation to be smaller\n        pygmt.config(FONT_ANNOT_PRIMARY = '10p,Helvetica,black')\n        # Forces the scale font to be smaller\n        pygmt.config(FONT_LABEL = '10p,Helvetica,black')\n\n\n        #  Basemap layer\n        fig.basemap(\n            region = region,\n            projection = projection,\n            frame = frame\n        )\n\n        \n        # topography layer\n        fig.grdimage(\n            # downloads a 3 arc second shuttle radar topography mission digital elevation model of the region extent\n            '@srtm_relief_03s',\n            # applies shading to the grid \n            shading = True, \n            cmap = grid_cmap,\n        )\n\n        \n        depth_series = [fm_depths.min(), fm_depths.max()]\n\n        # makes a color palette table to color the events by depth\n        pygmt.makecpt(\n            cmap = depth_color,\n            series = depth_series\n        )\n\n        for focal_mechanism in focal_mechanisms:\n\n            fig.meca(\n                # requires a file to be provided directly to the spec parameter if offseting the focal mechanisms is desired\n                spec = focal_mechanism,\n                # sets the focal mechanism convention\n                convention = convention,\n                # linear scaling factor with respect to M5 earthquakes\n                scale = fm_scale,\n                # sets the focal mechanisms to be colored by the previously created color palette table\n                C = True,\n                # controls whether to offset the focal mechanims\n                offset = fm_offset,\n            )\n\n\n        # Plots the color bar for depth\n        fig.colorbar(\n            frame = cbar_frame\n        )\n\n        \n        # plots the legend\n        fig.legend(\n            spec = legend,\n            position = legend_position,\n        )\n        \n\n        # plots the map scale\n        fig.basemap(\n            map_scale = map_scale\n            )\n\n\n        \n        # Plots a mini coast map as an offset inset map\n        fig.coast(\n            region = i_region,\n            projection = i_projection,\n            frame = i_frame,\n            land = i_land,\n            borders = i_borders,\n            shorelines = i_shorelines,\n            water = i_water,\n            xshift = i_xshift,\n            yshift = i_yshift\n        )\n\n        # Plots a rectangle of the study area in the inset map\n        fig.plot(\n            data = rectangle,  \n            style = i_style, \n            pen = i_pen,\n            projection = i_projection\n        )\n\n\n        # Saves a copy of the generated figure\n        fig.savefig(save_name, transparent = save_transparency)\n\n        print('Map Saved!')\n```\n\n# Result\n<p align=\"center\">\n  <img alt=\"\" style=\" margin-left: 50px;margin-right: 50px;\" src=\"https://user-images.githubusercontent.com/74040471/140019294-a8dfb2f1-cb87-4a3b-b0b2-a0ca3b1f477a.png\"/>\n</p>\n\nI hope that this has been helpful for anyone looking to plot and offset focal mechanisms with PyGMT.\n\n# Complete Code\n```\n'''\nPyGMT v0.4.1\n\nDecription:\nThis script creates a map of focal mechanisms that are scaled in size according to magnitude and colored according to depth. \nIt offsets selected focal mechanisms from their point of origin and draws a line to them.\n\nInstructions:\n1) Run the create_folder.py script to create the directory paths\n2) Perform a focal mechanism search of the Souther California Earthquake Data Center focal mechanism catalog using the provided parameters\n3) Copy the focal mechanism data and headers into a text file saved to the Data folder of this demo\n4) Run this script\n\nFocal Mechanism data source: https://service.scedc.caltech.edu/eq-catalogs/FMsearch.php\n\nDate range: 2019/01/01 to 2020/01/01\nMagnitude range: 5 to 10\nLatitude range: 35.5 to 36.1\nLongitude range: -118 to -117.15\nDepth range: -5 to 100\n'''\n\nclass Map_Builder():\n\n    # Path to main project folder\n    main_dir = r'C:\\Users\\USER\\Desktop\\Focal_Mechanism_Demo'\n\n\n    # Conditions focal mechanism files downloaded from the Southern California Earthquake Data Center so that they can be used\n    def Condition_Focal_Mechanism_Data(self):\n        import os\n        import re\n\n        print('Conditioning focal mechanisms...')\n\n\n        input_file = os.path.join(self.main_dir, 'Data', 'focal_mechanism_data.txt')\n        # reads in the file by line rather than the default of by character\n        with open(input_file, 'r') as f:\n            lines = f.readlines()\n\n        # holds the conditioned lines, namely the removal of empty lines\n        conditioned_lines = []\n        # iterates over each line and adds them to a list as long as they are not empty\n        for line in lines:\n            if line == '\\n':\n                continue\n            conditioned_lines.append(line)\n        \n        # empty placeholder for the conditioned lines list to be rejoined into a string\n        conditioned_data = ''\n        # rejoins the conditioned lines into a string so that it can be further conditioned\n        conditioned_data = conditioned_data.join(conditioned_lines)\n\n        # regular expression pattern for finding 1 or more white spaces (key) and the replacment value for matches (value)\n        replacements = {' +':'\\t'}\n        # subsitutes text using the patterns in \"replacements\"\n        for key, value in replacements.items():\n            data = re.sub(key, value, conditioned_data)\n\n        output_file = os.path.join(self.main_dir, 'Data', 'focal_mechanism_data.csv')\n        with open(output_file, 'w') as f:\n            f.write(data)\n\n\n\n    # Creates a focal mechanism dataframe in the \"aki\" format from the focal mechanism data\n    def Filter_AKI_Format_Focal_Mechanism_Data(self):\n        import os\n        import pandas as pd\n\n        print('Writing focal mechanisms to CSV...')\n\n\n        focal_mechanisms =  os.path.join(self.main_dir, 'Data', 'focal_mechanism_data.csv')\n        df_focal_mechanisms = pd.read_csv(focal_mechanisms , sep='\\t')\n        \n        # saves the unified depth column as a pandas series so that it can be recalled later for coloring the focal mechanisms by depth\n        fm_depths = df_focal_mechanisms['DEPTH']\n        # saves the unified depth magnitude column as a pandas series so that it can be recalled later for creating the legend\n        fm_mags = df_focal_mechanisms['MAG']\n\n        # magnitude of earthquakes used to filter out the focal mechanisms that will be offset\n        magnitude_filter = 7\n\n        # focal mechanisms with earthquake magnitude less than the magnitude filter\n        df_mag_filtered_focal_mecha = df_focal_mechanisms[(df_focal_mechanisms.MAG < magnitude_filter)]\n        # focal mechanisms with earthquake magnitude greater than or equal to the magnitude filter\n        df_offset_mag_filtered_focal_mecha =df_focal_mechanisms[(df_focal_mechanisms.MAG >= magnitude_filter)]\n\n        # reads the unmodifed magnitude of the focal mechanisms that will be offset into a list so it can be later used to label them\n        fm_label = []\n        for i in df_offset_mag_filtered_focal_mecha.MAG:\n            fm_label.append(i)\n\n        fm_dataframes = [df_mag_filtered_focal_mecha, df_offset_mag_filtered_focal_mecha]\n\n        # iterates through each focal mechanism dataframe and scales them expoentially so that they will be plotted as exponentially larger sizes, and resets the index in place while not including the old index as a column which is done for tidyness \n        for dataframe in fm_dataframes:\n            dataframe.reset_index(inplace=True, drop=True)\n            dataframe['MAG'] = 0.1 * (2 ** dataframe.MAG)\n\n        # new dataframes to hold only the columns used for the \"aki\" focal mechanism format\n        fm_aki_format = pd.DataFrame()\n        fm_aki_format_offset = pd.DataFrame()\n        aki_dataframes = [fm_aki_format, fm_aki_format_offset]\n        # column names of the aki format, with keys being columns from the focal mechanism dataframes and values being the new column names to be used\n        aki_format_attributes = {'LON':'longitude', 'LAT':'latitude', 'DEPTH':'depth', 'STRIKE':'strike', 'DIP':'dip', 'RAKE':'rake', 'MAG':'magnitude'}\n\n        # add the desired columns for the 'aki' focal mechanism format to the new dataframes in the desired order\n        for aki_data, fm_data in zip(aki_dataframes, fm_dataframes):\n            for key, value in aki_format_attributes.items():\n                aki_data[value] = fm_data[key]\n\n        # offset coordinates (lon:lat) used for offsetting the selected focal mechanisms\n        offset_coordinates = {-117.70:35.78}\n        fm_aki_format_offset['offset_longitude'] = offset_coordinates.keys()\n        fm_aki_format_offset['offset_latitude'] = offset_coordinates.values()\n\n        # adds label data for plotting above each focal mechanism beachball\n        fm_aki_format_offset['label_mag'] = fm_label\n\n        # path to the \"small\" EQ focal mechanisms \n        focal_mechanisms_lesser_output =  os.path.join(self.main_dir, 'Data', 'focal_mechanism_data_less_than_M{}.csv').format(magnitude_filter)\n        # path to the \"large\" EQ focal mechanisms \n        focal_mechanisms_greater_output =  os.path.join(self.main_dir, 'Data', 'focal_mechanism_data_greater_than_or_equal_to_M{}.csv').format(magnitude_filter)\n        focal_mechanism_outputs = [focal_mechanisms_lesser_output, focal_mechanisms_greater_output]\n\n        # writes the aki format dataframes to csv files, leaving out the header. This is important because if the header is included, fig.meca will plot the header text\n        for dataframe, path in zip(aki_dataframes, focal_mechanism_outputs):\n            dataframe.to_csv(path, sep='\\t', index=False, header=None)\n\n        return fm_depths, fm_mags, magnitude_filter\n\n\n\n    # Creates a postscript legend file\n    def Create_Legend(self, fm_mags):\n        import os\n        import math\n\n        # File path to postscript legend file to be created\n        focal_mechanism_legend = os.path.join(self.main_dir, 'Data', 'focal_mechanism_legend.txt')\n\n        # Creates a postscript file and writes some explainer, legend header text, and the column format\n        with open(focal_mechanism_legend, 'w') as f:\n            f.write(\n                '# G is vertical gap, V is vertical line, N sets # of columns,\\n'\n                '# D draws horizontal line, H is header, L is column header,\\n'\n                '# S is symbol <symbol size><symbol color><symbol border thickness><text position><text>\\n'\n                '\\n'\n                'H 12p,Helvetica Magnitude\\n'\n                'D 0.1i 1p\\n'\n                'G 0.05i\\n'\n                'N 1\\n'\n                '\\n'\n            )\n\n        # sets the index to the minimum magnitude of the unified dataset. This is used to set the minimum size for the circles used to represent magnitude within the legend, as well as label them\n        i = math.floor(fm_mags.min())\n        # sets the vertical space between each magnitude circle/text line\n        v_spacer = 0.08\n\n        # creteas a legend entry for each integer magnitude within the dataset and appends it to the legend file\n        with open(focal_mechanism_legend, 'a') as f:\n            while i <= round(fm_mags.max(), 0):\\\n                # replicates the exponential scaling of the magnitide and linear scaling of the fig.meca function so that the magnitude circles in the legend are the same size as the plotted focal mechanisms\n                size = 0.1*(0.1*(2**i))\n                size = round(size, 2)\n                \n                # creates a legend of transparent circles that are labeld with the associated magnitude\n                f.write(\n                    'S 0.20i c {} white@100 0.5p 0.6i M{}\\n'\n                    'G {}i\\n'\n                    .format(size, i, v_spacer)\n                )\n\n                # exponentially scales the vertical spacing between each legend entry so that they are evenly spaced in this particular demo\n                v_spacer = v_spacer**0.5\n                i += 1\n                \n\n    # Plots the map    \n    def Plot_Map(self, fm_depths, magnitude_filter):\n        import os\n        import pygmt\n\n        print('Building map...')\n\n        ### BASEMAP -----------------------------------------------------------------------------\n        # sets the region to be displayed in the map <min lon><max lon><min lat><max lat>\n        region = [-118.0, -117.15, 35.5, 36.1]  \n        # map projection (Mercator): <type><size><units>\n        projection = 'M6i' \n        # frame  annotations: [<frame sides with/without ticks>, <x-axis tick rate>, <y-axis tick rate>]\n        frame = ['SWne', 'xa', 'ya']\n        # map scale - displays a fancy (f) distance scale (lScale) in kilometers (u). Format of <type><longitude><latitude>/<latitude of km symbol>/<scale length in km>+<unit>+<label><label text>\n        map_scale = 'f-118.0/32.15/20/100+u+lScale:'\n        \n\n\n        ### GRID -----------------------------------------------------------------------------\n        # sets the color palette table for the digital elevation map\n        grid_cmap = 'grayC'\n\n\n\n        ### MECA -----------------------------------------------------------------------------\n        # focal mechanism data of earthquakes less than the magnitude filter\n        focal_mechanisms_less_than_magnitude_filter =  os.path.join(self.main_dir, 'Data', 'focal_mechanism_data_less_than_M{}.csv').format(magnitude_filter)\n        # focal mechanism data of earthquakes greater than or equal to the magnitude filter\n        focal_mechanisms_greater_than_magnitude_filter =  os.path.join(self.main_dir, 'Data', 'focal_mechanism_data_greater_than_or_equal_to_M{}.csv').format(magnitude_filter)\n        focal_mechanisms = [focal_mechanisms_less_than_magnitude_filter, focal_mechanisms_greater_than_magnitude_filter]\n        # column format: <longitiude><latitude><strike><dip><rake><magnitude>. Optionally add <offset longitude><offset latitude><label> to the end\n        convention = 'aki'\n        # scale defines the size for magnitude = 5 (0.5). Also optionally plots labels above the focal mechanisms (+f15p,Helvetica,black). \n        fm_scale = '0.5+f15p,Helvetica,black'\n        # thickness of the lines drawn from the offset focal mechanisms back to their point of origin\n        fm_offset = '1p'\n        # colors by which to color focal mechanisms\n        depth_color = 'yellow,red,purple'\n\n\n\n        ### COLORBAR -----------------------------------------------------------------------------\n        # automaticly determined fancy frame with label. Format of: <type>+<label><label text>\n        cbar_frame = 'af+l\"Depth (km)\"'\n\n\n\n        ### LEGENDS -----------------------------------------------------------------------------\n        # path to the legend file\n        legend = os.path.join(self.main_dir, 'Data', 'focal_mechanism_legend.txt')\n        # <position>+<width>+<offset x-dir>/<offset y-dir>\n        legend_position = 'JMR+w1.05i+o0.15i/-0.02i'\n        # <background color>+<line thickness>\n        #legend_box = '+gwhite+p1p'\n\n\n\n        ### INSET MAP -----------------------------------------------------------------------------\n        # extent of the inset map\n        i_region = [-130, -105, 27, 45]\n        i_projection = 'M1.75i'\n        # sets the border types, line thickness, and color <border type><line thickness><color>. In this case 1 = national boundaries, 2 = state boundaries within America\n        i_borders = ['1/0.8p,gray40', '2/0.8p,gray40']\n        # sets the color of the land\n        i_land = 'gray'\n        # sets the shorline type, line thickness, and color <shoreline type><line thickness><line color>. In this case, 1 = coastline\n        i_shorelines = '1/0.1p,gray40'\n        i_pen = '1p,black'\n        # sets the water color\n        i_water = 'azure2'\n        # shifts the inset map in the x-direction. Positive numbers shift up, negative numbers shifts down. <amount><units>\n        i_xshift = '12.5c'\n        # shifts the inset map in the y-direction. Positive numbers shift up, negative numbers shifts down. <amount><units>\n        i_yshift ='11.0c'\n        # first two list elements of 'rectangle' are the longitude and latitude of the bottom left corner of the rectangle, and the last two elements are the longitude and latitude of the uppper right corner. \n        rectangle = [[region[0], region[2], region[1], region[3]]]\n        # sets style of plot as rectagle (r) with first two list items as the longitude and latitude of the bottom left corner of the rectangle, and the last two list items as the longitude and latitude of the uppper right corner (+s)\n        i_style = 'r+s'\n        # sets an auto-detrmined frame with ticks in intervals of 10 (a10), then displays the frame with ticks on the top and right sides of the map (NE), but doesn't show ticks for the left and bottoms sides (sw)\n        i_frame = ['a10', 'swNE']\n        \n\n\n        ### Save File -----------------------------------------------------------------------------\n        # map save name\n        save_name = os.path.join(self.main_dir, 'Results', 'Focal_Mechanism_Demo.png')\n        # sets the transparency of the white space around the map    \n        save_transparency = False\n\n\n\n        # Establishes a figure to hold map layers\n        fig = pygmt.Figure()\n\n        # Forces Lat/Lon to display as degree decimal\n        pygmt.config(FORMAT_GEO_MAP = 'D')\n        # Forces Lat/Lon to display with two decimal places <f>, as opposed the default that limits by significan digits <g>\n        pygmt.config(FORMAT_FLOAT_OUT = '%.2f')\n        # Forces the map frame annotation to be smaller\n        pygmt.config(FONT_ANNOT_PRIMARY = '10p,Helvetica,black')\n        # Forces the scale font to be smaller\n        pygmt.config(FONT_LABEL = '10p,Helvetica,black')\n\n\n        #  Basemap layer\n        fig.basemap(\n            region = region,\n            projection = projection,\n            frame = frame\n        )\n\n        \n        # topography layer\n        fig.grdimage(\n            # downloads a 3 arc second shuttle radar topography mission digital elevation model of the region extent\n            '@srtm_relief_03s',\n            # applies shading to the grid \n            shading = True, \n            cmap = grid_cmap,\n        )\n\n        \n        depth_series = [fm_depths.min(), fm_depths.max()]\n\n        # makes a color palette table to color the events by depth\n        pygmt.makecpt(\n            cmap = depth_color,\n            series = depth_series\n        )\n\n        for focal_mechanism in focal_mechanisms:\n\n            fig.meca(\n                # requires a file to be provided directly to the spec parameter if offseting the focal mechanisms is desired\n                spec = focal_mechanism,\n                # sets the focal mechanism convention\n                convention = convention,\n                # linear scaling factor with respect to M5 earthquakes\n                scale = fm_scale,\n                # sets the focal mechanisms to be colored by the previously created color palette table\n                C = True,\n                # controls whether to offset the focal mechanims\n                offset = fm_offset,\n            )\n\n\n        # Plots the color bar for depth\n        fig.colorbar(\n            frame = cbar_frame\n        )\n\n        \n        # plots the legend\n        fig.legend(\n            spec = legend,\n            position = legend_position,\n        )\n        \n\n        # plots the map scale\n        fig.basemap(\n            map_scale = map_scale\n            )\n\n\n        \n        # Plots a mini coast map as an offset inset map\n        fig.coast(\n            region = i_region,\n            projection = i_projection,\n            frame = i_frame,\n            land = i_land,\n            borders = i_borders,\n            shorelines = i_shorelines,\n            water = i_water,\n            xshift = i_xshift,\n            yshift = i_yshift\n        )\n\n        # Plots a rectangle of the study area in the inset map\n        fig.plot(\n            data = rectangle,  \n            style = i_style, \n            pen = i_pen,\n            projection = i_projection\n        )\n\n\n        # Saves a copy of the generated figure\n        fig.savefig(save_name, transparent = save_transparency)\n\n        print('Map Saved!')\n\n\n\ndata = Map_Builder()\ndata.Condition_Focal_Mechanism_Data()\nfm_depths, fm_mags, magnitude_filter = data.Filter_AKI_Format_Focal_Mechanism_Data()\ndata.Create_Legend(fm_mags)\ndata.Plot_Map(fm_depths, magnitude_filter)\n```\n",
        "createdAt": "2021-11-03T06:25:41.000Z",
        "updatedAt": "2025-02-17T22:58:58.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/hazardgoat/Focal_Mechanisms_Demo/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Nandini-678/AI-Learning-Models-In-Seismology",
        "url": "https://github.com/Nandini-678/AI-Learning-Models-In-Seismology",
        "description": "Application Of AI Learning Models In Seismology",
        "stars": 0,
        "forks": 0,
        "readme": "# Application of AI Learning Models in Seismology\n\n## Overview\n\nNatural disasters, especially earthquakes, have devastating effects on human lives and infrastructure. Predicting earthquake magnitude and depth remains a critical challenge in seismology. Our project leverages machine learning and deep learning techniques to develop predictive models that can help mitigate the impacts of earthquakes by improving resource allocation and preventive measures.\n\nThis repository showcases our approach to earthquake prediction using advanced AI models, feature selection techniques, and a comprehensive dataset.\n\n## Features\n\n- **Dataset**: A rich seismic dataset from Kaggle, spanning from 1990 to 2023, with over 3 million records.\n- **Models Used**:\n  - Extreme Gradient Boosting (XGBoost)\n  - Random Forest\n  - Recurrent Neural Networks (LSTM, GRU)\n  - K-Nearest Neighbors (KNN)\n- **Optimization**: Metaheuristic optimization using Particle Swarm Optimization (PSO) to select the most relevant features.\n- **Performance Metrics**:\n  - R² Score\n  - Mean Absolute Error (MAE)\n  - Mean Squared Error (MSE)\n  - Root Mean Squared Error (RMSE)\n\n## Dataset\n\nThe dataset consists of seismic events with attributes like:\n- Date and time\n- Geographical coordinates (latitude, longitude)\n- Magnitude and depth\n- Affected region and other seismic parameters\n\n### Source\n\nThe dataset is publicly available on [Kaggle]([https://www.kaggle.com/](https://www.kaggle.com/datasets/alessandrolobello/the-ultimate-earthquake-dataset-from-1990-2023)). It has been processed to remove unnecessary features, handle missing values, and encode categorical variables.\n\n## Methodology\n\n1. **Data Preprocessing**:\n   - Cleaning the dataset\n   - Encoding categorical variables\n   - Handling missing values\n2. **Feature Selection**:\n   - Using PSO for optimal feature selection\n3. **Model Training**:\n   - Training and testing various models\n   - Hyperparameter tuning for optimal performance\n4. **Performance Evaluation**:\n   - Comparing models based on accuracy and error metrics\n\n## Results\n\nThe GRU-based RNN demonstrated the best performance, achieving high accuracy and generalization ability. PSO significantly improved model performance by selecting relevant features.\n\n| Model              | R² Score | MAE   | MSE   | RMSE  |\n|--------------------|----------|-------|-------|-------|\n| XGBoost            | 0.9949   | 0.0211| 0.0067| 0.0819|\n| Random Forest      | 0.9956   | 0.0179| 0.0058| 0.0760|\n| KNN                | 0.9949   | 0.0247| 0.0068| 0.0824|\n| LSTM               | 0.9956   | 0.0238| 0.0058| 0.0761|\n| GRU                | 0.9959   | 0.0206| 0.0054| 0.0737|\n\n## Applications\n\n- Earthquake Early Warning Systems\n- Identification of high-risk zones\n- Improved disaster management and planning\n- Development of earthquake-resistant structures\n\n## Future Work\n\n- Incorporating additional data sources like satellite imagery and geological surveys.\n- Refining models to improve real-time prediction and alert systems.\n- Developing region-specific models for more localized predictions.\n\n",
        "createdAt": "2024-12-14T04:58:39.000Z",
        "updatedAt": "2024-12-14T05:15:06.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Nandini-678/AI-Learning-Models-In-Seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "mprocha/bdfb-urban-noise",
        "url": "https://github.com/mprocha/bdfb-urban-noise",
        "description": "Tools to process seismological data to get urban noise",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2020-04-29T16:06:52.000Z",
        "updatedAt": "2020-06-29T01:31:40.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "gcalmettes/space-app-moon-scraper",
        "url": "https://github.com/gcalmettes/space-app-moon-scraper",
        "description": "Web scraper of seismologic data using Scrapy",
        "stars": 0,
        "forks": 0,
        "readme": "# space-app-moon-scraper\n\n**space-app-moon-scraper** is a web scraper made with `Scrapy` to parse data stored in https://pds-geosciences.wustl.edu/lunar/urn-nasa-pds-apollo_pse/data/xa/continuous_waveform/ for the [Nasa Space Apps Challenge](https://www.spaceappschallenge.org/).\n\n## Environment Variables\n\nYou can add the following environment variables, to control the behaviour of the scraper.\n\n`STATION`\n\nWhich station to parse, default to s11. Available values are {s11,s12,s14,s15,s16}.\n\n`FILES_STORE`\n\nWhere to store the files, default to `tmp/data`.\n## Usage/Examples\n\nWith `docker`\n```bash\ndocker run \\\n-e STATION=s12 \\\n-e FILES_STORE=/var/scraping/data \\\n-v \"$(pwd)\"/data:/var/scraping/data \\\nspace-app-scraper moonscraper\n```\n\nWith `python`\n\n```bash\npip install -r requirements.txt\ncd app/spaceAppScraper\nSTATION=s12 FILES_STORE=/var/scrapy/data crawl moonscraper\n```\n",
        "createdAt": "2022-10-03T12:04:45.000Z",
        "updatedAt": "2022-10-02T17:54:57.000Z",
        "language": null,
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/gcalmettes/space-app-moon-scraper/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "braunfuss/Palantiri",
        "url": "https://github.com/braunfuss/Palantiri",
        "description": "Seismological Backprojection array tool",
        "stars": 27,
        "forks": 5,
        "readme": "# Palantiri\n\n### A seismological backprojection array tool\n\nGitHub is restricting access to their services based on user nationality and residence. Such restrictions are incompatible with scientific standards in international research communities like seismology. See the statement at https://pyrocko.org/. \n\nAs researchers, we are obligated to retain open access to all. To achieve this, we are now migrating our code repositories away from GitHub to a new safe home. The new home of the Palantiri repository is at https://git.pyrocko.org/asteinbe/Palantiri.\n\nTo ensure a smooth transition, we will keep a version of the code repository at GitHub until 2020-01-01.\n\n\n## Documentation\n\nWIP Documentation: https://braunfuss.github.io/Palantiri/\n\n\n## Citation\n\n\n## License \nGNU General Public License, Version 3, 29 June 2007\n\nCopyright © 2018 University Potsdam, Potsdam, Germany and  University of Kiel, Kiel, Germany\nPalantiri is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\nPalantiri is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.\nYou should have received a copy of the GNU General Public License along with this program. If not, see <http://www.gnu.org/licenses/>.\n\n## Contact\n* Andreas Steinberg; \n  andreas.steinberg@ifg.uni-kiel.de\n\n* Frank Krüger; \n  kruegerf@geo.uni-potsdam.de\n\n\n```\n University of Kiel\n Institute of Geosciences\n Otto-Hahn-Platz 1\n 24118 Kiel, Germany, Germany\n\n```\n\nAvatar Image by By xDisciplExX, https://www.deviantart.com/xdisciplexx/art/Palantir-Stock-PNG-458559037 under Creative Commons Attribution-Noncommercial 3.0 License\n\n",
        "createdAt": "2018-05-10T21:49:33.000Z",
        "updatedAt": "2025-06-12T08:52:08.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/braunfuss/Palantiri/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "SeisBlue/BlueDisc",
        "url": "https://github.com/SeisBlue/BlueDisc",
        "description": "Adversarial Shape Learning for Seismic Phase Picking",
        "stars": 0,
        "forks": 0,
        "readme": "# BlueDisc: Adversarial Shape Learning for Seismic Phase Picking\n\n> **Note**: BlueDisc (SeisBlue Discriminator) is a core component of the [SeisBlue](https://github.com/SeisBlue/SeisBlue)\n\nThis repo is a minimal, reproducible implementation to validate the paper [\"Diagnosing and Breaking Amplitude Suppression in Seismic Phase Picking Through Adversarial Shape Learning.\"](\nhttps://doi.org/10.48550/arXiv.2511.06731) It augments a PhaseNet generator with a lightweight conditional discriminator (BlueDisc) to enforce label shape learning, which eliminates the 0.5-amplitude suppression band and increases effective S-phase detections.\n\n- Core idea: combine BCE Loss with a cGAN shape critic to decouple shape learning from temporal alignment\n\n<img src=\"docs/fig/model_architecture.png\" alt=\"BlueDisc architecture\" width=\"400\" />\n\n## Quick start\n\nPrereqs\n- Python 3.10+\n- PyTorch (install per your platform: https://pytorch.org/get-started/locally/)\n- MLflow 2.x (already in requirements)\n- GPU: NVIDIA GPU recommended for training\n\n> **Reproducibility Note**: GPU architecture affects GAN convergence. Newer GPUs (e.g., RTX 3090) support lower-precision computation and show better convergence than older models (e.g., GTX 1080 Ti) in our tests. Results in the paper were obtained using RTX 3090. When using different GPU architectures, you may need to adjust the `--data-weight` (λ) parameter to achieve similar results.\n\nSetup\n```bash\npython -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\n# Install PyTorch separately per platform (CPU/CUDA/MPS), e.g.:\n# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n```\n\nStart MLflow (required)\n```bash\nmlflow ui\n# or\npython -m mlflow ui\n```\n\nTrain\n- BCE only (no GAN):\n```bash\npython 01_training.py \\\n  --label N \\\n  --dataset InstanceCounts \\\n  --max-steps 10000\n```\n- Conditional GAN: set a data loss weight (λ), e.g. 4000 per paper\n```bash\npython 01_training.py \\\n  --label N \\\n  --dataset InstanceCounts \\\n  --data-weight 4000 \\\n  --max-steps 10000\n```\nNotes\n- `--dataset` is a [SeisBench dataset class name](https://seisbench.readthedocs.io/en/stable/pages/documentation/data.html#seisbench.data.instance.InstanceCounts) (e.g., `InstanceCounts`, `ETHZ`). The dataset will be downloaded by SeisBench on first use.\n- `--label` controls the output channel order: `N` (noise) or  `D` (detection).\n\nInfer\n1) Find the `run_id` from MLflow UI or `mlruns/*/*/meta.yaml`.\n2) Run inference (choose split and optional checkpoint by step/epoch):\n```bash\npython 02_inference.py \\\n  --run-id <RUN_ID> \\\n  --dataset InstanceCounts \n```\n\nEvaluate\n```bash\npython 03_evaluation.py \\\n  --run-id <RUN_ID> \n```\nOutputs are saved under `mlruns/<experiment>/<run_id>/artifacts/` (waveforms, labels, predictions as HDF5; checkpoints under `checkpoint/`; matching CSVs under `<split>/matching_results/`).\n\n## Visualization\n\nThe repository includes several plotting scripts to analyze model behavior during and after training:\n\n### Training-based visualization (using logged tracking data)\nDuring training, the model automatically logs sample predictions at each step. You can visualize training progression using:\n- `plot_compare_runs.py`: side-by-side comparison of predictions from different runs at the same step\n\n<img src=\"docs/fig/compare_runs_example.png\" alt=\"compare_runs_example\" width=\"400\" />\n\n- `plot_compare_shape.py`: compare prediction shapes at selected training steps\n\n<img src=\"docs/fig/compare_shape_example.png\" alt=\"compare_shape_example\" width=\"400\" />\n\n- `plot_compare_time.py`: visualize how predictions evolve over training steps for a specific sample\n\n<img src=\"docs/fig/compare_time_example.png\" alt=\"compare_time_example\" width=\"400\" />\n\n\nThese scripts work directly with the tracking data logged during training (`mlruns/<experiment>/<run_id>/artifacts/track/`).\n\n### Inference-based visualization (requires test dataset predictions)\n- `plot_compare_peak.py`: analyze peak detection accuracy by comparing predicted peaks with ground-truth labels. **Requires running both inference (`02_inference.py`) and evaluation (`03_evaluation.py`)** on the test dataset first. The evaluation step generates matching results (`matching_results/` CSVs) that pair each predicted peak with its corresponding label peak, enabling quantitative analysis of detection performance.\n\n<img src=\"docs/fig/compare_peak_example.png\" alt=\"compare_peak_example\" width=\"400\" />\n\n### Data exploration\n- `plot_compare_phase.py`: visualize P and S phase label arrangements in the dataset. This is a data exploration tool independent of model training.\n\n<img src=\"docs/fig/compare_p_s.png\" alt=\"compare_p_s\" width=\"400\" />\n\n## Repo layout\n- `01_training.py`, `02_inference.py`, `03_evaluation.py`: train → infer → evaluate pipeline\n- `module/`: generator (PhaseNet wrapper), discriminator (BlueDisc), GAN training loop, data pipeline, logger\n- `plot_*.py`: visualization scripts for analyzing training, inference, and data\n- `mlruns/`: MLflow experiments and artifacts\n- `docs/`: short documentation\n- [loss_landscape/](loss_landscape): standalone loss-landscape simulations (BCE toy experiments)\n  - `loss_landscape_analysis.py`: BCE loss surface visualization (height vs. time/peak)\n  - `no_model_bce_test.py`: point-wise vs Gaussian-parameterized BCE optimization\n\n\n## Citation\n\nPlease cite the paper when using this code:\n\n```bibtex\n@misc{huang2025bluedisc,\n  title={Diagnosing and Breaking Amplitude Suppression in Seismic Phase Picking Through Adversarial Shape Learning},\n  author={Chun-Ming Huang and Li-Heng Chang and I-Hsin Chang and An-Sheng Lee and Hao Kuo-Chen},\n  year={2025},\n  publisher={arXiv},\n  doi={10.48550/arXiv.2511.06731},\n  eprint={2511.06731},\n  archivePrefix={arXiv}\n}\n```\n\n## References\n\nKey papers referenced in this work:\n\n- **PhaseNet**: Zhu, W., & Beroza, G. C. (2019). PhaseNet: a deep-neural-network-based seismic arrival-time picking method. *Geophysical Journal International*, 216(1), 261-273.  \n  DOI: [10.1093/gji/ggy423](https://doi.org/10.1093/gji/ggy423)\n\n- **GAN**: Goodfellow, I., Pouget-Abadie, J., Mirza, M., et al. (2014). Generative adversarial nets. *NeurIPS*.  \n  [Paper](https://papers.nips.cc/paper_files/paper/2014/hash/f033ed80deb0234979a61f95710dbe25-Abstract.html) | [arXiv:1406.2661](https://arxiv.org/abs/1406.2661)\n\n- **Conditional GAN**: Mirza, M., & Osindero, S. (2014). Conditional generative adversarial nets. *arXiv preprint arXiv:1411.1784*.  \n  [arXiv:1411.1784](https://arxiv.org/abs/1411.1784)\n\n- **pix2pix**: Isola, P., Zhu, J. Y., Zhou, T., & Efros, A. A. (2017). Image-to-image translation with conditional adversarial networks. *CVPR*.  \n  DOI: [10.1109/CVPR.2017.632](https://doi.org/10.1109/CVPR.2017.632) | [arXiv:1611.07004](https://arxiv.org/abs/1611.07004)\n\n- **U-Net**: Ronneberger, O., Fischer, P., & Brox, T. (2015). U-net: Convolutional networks for biomedical image segmentation. *MICCAI*.  \n  DOI: [10.1007/978-3-319-24574-4_28](https://doi.org/10.1007/978-3-319-24574-4_28) | [arXiv:1505.04597](https://arxiv.org/abs/1505.04597)\n\n- **SeisBench**: Woollam, J., Rietbrock, A., Bueno, A., & De Angelis, S. (2022). SeisBench—A toolbox for machine learning in seismology. *Seismological Research Letters*, 93(3), 1695-1709.  \n  DOI: [10.1785/0220210324](https://doi.org/10.1785/0220210324) | [GitHub](https://github.com/seisbench/seisbench)\n\n- **Pick-Benchmark**: Münchmeyer, J., Bindi, D., Leser, U., & Tilmann, F. (2022). Which picker fits my data? A quantitative evaluation of deep learning based seismic pickers. *JGR: Solid Earth*, 127(1).  \n  DOI: [10.1029/2021JB023499](https://doi.org/10.1029/2021JB023499) | [GitHub](https://github.com/seisbench/pick-benchmark)\n\n- **INSTANCE Dataset**: Michelini, A., Cianetti, S., Gaviano, S., et al. (2021). INSTANCE–the Italian seismic dataset for machine learning. *Earth System Science Data*, 13(12), 5509-5544.  \n  DOI: [10.5194/essd-13-5509-2021](https://doi.org/10.5194/essd-13-5509-2021)\n\n## Contributor List\njimmy60504, atihsin118324, qwert159784623\n",
        "createdAt": "2025-10-29T06:10:36.000Z",
        "updatedAt": "2025-11-11T16:06:18.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/SeisBlue/BlueDisc/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Sarvandani/MUTE_SEGY_CODE",
        "url": "https://github.com/Sarvandani/MUTE_SEGY_CODE",
        "description": "Muting seismic data, data processing, imaging, Full waveform Inversion",
        "stars": 7,
        "forks": 0,
        "readme": "## Muting seismic files before modeling or before FWI\nWe may need to muffle some parts of the data set before modeling seismic data. This work is vital in full wave inversion (FWI).\n\nI have created a free graphical code to do this task, called `MUTE_SEGY_CODE`. Muting the data is applied to one Segy file in the folder called `synthetic_data1.segy`.\n\nAfter defining the initial parts of the code (explained in the code), the first step is to define the upper and lower layers of the favorite area of data. As you see in the figure, the layers should be selected from left to right, and the layer is chosen by double clicking the mouse on the right part of the figure.\n\n<img src=\"OBS_1_PICKING.png\" width=\"800\" height=\"600\">\n\n\n\n\nThe code starts interpolating the selected region as follows:\n\n<img src=\"OBS_1_KM_SEC.png\" width=\"800\" height=\"600\">\n\n\n\n\n\n\nand at the end, the selected region is as follows:\n\n<img src=\"OBS_1_AFTER_MUTING_.png\" width=\"800\" height=\"600\">\n\n\n\nThe muted Segy file can be found in the folder MUTED_SEGY_FILES.\n\n`DISCLAIMER`:  I don't warrant this code in any way whatsoever. This code is provided \"as-is\" to be used at your own risk.\n\nThis work was done as part of my PhD, I would be happy if you could cite my PhD thesis:\n\nhttps://theses.hal.science/tel-04020124/\n\n\n\n",
        "createdAt": "2023-01-20T15:22:41.000Z",
        "updatedAt": "2025-11-05T10:45:07.000Z",
        "language": "MATLAB",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Sarvandani/MUTE_SEGY_CODE/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "taylor-kenyon/FluvialSeismology_Analysis",
        "url": "https://github.com/taylor-kenyon/FluvialSeismology_Analysis",
        "description": "Scripts meant for the collection, visualization, filtering, and analysis of seismic data targeted at fluvial processes. Topics will cover both flow and sediment transport, with the objective of producing tools for the analysis of both discharge and bedload sediment flux.",
        "stars": 0,
        "forks": 2,
        "readme": "",
        "createdAt": "2024-05-06T16:43:22.000Z",
        "updatedAt": "2025-07-18T18:16:33.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "NoiseCIEI/Seed2Cor",
        "url": "https://github.com/NoiseCIEI/Seed2Cor",
        "description": "Seismic Ambient Noise Cross-Correlation in Parallel",
        "stars": 9,
        "forks": 1,
        "readme": "# Seed2Cor\n\nParallel single component ambient noise correlation from `SEED`\n- Extract `SAC` & response from `SEED`\n- Remove instrument response\n- Temporal (1-bit, run-absolute-mean, cut events) &\n  spectral (whiten) normalization\n- Cross/Auto-correlate & stack\n\n# Directory structures\n\n- `input/`: template files for input\n- `src/`: source code\n- `tools/`: auxiliary scripts\n\n# Usage\n\n## Dependencies\n\nCurrently tested on\n\n- RHEL (Red Hat Enterprise Linux): 6.9, 7.3\n- gcc: 6.1.0, 4.4.7 20120313 (Red Hat 4.4.7-18)\n- FFTW: 3.3.7, 3.3.4 (failed!), 3.2.3\n- rdseed 5.3.1\n- evalresp 3.3.3\n\n## Compile\n\n```\n$ cd src/\n$ make\n```\n\nThe output could be compared with `src/make.log`.\n\n## Input\n\n- `parameters`: processing parameters\n  - see `input/parameters.lst` for template\n- `seed.lst`: list of `SEED` files\n  - see `input/seed.lst` for template\n  - use `tools/generate_seedlst.sh` to generate\n- `station.lst`: list of stations\n  - see `input/station.lst` for template\n  - use `tools/generate_stationlst.sh` to generate\n  - optional flag\n    ```\n    0-N\n    For stations with same number n (except 0), do cc,\n    also do cc with group 0,\n    but do not do cc with other numbers.\n    For number 0, do cc with all other group,\n    but do not do cc with all other group 0 members.\n    ```\n\n## Run\n\n```shell\n$ Seed2Cor parameter_file [nthreads]\n```\n\nA `Python` wrapper `tools/s2c_wrapper.py` with a `YAML` parameter file\n`input/param.yml` might be helpful.\n\n## Output\n\n### File Structure\n\n- `yyyy.MMM/`\n  - `yyyy.MMM.dd/`\n  - `COR/`: monthly correlations\n    - `STA/`\n      - `COR_MASTER_SLAVE.SAC`\n  - `COR_D/`: daily correlations\n    - `STA/`\n      - `COR_MASTER_SLAVE_dd.SAC`\n  - `Cor_dayflag.lst`: days if data available\n\n### SAC Header\n\n- `KEVNM`: source station\n- `KSTNM`: receiver station\n- `USER0`: # of days stacked\n\n## Attention\n\n- The temperal normalization method of 'earthquake cutting' has NOT been well tested. Use with caution!\n- The spectrum reshape function is currently removed from the code. (`Smooth2()` in `Whiten.c` does nothing)\n\n# Reference\n\nBensen, G. D., Ritzwoller, M. H., Barmin, M. P., Levshin, A. L., Lin, F., Moschetti, M. P., et al. (2007). Processing seismic ambient noise data to obtain reliable broad-band surface wave dispersion measurements. *Geophysical Journal International*, 169(3), 1239–1260.\n\nLin, F.-C., Ritzwoller, M. H., & Shen, W. (2011). On the reliability of attenuation measurements from ambient noise cross-correlations. *Geophysical Research Letters*, 38(11).\n",
        "createdAt": "2019-01-10T07:34:15.000Z",
        "updatedAt": "2025-11-03T13:04:50.000Z",
        "language": "C++",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/NoiseCIEI/Seed2Cor/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "HouseJaay/Geopy",
        "url": "https://github.com/HouseJaay/Geopy",
        "description": "Useful scripts for seismology research",
        "stars": 13,
        "forks": 2,
        "readme": "# Geopy\n## 介绍\n地震学科研实用程序，包括：（1）振幅比计算程序，（2）双台法计算相速度程序，（3）调用其它地震研究程序的接口等。\n## 功能介绍\n### 用双台法计算面波相速度-FastTS\n基于TwoStation改进，通过把常用结果保存在内存，提高计算效率。\n### 计算振幅比-ZHratio\n计算单台的振幅比。\n### 发送数据申请-dwseis\n利用breq_fast服务申请地震数据。\n### cps接口-cps.py\n调用cps面波相关程序的接口，包括计算理论相速度、群速度频散曲线等。\n### 画图辅助-Plot\n用gmt将多张图画在一起需要比较繁琐的调整，本程序提供了拼贴功能，把不同深度或周期的成像结果拼成两列或三列，并自动裁去gmt输出中的白边。\n",
        "createdAt": "2017-07-07T08:35:13.000Z",
        "updatedAt": "2025-05-26T18:23:07.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/HouseJaay/Geopy/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ShiroHirano/DiallelX",
        "url": "https://github.com/ShiroHirano/DiallelX",
        "description": "DiallelX is a CPU-oriented modern fortran program to approximate Network Cross-Correlation coefficients (NCCs) among multiple continuous records and template waveforms observed at multiple seismic stations. The results, relatively less accurate but sufficient to find new seismic events, are obtained several-fold faster than a conventional scheme.",
        "stars": 3,
        "forks": 0,
        "readme": "# *DiallelX*: Seismic network cross-correlation calculator\n\n<address><a href=\"https://interfacial.jp/\">Shiro Hirano</a></address>\n\nTable of Contents\n\n- [*DiallelX*: Seismic network cross-correlation calculator](#diallelx-seismic-network-cross-correlation-calculator)\n  - [Give it a try](#give-it-a-try)\n  - [Introduction](#introduction)\n  - [Installation](#installation)\n    - [1. Install `gfortran` and `libfftw3-dev`](#1-install-gfortran-and-libfftw3-dev)\n    - [2. Install Intel oneAPI Base Toolkit and Intel HPC Toolkit](#2-install-intel-oneapi-base-toolkit-and-intel-hpc-toolkit)\n    - [3. Use Intel Fortran Compiler Classic (`ifort`) that is deprecated and no longer distributed](#3-use-intel-fortran-compiler-classic-ifort-that-is-deprecated-and-no-longer-distributed)\n    - [To specify compiler](#to-specify-compiler)\n  - [Preparing data files: nomenclature](#preparing-data-files-nomenclature)\n    - [Continuous waveform records](#continuous-waveform-records)\n    - [Template waveforms](#template-waveforms)\n  - [Algorithm: an overview](#algorithm-an-overview)\n  - [Diallel calculation](#diallel-calculation)\n  - [Interpretation of results](#interpretation-of-results)\n    - [Event candidates](#event-candidates)\n    - [histogram.dat](#histogramdat)\n  - [Appendix A: Visual example of a diallel](#appendix-a-visual-example-of-a-diallel)\n  - [Appendix B: Details of algorithm](#appendix-b-details-of-algorithm)\n  - [Reference](#reference)\n\n\n## Give it a try\n\n<details>\n<summary>Ubuntu/Linux Mint</summary>\n\n```bash\n# Install required packages (make and locate are installed in Linux Mint by default)\nsudo apt install make locate gfortran libfftw3-dev gnuplot -y\n# Build DiallelX\nmake\n# Generate dummy data in a directory named \"test\"\n./DiallelX -s -d test\n# Calculate NCCs among dummy data\n./DiallelX -d test\n# Plot top 100 similar waveform pairs\n./DiallelX -p 100 -d test\n```\n</details>\n\n<details>\n<summary>AlmaLinux</summary>\n\n```bash\n# Install required packages\nsudo dnf install gfortran fftw3-devel gnuplot -y \n# Build DiallelX\nmake\n# Generate dummy data in a directory named \"test\"\n./DiallelX -s -d test\n# Calculate Network Cross-correlation Coefficients among dummy continuous records and dummy templates\n./DiallelX -d test\n```\n</details>\n\n## Introduction\n\n*DiallelX* is a CPU-oriented modern fortran program that approximates Network Cross-Correlation coefficients (NCCs) among multiple continuous records and template waveforms observed at multiple seismic stations.\nThe results, while relatively less accurate but sufficient to find new seismic events, are obtained several-fold faster than a conventional scheme.\n\nRequired steps are as follows:\n\n0. Install *DiallelX* (gfortran and single-precision FFTW3 are required)\n0. Prepare waveform records in float32 binary formats (w/o record boundaries) or SAC format\n0. Run `./DiallelX -d [IOdirname]`\n0. Define new events based on results and stats\n\nApropos, *DiallelX* is derived from a terminology in genetics as follows<sup>[[Hayman 1954]](https://doi.org/10.1093/genetics/39.6.789)</sup>:\n> A *diallel cross* is the set of all possible matings between several genotypes.\n\nTherefore, the term *diallel* in this context represents the set of cross-correlation coefficients (or functions) between all possible pairs of the continuous and template waveforms.\n\n## Installation\n\nJust running `make` in the `DiallelX` directory compiles the program.\nHowever, other tools must have been installed before `make`.\nUsers have three installation options (the first is recommended, given the official deprecation of `ifort` as in the third):\n\n1. Install `gfortran` and `fftw3`\n1. Install [Intel oneAPI Base Toolkit](https://www.intel.com/content/www/us/en/developer/tools/oneapi/base-toolkit-download.html) and [Intel HPC Toolkit](https://www.intel.com/content/www/us/en/developer/tools/oneapi/hpc-toolkit-download.html)\n1. Use [Intel Fortran Compiler Classic](https://www.intel.com/content/www/us/en/developer/articles/guide/porting-guide-for-ifort-to-ifx.html) (`ifort`) that is deprecated and no longer distributed\n\nHere is the installation guide for Linux Mint 21.x (the development environment of *DiallelX*), Ubuntu 22.04, and AlmaLinux 9.3 <!--macOS Sonoma-->.\n\n### 1. Install `gfortran` and `libfftw3-dev`\n\nFor Ubuntu (`make` and `locate` have not been installed by default) or Linux Mint:\n```bash\nsudo apt install make locate gfortran libfftw3-dev gnuplot -y\n```\n\nFor AlmaLinux:\n```bash\nsudo dnf install gfortran fftw3-devel gnuplot -y \n```\n<!--\nFor macOS Sonoma:\n```bash\nbrew install gcc fftw\n```\n-->\n\n### 2. Install [Intel oneAPI Base Toolkit](https://www.intel.com/content/www/us/en/developer/tools/oneapi/base-toolkit-download.html) and [Intel HPC Toolkit](https://www.intel.com/content/www/us/en/developer/tools/oneapi/hpc-toolkit-download.html)\n\n> [!WARNING]\n> (2024-06-09)\n> After upgrading to ifort Version 2021.12.0 Build 20240211_000000 on Ubuntu/Linux Mint, ifort does not work because of [missing omp_lib](https://community.intel.com/t5/Intel-Fortran-Compiler/After-upgrading-ifort-no-longer-finds-include-dir-for-OpenMP/td-p/1600614).\n> Tentatively, to avoid this problem with ifort, copy and paste\n> ```\n> $(shell ./omp_lib_finder.sh)\n> ```\n> into the right hand (blank) side of 13th line in `src/Makefile` (INCLUDE = ).\n\nFor Linux Mint/Ubuntu, the following procedures are provided in the above two links:\n```bash\nwget -O- https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB \\ | gpg --dearmor | sudo tee /usr/share/keyrings/oneapi-archive-keyring.gpg > /dev/null\necho \"deb [signed-by=/usr/share/keyrings/oneapi-archive-keyring.gpg] https://apt.repos.intel.com/oneapi all main\" | sudo tee /etc/apt/sources.list.d/oneAPI.list\nsudo apt update\nsudo apt install intel-basekit intel-hpckit gnuplot -y\n```\n\nAfter these installations, running\n```bash\necho \"source /opt/intel/oneapi/setvars.sh > /dev/null\" >> ~/.bashrc\n```\nis required to set environment variables.\n\nThe same option is unavailable for macOS because of [the official announcement](https://www.intel.com/content/www/us/en/docs/oneapi/installation-guide-macos/2024-0/overview.html):\n>  NOTE: Starting with the 2024.0 release, macOS* is no longer supported in Intel® oneAPI Toolkits and components.\n\n### 3. Use [Intel Fortran Compiler Classic](https://www.intel.com/content/www/us/en/developer/articles/guide/porting-guide-for-ifort-to-ifx.html) (`ifort`) that is deprecated and no longer distributed\n\nIntel Fortran Compiler Classic (`ifort`) may provide the fastest executable file of *DiallelX*.\nHowever, `ifort` says:\n> Intel(R) Fortran Compiler Classic (ifort) is now deprecated and will be discontinued late 2024. Intel recommends that customers transition now to using the LLVM-based Intel(R) Fortran Compiler (ifx) for continued Windows* and Linux* support, new language support, new language features, and optimizations.\n\nAt least for Jul. 2024, unfortunately, `ifx` provides a significantly slower executable file than `ifort` and `gfortan` does.\nTherefore, `Makefile` of *DiallelX* uses `ifort` if available.\n\n### To specify compiler\n\nDuring `make`, the compiler will be chosen automatically.\nHowever, if you prefer a compiler out of multiple compilers, you can run one of the following three:\n```bash\nmake compiler=gfortran\nmake compiler=ifort\nmake compiler=ifx\n```\n\n## Preparing data files: nomenclature\n\nHere is an example of the nomenclature of waveform files.\nInput waveforms must be SAC or float32 binary files w/o record boundaries.\nIn the following, \".sac\" can be replaced by \".bin\".\nInside of the brackets [ &nbsp; ] can be defined by the user.\nAll data files are assumed to be in subdirectories under `[IOdirname]`, the name of which is arbitrary and should be be specified by the user when `./DiallelX` is executed.\n`[IOdirname]` can be a relative or absolute path.\n\n### Continuous waveform records\n\nAll continuous records must consist of the same length.\nFor example, a daily record with 100 Hz sampling rate results in 100 $\\times$ 60 $\\times$ 60 $\\times$ 24 = 8,640,000 samples, which is 34,560,000 Bytes for a float32 binary file or 34,560,632 Bytes for a SAC file.\n\nUsers have to place continuous records in `[IOdirname]/continuous_records/`, where the file names are assumed to be `[RecordID]_[ChannelID].sac`.\n\n`[RecordID]` are the IDs of each continuous record; e.g., hourly, daily, weekly, etc. However, monthly-long data are not suitable due to their different lengths.\nNote that too long (e.g., 100 Hz yearly) data per file is not acceptable because the number of samples for each file is represented by int32, and its maximum is $2^{31}-1 = 2,147,483,647$.\nThe record IDs must be free of an underscore ( _ ) because the first underscore is considered a separator.\n`[ChannelID]` (e.g., station codes and components) must be common among continuous records and template waveforms and can include underscores; for example, the following format is possible for daily records:\n```bash\nls -1v [IOdirname]/continuous_records/\n20231231_N.KYTH_E.sac # 1st continuous record, channel 1\n20231231_N.KYTH_N.sac # 1st continuous record, channel 2\n20231231_N.KYTH_U.sac # 1st continuous record, channel 3\n20231231_N.KMMH_E.sac # 1st continuous record, channel 4\n20231231_N.KMMH_N.sac # 1st continuous record, channel 5\n20231231_N.KMMH_U.sac # 1st continuous record, channel 6\n20240101_N.KYTH_E.sac # 2nd continuous record, channel 1\n⋮\n20240101_N.KMMH_U.sac # 2nd continuous record, channel 6\n20240102_N.KYTH_E.sac # 3rd continuous record, channel 1\n⋮\n```\nwhere `YYYYMMDD`-format before the first underscores refers to the date of the continuous record, \"N.xxxH\" are Hi-net station codes, and \"E\", \"N\", and \"U\" represent EW, NS, and UD components, respectively.\nInstead, `YYYYMMDD` may be replaced by sequential numbers, and  `[ChannelID]` can be each channel number or channel CD (e.g., N.KYTH_U = 55e3, N.KMMH_E = 5625, etc.)\n\n### Template waveforms\n\nThe length of all template waveforms must be the same and may be a power of 2 given a property of FFT.\nLengths of other composite numbers (e.g., $10^n$) are possible but may slow down the calculation speed.\n\nUsers have to locate template waveforms in `[IOdirname]/templates/`, where the file names are assumed to be `[TemplateID]_[ChannelID].sac`.\n\n`[TemplateID]` is IDs of each template event and is free of underscore ( _ ) because only the first underscores are considered separators.\n`[ChannelID]` (e.g., station codes and components) must be common among continuous records and template waveforms.\nThe following format is possible for three template events in total:\n```bash\nls -1v [IOdirname]/templates/\n20231231-23583103_N.KYTH_E.sac # 1st template, channel 1\n20231231-23583103_N.KYTH_N.sac # 1st template, channel 2\n20231231-23583103_N.KYTH_U.sac # 1st template, channel 3\n20231231-23583103_N.KMMH_E.sac # 1st template, channel 4\n20231231-23583103_N.KMMH_N.sac # 1st template, channel 5\n20231231-23583103_N.KMMH_U.sac # 1st template, channel 6\n20240101-01392817_N.KYTH_E.sac # 2nd template, channel 1\n⋮\n20240101-01392817_N.KMMH_U.sac # 2st template, channel 6\n20240101-03022439_N.KYTH_E.sac # 3rd template, channel 1\n⋮\n20240101-03022439_N.KMMH_U.sac # 3rd template, channel 1\n```\nwhere `[YYYYMMDD]-[hhmmssxx]`-format before underscores refers to the date and start time of the template waveform.\nOf course, other unique IDs (e.g., `[SequencialNumber]_[ChannelID].sac`) are also possible.\n\n## Algorithm: an overview\n\n*DiallelX* *approximates* Network Cross-correlation Coefficients (NCCs) among all continuous and template waveform pairs.\nNote that even the autocorrelation of a template is not necessarily equal to 1.0 due to the approximation.\nHowever, users can control the accuracy.\n\nIn the calculation, each continuous record with length `r` is divided into `n` windows with length `w` equivalent to the template length.\nThe division of the `k`-th record is schematically as follows.:\n```\n┌----------k-th record----------┐┌--------(k+1)-th record--------┐\n1......s.....w..................r1......s..p\n└-1st window-┘                  |└-1st window-┘\n        └-2nd window-┘          |        └-2nd window-┘\n└stride┘        └-3rd window-┘  |└stride┘\n        └stride┘            ⋱\n                              └n-th window-┘\n                                 └-padding-┘\n```\nThe windows overlap with length `w-s`, where `s:=w/a` is the stride length and `a` is the specified accuracy (default value is 2).\nIf `a=w` (i.e., `s=1`), NCC values are calculated for all possible windows of length `w`, which provides the most accurate results but is time-consuming.\nThus, `a=2` or `a=4` are recommended.\nThe padding that makes the final (`n`-th) window length of `w` will automatically be extracted from the next continuous record's initial part or filled by zero if the `k`-th record is final.\n\nThe following parameters are examples for 1-day continuous and 10.24-second template waveforms sampled at 100 Hz, and `a=4`.:\n```\nr: length of record   : 8640000 samples (=100Hz × 24h)\nw: length of template :    1024 samples (=length of window)\ns: length of stride   :     256 samples (=w/a)\np: length of padding  :     768 samples (=s*(n-1)+w-r)\nn: number of windows  :   33750 (=floor((r-1)/s)+1)\n```\nwhere `n=floor((r-1)/s)+1` is derived so that `s*(n-1) + 1 ≦ r < s*n + 1` holds, where the left and right-hand side are the initial point of the `n`-th and `n+1`-th windows, respectively.\n\nAlso, [Appendix A: Visual example of a diallel](#appendix-a-visual-example-of-a-diallel) may help readers understand the meaning of diallel.\n\n## Diallel calculation\n\nTo calculate the diallel, run the following command line:\n```bash\n./DiallelX -d [IOdirname]\n```\n\nFor more accurate results, a possible option is as follows:\n```bash\n./DiallelX -a [A] -d [IOdirname]\n```\nwith `-a 4` et cetera, where a positive integer `[A]` must be a divisor of the template waveform length, and the default is `-a 2`.\nThe larger value of `[A]` raises accuracy and computation time; see [Algorithm: an overview](#algorithm-an-overview) section for details.\n\nThe standard output like below will be shown when it works:\n```bash\n./DiallelX -d [IOdirname]\nDiallelX: IO\n  IO directory       :  [IOdirname]\n  output file        :  [IOdirname]/results/candidates.csv\nDiallelX: fundamental parameters\n  number of records  :           4 records\n  length of record   :     8640000 samples\n  number of templates:        1000 templates\n  length of template :        1024 samples (= fft length)\n  number of channels :          15 channels\nDiallelX: optional parameters\n  accuracy           :           2\n  number of threads  :          24 OMP threads\nDiallelX: derived parameters\n  number of windows  :       16875 windows\n  length of stride   :         512 samples\n  length of padding  :         512 samples\n  required memory    :       1.620 Gbytes (infimum)\n  computation cost   :     1.0E+12 \n \nDiallelX: Main loop started\n  RecordID=20231224, progress=1/4, time=00:00:04.677(+4.68E+00sec.)\n  RecordID=20231225, progress=2/4, time=00:00:09.443(+4.77E+00sec.)\n  RecordID=20231226, progress=3/4, time=00:00:14.224(+4.78E+00sec.)\n  RecordID=20231227, progress=4/4, time=00:00:18.966(+4.74E+00sec.)\nDiallelX: Done\n```\nHowever, running with `-l` option:\n```bash\n./DiallelX -l -d [IOdirname]\n```\nis recommended before starting a calculation with a huge dataset to show the list of parameters without running the diallel calculation.\n\nIn the list, `required memory` is roughly estimated given the size of some large arrays.\nThrough the same environment, the total processing time is proportional to `computation cost` if the `number of templates` is sufficiently large ($\\gtrsim$ 500).\nFor example, checking processing time with one continuous record and $\\sim$ 1,000 templates in advance enables us to estimate the time with the huge dataset.\n\n\nFor other options, run\n```bash\n./DiallelX -h\n```\nto show help.\n\n## Interpretation of results\n\nThe command in the [previous section](#diallel-calculation) provides two following files:\n+ `[IOdirname]/results/candidates.csv`\n+ `[IOdirname]/results/histogram.dat`\n\n### Event candidates\n\nThe output file `candidates.csv` includes a list of similar waveforms, sorted by their NCC value in descending order.\n```bash\nhead [IOdirname]/results/candidates.csv | column -s, -t\n0.97231  4  4429856  14\n0.96088  1  1948106  12\n0.96004  1  5761983  7\n0.94624  4  6657165  16\n0.93600  3  8161597  24\n0.92957  2  1607820  23\n0.92449  1  194523   9\n0.92246  3  4860318  7\n0.91865  2  2760234  9\n0.91829  4  2838100  23\n```\nEach column represents the NCC value, the continuous record number, the initial sample number, and the most similar template waveform.\nHence, the meaning of the first line in the above list is \"*A waveform starting from the 4429856th sample in the 4th continuous record is similar to the 14th template with an NCC value of 0.97231.*\"\n\nEach line is a result from each window explained in [Algorithm: an overview](#algorithm-an-overview) section.\nHowever, each NCC value is compared to those of their previous and next window, and only the windows with relatively higher NCC values than their neighbors are in the output.\nThis comparison reduces useless results because the neighboring windows overlap and are similar to the same template.\n\nEven if a window is similar to multiple templates, the most similar one is listed as sufficient for event detection.\nRefer to the line number in `[IOdirname]/parameters/continuous_records.csv` and `[IOdirname]/parameters/templates.csv` to confirm the filename of the continuous record and template corresponding to the number in the second and forth column, respectively.\n\nEven if a window consists only of noises and is similar to nothing, at least one template number is listed.\nHowever, users can exclude such a window in the user-defined event detection process.\n\nSometimes, the waveform starting from the sample number in the third column may protrude from the record.\nFor example, the following output is possible if the record and template lengths are 8,640,000 (=100Hz $\\times$ 24hours) and 1,024, respectively:\n```bash\nhead [IOdirname]/results/candidates.csv | column -s, -t\n           ⋮\n0.74035  1  8639986  25\n           ⋮\n```\nThe record starting from `8639986` lasts until `8639986+1023=8641009`.\nThis overflow can happen because *DiallelX* adds the samples of the head of the next record to the tail of the current record.\nTherefore, the waveform similar to the 25th template consists of 8639986th to 8640000th samples of the 1st record and 1st to 1009th samples of the 2nd record.\n\n### histogram.dat\n\nThis file includes a histogram of NCC values for the diallel.\nTherefore, total number in whole bins is (# of records) $\\times$ (# of windows) $\\times$ (# of templates).\nThe first, second, and third columns are NCC values, the number in the bin, and the accumulated number from the largest (=1.0), respectively.\nUsers may define a threshold for event definition based on this histogram.\n\n## Appendix A: Visual example of a diallel\n\nHere is a visual example of a diallel (set of cross-correlation functions among the windows and templates) and output from ten windows and five templates with the length of 64 samples and `a=2` (i.e., stride length = 64/2=32).\n\nFor simplicity, we ignore the padding.\nMoreover, we consider only single-channel waveforms in this appendix; see [Appendix B: Details of algorithm](#appendix-b-details-of-algorithm) to treat network cross-correlation coefficients.\n\nThe continuous record and windows are as in the following figure:\n\n![./img/windows.svg](./img/windows.svg)\n\nThe template waveforms are the following five:\n\n![./img/templates.svg](./img/templates.svg)\n\nThen, the diallel is the following figure, where the windows and templates (black) are in the top row and right column, respectively, and \"×\" accompanied by decimals indicates the maxima of each cross-correlation function:\n\n![./img/diallel.svg](./img/diallel.svg)\n\nFor each column in the diallel, the (light)blue function has the largest CC value and is a candidate to be output.\nHowever, the neighboring windows tend to show higher CC values because they overlap.\nOnly their local peaks along the lateral direction (saturated blue) are output to avoid duplicate detection.\n\n## Appendix B: Details of algorithm\n\nAs in [Algorithm: an overview](#algorithm-an-overview) section, *DiallelX* calculates NCC values between windows cut out from the continuous records and templates.\nHere, we explain how this algorithm reduces computation time.\n\nLet $r$, $w$, and $s$ be the lengths of the continuous record, template waveform, and stride, respectively, as in the main text.\nFor each window and template, we first calculate a cross-correlation function between them and extract its maximum.\nTherein, we calculate the cross-correlation function using FFT.\n\nWe have multiple templates, and all templates will be heavily used.\nTherefore, we calculate and store spectra of all template waveforms in advance.\nAnother point is that all waveforms are observed in multiple channels, where $m$ represents the number of whole channels.\n\nFor $k=1,2,\\ldots,w$ and $\\textrm{Ch}=1,2,\\ldots, m$, we assume that $x_k^{(\\textrm{Ch})}$ and $y_k^{(\\textrm{Ch})}$ are $k$-th sample of continuous and template waveforms in $\\textrm{Ch}$-th channel, respectively.\nAfter offset elimination and $L^2$ normalization, we calculate FFT of the waveforms, denoted as $X_k^{(\\textrm{Ch})}$ and $Y_k^{(\\textrm{Ch})}$.\nThe time domain cross-correlation function in the $\\textrm{Ch}$-th channel, $CF_j^{(\\textrm{Ch})}$, is obtained as\n```math\nCF_j^{(\\textrm{Ch})} = \\frac{1}{w} \\sum_{k=1}^w X_k^{(\\textrm{Ch})} \\overline{Y_k^{(\\textrm{Ch})}} \\exp\\left(2 \\pi i \\frac{j k}{w}\\right),\n```\nwhere the overline denotes the complex conjugate, and $i$ is the imaginary unit.\nThe NCC value that ranges between $-1$ and $+1$ is obtained as follows:\n```math\nNCC = \\max_j \\frac{1}{m} \\sum_{\\textrm{Ch}=1}^{m} CF_j^{(\\textrm{Ch})}. \\tag{1}\n```\nHowever, the summation and IFFT are linear operators, which enable us to change the order of operation as:\n```math\nNCC = \\max_j \\frac{1}{m w} \\sum_{k=1}^w \\left( \\sum_{\\textrm{Ch}=1}^{m} X_k^{(\\textrm{Ch})} \\overline{Y_k^{(\\textrm{Ch})}} \\right) \\exp\\left(2 \\pi i \\frac{j k}{w}\\right), \\tag{2}\n```\nwhere the summation w.r.t. all channels $\\left( \\sum_{\\textrm{Ch}=1}^{m} \\right)$ in eq.(2) is done before IFFT.\nThanks to this formulation, we can reduce the number of required IFFT from $m$ in eq.(1) into only once in eq.(2).\n\nThe following table shows computation times for the main loop, which almost consists of calculations of eq.(2) with a certain dataset.\n\n|# of channels ( $m$ )|time for the main loop ( $T$ )|\n|-:|--:|\n| 1|2.9 seconds|\n| 3|3.5 seconds|\n| 5|4.0 seconds|\n|15|6.8 seconds|\n\nThe above relation is well approximated by the linear equation:\n```math\nT = 0.278 \\ m + 2.634,\n```\nwhich means that 2.634 seconds is required for only one IFFT for each window and template.\nIF our algorithm is along eq.(1), IFFT is required $m$ times, and the equation will become as follows:\n```math\nT = (0.278 + 2.634) m = 2.912 \\ m.\n```\nTherefore, eq.(2) gets relatively faster as the number of channels increases.\n\n## Reference\n\nTo cite this code:\n+ Hirano, S., &amp; Naoi, M. (2025), *DiallelX*: a modern fortran code for calculating network cross-correlation, *Prog. Earth Planet. Sci.*, 12:31, [https://doi.org/10.1186/s40645-025-00701-x](https://doi.org/10.1186/s40645-025-00701-x)\n",
        "createdAt": "2024-05-19T13:11:43.000Z",
        "updatedAt": "2025-09-12T08:34:39.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ShiroHirano/DiallelX/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "nshapiro67/M2_2020",
        "url": "https://github.com/nshapiro67/M2_2020",
        "description": "python scripts fo computer labs on seismology",
        "stars": 0,
        "forks": 1,
        "readme": "# M2_2020\npython scripts for computer labs on seismology\n\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nshapiro67/M2_2020/HEAD)\n\n",
        "createdAt": "2020-10-29T15:40:13.000Z",
        "updatedAt": "2020-11-03T12:59:38.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/nshapiro67/M2_2020/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Shihao-Yuan/hvsr-lite",
        "url": "https://github.com/Shihao-Yuan/hvsr-lite",
        "description": "A Python package for HVSR computation, supporting both single-station analysis and batch processing of dense nodal array data.",
        "stars": 2,
        "forks": 1,
        "readme": "# hvsr-lite\n\n**Minimal HVSR (Horizontal-to-Vertical Spectral Ratio) analysis for seismic data**\n\nA Python package for HVSR computation, supporting both single-station analysis and batch processing of dense nodal array data.\n\n**Author:** Shihao Yuan (syuan@mines.edu)\n\n> **DISCLAIMER:** This is a development build. The code may contain errors or unstable functionality. Contributions and feedback are welcome.\n\n## Features\n\n- **Minimal dependencies** - Only NumPy, SciPy, and ObsPy required (Numba optional for acceleration)\n- **Parallel processing** - Multi-core batch processing for dense arrays\n- **Quality control** - Automatic window rejection based on STA/LTA and amplitude thresholds\n\n## Installation\n\nInstall from source:\n\n```bash\ngit clone https://github.com/yourusername/hvsr-lite.git\ncd hvsr-lite\npip install -e .\n```\n\n## Quick Start\n\n```python\nfrom obspy import read\nimport numpy as np\nfrom hvsr_lite.core import compute_hvsr\nfrom hvsr_lite.utils import stream_to_dict\n\n# Load seismic data (3 components: N, E, Z)\nst = read('path/to/data/*.mseed')\n\n# Option 1: Use the stream_to_dict utility\ndata = stream_to_dict(st)\nhorizontal_data = np.column_stack([data['north'], data['east']])\nvertical_data = data['vertical']\nsampling_rate = data['sampling_rate']\n\n# Option 2: Extract components manually\n# north = st.select(component='N')[0].data\n# east = st.select(component='E')[0].data\n# vertical = st.select(component='Z')[0].data\n# sampling_rate = st[0].stats.sampling_rate\n# horizontal_data = np.column_stack([north, east])\n\n# Compute HVSR\nresult = compute_hvsr(\n    horizontal_data=horizontal_data,\n    vertical_data=vertical_data,\n    sampling_rate=sampling_rate,\n    window_length=40.0,  # seconds\n    overlap=0.5,  # 50% overlap\n    smoothing_method='custom_ko',  # improved Konno-Ohmachi\n    ko_bandwidth=40.0,  # smoothing bandwidth\n    min_frequency=0.1,  # Hz\n    max_frequency=20.0  # Hz\n)\n\n# Access results\nfrequencies = result.frequencies\nhvsr_curve = result.hvsr_values\nprint(f\"Peak frequency: {frequencies[hvsr_curve.argmax()]:.2f} Hz\")\n```\n\n### Parallel Processing Functions\n\n**`compute_hvsr_batch(stations_data, n_workers=None, use_threading=True, **kwargs)`** - Batch processing\n\nProcess multiple stations in parallel using ThreadPoolExecutor or ProcessPoolExecutor.\n\n```python\nstations_data = [\n    {'station_id': 'A001', 'horizontal_data': h1, 'vertical_data': v1, 'sampling_rate': 250},\n    {'station_id': 'A002', 'horizontal_data': h2, 'vertical_data': v2, 'sampling_rate': 250},\n]\nresults = compute_hvsr_batch(stations_data, n_workers=4)\n# Returns: List[Tuple[str, HVSRResult]]\n```\n\n**`compute_hvsr_array(array_data, station_ids, n_workers=None, **kwargs)`** - Array processing\n\nProcess dense nodal arrays with standardized data format.\n\n```python\narray_data = {\n    'horizontal_data': {'A001': h1, 'A002': h2},\n    'vertical_data': {'A001': v1, 'A002': v2},\n    'sampling_rate': 250\n}\nresults = compute_hvsr_array(array_data, ['A001', 'A002'], n_workers=4)\n# Returns: Dict[str, HVSRResult]\n```\n\n### `compute_hvsr()`\n\nMain function for HVSR computation with extensive parameter control:\n\n**Parameters:**\n- `horizontal_data` - North and East component data (2D array or 1D)\n- `vertical_data` - Vertical component data (1D array)\n- `sampling_rate` - Sampling rate in Hz\n- `window_length` - Time window length in seconds (default: 60.0)\n- `overlap` - Window overlap fraction (default: 0.66)\n- `smoothing_method` - Smoothing method: 'custom_ko', 'custom_ko_smooth', 'moving_average', 'konno_ohmachi'\n- `ko_bandwidth` - Konno-Ohmachi bandwidth parameter (default: 40.0)\n- `min_frequency` - Minimum frequency in Hz (default: 0.1)\n- `max_frequency` - Maximum frequency in Hz (default: None)\n- And many more QC and processing options...\n\n**Returns:**\n- `HVSRResult` object with:\n  - `frequencies` - Frequency array\n  - `hvsr_values` - Smoothed HVSR curve (recommended for inversion)\n  - `hvsr_mean` - Mean HVSR across windows\n  - `hvsr_std` - Standard deviation\n  - `window_hvsr` - Individual window HVSR curves\n  - `metadata` - Processing metadata\n\n",
        "createdAt": "2025-09-15T01:42:58.000Z",
        "updatedAt": "2025-10-21T12:05:55.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Shihao-Yuan/hvsr-lite/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "zhitengyu/seismology.github.io",
        "url": "https://github.com/zhitengyu/seismology.github.io",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2017-01-01T03:59:06.000Z",
        "updatedAt": "2017-01-01T05:50:45.000Z",
        "language": "HTML",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Tdelaselle/ObsPy-AE",
        "url": "https://github.com/Tdelaselle/ObsPy-AE",
        "description": "Preprocessing Acoustic Emission recordings for signal processing through ObsPy (Python framework for processing seismological data).",
        "stars": 1,
        "forks": 0,
        "readme": "\n# Welcome to the __ObsPy Acoustic Emission__ repository\n\n<div>\nScripts for Converting and processing Acoustic Emission continuous recordings to Obspy stream for signal processing, plotting and facilitate rapid application development.\n\n<div>\n  \n\n\n## About\n\nThis script processes acoustic emission data stored in _txt_ or _tdms_ files and converts it into an ObsPy stream format, saved in mseed format or serialized with pickle. It uses command-line arguments to specify various parameters including data file paths, export formats, and sensor configurations.\n    \nFrom ObsPy documentation: \n    \n    \"ObsPy is an open-source project dedicated to provide a Python framework for processing seismological data. \n    It provides [...] seismological signal processing routines which allow the manipulation of seismological time series \n    (see [Beyreuther2010], [Megies2011], [Krischer2015]). \n    The goal of the ObsPy project is to facilitate rapid application development for seismology.\"\n\nAs Acoustic emission datastreaming are analogous to seismograms and as there is no open access tools to recompose AE streaming, read, pre-processed (clean, filter, downsample, ...), plot and analyse it with signal processing classical methods, we develop this script to run  Obspy procedures and performed easily and rapidly these operations.    \n\nThis package was written and documented by [Théotime de la Selle](https://github.com/ThéotimedeLaSelle). Any contributions are very welcomed.\n\n\nThis work was supported by the __ANR project e-Warnings__ (ANR-19-CE42-001).\n\n> __Copyright ©️ 2024 Théotime de la Selle__\n>\n> This program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n> This program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n> You should have received a copy of the GNU General Public License\nalong with this program. If not, see <https://www.gnu.org/licenses/>.\n\n## Installation\n\nApart from depandencies (see scripts README), no installation is needed.\n\n## Content description\n\nThis package provide 2 similar and independant scripts (supported with a respective README.m) dedicated to a different format of acoustic emission data : \n\n- AE_ASCII_Obspy.py for reading, stacking and converting _.txt_ files (see AE_ASCII_Obspy_README.m)\n- AE_TDMS_Obspy.py for reading, stacking and converting _.tdms_ files (see AE_TDMS_Obspy_README.m)\n\nWe also provide a 2-channels AE datastreaming in ASCII format splitted in 3 _.txt_ files for testing.\n",
        "createdAt": "2024-10-24T08:52:15.000Z",
        "updatedAt": "2025-11-25T10:37:38.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Tdelaselle/ObsPy-AE/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "wangliang1989/sacio_Fortran",
        "url": "https://github.com/wangliang1989/sacio_Fortran",
        "description": "provides a Fortran 90 module named `sacio` for reading and writting evenly-spaced SAC binary format files",
        "stars": 7,
        "forks": 2,
        "readme": "# sacio_Fortran\n\nThis project provides a Fortran 90 module named `sacio` for reading and writting evenly-spaced SAC binary format files.\n\nSAC I/O subroutines in the module:\n\n1. `sacio_readhead`: Read SAC binary header only\n2. `sacio_readsac`: Read SAC binary file\n3. `sacio_writesac`: Write SAC binary file\n4. `sacio_readsac_cut`: Read SAC binary file with cut option\n5. `sacio_nullhead`: Change a SAC header to undefined\n6. `sacio_newhead`: Create a ready-to-use SAC header for evenly-spaced SAC data\n\nTo contact me: wangliang0222@foxmail.com\n\n## How To Get\n\n### Method 1\n\nClick \"Download ZIP\" button on the webpage.\n\n### Method 2\n\nDownload it with the address directly:\nhttps://codeload.github.com/wangliang1989/sacio_Fortran/zip/master\n\n### Method 3\n\nUse git:\n\n~~~bash\n$ git clone https://github.com/wangliang1989/sacio_Fortran.git\n~~~\n\n## FileList\n\n1. `sacio.f90`: source code of `sacio` module (THIS IS WHAT YOU REALLY NEED!)\n2. examples: `test/test_sacio_*.f90` show the usage of subroutines provided by `sacio` module\n3. `Makefile`: makefile showing how to compile and link\n4. `README.md`: this file\n5. `.log.md`: the log file\n6. example data: `test/testin.sac`: SAC file in binary format\n\n## How to use\n\n1. **Read examples for more details.**\n\n2. Compile & Link\n\n   ~~~bash\n   $ gfortran -c sacio.f90\n   $ gfortran -c your_program.f90\n   $ gfortran your_program.o sacio.o -o your_program\n   ~~~\n\n## Revision History\n\n- 2015-09-11: v1.0\n\n## License\n\nCopyright  2015 Liang Wang @ Guilin University of Technology, Dongdong Tian @ USTC\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.!\n",
        "createdAt": "2015-09-11T03:17:42.000Z",
        "updatedAt": "2024-01-06T14:19:28.000Z",
        "language": "Fortran",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/wangliang1989/sacio_Fortran/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Hy-X/Seismology_Plotting",
        "url": "https://github.com/Hy-X/Seismology_Plotting",
        "description": "Here is the script that I used to make plots along with some of my tutorials ",
        "stars": 0,
        "forks": 0,
        "readme": "# Seismology_Plotting\nHere is the script that I used to make plots along with some of my tutorials \n",
        "createdAt": "2025-02-05T19:42:52.000Z",
        "updatedAt": "2025-02-05T19:43:58.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Hy-X/Seismology_Plotting/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "sapienzaapps/galileo-terremoti",
        "url": "https://github.com/sapienzaapps/galileo-terremoti",
        "description": "SeismoCloud is a warning system for earthquakes (Earthquake Early Warning -- EEW) based on a low cost sismometers' network.",
        "stars": 13,
        "forks": 5,
        "readme": "[![Build Status](https://travis-ci.org/sapienzaapps/galileo-terremoti.svg?branch=master)](https://travis-ci.org/sapienzaapps/galileo-terremoti)\n\n**Intel Galileo support is dropped**, so please choose **Raspberry PI** or **NodeMCU**.\n\n# NodeMCU?\n\nTo use NodeMCU boards you must use sketch at https://github.com/sapienzaapps/seismoclouddevice-nodemcu . The code here is for Raspberry PI only.\n\n# License\n\nSee LICENSE file\n\n# Requirements\n\n* GNU/Linux or OSX (others \\*BSD are not supported but it should works)\n* GNU make\n* GCC compiler (if you plan to compile for `linux-x86`)\n* Raspberry PI cross-compilation toolchain (if you plan to compile for `raspi`)\n\n## Network requirements\n\nIf you have any firewall in your network, please allow these ports to Internet (outbound):\n\n* TCP: 80, 443, 1883\n* UDP: 123\n\n# Hardware\n## Hardware needed\n\n* 3 LEDs (Red, Green, Blue) with 3 resistor\n* 1 Accelerometer (ADXL345)\n* Raspberry PI\n* Donuts cables\n* Ethernet wired connection\n\n**Important**: if `i2c` bus is not enabled, please refer to `Platform specific` chapter of this README to enable i2c on Raspberry PI.\n\n## LEDs\n\nLEDs can be in these different states:\n\n* **Green**: device is ready\n* **Green + Yellow**: device is ready but there is an issue connecting to SeismoCloud APIs\n* **Green (still) + Yellow (blinking)**: device is calibrating\n* **Green + Red (only for about 5 seconds)**: shake detected\n* **Red ONLY** still: reboot/upgrade in progress\n* **Green + Yellow + Red - ALL rotating**: software is loading\n* **Green + Yellow + Red - ALL blinking fast**: software is loaded, starting accelerometer\n\n### LED pins on Raspberry PI\n\n* GPIO-17 (wiringpi addr #0) : Green\n* GPIO-18 (wiringpi addr #1) : Yellow\n* GPIO-21 (wiringpi addr #2) : Red\n\n## Link ADXL345 Accelerometer to Raspberry PI\n\n* 3.3v : 3.3v\n* GND : GND\n* SDA : SDA\n* SCL : SCL\n\nRefer to Raspberry PI pinout, as https://jeffskinnerbox.files.wordpress.com/2012/11/raspberry-pi-rev-1-gpio-pin-out1.jpg\n\n# Setup your build environment\n## Compile on Raspberry Pi (Raspbian Jessie)\n\nMake sure that you have these (debian) packages: `build-essential git wiringpi` and you're good to go.\n\n**Note**: you need to remove `libi2c-dev` if you have it because there are compatibility issues with some headers (you need to use i2c headers that are bundled with Linux kernel headers/source).\n\n# How to build from command line\n\nYou should issue `make` command into project root directory. Make targets availables are: `all`, `clean` and `upload` (see below).\n\nYou may use these options to compile a particular version:\n\n* **PLATFORM** : can be `linux-x86`, `mac-osx`, `raspi` or `galileo` depending on your target system\n* **VARIANT** : platform variant; `galileo` has two \"variants\":\n\t* **galileo_fab_d** : Galileo Gen 1 (default)\n\t* **galileo_fab_g** : Galileo Gen 2\n* **SDK_ROOT** : SDK/Toolchain root path (see \"Toolchains\") if it's not in default paths (`/opt` for GNU/Linux, /Applications for OS X)\n* **DEBUG** : if nonempty, enables debug options (i.e. debug messages and commands, crash reports)\n* **DEBUG_SERVER** : if nonempty, device will use testing APIs\n* **NOWATCHDOG** : if nonempty, watchdog will be disabled\n* **REMOTEHOST** : if nonempty, enables `upload` target (Galileo only)\n\n# How to add a new platform\n\nProject code is written as much generic as possible. Platform-specific code is placed into `vendor/` directory.\nSpecific code includes: LED control, 3-axis sensor code, etc.\n\nYou should implement these classes as a .cpp file into platform specific directory (example: `vendor/linux-x86/LED.cpp`):\n* `LED class`\n* `generic.cpp`\n\nYou'll find definitions into `.h` files in project root directory.\nAlso you may need to create a child class of `Accelerometer.h` and a `vendor_specific.h` into vendor directory.\n\nSee `linux-x86`, `mac-osx` and `galileo` for more infos.\n\n# Platform specific informations\n\n## Linux\n\nIn order to test latency you need to run `sketch.elf` as root OR grant `CAP_NET_RAW` capability with a command like:\n\n    $ sudo setcap cap_net_raw=ep build/out_linux-x86/sketch.elf\n\n## I2C bus not available on latest Raspbian releases\n\nPlease refer to this post: https://www.raspberrypi.org/forums/viewtopic.php?f=28&t=97314\n\nIn short, you should run `raspi-config` and enable `i2c` under `Advanced Options` menu (and then reboot).\n",
        "createdAt": "2015-10-29T08:57:54.000Z",
        "updatedAt": "2025-04-20T04:31:03.000Z",
        "language": "C++",
        "homepage": "http://www.seismocloud.com",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/sapienzaapps/galileo-terremoti/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "kura-okubo/ParseMiniseed.jl",
        "url": "https://github.com/kura-okubo/ParseMiniseed.jl",
        "description": "Parse miniseed format for processing seismological data.",
        "stars": 0,
        "forks": 0,
        "readme": "# ParseMiniseed\nParse miniseed format for processing seismic data.\n\n## About SeisRequests\nThis package enable to use __parsemseed()__ in [jpjones76/SeisIO.jl](https://github.com/jpjones76/SeisIO.jl).\n\n## Installing\n\nAdd ParseMiniseed like so:\n\n```julia\n(v1.0) pkg> add \"https://github.com/kura-okubo/ParseMiniseed.jl\"\n```\n\n## Example\n\n```\nusing Dates\nusing SeisRequests, ParseMiniseed\n\nstarttime = DateTime(2009, 12, 1, 0, 0, 0)\nendtime = DateTime(2009, 12, 2, 0, 0, 0)\n\nreq_NCEDC = FDSNDataSelect(network=\"BP\", station=\"GHIB\", channel=\"BP1\", starttime=starttime, endtime=endtime, format=\"miniseed\")\ndata_NCEDC = get_request(req_NCEDC; server=\"NCEDC\", verbose=false)\n\nseis_NCEDC = ParseMiniseed.SeisData()\nseis_NCEDC += parsemseed(IOBuffer(data_NCEDC.body), false, 0)[1]\n```\n",
        "createdAt": "2019-02-19T00:40:03.000Z",
        "updatedAt": "2019-02-19T01:28:09.000Z",
        "language": "Julia",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/kura-okubo/ParseMiniseed.jl/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "cherscarlett/mt-rainier-seismicity",
        "url": "https://github.com/cherscarlett/mt-rainier-seismicity",
        "description": "Final Project for Miami University Seismology Skill Building Workshop (2024)",
        "stars": 1,
        "forks": 0,
        "readme": "# Can we predict the eruption of Mount Rainier?\nFinal Project for Earthscope Consortium's Seismology Skill Building Workshop (2024)\n\nDownload the notebook directly or [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/cherscarlett/mt-rainier-seismicity/main?labpath=mt-rainier-seismicity.ipynb)\n",
        "createdAt": "2024-08-21T15:32:33.000Z",
        "updatedAt": "2024-08-23T09:54:08.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/cherscarlett/mt-rainier-seismicity/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "SeisBlue/SeisBlue",
        "url": "https://github.com/SeisBlue/SeisBlue",
        "description": "Deep learning seismic phase picking framework with SEISAN ",
        "stars": 72,
        "forks": 14,
        "readme": "# SeisBlue\n\nA deep-learning data processing platform for seismology\n\n---\n# Warning \n\nThis project is turned into internal development. The code is not maintained and updated.\n\nPlease contact the author or [SGYLAB](https://sgylab.gl.ntu.edu.tw/) for more information.\n\n---\n\n![workflow](workflow.png)\n\n![example](example.png)\n\n---\n\n# Related Publications\n\n- Huang, C.-M., Chang, L.-H., Kuo-Chen, H., and Zhuang, Y.: SeisBlue: a deep-learning data processing platform for seismology, EGU General Assembly 2023, Vienna, Austria, 24–28 Apr 2023, EGU23-13927, https://doi.org/10.5194/egusphere-egu23-13927, 2023.\n- Sun, WF., Pan, SY., Huang, CM. et al. Deep learning-based earthquake catalog reveals the seismogenic structures of the 2022 MW 6.9 Chihshang earthquake sequence. Terr Atmos Ocean Sci 35, 5 (2024). https://doi.org/10.1007/s44195-024-00063-9\n- Kuo-Chen, H., Sun, W., Huang, C., Pan, S., 2022, Near real-time seismic data processing helps scientist understand aftershocks, Temblor, http://doi.org/10.32858/temblor.276\n\n---\n\nReference:\n \n [EQTansfomer](https://www.nature.com/articles/s41467-020-17591-w) | [Github](https://github.com/smousavi05/EQTransformer)\n\n Mousavi, S. M., Ellsworth, W. L., Zhu, W., Chuang, L. Y., & Beroza, G. C. (2020). Earthquake transformer—an attentive deep-learning model for simultaneous earthquake detection and phase picking. Nature communications, 11(1), 1-12.\n\n [PhaseNet](https://doi.org/10.1093/gji/ggy423) | [Github](https://github.com/wayneweiqiang/PhaseNet)\n\n Zhu, W., & Beroza, G. C. (2018). PhaseNet: A Deep-Neural-Network-Based Seismic Arrival Time Picking Method. arXiv preprint arXiv:1803.03211.\n\n [U-net](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/)\n\n Ronneberger, O., Fischer, P., & Brox, T. (2015, October). U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention (pp. 234-241). Springer, Cham.\n\n [U-net ++](https://doi.org/10.1007/978-3-030-00889-5_1) | [Github](https://github.com/MrGiovanni/UNetPlusPlus)\n\n Zhou, Z., Siddiquee, M. M. R., Tajbakhsh, N., & Liang, J. (2018). Unet++: A nested u-net architecture for medical image segmentation. In Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (pp. 3-11). Springer, Cham.\n\n [PhasePApy](https://doi.org/10.1785/0220160019) | [GitHub](https://github.com/austinholland/PhasePApy)\n\n Chen, C., & Holland, A. A. (2016). PhasePApy: A robust pure Python package for automatic identification of seismic phases. Seismological Research Letters, 87(6), 1384-1396.\n\n [PyAPA](https://doi.org/10.3319/TAO.2018.12.23.01) | [GitHub](https://github.com/SeanMica/PyAPA)\n\n Chang, Y. H., Hung, S. H., & Chen, Y. L. (2019). A fast algorithm for automatic phase picker and event location: Application to the 2018 Hualien earthquake sequences. Terr. Atmos. Ocean. Sci, 30, 435-448.\n\n---\n\nPersonal Blog (Traditional Chinese only):\n\n[Jimmy Lab wordpress](https://jimmylab.wordpress.com/)",
        "createdAt": "2018-04-03T05:00:58.000Z",
        "updatedAt": "2025-12-05T01:03:21.000Z",
        "language": "Python",
        "homepage": "https://seisblue.github.io/SeisBlue/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/SeisBlue/SeisBlue/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "GenericMappingTools/pygmt",
        "url": "https://github.com/GenericMappingTools/pygmt",
        "description": "A Python interface for the Generic Mapping Tools.",
        "stars": 830,
        "forks": 233,
        "readme": "# PyGMT\n\n> A Python interface for the Generic Mapping Tools\n\n[Documentation (development version)](https://www.pygmt.org/dev) | [Contact](https://forum.generic-mapping-tools.org) | [TryOnline](https://github.com/GenericMappingTools/try-gmt)\n\n[![Latest version on PyPI](https://img.shields.io/pypi/v/pygmt)](https://pypi.org/project/pygmt)\n[![Latest version on conda-forge](https://img.shields.io/conda/v/conda-forge/pygmt)](https://anaconda.org/conda-forge/pygmt)\n[![GitHub license](https://img.shields.io/github/license/GenericMappingTools/pygmt)](https://github.com/GenericMappingTools/pygmt/blob/main/LICENSE.txt)\n[![Compatible Python versions](https://img.shields.io/python/required-version-toml?tomlFilePath=https%3A%2F%2Fraw.githubusercontent.com%2FGenericMappingTools%2Fpygmt%2Frefs%2Fheads%2Fmain%2Fpyproject.toml)](https://www.pygmt.org/dev/minversions.html)\n[![Digital Object Identifier for the Zenodo archive](https://zenodo.org/badge/DOI/10.5281/3781524.svg)](https://doi.org/10.5281/zenodo.3781524)\n[![Discourse forum](https://img.shields.io/discourse/status?label=forum&server=https%3A%2F%2Fforum.generic-mapping-tools.org)](https://forum.generic-mapping-tools.org)\n[![PyOpenSci](https://tinyurl.com/y22nb8up)](https://github.com/pyOpenSci/software-submission/issues/43)\n[![Contributor Code of Conduct](https://img.shields.io/badge/Contributor%20Covenant-v2.1%20adopted-ff69b4.svg)](https://github.com/GenericMappingTools/.github/blob/main/CODE_OF_CONDUCT.md)\n[![GitHub Actions Tests status](https://github.com/GenericMappingTools/pygmt/actions/workflows/ci_tests.yaml/badge.svg)](https://github.com/GenericMappingTools/pygmt/actions/workflows/ci_tests.yaml)\n[![GitHub Actions GMT Dev Tests status](https://github.com/GenericMappingTools/pygmt/actions/workflows/ci_tests_dev.yaml/badge.svg)](https://github.com/GenericMappingTools/pygmt/actions/workflows/ci_tests_dev.yaml)\n[![Test coverage status](https://codecov.io/gh/GenericMappingTools/pygmt/graph/badge.svg?token=78Fu4EWstx)](https://app.codecov.io/gh/GenericMappingTools/pygmt)\n[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)\n[![CodSpeed Performance Benchmarks](https://img.shields.io/endpoint?url=https://codspeed.io/badge.json)](https://codspeed.io/GenericMappingTools/pygmt)\n\n<!-- doc-index-start-after -->\n\n## Why PyGMT?\n\nA beautiful map is worth a thousand words. To truly understand how powerful PyGMT is,\nplay with it online on [Binder](https://github.com/GenericMappingTools/try-gmt)! For a\nquicker introduction, check out our [3 minute overview](https://youtu.be/4iPnITXrxVU)!\n\nAfterwards, feel free to look at our [Tutorials](https://www.pygmt.org/latest/tutorials),\nvisit the [Gallery](https://www.pygmt.org/latest/gallery), and check out some\n[external PyGMT examples](https://www.pygmt.org/latest/external_resources.html)!\n\n[![Quick Introduction to PyGMT YouTube Video](https://raw.githubusercontent.com/GenericMappingTools/pygmt/refs/heads/main/doc/_static/scipy2022-youtube-thumbnail.jpg)](https://www.youtube.com/watch?v=4iPnITXrxVU)\n\n## About\n\nPyGMT is a library for processing geospatial and geophysical data and making\npublication-quality maps and figures. It provides a Pythonic interface for the\n[Generic Mapping Tools (GMT)](https://github.com/GenericMappingTools/gmt), a command-line\nprogram widely used across the Earth, Ocean, and Planetary sciences and beyond.\n\n## Project goals\n\n- Make GMT more accessible to new users.\n- Build a Pythonic API for GMT.\n- Interface with the GMT C API directly using ctypes (no system calls).\n- Support for rich display in the Jupyter notebook.\n- Integration with the [scientific Python ecosystem](https://scientific-python.org/):\n  `numpy.ndarray` or `pandas.DataFrame` for data tables, `xarray.DataArray` for grids,\n  and `geopandas.GeoDataFrame` for geographical data.\n\n## Quickstart\n\n### Installation\n\nSimple installation using [mamba](https://mamba.readthedocs.org/):\n\n```bash\nmamba install --channel conda-forge pygmt\n```\n\nIf you use [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/index.html):\n\n```bash\nconda install --channel conda-forge pygmt\n```\n\nFor other ways to install `pygmt`, see the [full installation instructions](https://www.pygmt.org/latest/install.html).\n\n### Getting started\n\nAs a starting point, you can open a [Python interpreter](https://docs.python.org/3/tutorial/interpreter.html)\nor a [Jupyter notebook](https://docs.jupyter.org/en/latest/running.html), and try the\nfollowing example:\n\n``` python\nimport pygmt\nfig = pygmt.Figure()\nfig.coast(projection=\"N15c\", region=\"g\", frame=True, land=\"tan\", water=\"lightblue\")\nfig.text(position=\"MC\", text=\"PyGMT\", font=\"80p,Helvetica-Bold,red@75\")\nfig.show()\n```\n\nYou should see a global map with land and water masses colored in tan and lightblue,\nrespectively. On top, there should be the semi-transparent text \"PyGMT\". For more examples,\nplease have a look at the [Gallery](https://www.pygmt.org/latest/gallery/index.html) and\n[Tutorials](https://www.pygmt.org/latest/tutorials/index.html).\n\n## Contacting us\n\n- Most discussion happens [on GitHub](https://github.com/GenericMappingTools/pygmt).\n  Feel free to [open an issue](https://github.com/GenericMappingTools/pygmt/issues/new)\n  or comment on any open issue or pull request.\n- We have a [Discourse forum](https://forum.generic-mapping-tools.org/c/questions/pygmt-q-a)\n  where you can ask questions and leave comments.\n\n## Contributing\n\n### Code of conduct\n\nPlease note that this project is released with a\n[Contributor Code of Conduct](https://github.com/GenericMappingTools/.github/blob/main/CODE_OF_CONDUCT.md).\nBy participating in this project you agree to abide by its terms.\n\n### Contributing guidelines\n\nPlease read our [Contributing Guide](https://github.com/GenericMappingTools/pygmt/blob/main/CONTRIBUTING.md)\nto see how you can help and give feedback.\n\n### Imposter syndrome disclaimer\n\n**We want your help.** No, really.\n\nThere may be a little voice inside your head that is telling you that you're not ready\nto be an open source contributor; that your skills aren't nearly good enough to\ncontribute. What could you possibly offer?\n\nWe assure you that the little voice in your head is wrong.\n\n**Being a contributor doesn't just mean writing code.** Equally important contributions\ninclude: writing or proof-reading documentation, suggesting or implementing tests, or\neven giving feedback about the project (including giving feedback about the contribution\nprocess). If you're coming to the project with fresh eyes, you might see the errors and\nassumptions that seasoned contributors have glossed over. If you can write any code at\nall, you can contribute code to open source. We are constantly trying out new skills,\nmaking mistakes, and learning from those mistakes. That's how we all improve and we are\nhappy to help others learn.\n\n*This disclaimer was adapted from the* [MetPy project](https://github.com/Unidata/MetPy).\n\n## Citing PyGMT\n\nPyGMT is a community developed project. See the\n[AUTHORS.md](https://github.com/GenericMappingTools/pygmt/blob/main/AUTHORS.md) file\non GitHub for a list of the people involved and a definition of the term \"PyGMT Developers\".\nFeel free to cite our work in your research using the following BibTeX:\n\n```\n@software{\n  pygmt_2025_17156962,\n  author       = {Tian, Dongdong and\n                  Leong, Wei Ji and\n                  Fröhlich, Yvonne and\n                  Grund, Michael and\n                  Schlitzer, William and\n                  Jones, Max and\n                  Toney, Liam and\n                  Yao, Jiayuan and\n                  Tong, Jing-Hui and\n                  Magen, Yohai and\n                  Materna, Kathryn and\n                  Belem, Andre and\n                  Newton, Tyler and\n                  Anant, Abhishek and\n                  Ziebarth, Malte and\n                  Quinn, Jamie and\n                  Uieda, Leonardo and\n                  Wessel, Paul},\n  title        = {{PyGMT: A Python interface for the Generic Mapping Tools}},\n  month        = oct,\n  year         = 2025,\n  publisher    = {Zenodo},\n  version      = {0.17.0},\n  doi          = {10.5281/zenodo.17156962},\n  url          = {https://doi.org/10.5281/zenodo.17156962}\n}\n```\n\nTo cite a specific version of PyGMT, go to our Zenodo page at <https://doi.org/10.5281/zenodo.3781524>\nand use the \"Export to BibTeX\" function there. It is also strongly recommended to cite\nthe [GMT 6 paper](https://doi.org/10.1029/2019GC008515) (which PyGMT wraps around). Note\nthat some modules like `dimfilter`, `surface`, and `x2sys` also have their dedicated\ncitations. Further information for all these can be found at <https://www.generic-mapping-tools.org/cite>.\n\n## License\n\nPyGMT is free software: you can redistribute it and/or modify it under the terms of the\n**BSD 3-clause License**. A copy of this license is provided in\n[LICENSE.txt](https://github.com/GenericMappingTools/pygmt/blob/main/LICENSE.txt).\n\n## Support\n\nThe development of PyGMT has been supported by NSF grants\n[OCE-1558403](https://www.nsf.gov/awardsearch/showAward?AWD_ID=1558403) and\n[EAR-1948603](https://www.nsf.gov/awardsearch/showAward?AWD_ID=1948602).\n\n## Related projects\n\nOther official wrappers for GMT:\n\n- [GMT.jl](https://github.com/GenericMappingTools/GMT.jl): A Julia wrapper for GMT.\n- [gmtmex](https://github.com/GenericMappingTools/gmtmex): A Matlab/Octave wrapper for GMT.\n\n<!-- doc-index-end-before -->\n\n## Minimum supported versions\n\nPyGMT has adopted [SPEC 0](https://scientific-python.org/specs/spec-0000/) alongside the\nrest of the scientific Python ecosystem, and made a few extensions based on the needs of\nthe project. Please see [Minimum Supported Versions](https://www.pygmt.org/dev/minversions.html)\nfor the detailed policy and the minimum supported versions of GMT, Python and core\npackage dependencies.\n",
        "createdAt": "2017-03-17T20:31:51.000Z",
        "updatedAt": "2025-12-05T09:57:19.000Z",
        "language": "Python",
        "homepage": "https://www.pygmt.org",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.17156962",
            "openAlex": "10.5281/zenodo.17156962",
            "openCitations": "10.5281/zenodo.3781524",
            "dataCite": "10.5281/zenodo.17156962",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/GenericMappingTools/pygmt/main/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.17156962",
            "title": "PyGMT: A Python interface for the Generic Mapping Tools",
            "journal": "Zenodo",
            "dateReleased": "2025-10-03T00:00:00.000Z",
            "abstract": "",
            "citationsArray": []
        },
        "repoDoi": "10.5281/zenodo.17156962",
        "publications": [
            {
                "doi": "10.5281/zenodo.17156962",
                "name": "PyGMT: A Python interface for the Generic Mapping Tools",
                "source": "",
                "authorNames": [],
                "abstract": "",
                "publicationDate": "2025-10-03T00:00:00.000Z"
            },
            {
                "doi": "10.1038/s41597-021-01030-6",
                "name": "Introduction to a community dataset from an infrasound array experiment at Mt. Etna, Italy",
                "source": "Zenodo",
                "authorNames": [
                    "De Angelis, S.",
                    "Zuccarello, L.",
                    "Rapisarda, S.",
                    "Minio, V."
                ],
                "url": [
                    "http://ui.adsabs.harvard.edu/#abs/2021NatSD...8..247D",
                    "http://doi.org/10.1038/s41597-021-01030-6"
                ]
            },
            {
                "doi": "10.1029/2021EA001675",
                "name": "Performance Assessment of Geophysical Instrumentation Through the Automated Analysis of Power Spectral Density Estimates",
                "source": "Zenodo",
                "authorNames": [
                    "Koymans, M. R.",
                    "Domingo Ballesta, J.",
                    "Ruigrok, E.",
                    "Sleeman, R.",
                    "Trani, L.",
                    "Evers, L. G."
                ],
                "url": [
                    "http://ui.adsabs.harvard.edu/#abs/2021E&SS....801675K",
                    "http://doi.org/10.1029/2021EA001675"
                ]
            },
            {
                "doi": "10.1093/gji/ggab297",
                "name": "Gradient-boosted equivalent sources",
                "source": "Zenodo",
                "authorNames": [
                    "Soler, Santiago R",
                    "Uieda, Leonardo"
                ],
                "url": [
                    null,
                    "http://ui.adsabs.harvard.edu/#abs/2021GeoJI.227.1768S",
                    "http://doi.org/10.1093/gji/ggab297",
                    "http://ui.adsabs.harvard.edu/#abs/2021GeoJI.tmp..297S"
                ]
            },
            {
                "doi": "10.1093/gji/ggaa388",
                "name": "Shear-wave splitting beneath Fennoscandia — evidence for dipping structures and laterally varying multilayer anisotropy",
                "source": "Zenodo",
                "authorNames": [
                    "Grund, Michael",
                    "Ritter, Joachim R R"
                ],
                "url": [
                    "http://ui.adsabs.harvard.edu/#abs/2020GeoJI.223.1525G",
                    null,
                    "http://doi.org/10.1093/gji/ggaa388",
                    "http://ui.adsabs.harvard.edu/#abs/2020GeoJI.tmp..960G"
                ]
            }
        ]
    },
    {
        "source": "GitHub",
        "name": "phekdey-pheng/Recent_developments_in_earthquake_seismologypresent",
        "url": "https://github.com/phekdey-pheng/Recent_developments_in_earthquake_seismologypresent",
        "description": "Auto-generated repository for Recent_developments_in_earthquake_seismologypresent",
        "stars": 0,
        "forks": 0,
        "readme": "# Recent_developments_in_earthquake_seismologypresent\n\nThis repository was auto-generated. \n\n## Description\n\nThis is a placeholder README for the Recent_developments_in_earthquake_seismologypresent repository.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details. \n\n## Author\n\nPhekdey PHORN \n\nCreated on 2025-08-30.\n\n## Contact\n\nFor any inquiries, please contact [Phekdey PHORN](+855 89755770). Thank you for visiting my GitHub profile!\n",
        "createdAt": "2025-08-29T18:10:04.000Z",
        "updatedAt": "2025-08-29T18:10:20.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/phekdey-pheng/Recent_developments_in_earthquake_seismologypresent/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "yvonnefroehlich/SplitLab-TemporalAlignment",
        "url": "https://github.com/yvonnefroehlich/SplitLab-TemporalAlignment",
        "description": "Relative temporal alignment of seismic traces in SplitLab",
        "stars": 2,
        "forks": 0,
        "readme": "# Temporal Alignment of Seismic Traces in _SplitLab_\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5805029.svg)](https://doi.org/10.5281/zenodo.5805029)\n\nThis material addresses an error source in the code of the MATLAB based shear wave splitting software package\n_SplitLab_ ([**_Wüstefeld et al. 2008_**](https://doi.org/10.1016/j.cageo.2007.08.002)). The error causes a\nwrong temporal alignment of the single traces (Z, N, E components) of one earthquake _relative_ to each other.\nThe resulting wrong horizontal particle motion and wrong waveforms in the ray (LQT) coordinate system lead to\na wrong shear wave splitting measurement.\n\nHere, I provide a test with seismological data for your personal _SplitLab_ version and a suggested correction\nof the _SplitLab_ function `getFileAndEQseconds.m` for the publicly available _SplitLab_ versions.\n\nThis modified _SplitLab_ function is also introduced by _StackSplit_\n([**_Grund 2017_**](https://doi.org/10.1016/j.cageo.2017.04.015)) upon version [v3.0](https://doi.org/10.5281/zenodo.5802051)\nduring the installation process.\n\n\n## Citation\n\nIf you make use of this material, please cite the relating publications in which this issue is described in detail:\n\n**Peer-reviewed journal article**\n- [**_Fröhlich Y, Grund M, Ritter J R R (2022)_**](https://doi.org/10.4401/ag-8781).\n  On the effects of wrongly aligned seismogram components for shear wave splitting analysis.\n  *Annals of Geophysics*, 66(2).\n  https://doi.org/10.4401/ag-8781.\n\n**Doctoral studies**\n- [**_Fröhlich Y (2025)_**](https://doi.org/10.5445/IR/1000183786).\n  Shear wave splitting analysis of long-term data: Anisotropy studies in the Upper Rhine Graben area, Central Europe.\n  Dissertation, *Karlsruhe Institute of Technology, Geophysical Institute*.\n  https://doi.org/10.5445/IR/1000183786.\n\n**Presentation**\n- [**_Fröhlich Y, Ritter J R R (2022)_**](https://doi.org/10.5281/zenodo.14510987).\n  Shear wave splitting analysis and temporal misalignment of seismogram components.\n  *48th German Seismology Working Group meeting*, Münster.\n  https://doi.org/10.5281/zenodo.14510987.\n\n\n## Content\n\n- [`00_SAC_files`](https://github.com/yvonnefroehlich/SplitLab-TemporalAlignment/tree/main/00_SAC_files)\n  - Subfolders corresponding to some of the filename formats supported by _SplitLab_\n  - Vertical (Z), North (N), East (E) components as SAC-files\n  - Sampling interval: 0.05 s\n\n- [`01_your_results`](https://github.com/yvonnefroehlich/SplitLab-TemporalAlignment/tree/main/01_your_results)\n  - Until now empty\n  - Output folder for your own shear wave splitting measurement results\n\n- [`02_SL_diagnosticplots`](https://github.com/yvonnefroehlich/SplitLab-TemporalAlignment/tree/main/02_SL_diagnosticplots)\n  - _SplitLab_ diagnostic plots for comparison (PDF and PNG formats)\n  - Without (`*_wrong`) and with (`*_correct`) consideration of the milliseconds (msecs)\n\n- [`03_SL_getFileAndEQseconds`](https://github.com/yvonnefroehlich/SplitLab-TemporalAlignment/tree/main/03_SL_getFileAndEQseconds)\n  - Modified _SplitLab_ function `getFileAndEQseconds.m`\n  - Publicly available _SplitLab_ versions (`*_SLxyz`)\n\n\n## Test\n\n### How to do\n\n**0) Choose one of the three provided examples (see subsection \"Details on earthquakes and traces\")**\n\n  - Stuttgart (STU), 2001/06/29 (2001.180) 18:35 (UTC)\n  - Stuttgart (STU), 2009/11/14 (2009.318) 19:44 (UTC)\n  - Échery (ECH), 2018/08/28 (2018.240) 22:35 (UTC)\n\n**1) Set up _SplitLab_ project**\n\n<details><summary>Click for single steps</summary>\n<p>\n\n  - General\n    - Seismic data directory\n\t  - Go to folder `00_SAC_files`\n\t  - Select subfolder with preferred filename format\n    - Output directory\n\t  - Select folder `01_your_results`\n  - Station\n    - Stuttgart\n      - Station code: STU\n      - Network code: GE\n      - Latitude in deg North: 48.771\n      - Longitude in deg East: 9.194\n    - Échery\n      - Station code: ECH\n      - Network code: G\n      - Latitude in deg North: 48.216\n      - Longitude in deg East: 7.159\n  - Event window\n    - Moment magnitude: 6.00 to 9.75\n    - Epicentral distance in deg: 90 to 140\n    - Hypocentral depth in km: 0 to 1000\n    - Start and end date: corresponding to the date of the chosen example / earthquake\n  - Phases\n    - Earth model: IASP91\n    - Phases: (at least) SKS, SKKS, PKS\n  - Find files\n    - File search string: corresponding to the chosen filename format\n    - Offset: 0 s\n    - Tolerance: 420 s\n\n</p>\n</details>\n\n**2) Perform shear wave splitting measurement**\n\n  - Bandpass filter: 0.020 Hz (lower corner), 0.20 Hz or 0.15 Hz (upper corner)\n  - Coordinate system: LQT\n  - Phase: SKS\n\n**3) Compare your result / _SplitLab_ diagnostic plot with the provided diagnostic plots**\n\n  - Folder `02_SL_diagnosticplots`: diagnostic plots for wrong and correct relative temporal alignment\n  - Shape of the E-N particle motion: elliptic or linear?\n  - SKS phase-related signal on the transverse (T) component: yes or no?\n\n### Details on earthquakes and traces\n\n<details><summary>Click for more information</summary>\n<p>\n\n**Stuttgart (STU), 2001/06/29 (2001.180)**\n- Earthquake\n  - Date: 2001/06/29 (2001.180)\n  - Time: 18:35:51 (UTC)\n  - Moment magnitude: 6.1\n  - Source region: Southern Bolivia\n  - Hypocentral depth: 274 km\n  - Backazimuth: 246.5 deg\n  - Epicentral distance: 95.29 deg\n- Traces\n  - msecs of start times: North = 0027, East = 0927, Vertical = 0627\n  - Relative msec difference: |E-N| = |900|, i.e., |18| samples\n\n**Stuttgart (STU), 2009/11/14 (2009.318)**\n- Earthquake\n  - Date: 2009/11/14 (2009.318)\n  - Time: 19:44:29 (UTC)\n  - Moment magnitude: 6.2\n  - Source region: Jujuy province, Argentina\n  - Hypocentral depth: 220 km\n  - Backazimuth: 244.5 deg\n  - Epicentral distance: 98.15 deg\n- Traces\n  - msecs of start times: North = 0145, East = 0895, Vertical = 0945\n  - Relative msec difference: |E-N| = |750|, i.e., |15| samples\n\n**Échery (ECH), 2018/08/28 (2018.240)**\n- Earthquake\n  - Date: 2018/08/28 (2018.240)\n  - Time: 22:35:13 (UTC)\n  - Moment magnitude: 6.5\n  - Source region: Mariana Islands\n  - Hypocentral depth: 60 km\n  - Backazimuth: 40.1 deg\n  - Epicentral depth: 106.00 deg\n- Traces\n  - msecs of start times: North = 0950, East = 0000, Vertical = 0950\n  - Relative msec difference: |E-N| = |950|, i.e., |19| samples\n\n</p>\n</details>\n\n\n## _SplitLab_ function `getFileAndEQseconds.m`\n\n### _SplitLab_ versions\n\n- [`03_SL_getFileAndEQseconds`](https://github.com/yvonnefroehlich/SplitLab-TemporalAlignment/tree/main/03_SL_getFileAndEQseconds)\n  - [_SplitLab_ 1.0.5](https://splitting.gm.univ-montp2.fr) (`*_SL105`) ([**_Wüstefeld et al. 2008_**](https://doi.org/10.1016/j.cageo.2007.08.002))\n  - [_SplitLab_ 1.2.1](https://robporritt.wordpress.com/software/) (`*_SL121`) (based on _SplitLab_ 1.0.5; **_Porritt 2014_**)\n  - [_SplitLab_ 1.3.0](https://github.com/nmcreasy/SplitLab1.3.0) (`*_SL130`) (based on _SplitLab_ 1.2.1; **_Creasy 2020_**)\n  - [_SplitLab_ 1.9.0](https://github.com/IPGP/splitlab) (`*_SL190`) (based on _SplitLab_ 1.0.5; **_IPGP_**)\n\n### How to do\n\n- The filename part `xyz` or `x.y.z` indicates the _SplitLab_ version\n- Go to the folder `~/SplitLabx.y.z/Tools/` on your computer\n- Rename the existing function `getFileAndEQseconds.m`, e.g., `getFileAndEQseconds_original.m` in this folder\n- Copy and past the modified function `getFileAndEQseconds_SLxyz.m` into this folder\n- Remove the end of the filename `_SLxyz`\n- **Assign earthquake catalog and seismological data in your _SplitLab_ project**\n\n\n## Releases\n\n| Release | Zenodo DOI |\n| --- | --- |\n| dev [main branch](https://github.com/yvonnefroehlich/SplitLab-TemporalAlignment) |  |\n| [v2.0](https://github.com/yvonnefroehlich/SplitLab-TemporalAlignment/releases/tag/v2.0) | [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.13760807.svg)](https://doi.org/10.5281/zenodo.13760807) |\n| [v1.0](https://github.com/yvonnefroehlich/SplitLab-TemporalAlignment/releases/tag/v1.0) | [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5805030.svg)](https://doi.org/10.5281/zenodo.5805030) |\n\nFor details on the single releases see the [changelog](https://github.com/yvonnefroehlich/SplitLab-TemporalAlignment/blob/main/changelog.md).\n\n\n## Contributing\n\nFor bug reports, suggestions, or recommendations feel free to\n[open an issue](https://github.com/yvonnefroehlich/SplitLab-TemporalAlignment/issues)\nor [submit a pull request](https://github.com/yvonnefroehlich/SplitLab-TemporalAlignment/pulls)\ndirectly here on [GitHub](https://github.com/yvonnefroehlich/SplitLab-TemporalAlignment).\n\n\n## Related topics\n\nIn the table below, some other tools for measuring shear wave splitting (besides _SplitLab_)\nare listed. Please note, that these tools were not checked regarding the issue\nof an incorrect relative temporal alignment of the single component traces.\n\n| Software | Language | Description | Authors |\n| --- | --- | --- | --- |\n| [SplitLab&nbsp;1.0.5](https://splitting.gm.univ-montp2.fr)       | MATLAB  | Original SplitLab version          | A Wüstefeld, G Bokelmann, C Zaroli |\n| [SplitLab&nbsp;1.2.1](https://robporritt.wordpress.com/software) | MATLAB  | Update of SplitLab 1.0.5           | R W Porritt                        |\n| [SplitLab&nbsp;1.3.0](https://github.com/nmcreasy/SplitLab1.3.0) | MATLAB  | Update of SplitLab 1.2.1           | N M Creasy                         |\n| [SplitLab&nbsp;1.9.0](https://github.com/IPGP/splitlab)          | MATLAB  | Latest SplitLab version (beta)     | IPGP                               |\n| SplitRacer                                                       | MATLAB  | Semi-automated workflow            | [M C Reiss, G Rümpker](https://doi.org/10.1785/0220160191) |\n| SplitRacerAUTO                                                   | MATLAB  | Automated workflow, large datasets | [F Link, M C Reiss, G Rümpker](https://doi.org/10.1016/j.cageo.2021.104961) |\n| [SplitPy](https://github.com/paudetseis/SplitPy)                 | Python  | Based on SplitLab                  | P Audet, A Schaeffer               |\n| [SWSPy](https://github.com/TomSHudson/swspy)                     | Python  | Automated workflow                 | T Hudson, A M Walker, J Asplet     |\n| [STADIUM-Py](https://github.com/earthinversion/STADIUM-Py)       | Python  | Automated workflow                 | U Kumar, C Legendre                |\n| [Pytheas](https://github.com/ispingos/pytheas-splitting)         | Python  | Local shear wave splitting         | I Spingos, G Kaviris               |\n| [Sheba](https://github.com/jwookey/sheba)                        | Fortran |                                    | J Wookey                           |\n| [SeisSplit.jl](https://github.com/anowacki/SeisSplit.jl)         | Julia   |                                    | A Nowacki                          |\n\n\n## References\n\n- [**_Bowman J R, Ando M (1987)_**](https://doi.org/10.1111/j.1365-246X.1987.tb01367.x).\n  Shear-wave splitting in the upper-mantle wedge above the Tonga subduction zone.\n  *Geophysical Journal International*, 88(1):25-41.\n  https://doi.org/10.1111/j.1365-246X.1987.tb01367.x.\n- **_Creasy N M (2020)_**. SplitLab version 1.3.0.\n  available at https://github.com/nmcreasy/SplitLab1.3.0.\n- **_GEOFON Data Centre (1993)_**. GEOFON (GeoForschungsNetz).\n  Deutsches GeoForschungsZentrum (GFZ), Seismic Network.\n  https://doi.org/10.14470/TR560404.\n- **_GEOSCOPE (1982)_**. French Global Network of broad band seismic stations.\n  Institut de physique du globe de Paris (IPGP) & Ecole et Observatoire des Sciences de la Terre de Strasbourg (EOST).\n  https://doi.org/10.18715/GEOSCOPE.G.\n- [**_Grund M (2017)_**](https://doi.org/10.1016/j.cageo.2017.04.015).\n  StackSplit - a plugin for multi-event shear wave splitting analyses in SplitLab.\n  *Computers & Geosciences*, 105:43-50.\n  https://doi.org/10.1016/j.cageo.2017.04.015.\n- **_Porritt R W (2014)_**. SplitLab version 1.2.1.\n  available at https://robporritt.wordpress.com/software/.\n- [**_Silver P G, Chan W W (1991)_**](https://doi.org/10.1029/91JB00899).\n  Shear wave splitting and subcontinental mantle deformation.\n  *Journal of Geophysical Research*, 96(B10):16429-16454.\n  https://doi.org/10.1029/91JB00899.\n- [**_Walsh E, Arnold R, Savage M K (2013)_**](https://doi.org/10.1002/jgrb.50386).\n  Silver and Chan revisited.\n  *Journal of Geophysical Research: Solid Earth*, 118(10):5500-5515.\n  https://doi.org/10.1002/jgrb.50386.\n- [**_Wüstefeld A, Bokelmann G, Zaroli C, Barruol G (2008)_**](https://doi.org/10.1016/j.cageo.2007.08.002).\n  SplitLab: A shear-wave splitting environment in Matlab.\n  *Computers & Geosciences*, 34(5):515-528.\n  https://doi.org/10.1016/j.cageo.2007.08.002.\n\n\n## Funding\n\nThe presented research and YF received support from:\n\n- [Graduate Funding from the German States](https://www.khys.kit.edu/english/graduate_funding.php) (scholarship)\n",
        "createdAt": "2021-11-14T14:42:19.000Z",
        "updatedAt": "2025-10-19T18:59:41.000Z",
        "language": "MATLAB",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.5805029",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.5805029",
            "dataCite": "10.5281/zenodo.5805029",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/yvonnefroehlich/SplitLab-TemporalAlignment/main/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.5805029",
            "title": "Temporal Alignment of Seismic Traces in SplitLab",
            "journal": "Zenodo",
            "dateReleased": "2024-09-18T00:00:00.000Z",
            "abstract": "Correct relative temporal alignment of seismic component traces in SplitLab (Wüstefeld et al. 2008) for the versions 1.0.5 (Wüstefeld et al. 2008), 1.2.1 (Porritt 2014), 1.3.0 (Creasy 2020), and 1.9.0 (IPGP).\n\nThis bug fix is inculded in StackSplit (Grund 2017) up on release v3.0.",
            "citationsArray": [
                "10.4401/ag-8781"
            ]
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ytamama/GuyotSeismology",
        "url": "https://github.com/ytamama/GuyotSeismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# GuyotSeismology\n\nPrograms to analyse seismological data, such as those of campus construction, at Guyot Hall, Princeton University. Seismic data were collected using a Nanometrics Meridian Compact PH 120 seismometer. Weather data were collected using a Vaisala WXT520 weather station, also stationed at Guyot Hall. This weather station is used in conjunction with a Septentrio PolaRx5 receiver. \n\nI wish to thank the IRIS Seismology Skill Building Workshop in Summer 2020, of the IRIS Education and \nPublic Outreach Program, for their helpful tutorials on scientific computing. \nIn addition, the facilities of IRIS Data Services, and specifically the IRIS Data Management Center, were used for access to earthquake catalog information, computing the distances and travel times from those earthquakes to Guyot Hall, and accessing software including Seismic Analysis Code (SAC) and evalresp. \nUnder Cooperative Support Argument EAR-1851048, the Seismological Facilities for the Advancement of Geosciences Award supports the services and facilities provided by IRIS.\n\nFor more on SAC, see Helffrich et al., (2013), The Seismic Analysis Code: a Primer and User's Guide; and the SAC manual, provided by IRIS (https://ds.iris.edu/files/sac-manual/manual.html)\n\n",
        "createdAt": "2020-03-17T14:16:47.000Z",
        "updatedAt": "2021-01-04T14:30:15.000Z",
        "language": "MATLAB",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ytamama/GuyotSeismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "dhilowitz/MSEED2Wav",
        "url": "https://github.com/dhilowitz/MSEED2Wav",
        "description": "A quick-and-dirty JUCE console app that takes seismological data in MSEED format and converts it into a WAV file.",
        "stars": 16,
        "forks": 3,
        "readme": "# MSEED2Wav\n\nA quick-and-dirty JUCE console app that takes seismological data in MSEED format and converts it into a WAV file.\n\n## Usage\n```\nMSEED2Wav <mseed-file-name>\n```",
        "createdAt": "2024-04-15T14:08:53.000Z",
        "updatedAt": "2025-10-06T09:26:06.000Z",
        "language": "C++",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/dhilowitz/MSEED2Wav/main/readme.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "emiliahartmann/computational_seismology",
        "url": "https://github.com/emiliahartmann/computational_seismology",
        "description": "A repository to put geophysical related codes and testing initial codes for own learning.",
        "stars": 0,
        "forks": 0,
        "readme": "\n# Computational Seismology\n\nThis repository contains codes related to the course \"Computers, waves, simulations\" of professor Heiner Igel.\n\n## My-Jupyter-Notebooks\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/emiliahartmann/computational_seismology/master)\n\n\n\n\n\n\n## Main Reference to the content of the codes\n - [Computers, waves, simulations course at coursera](https://www.coursera.org/learn/computers-waves-simulations)\n\n\n## Reference to write the readm.md file\n\n - [Awesome Readme Templates](https://awesomeopensource.com/project/elangosundar/awesome-README-templates)\n - [Awesome README](https://github.com/matiassingers/awesome-readme)\n - [How to write a Good readme](https://bulldogjob.com/news/449-how-to-write-a-good-readme-for-your-github-project)\n\n",
        "createdAt": "2022-07-19T12:44:34.000Z",
        "updatedAt": "2022-07-19T13:56:09.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/emiliahartmann/computational_seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "seismomomo/madpy",
        "url": "https://github.com/seismomomo/madpy",
        "description": "Measure Amplitude and Duration in Python",
        "stars": 5,
        "forks": 1,
        "readme": "# MADPy\n### <ins>M</ins>easure <ins>A</ins>mplitude and <ins>D</ins>uration in <ins>Py</ins>thon \n#### For seismic time series analysis\n<br>\n\n## Overview\n_____\n\nMADPy is a Python package that measures the amplitude and duration of a seismogram. The amplitudes and durations can be used in many seismic applications, such as magnitude calculation and seismic source discrimination.\n\nMADPy relies heavily on <a href=https://github.com/obspy/obspy>Obspy</a>. The tool reads in Obspy Stream objects for measurement. Each Trace within the Stream must include the origin time, P- arrival, and S- arrival. Additionally, the Trace data must be pre-processed and ready for measurement. This tool does not include any post-processing. A brief tutorial for MADPy can be found [here](tutorial/madpy.ipynb).<br><br>\n\n<div>\n    <figure style=text-align:left>\n        <img src=tutorial/amp-WY.YNR.01.HHE.png width=550><br>\n        <figcaption>Example amplitude measurement</figcaption>\n    </figure>\n</div>\n<br><br>\n<div>\n    <figure style=text-align:left>\n        <img src=tutorial/dur-log-WY.YNR.01.HHZ.png width=550><br>\n        <figcaption>Example duration measurement</figcaption>\n    </figure>\n</div>\n<br>\n\n## Getting started\n_____\n\nMADPy relies heavily on the editing of the [configuration file](madpy/config.py). Users who do not expect to edit the config file often should install the package using pip. Users who will edit the config file, or who are unsure, should use MADPy as a module in their working directory.\n\nThe current version of MADPy is: 0.1.0\n<br><br>\n\n#### <ins>Pip install</ins>\n\nTo install MADPy using pip, type the following command in the desired Python environment.\n\n```pip install madpy-seis```\n\nIf using anaconda, the source code will likely be located in\n\n```[conda path]/envs/[environment]/lib/python3.9/site-packages/madpy```\n\n_Note: Users should exercise caution if editing modules other than config.py_\n<br><br>\n\n#### <ins>Modular use</ins>\n\nTo use MADPy as a collection of modules, navigate to your working directory.\n\n```cd [path]/madpy```\n\nThen type one of the following into the command line:\n\n```pip download --no-deps madpy-seis```\n\n\n```git clone https://github.com/seismomomo/madpy```\n<br><br>\n\n## Noise Measurement\n_____\n\nThe noise level is the root mean square of the data and is calculated for both amplitude and duration measurements. The procedure is outlined below. Parameters from the [config file](madpy/config.py) are __bolded__ for clarity.\n1. <ins>Trim noise window</ins> – The waveform is trimmed to the user-specified noise window. The __noise_window_begin__ and __noise_window_end__ parameters define the noise window, and are relative to the arrival specified in __noise_phase__. \n\n2. <ins>Measure noise</ins> – The noise level is the RMS of the data within the trimmed window. It is used differently for the different measurement types.\n    - Amplitude measurements: The noise level can be used as a quality constraint for the amplitude measurement by calculating the signal-to-noise ratio.\n    - Duration measurements: The noise level is used to determine the fitting window for the best fit line. It can be used as a threshold for the duration value (see Duration Measurement details below).\n<br><br>\n\n## Amplitude measurement\n_____\n\nThe amplitude is defined as a user-specified factor of the maximum peak-to-peak amplitude of the seismogram. The procedure is outlined below. Parameters from the [config file](madpy/config.py) are __bolded__ for clarity. \n1. <ins>Trim signal window</ins> – The waveform is trimmed to the user-specified signal window. The __signal_window_begin__ and __signal_window_end__ parameters define the signal window, and are relative to the arrival specified in __signal_phase__. _Note: The amplitude is the maximum peak-to-peak amplitude within the signal window. This does not guarantee that it is the maximum peak-to-peak amplitude of the full waveform. Choose the signal window carefully._\n   \n2. <ins>Measure amplitude</ins> – The peak-to-peak amplitude is measured by using differentials to isolate the inflection points. The maximum difference between these inflection points is the peak-to-peak amplitude. The __amp_factor__ parameter controls which amplitude gets reported. For example, __amp_factor__=0.5 indicates that the reported value is half the maximum peak-to-peak amplitude.\n\n3. <ins>Amplitude output</ins> – The amplitude information for the Stream is returned as a pandas Dataframe. Users have the option to save this output to file by setting __save_output__ to True. The file name is \"amp-output.csv\" and is saved in the path specified in __output_path__.\n   \n\n3. <ins>Plot</ins> – If __plot__ is set to True, the module will generate the waveform with the maximum amplitude marked. The time axis is relative to PLOT_PHASE, a global parameter set in [amp.py](madpy/plotting/amp.py). The plotting parameters are available in [params.py](madpy/plotting/params.py). The plot must be generated if the user wishes to save the figure (__save_figure__=True). The figure is saved in __figure_path__ and is named \"amp-[trace id].png.\"\n<br><br>\n\n## Duration measurement\n_____\n\nThe duration is defined as the time from the P- arrival until the seismic energy reaches a user-specified energy threshold. The procedure is outlined below. Parameters from the [config file](madpy/config.py) are __bolded__ for clarity.\n    \n1. <ins>Apply smoothing</ins> – A moving average is applied to the data before duration measurement. __moving_average_window__ defines the averaging in seconds. For example, __moving_average_window__=2 means a 2-second moving average will be applied to the data. This parameter should be set to 0 if smoothing is not desired.\n\n2. <ins>Convert to envelope</ins> – The coda envelope is calculated by taking the log10 of the real part of the Hilbert transform. Bad values are set to NaN at this stage.\n\n3. <ins>Determine fitting window</ins> – The duration is measured using a line that is fit to the envelope data within the fitting window. Conventionally, the fitting window starts at the maximum value of the envelope within __signal_window_begin__ and __signal_window_end__ (with respect to __signal_phase__). The fitting window ends at __end_fit_noise__, which is a factor of the noise. If __end_fit_noise__=2, then the fitting window ends at twice the noise level. For this conventional fitting scheme, __start_fit_max__=1. To start the fitting window elsewhere, relative to the envelope maximum, __start_fit_max__ describes the inverse of the location. For example, __start_fit_max__=4 starts the fitting window 25% of the way between the envelope maximum and __end_fit_noise__. This is useful for envelopes that are curved in log space.\n\n4. <ins>Fit line to coda</ins> – The duration measurement necessitates fitting a straight line to the coda envelope. This is done using an L2 norm following Gm=d, where m includes the slope and intercept of the best fit line. The constraints force the slope to be negative since that is sensible for coda decay. There is no constraint on the intercept. _Note: The threadpoolctl library is invoked here to prevent Numpy parallelization in the background_.\n\n5. <ins>Measure duration</ins> – The duration occurs where the best fit line from step 4 intercepts a pre-defined ground motion threshold. There are two options for defining this threshold. \n    - __threshold_type__='absolute': The line must cross a static threshold that is specified in __duration_absolute_threshold__. For example, if __duration_absolute_threshold__=-7.7, then the duration is the time between the P- arrival and where the best fit line intersects with -7.7. Be sure to specify this parameter in log space.\n    - __threshold_type__='noise': The line must cross a factor of the noise level that is specfied in __duration_noise_threshold__. For example, if __duration_noise_threshold__=1, then the duration is the time between the P- arrival and where the best fit line intersects with the noise level.<br>\n    \n    _Note: Oftentimes the best fit line has to be extrapolated to reach the duration threshold. Sometimes, this intersection will occur beyond the waveform segment it is provided. The duration module will raise a Warning if this occurs_.\n    \n6. <ins>Calculate correlation coefficient</ins> – The Pearson correlation coefficient (CC) between the best fit line and the data is calculated to provide a measure of quality control. The resulting CC value should be negative, since the relationship between time and ground motion is inversely proportional.\n\n7. <ins>Duration output</ins> – The duration information for the Stream is returned as a pandas Dataframe. Users have the option to save this output to file by setting __save_output__ to True. The file name is \"dur-output.csv\" and is saved in the path specified in __output_path__.\n\n8. <ins>Plot</ins> – If __plot__ is set to True, the module will return a duration plot. There are two options for the plot, and both are specified when calling the duration module.\n    - 'linear': This option plots the normal time series with the phases and duration marked.\n    - 'log': This option plots the envelope of the waveform that is used for the duration measurement. This plot includes phases, moving average, best fit line, duration, and ground motion threshold. The best fit line will become dashed if it is extrapolated.<br>\n    \n    The time axis is relative to PLOT_PHASE, a global parameter set in [dur.py](madpy/plotting/dur.py). The plotting parameters can be changed in [params.py](madpy/plotting/params.py). The plot must be generated if the user wishes to save the figure (__save_figure__=True). The figure is saved in __figure_path__ and is named \"dur-[plot type]-[trace id].png.\" \n    \n    _Note: The 'log' option is best for debugging. The 'linear' option is best for a quick check_.",
        "createdAt": "2021-09-30T01:00:07.000Z",
        "updatedAt": "2025-04-29T17:53:00.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/seismomomo/madpy/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "tktmyd/seismolib",
        "url": "https://github.com/tktmyd/seismolib",
        "description": "A small python codes to help data analysis in seismology",
        "stars": 0,
        "forks": 0,
        "readme": "# seismolib\n\nSmall Python codes to help with data analysis in seismology.",
        "createdAt": "2024-09-07T10:03:28.000Z",
        "updatedAt": "2025-08-31T03:20:40.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/tktmyd/seismolib/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "BackTrackBB/backtrackbb",
        "url": "https://github.com/BackTrackBB/backtrackbb",
        "description": "Multi-band array detection and location of seismic sources",
        "stars": 62,
        "forks": 18,
        "readme": "## BackTrackBB\nMulti-band array detection and location of seismic sources\n\n(c) 2015-2018  Natalia Poiata <poiata@ipgp.fr>, Claudio Satriano <satriano@ipgp.fr>;\n\n(c) 2013-2014  Natalia Poiata <poiata@ipgp.fr>, Claudio Satriano <satriano@ipgp.fr>, Pierre Romanet <romanet@ipgp.fr>\n\nBackTrackBB is a program for detection and space-time location of seismic sources\nbased on multi-scale, frequency-selective statistical coherence of the wave field\nrecorded by dense large-scale seismic networks and local antennas.\nThe method is designed to enhance coherence of the signal statistical features\nacross the array of sensors and consists of three steps:\n  * signal processing;\n  * space-time imaging;\n  * detection and location.\n\n\n\n## Getting Started\n\nClone or download the project from GitHub, if needed uncompress the archive.\n\n### Installation:\n\n#### Linux and macOS:\nNote: You will need a C compiler (ex., `gcc` or `clang`).\n\nRun the following command from within the main directory:\n\n    pip install .\n\nor to install developer mode use:\n\n    pip install -e .\n\n#### Windows\nYou can use the above instructions for Linux and macOS, if you have a recent version of Visual Studio.\n\nAlternatively, download the most recent Windows build of backtrackbb from\n[this link](https://www.dropbox.com/s/borfq99yajyi1ii/backtrackbb-0.0.0_gea9d-cp36-cp36m-win_amd64.whl?dl=1)\n(python wheel file) and install it with:\n\n    pip install backtrackbb-VERSION.whl\n    \nwhere `VERSION` is the current backtrackbb version string (part of the file name).\n\n### Running examples:\nFirst, download the file [examples.zip](https://www.dropbox.com/s/emlz4lbd6dpu9a9/examples.zip?dl=1) containing additional data (seismograms and theoretical travel-time grids).\n\n\nRun the main detection and location code on an example dataset:\n\n    btbb  examples/BT_ChileExample.conf\n\nRun an example illustrating the procedure of Multi-Band Filter Characteristic Function calculation:\n\n    mbf_plot  examples/MBF_ChileExample.conf\n\n\n## Documentation\n\nA detailed documentation is available here: [backtrackbb.readthedocs.io](http://backtrackbb.readthedocs.io/en/latest/)\n\n\n### Contact Information:\n  * [Natalia Poiata](mailto:poiata@ipgp.fr)\n  * [Claudio Satriano](mailto:satriano@ipgp.fr)\n\n\n### References\nPoiata, N., C. Satriano, J.-P. Vilotte, P. Bernard, and K. Obara (2016). Multi-band array detection and location of seismic sources recorded by dense seismic networks, Geophys. J. Int., 205(3), 1548-1573, doi:[10.1093/gji/ggw071](https://doi.org/10.1093/gji/ggw071).\n\n\nPoiata, N., J.-P., Vilotte, P., Bernard, C., Satriano, and K. Obara (2018). Imaging different components of a tectonic tremor sequence in southwestern Japan using an automatic statistical detection and location method, Geophys. J. Int., 213(3), 2193–2213, doi:[10.1093/gji/ggy070](https://doi.org/10.1093/gji/ggy070).\n",
        "createdAt": "2017-05-31T21:04:54.000Z",
        "updatedAt": "2025-05-01T15:45:31.000Z",
        "language": "Python",
        "homepage": "http://backtrackbb.github.io",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/BackTrackBB/backtrackbb/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "benjaminpope/starwars",
        "url": "https://github.com/benjaminpope/starwars",
        "description": "Seismology with Adversarial Recurrent Stars",
        "stars": 0,
        "forks": 1,
        "readme": "[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)\n\n# starwars\nSeismology with Adversarial Recurrent Stars\n\n## Contributors\n[Benjamin Pope](http://benjaminpope.github.io), [Guy Davies](https://www.birmingham.ac.uk/staff/profiles/physics/davies-guy.aspx)\n\n## The Idea\n\nCan we use Generative Adversarial Networks to model red giant light curves, and then use the GANs to model their noise and look for planets?\n\n## The Data\n\nWe've got a thousand of the best and brightest red giants, and we're going to train our GAN on one-day segments of long-cadence data.\n\n## The Context\n\nThis is a project developed for [TESS Ninja 2](https://tess.ninja/two/) held at the University of Chicago, March 25-29 2019. ",
        "createdAt": "2019-03-25T18:16:07.000Z",
        "updatedAt": "2019-03-28T22:02:28.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/benjaminpope/starwars/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "KUM-Kiel/6d6-compat",
        "url": "https://github.com/KUM-Kiel/6d6-compat",
        "description": "Compatibility Tools for the 6D6 Datalogger",
        "stars": 2,
        "forks": 1,
        "readme": "# 6D6 Compatibility Tools\n\nThis is a small collection of tools to convert data from the new 6D6 datalogger to other formats.\n\nThe main supported format is the MiniSEED format.\nIt is widely accepted in the seismological community and ensures maximum compatibility with different workflows.\n\nFor compatibility with SEND software, the s2x format can be used.\nThis is especially useful when you are in the process of migrating to the new 6D6 Datalogger but still have a lot of old recorders and wish to use all the models side by side.\n\nAs a planned feature, the implementation of the SEG-Y format is in progress.\n\n## Installation\n\nTo install, copy this into your terminal and press enter:\n\n```text\ncurl -fsSL https://raw.githubusercontent.com/KUM-Kiel/6d6-compat/master/install | bash\n```\n\nIf that does not work, try this instead:\n\n```text\nwget -qO- https://raw.githubusercontent.com/KUM-Kiel/6d6-compat/master/install | bash\n```\n\nOr [download one of the binary realeases](https://github.com/KUM-Kiel/6d6-compat/releases/latest).\n\n## Building from source\n\nTo compile the programs from source you need to have installed a C compiler, Ruby and the rake-c gem.\n\n```text\n$ sudo apt-get install build-essential gcc-multilib git ruby\n$ sudo gem install rake-c\n$ git clone https://github.com/KUM-Kiel/6d6-compat.git\n$ cd 6d6-compat\n$ rake\n$ rake install\n```\n## Finding the device path of a StiK or SD card\n\nSeveral commands need the device path of a StiK or SD card.\nA device path normally looks like `/dev/sdb`.\n\n### Linux\n\nOn newer Linux distributions the following command can be tried:\n\n```text\n$ sudo dmesg -w\n```\n\nIf that does not work one can try\n\n```text\n$ sudo tail -f /var/log/syslog\n```\n\nNow the StiK or SD card can be plugged in.\nOnce it is detected by the computer, a message with the device path should appear in the terminal.\nThe device path normally looks like `/dev/sdb` but it could also be like `/dev/mmcblk0`.\n\nThe message log can now be stopped with `Ctrl`+`C`.\n\n### macOS\n\nOn macOS the device path looks like `/dev/disk2`.\n\nThe device path can be found by opening the `Disk Utility.app` (search for “Disk Utility” in Spotlight).\nNow select the proper device in the list on the left.\nThe device name will now be displayed on the bottom right.\nIf the device name is `disk2`, the device path will be `/dev/disk2`.\n\n![Disk Utility](disk-utility.png)\n\n## Frequently Asked Questions\n\nSee our [FAQ file](FAQ.md).\n\n## Licence\n\nThe program is published under the terms of the GNU GPL 3.0. See the `LICENCE` file.\n\nSome parts of the program incorporate public domain or BSD licensed code.\n",
        "createdAt": "2016-05-27T07:24:26.000Z",
        "updatedAt": "2025-07-22T12:26:45.000Z",
        "language": "C",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/KUM-Kiel/6d6-compat/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "insersir/statistical-seismology",
        "url": "https://github.com/insersir/statistical-seismology",
        "description": null,
        "stars": 1,
        "forks": 0,
        "readme": "# statistical-seismology",
        "createdAt": "2021-12-26T07:27:57.000Z",
        "updatedAt": "2021-12-27T15:07:06.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/insersir/statistical-seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "SereneAnt/segystream",
        "url": "https://github.com/SereneAnt/segystream",
        "description": "Reactively streaming SEG-Y parser",
        "stars": 5,
        "forks": 0,
        "readme": "# SegyStream\n\n[![TravisCI build](https://travis-ci.org/SereneAnt/segystream.svg?branch=master)](https://travis-ci.org/SereneAnt/segystream)\n\nReactive streaming SEG-Y parser\n\n## Features\n* [Seg-Y version 1](https://seg.org/Portals/0/SEG/News%20and%20Resources/Technical%20Standards/seg_y_rev1.pdf) format supported.\n* Supports asynchronous stream processing with non-blocking adaptive pull/push back pressure, as it declared by [Reactive Streams](http://www.reactive-streams.org/).\n* Built with [Akka Streams](https://doc.akka.io/docs/akka/2.5/stream/index.html).\n* Contains examples of different use cases: streaming from file source,\n  [AWS S3](aws.amazon.com/s3), transformation, visualization, statistics, parallel processing, etc.\n* API for both [scala](https://www.scala-lang.org/) and [java](https://docs.oracle.com/javase/8/docs/technotes/guides/language/index.html) languages.\n\n## Further work\n* Configurable segy data chunk size and text reading encoding\n* Add Github badges - code coverage, stable version, etc\n* Add more examples for streaming from file source, S3, transformation, visualization, parallel processing\n* Add benchmarks, taking commonly used Seg-Y parsers as a baseline\n* Add full support for set of Seg-Y v1 features (variable ext text headers, etc.)\n* Add Seg-Y v2 support\n* Cross-validation against other commonly used Seg-Y parsers\n\n## Prerequisites\n* java 1.8\n* [sbt](https://www.scala-sbt.org/) 1.x\n\n## How to use\nAdd dependency:\n\n**Sbt**\n```sbt\nlibraryDependencies += \"com.github.sereneant.segystream\" %% \"segystream-core\" % \"0.1.0\"\n```\n\n**Maven**\n```xml\n<dependency>\n    <groupId>com.github.sereneant.segystream</groupId>\n    <artifactId>segystream-core_2.12</artifactId>\n    <version>0.1.0</version>\n</dependency>\n```\n\n**Gradle**\n```groovy\ndependencies {\n  compile group: 'com.github.sereneant.segystream', name: 'segystream-core_2.12', version: '0.1.0'\n}\n```\n\nStreaming implementation is based on [Akka Streams](https://doc.akka.io/docs/akka/2.5/stream/index.html).\n\n**Scala**\n\nSetup streams:\n```Scala\n  implicit val system: ActorSystem = ActorSystem(\"segystream-examples\")\n  implicit val mat: ActorMaterializer = ActorMaterializer()\n```\n\nConstruct Stream blueprint from Seg-Y file or another byte sources (S3, HDFS, etc).\n```scala\n  val segySource: Source[SegyPart, Future[SegyHeaders]] = fileSource.viaMat(SegyFlow())(Keep.right)\n```\nFull spectre of [Alpakka Connectors](https://developer.lightbend.com/docs/alpakka/current/) can be used for streaming from different sources / to different sinks.\n\nRun the flow, make actions/transformations:\n```scala\n  val done: Future[Done] = segySource\n    .map {\n      case th: TraceHeader => println(s\"Trace Header: ${th.traceSequenceNumberWithinLine}\")\n      case td: TraceDataChunk => println(s\"Trace Data Chunk: length=${td.length}\")\n      case _ => // NoOp\n    }\n    .toMat(Sink.ignore)(Keep.right) // wait for the Sink to complete\n    .run()\n```\n\nWait for stream termination and print the stats:\n```scala\n  implicit val ec: ExecutionContextExecutor = system.dispatcher\n  done.onComplete { _ =>\n    system.terminate()\n    println(\"Stream completed\")\n  }\n```\n\n### Scala examples\n* [Collect and print Seg-Y data stats](examples/src/main/scala/com/github/sereneant/segystream/examples/CollectSegyStats.scala)\n* [Output info from Seg-Y headers](examples/src/main/scala/com/github/sereneant/segystream/examples/GetSegyHeaders.scala)\n* [Collect data for given in-line/cross-line section](examples/src/main/scala/com/github/sereneant/segystream/examples/GetDataForSlice.scala)\n* More to come...\n\n**Java**\n\nThe full power of Akka streams is available in Java as well.\n\n### Java Examples\n* [Print some Seg-Y Info](examples/src/main/java/com/github/sereneant/segystream/examples/PrintDebugInfo.java)\n\n## Configuration\nStream of Seg-Y data in traces is split into chunks of configurable length, default is 1024 bytes.\n\nCustom configuration can be passed to `SegyFlow` constructor:\n```scala\nval segyFlow = new SegyFlow(SegyConfig(\n  charset: Charset = Charset.forName(\"CP037\"), //textual data charset\n  dataChunkSize: Int = 1024 //bytes\n))\n```\n\n### Building from sources\n```bash\nsbt package\n```\n\n### Publish to local repo repository\n**Ivy**\n```bash\nsbt publishLocal\n```\n**Maven**\n```bash\nsbt publishM2\n```\n\n### Running tests\n```bash\nsbt test\n```\n\n### Running benchmarks\n_TBD_\n\n### Running examples\nExamples are located in [examples](examples) folder.\n```bash\nsbt \"examples/runMain com.github.sereneant.segystrem.examples.CollectSegyStats SegY_file_name.segy\"\n```\n\n## Known Issues\n* Parser does not support variable extended text headers.\n* Parser does not support Data Sample Format Code 4 (4-byte fixed-point with gain, obsolete).\n\n## Contributing\nAny contributions are welcome!\nIt can be done by creating issues and pull requests on a [project GitHub page](https://github.com/SereneAnt/segystream).\n\nPlease keep code clean (whatever it means for you) and comply with coding style standards:\n* [Scala style guide](https://docs.scala-lang.org/style)\n* [Google Java Style Guide](https://google.github.io/styleguide/javaguide.html)\n\nPlease keep a [CHANGELOG.md](CHANGELOG.md) file in actual state;\nthe format is based on [Keep a Changelog](http://keepachangelog.com/en/1.0.0/).\n\n## Versioning\n[SemVer](http://semver.org/) is used as versioning standard.\nFor the version references, see the [git tags](https://github.com/SereneAnt/segystream/tags).\n\n## License\nLicensed under the MIT License - see the [LICENSE](LICENSE) file.\n\n## Acknowledgments\n* Inspired by [Reactive Manifesto](https://www.reactivemanifesto.org)\n* Thanks to [Mikhail Aksenov](https://github.com/thecoldwine) for [sigrun](https://github.com/thecoldwine/sigrun), used as a good starter in Seg-Y parsing.\n* Thanks to [Andriy Plokhotnyuk](https://github.com/plokhotnyuk) for his [jsoniter-scala](https://github.com/plokhotnyuk/jsoniter-scala) as an example of technical excellence and well shaped scala project, where build configuration and project structure were borrowed from.\n\n## Alternative noteworthy implementations\nAll references are given in alphabetical order.\n#### Java\n* https://github.com/cloudera/seismichadoop\n* https://github.com/dhale/idh/tree/master/bench/src/segy\n* https://github.com/thecoldwine/sigrun\n#### Python\n* https://github.com/obspy/obspy\n* https://github.com/sixty-north/segpy\n* https://github.com/Statoil/segyio\n",
        "createdAt": "2018-05-12T01:50:48.000Z",
        "updatedAt": "2024-03-13T05:18:33.000Z",
        "language": "Scala",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/SereneAnt/segystream/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "VedangW/GroundMotionClassifier",
        "url": "https://github.com/VedangW/GroundMotionClassifier",
        "description": "Classify ground motion waves into earthquakes or blasts using traditional Machine Learning algorithms.",
        "stars": 10,
        "forks": 4,
        "readme": "# GroundMotionClassifier\nA project to differentiate between earthquakes and blasting waves using Support Vector Machines.\n\n## Prerequisites:\n\nTo run this project, you would need a linux-based operating system (Ubuntu or Fedora would work best).\n\nThe code is written in Python 2.7.12+, but any version of Python 2 would work. \n\nYou would also need the following installed in your system:\n- Scipy\n- Numpy\n- Matplotlib\n- Scikit-Learn\n- Peakutils\n- Plotly\n\nThese can be downloaded using a download manager such as pip. \n\nInstall pip:\n```\nsudo apt-get install python-pip\n```\nInstall any of the dependencies with pip. For eg,:\n```\npip install scikit-learn\npip install numpy\n```\n\n## Running the code:\n\nThe feature vector is stored in store.txt present in isrsvm/PS/Code.\nTo create a new feature vector (while erasing the previous one):\n```\nsh run.sh\n```\nTo test the working of any module, you can simply compile it with Python 2 with the appropriate command-line arguments.\nCheck in the comments in the relevant file to know the command-line arguments. For eg.:\n```\npython Seismogram.py Kachchh pitsa001.044\npython rsp.py /path/to/PS/Datasets/Surendranagar pitsa001.003 r\n```\nTo train the classifier and plot the decision boundary along with the scatterplot, compile the classifier file.\nThis however should be done after creating the feature vector:\n```\npython classifier.py\n```\n\n## Datasets:\n\nThe datasets are present in isrsvm/PS/Datasets.\n\n#### Note: These datasets are owned by the Institute of Seismological Research, Gandhinagar, India.\n\n\n",
        "createdAt": "2017-06-30T06:26:32.000Z",
        "updatedAt": "2025-11-30T19:22:34.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/VedangW/GroundMotionClassifier/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "keurfonluu/fteikpy",
        "url": "https://github.com/keurfonluu/fteikpy",
        "description": "Accurate Eikonal solver for Python",
        "stars": 65,
        "forks": 20,
        "readme": "fteikpy\n=======\n\n|License| |Stars| |Pyversions| |Version| |Downloads| |Code style: black| |Codacy Badge| |Codecov| |Build| |Docs| |DOI|\n\n**fteikpy** is a Python library that computes accurate first arrival traveltimes in 2D and 3D heterogeneous isotropic velocity models. The algorithm handles properly the curvature of wavefronts close to the source which can be placed without any problem between grid points.\n\nThe code is based on `FTeik <https://github.com/Mark-Noble/FTeik-Eikonal-Solver>`__ implemented in Python and compiled `just-in-time <https://en.wikipedia.org/wiki/Just-in-time_compilation>`__ with `numba <https://numba.pydata.org/>`__.\n\n.. figure:: https://raw.githubusercontent.com/keurfonluu/fteikpy/master/.github/sample.gif\n   :alt: sample-marmousi\n   :width: 100%\n   :align: center\n\n   Computation of traveltimes and ray-tracing on smoothed Marmousi velocity model.\n\nFeatures\n--------\n\nForward modeling:\n\n-  Compute traveltimes in 2D and 3D Cartesian grids with the possibility to use a different grid spacing in Z, X and Y directions,\n-  Compute traveltime gradients at runtime or a posteriori,\n-  A posteriori 2D and 3D ray-tracing.\n\nParallel:\n\n-  Traveltime grids are seemlessly computed in parallel for different sources,\n-  Raypaths from a given source to different locations are also evaluated in parallel.\n\nInstallation\n------------\n\nThe recommended way to install **fteikpy** and all its dependencies is through the Python Package Index:\n\n.. code:: bash\n\n   pip install fteikpy --user\n\nOtherwise, clone and extract the package, then run from the package location:\n\n.. code:: bash\n\n   pip install . --user\n\nTo test the integrity of the installed package, check out this repository and run:\n\n.. code:: bash\n\n   pytest\n\nDocumentation\n-------------\n\nRefer to the online `documentation <https://keurfonluu.github.io/fteikpy/>`__ for detailed description of the API and examples.\n\nAlternatively, the documentation can be built using `Sphinx <https://www.sphinx-doc.org/en/master/>`__:\n\n.. code:: bash\n\n   pip install -r doc/requirements.txt\n   sphinx-build -b html doc/source doc/build\n\nUsage\n-----\n\nThe following example computes the traveltime grid in a 3D homogeneous velocity model:\n\n.. code-block:: python\n\n   import numpy as np\n   from fteikpy import Eikonal3D\n\n   # Velocity model\n   velocity_model = np.ones((8, 8, 8))\n   dz, dx, dy = 1.0, 1.0, 1.0\n\n   # Solve Eikonal at source\n   eik = Eikonal3D(velocity_model, gridsize=(dz, dx, dy))\n   tt = eik.solve((0.0, 0.0, 0.0))\n\n   # Get traveltime at specific grid point\n   t1 = tt[0, 1, 2]\n\n   # Or get traveltime at any point in the grid\n   t2 = tt(np.random.rand(3) * 7.0)\n\nContributing\n------------\n\nPlease refer to the `Contributing\nGuidelines <https://github.com/keurfonluu/fteikpy/blob/master/CONTRIBUTING.rst>`__ to see how you can help. This project is released with a `Code of Conduct <https://github.com/keurfonluu/fteikpy/blob/master/CODE_OF_CONDUCT.rst>`__ which you agree to abide by when contributing.\n\n.. |License| image:: https://img.shields.io/github/license/keurfonluu/fteikpy\n   :target: https://github.com/keurfonluu/fteikpy/blob/master/LICENSE\n\n.. |Stars| image:: https://img.shields.io/github/stars/keurfonluu/fteikpy?logo=github\n   :target: https://github.com/keurfonluu/fteikpy\n\n.. |Pyversions| image:: https://img.shields.io/pypi/pyversions/fteikpy.svg?style=flat\n   :target: https://pypi.org/pypi/fteikpy/\n\n.. |Version| image:: https://img.shields.io/pypi/v/fteikpy.svg?style=flat\n   :target: https://pypi.org/project/fteikpy\n\n.. |Downloads| image:: https://pepy.tech/badge/fteikpy\n   :target: https://pepy.tech/project/fteikpy\n\n.. |Code style: black| image:: https://img.shields.io/badge/code%20style-black-000000.svg?style=flat\n   :target: https://github.com/psf/black\n\n.. |Codacy Badge| image:: https://img.shields.io/codacy/grade/bec3d2ad6b8c45cf9bb0da110fe04838.svg?style=flat\n   :target: https://www.codacy.com/gh/keurfonluu/fteikpy/dashboard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=keurfonluu/fteikpy&amp;utm_campaign=Badge_Grade\n\n.. |Codecov| image:: https://img.shields.io/codecov/c/github/keurfonluu/fteikpy.svg?style=flat\n   :target: https://codecov.io/gh/keurfonluu/fteikpy\n\n.. |DOI| image:: https://zenodo.org/badge/DOI/10.5281/zenodo.4269352.svg?style=flat\n   :target: https://doi.org/10.5281/zenodo.4269352\n\n.. |Build| image:: https://img.shields.io/github/workflow/status/keurfonluu/fteikpy/Python%20package\n   :target: https://github.com/keurfonluu/fteikpy\n\n.. |Docs| image:: https://img.shields.io/github/workflow/status/keurfonluu/fteikpy/Build%20documentation?label=docs\n   :target: https://keurfonluu.github.io/fteikpy/\n",
        "createdAt": "2017-05-16T22:25:10.000Z",
        "updatedAt": "2025-12-03T10:31:07.000Z",
        "language": "Python",
        "homepage": "https://github.com/keurfonluu/fteikpy",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.4269352",
            "openAlex": "10.5281/zenodo.4269352",
            "openCitations": "10.5281/zenodo.4269352",
            "dataCite": "10.5281/zenodo.4269352",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/keurfonluu/fteikpy/master/README.rst",
        "mainPaper": {
            "doi": "10.5281/zenodo.4269352",
            "title": "fteikpy: Accurate Eikonal solver for Python",
            "journal": "Zenodo",
            "dateReleased": "2022-08-14T00:00:00.000Z",
            "abstract": "",
            "citationsArray": []
        },
        "repoDoi": "10.5281/zenodo.4269352",
        "publications": [
            {
                "doi": "10.5281/zenodo.4269352",
                "name": "fteikpy: Accurate Eikonal solver for Python",
                "source": "",
                "authorNames": [],
                "abstract": "",
                "publicationDate": "2022-12-05T12:13:02.388Z"
            }
        ]
    },
    {
        "source": "GitHub",
        "name": "jpampuero/jpampuero.github.io",
        "url": "https://github.com/jpampuero/jpampuero.github.io",
        "description": "J. P. Ampuero - Seismology and Earthquake Dynamics",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2019-02-19T22:37:03.000Z",
        "updatedAt": "2024-02-06T15:26:50.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "rwalkerlewis/cps_src",
        "url": "https://github.com/rwalkerlewis/cps_src",
        "description": "Source for Dr. Robert Herrmann's Computer Programs in Seismology",
        "stars": 1,
        "forks": 2,
        "readme": "# cps_src\n",
        "createdAt": "2024-04-08T00:53:21.000Z",
        "updatedAt": "2024-10-16T19:24:12.000Z",
        "language": "C",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/rwalkerlewis/cps_src/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "bclswl0827/eewgo",
        "url": "https://github.com/bclswl0827/eewgo",
        "description": "An EEW (Earthquake Early Warning) library implemented in pure Go.",
        "stars": 4,
        "forks": 0,
        "readme": "# eewgo\n\nAn EEW (Earthquake Early Warning) library implemented in pure Go.\n",
        "createdAt": "2024-10-14T12:49:18.000Z",
        "updatedAt": "2025-06-02T18:11:05.000Z",
        "language": "Go",
        "homepage": "https://pkg.go.dev/github.com/bclswl0827/eewgo",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/bclswl0827/eewgo/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Glad-analytics/Seismic-Anomaly-Detection",
        "url": "https://github.com/Glad-analytics/Seismic-Anomaly-Detection",
        "description": "Anomaly detection in global seismology data using Autoencoders and Random Forests on Raspberry Shake waveforms.",
        "stars": 0,
        "forks": 0,
        "readme": "# Seismic-Anomaly-Detection\nThis Project explores anomaly detection in global seismology data using Autoencoders (AE) and Random Forests (RF) on Raspberry Shake waveforms.\n\n## Project Aim  \nDetect earthquake (EQ) events by training an AE on non-earthquake (Non-EQ) background data and treating high reconstruction errors as anomalies.  \n\n## Methods  \n- **Data**: 20 Raspberry Shake stations (rural, urban, industrial).  \n- **Preprocessing**: Segmentation (5s windows), z-score normalization.  \n- **Models**:  \n  - Autoencoder (unsupervised)  \n  - Random Forest on 64D bottleneck features  \n- **Thresholding**: q95 (95 percentile) vs ROC-optimal (Youden’s J).  \n- **Exploratory Analysis**: RF probabilities aggregated to detect human activity trends.  \n\n## Results  \n- AE achieved ROC-AUC ≈ 0.72 (F1 ≈ 0.79).  \n- RF classifier on latent features: ROC-AUC ≈ 0.75.  \n- EQs show higher reconstruction error than Non-EQ, but overlap remains.  \n- Human activity patterns (urban/industrial) often look EQ-like.  \n\n## Future Work  \n- Continuous background monitoring.  \n- More advanced models (CNN, LSTM, Transformer, VAE(Variational Autoencoders)).  \n- Station-specific thresholds.  \n\n## Technologies  \n- Python (NumPy, pandas, scikit-learn, TensorFlow/Keras, Matplotlib)  \n- Jupyter Notebook  \n- GitHub for version control.\n  \n## Notebooks\n1. **Baseline notebook** — waveform download, segmentation, first AE baseline  \n   - [Notebook 1 (IPYNB)](notebooks/PROJECT.ipynb)  \n   - If GitHub can’t render it, open the **[HTML view](notebooks/PROJECT.html)** (same content, with outputs).\n     *(⚠️ Large file — please download and open locally in a browser to see code + outputs).*\n\n2. **Improved model + Human Activity Trends** — refined AE, RF on bottlenecks, activity analysis  \n   - [Notebook 2 (IPYNB)](notebooks/ProjectCONTD2.ipynb)\n\n3. **Stress test (M7.5–M8.8 earthquakes)** — separation checks on large events  \n   - [Notebook 3 (IPYNB)](notebooks/PROJT3.ipynb)\n\n## Reports\n- **[Final Project Report (PDF)](reports/Final%20Project%20Report.%20Anomaly%20Detection.pdf)**  \n  A comprehensive write-up covering background, methodology, experiments, results, discussion, and future directions.  \n",
        "createdAt": "2025-09-22T15:07:30.000Z",
        "updatedAt": "2025-09-25T17:14:19.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Glad-analytics/Seismic-Anomaly-Detection/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "0Quake/Zero-Quake",
        "url": "https://github.com/0Quake/Zero-Quake",
        "description": "地震情報アプリ「Zero Quake」のリポジトリです。地震関連の様々な情報を、このアプリ一つで確認できます。速報性や信頼性、ユニバーサルデザイン等、いざというときにスムーズに情報を取得できる工夫をしています。",
        "stars": 49,
        "forks": 5,
        "readme": "# Zero-Quake\n\n![](https://img.shields.io/github/downloads/0quake/Zero-Quake/total)\n![](https://img.shields.io/github/v/release/0Quake/Zero-Quake)\n![](https://img.shields.io/github/license/0quake/Zero-Quake)\n\n## ご注意\n\n- 地震に関する正確な知識に基づき、正しくお使いください。\n- インターネットを使用して情報を取得する都合で、情報の遅延・欠損等があり得ます。\n- 動画・配信等での使用は制限しませんが、当アプリで使用する情報源・合成音声システム等に付帯する第三者の権利に関しては、ご自身で確認うえ、自己責任でご使用ください。\n- 地図に表示されるリアルタイムの揺れ情報には、生活振動などを含みます。\n- TREM-RTS から取得する台湾などの揺れ情報は、日本とは異なる尺度の震度ですが、おおむね日本の震度と対応するため、そのまま表示しています。\n- Early-Estによる情報は実験的なものであり、専門家による検証を経ていません。自己責任にて有効化してください。\n\n## 受信できる防災情報の一覧／出典\n\n- ### 緊急地震速報 (気象庁)\n\n  予報・警報／キャンセル報／PLUM 法など含む\n  <details>\n  <summary>出典・情報源</summary>\n  <ul>\n    <li>Wolfx (By: Wolfx Project)</li>\n    <li>ProjectBS (By: CrossRoad)</li>\n    <li>Axis (By:Prioris)（無料登録必須）</li>\n    <li>P2P 地震情報 API (By: P2P地震情報)（警報のみ）</li>\n  </ul>\n  </details>\n\n- ### 地震情報 (気象庁)\n\n  震度速報 ／ 震源に関する情報 ／ 震度・震源に関する情報 _[震度・震源情報/遠地地震に関する情報]_  \n  長周期地震動に関する観測情報※ ／ 顕著な地震の震源要素更新のお知らせ ／ 推計震度分布※  \n  南海トラフ地震に関連する情報 _[南海トラフ地震臨時情報/南海トラフ地震関連解説情報]_\n  地震の活動状況等に関する情報／北海道・三陸沖後発地震注意情報／地震回数に関する情報\n  <details>\n  <summary>出典・情報源</summary>\n  <ul>\n    <li>出典：<a href=\"https://xml.kishou.go.jp/xmlpull.html\">気象庁防災情報 XML</a></li>\n    <li>出典：<a href=\"https://www.jma.go.jp/bosai/map.html?contents=earthquake_map\">気象庁ホームページ</a>（※の項目のみ）</li>\n    <li>nTool Earthquake API</li>\n    <li>Axis(By:Prioris)（無料登録必須）</li>\n  </ul>\n  </details>\n\n- ### 津波情報 (気象庁)\n\n  大津波警報／津波警報／津波注意報／津波予報※  \n  津波情報／沖合の津波観測に関する情報※\n  国際津波関連情報(WEPA40)☆\n  <details>\n  <summary>出典・情報源</summary>\n  <ul>\n    <li>出典：<a href=\"https://xml.kishou.go.jp/xmlpull.html\">気象庁防災情報 XML</a></li>\n    <li>出典：<a href=\"https://www.jma.go.jp/bosai/map.html#contents=pacifictsunami\">気象庁ホームページ</a>（☆の項目のみ）</li>\n    <li>P2P 地震情報 API（※の項目を除く）</li>\n  </ul>\n  </details>\n\n- ### リアルタイム揺れ情報\n\n  K-NET, KiK-net（防災科学技術研究所 強震観測網）  \n  S-net, 相模湾地震観測施設（防災科学技術研究所 海底観測網）  \n  TREM-RTS (SE-Net, MS-Net)※（台湾など コミュニティ観測網）  \n  Wolfx SeisJS（中国本土など　コミュニティ観測網）\n\n  ※<a href=\"https://github.com/whes1015\">YuYu1015 様</a>のご厚意でデータを提供していただいています\n\n  <details>\n  <summary>出典・情報源</summary>\n  <ul>\n    <li>出典：強震モニタ <a href=\"http://www.kmoni.bosai.go.jp/\">© 防災科学技術研究所</a>（K-NET, KiK-net）</li>\n    <li>出典：長周期地震動モニタ <a href=\"http://www.lmoni.bosai.go.jp/\">© 防災科学技術研究所</a>（K-NET, KiK-net）</li>\n    <li>出典：海しる <a href=\"https://www.msil.go.jp/\">© Japan Coast Guard, 防災科学技術研究所</a>（S-net, 相模湾地震観測施設）</li>\n    <li>出典：TREM-RTS API（TREM-RTS）</li>\n    <li>出典：Wolfx Project（Wolfx SeisJS）</li>\n  </ul>\n  </details>\n\n- ### 解説情報\n\n  各月の地震活動のまとめ\n  <details>\n  <summary>出典・情報源</summary>\n    <ul><li>出典：<a href=\"https://www.data.jma.go.jp/svd/eqev/data/gaikyo/\">気象庁ホームページ「各月の地震活動のまとめ」</a></li>\n  </ul>\n  </details>\n\n- ### 海外の地震情報\n\n  1. USGS Earthquake Catalog\n  2. Early-Est リアルタイム地震情報\n  <details>\n  <summary>出典・情報源</summary>\n  <ol>\n    <li>© U.S. Geological Survey</li>\n    <li>© INGV - National Institute of Geophysics and Volcanology (イタリア国立地球物理学火山学研究所)</li>\n  </ol>\n  </details>\n\n## その他の出典表示\n\n### 地図データ\n\n- 国内陸地 出典： 気象庁「[地震情報／細分区域](https://www.data.jma.go.jp/developer/gis.html)」※\n- 国内湖沼 出典：国土交通省「[国土数値情報(湖沼データ)](https://nlftp.mlit.go.jp/ksj/gml/datalist/KsjTmplt-W09-v2_2.html)」※\n- 国外陸地 出典：Natural Earth※\n- プレート境界 出典\n\n  1. [© Hugo Ahlenius, Nordpil, Peter Bird](http://opendatacommons.org/licenses/by/1.0/)\n  2. 気象庁「[震度データベース検索](https://www.data.jma.go.jp/eqdb/data/shindo/)」(地図部分を編集して使用)\n- 海底地形 出典：GEBCO\n- [© OpenStreetMap contributors](https://www.openstreetmap.org/copyright/)\n- 最適化ベクトルタイル, 地理院タイル [© 国土地理院](https://maps.gsi.go.jp/development/ichiran.html)  \n\n※加工（単純化処理）して使用\n\n### フォント\n\n- © 2022 The BIZ UDGothic Project Authors /\n  [OFL](https://openfontlicense.org/open-font-license-official-text/)\n- © 2016 The M+ Project Authors. /\n  [OFL](https://openfontlicense.org/open-font-license-official-text/)\n- © 2021 The Azeret Project Authors /\n  [OFL](https://openfontlicense.org/open-font-license-official-text/)\n- © Google /\n  [Apache License, Version 2.0](https://raw.githubusercontent.com/google/material-design-icons/refs/heads/master/LICENSE)\n\n### その他\n- turf-distance（変更して使用） [© 2014 Morgan Herlocker](http://mit-license.org/)\n- KyoshinShindoColorMap © ingen084, こんぽ\n- 表層地盤増幅率：防災科学技術研究所 ／ 下記文献\n\n  1. 若松加寿江・松岡昌志(2013)：全国統一基準による地形・地盤分類 250m メッシュマップの構築とその利用，地震工学会誌 No.18, pp.35-38.\n  2. Wakamatsu, K. and Matsuoka, M. (2013): \" Nationwide 7.5-Arc-Second Japan Engineering Geomorphologic Classification Map and Vs30 Zoning\", Journal of Disaster Research Vol.8 No.5, pp.904-911.\n  3. 松岡昌志・若松加寿江(2008) ： 地形・地盤分類 250m メッシュマップ全国版に基づく地盤のゆれやすさデータ，産業技術総合研究所，知的財産管理番号 H20PRO-936．\n  4. 藤本一雄・翠川三郎(2006) : 近接観測点ペアの強震観測記録に基づく地盤増幅度と地盤の平均 S 波速度の関係,日本地震工学会論文集,Vol.6,No.1,pp.11-22.\n",
        "createdAt": "2022-11-03T09:58:10.000Z",
        "updatedAt": "2025-11-29T12:23:45.000Z",
        "language": "JavaScript",
        "homepage": "https://0quake.github.io/ZeroQuake_Website/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/0Quake/Zero-Quake/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "mathworks/Seismic-HAzard-Parameters-Evaluation-Interface-SHAppE",
        "url": "https://github.com/mathworks/Seismic-HAzard-Parameters-Evaluation-Interface-SHAppE",
        "description": "SHAppE is an interface for time-dependent seismic hazard evaluation, designed to support both researchers and students in natural and anthropogenic seismology.",
        "stars": 7,
        "forks": 1,
        "readme": "[![Open in MATLAB Online](https://www.mathworks.com/images/responsive/global/open-in-matlab-online.svg)](https://matlab.mathworks.com/open/github/v1?repo=mathworks/Seismic-HAzard-Parameters-Evaluation-Interface-SHAppE)\n[![View Seismic-HAzard-Parameters-Evaluation-Interface-SHAppE on File Exchange](https://www.mathworks.com/matlabcentral/images/matlab-file-exchange.svg)](https://uk.mathworks.com/matlabcentral/fileexchange/180879-shappe-seismic-hazard-parameters-evaluation-interface)\n\n# SHAppE: Seismic HAzard Parameters Evaluation App\n***WARNING**: SHAppE is compatible with MATLAB R2025a and newer. If you use an earlier MATLAB release, please select the [SHAppE version V1.0.0](https://github.com/mathworks/Seismic-HAzard-Parameters-Evaluation-Interface-SHAppE/releases/tag/v1.0.0)*\n\nSHAppE is an intuitive interface for time-dependent seismic hazard evaluation, designed to support both researchers and practitioners in natural and anthropogenic seismology.\n\nSHAppE is an interactive MATLAB&reg; app designed for time-dependent seismic hazard analysis. It extends the capabilities of the existing SHAPE toolbox ([Leptokaropoulos and Lasocki, 2020](https://doi.org/10.1785/0220190319)), improving usability through an intuitive graphical interface. SHAppE allows users to explore datasets interactively and evaluate probabilistic changes in seismic hazard parameters, such as mean return periods (MRP) and exceedance probabilities (EP), along with their confidence intervals. SHAppE also produces output figures and reports. Each report documents the specific combination of methods, selected data, and parameter values used in the analysis. This ensures better traceability and simplifies reproducibility for future studies.\n\n![screenshot](SHAppE_ScreenShot.png){ width=900px }\n\n## Key Features\n\n- **New Interactive Interface**: SHAppE enhances the SHAPE toolbox by providing a graphical interface for improved user experience.\n- **User-Friendly Design**: The intuitive layout ensures accessibility for users with limited programming skills.\n- **Flexible Data Import & Filtering**: Users can import earthquake catalogues and correlated data (e.g., fluid injection rates, reservoir volume, tidal water levels) and filter by:\n  - Time\n  - Space\n  - Depth\n  - Magnitude\n- **Multiple Windowing Options**: Users can create time windows based on:\n  - Equal time intervals\n  - Equal event numbers\n  - Graphical selections\n  - Imported files\n- **Magnitude Distribution Models**: Four models for seismic hazard parameter evaluation are supported:\n  - Unbounded Gutenberg-Richter (GRU)\n  - Truncated Gutenberg-Richter (GRT)\n  - Unbounded Non-Parametric Kernel (NPU)\n  - Truncated Non-Parametric Kernel (NPT)\n- **Parameter Evaluation**: Users can define the target period, magnitude, maximum magnitude (Mmax), and number of bootstrap iterations for evaluating the confidence interval of the results.\n- **Results Export**: Generate comprehensive reports and visualizations of:\n  - Original data\n  - Filtered data\n  - Selected parameters\n  - Earthquake occurrence rates and b-values (or mean magnitudes for non-parametric approaches)\n  - Mean Return Periods and Exceedance Probabilities with confidence intervals\n\n## Required Products\n\nSHAPE functions are compatible with R2017b or later and require the following:\n\n-   [MATLAB](https://www.mathworks.com/products/matlab.html)\n-   [Statistics and Machine Learning Toolbox&trade;](https://www.mathworks.com/help/stats/)\n-   [Image Processing Toolbox&trade;](https://uk.mathworks.com/products/image-processing.html)\n\n## 3rd Party Content\n\n- **Original MATLAB Functions for Hazard Analysis**: SHAppE relies on the original MATLAB functions for seismic hazard parameter evaluations available in the SHAPE toolbox repository: https://epos-apps.grid.cyfronet.pl/tcs-ah/sera-applications/src/branch/master/SHAPE_Package/SHAPE_ver2b.0/SSH.\n- **Sample data can be downloaded from Thematic Core Service - Anthropogenic Hazards (TCS-AH)** on the Episodes Platform: https://episodesplatform.eu/?lang=en#episode:SONG_TRANH. The dataset includes seismicity records and water level measurements from the Song Tranh 2 reservoir in Vietnam. These data are shared under the Creative Commons Attribution 4.0 International License (CC:BY). We processed the original data in the Sample_Data directory to be compatible with the app. We duplicated the ML as MW in seismic data and Water Level as Pressure in production data for demonstration purposes.\n\n## Installation and Usage\n\n- **Get SHAppE**: You can download or clone SHAppE from GitHub&reg; or download/install from File Exchange.\n- **Download SHAPE Functions**: The original SHAPE toolbox will be installed automatically when you install SHAppE into MATLAB. It can also be downloaded from https://epos-apps.grid.cyfronet.pl/tcs-ah/sera-applications/src/branch/master/SHAPE_Package/SHAPE_ver2b.0/SSH to enable SHAppE's functionality.\n- **Installation**: Follow the provided instructions to add SHAppE to your MATLAB environment.\n- **Data Preparation**: Users may either download sample data or follow provided instructions to retrieve and preprocess data from the TCS-AH platform.\n- **Usage Guide**: SHAppE's interface is designed for interactive exploration and parameter evaluation with minimal setup.\n\n## Examples\n\nFor further information check the dynamic poster from the AGU&reg; 2024: https://agu24.ipostersessions.com/Default.aspx?s=70-48-54-74-A7-26-61-43-DA-37-35-81-88-E0-C0-57.\n\n## Citation\n\nIf you use SHAppE in your research, please cite:\n\nLeptokaropoulos, K. and Lasocki, S. (2020). SHAPE: A MATLAB software package for time-dependent seismic hazard analysis. Seismol. Res. Lett., 91, 1867-1877. https://doi.org/10.1785/0220190319\n\n## Contact, Feedback and Support\n\nFor inquiries and feedback please contact:\n\nKostas Leptokaropoulos (kleptoka@mathworks.com)\n\nor\n\nAndrew Redfearn (aredfear@mathworks.com)\n\nAlternatively, connect via LinkedIn&reg;: https://www.linkedin.com/in/kostas-leptokaropoulos-391a0665/\n```\n",
        "createdAt": "2025-04-15T10:41:45.000Z",
        "updatedAt": "2025-12-03T14:53:55.000Z",
        "language": "MATLAB",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/mathworks/Seismic-HAzard-Parameters-Evaluation-Interface-SHAppE/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "JorgeGines/quakes",
        "url": "https://github.com/JorgeGines/quakes",
        "description": "seismological scripts",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2016-09-12T15:10:16.000Z",
        "updatedAt": "2016-09-12T15:10:16.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "damb/mseed",
        "url": "https://github.com/damb/mseed",
        "description": "libmseed bindings for Rust",
        "stars": 4,
        "forks": 3,
        "readme": "# mseed\n\n[![Crates.io](https://img.shields.io/crates/v/mseed)](https://crates.io/crates/mseed)\n[![License](https://img.shields.io/badge/license-Apache%202.0-blue)](LICENSE)\n[![Build Status](https://img.shields.io/github/actions/workflow/status/damb/mseed/rust.yml?branch=main)](https://github.com/damb/mseed/actions/workflows/rust.yml?query=branch%3Amain)\n\nRust bindings for [libmseed](https://github.com/EarthScope/libmseed) - The miniSEED data format library.\n\n## Usage\n\nmseed uses [Cargo](https://crates.io), so add it with `cargo add mseed` or\nmodify `Cargo.toml`:\n\n```toml\n[dependencies]\nmseed = \"0.7\"\n```\n\n## Documentation\n\nFor the crate's documentation please refer to\n[docs.rs/mseed](https://docs.rs/mseed/).\n\n## Examples\n\nPlease refer to the libraries' [examples](examples/).\n\n## Building mseed\n\n```sh\ngit clone https://github.com/damb/mseed\ncd mseed\ncargo build\n```\n\n## Version of libmseed\n\nCurrently this library requires `libmseed` version 3.1.3 (or newer patch\nversions). The source for `libmseed` is included in the `libmseed-sys` crate so\nthere's no need to pre-install the `libmseed` library, the `libmseed-sys` crate\nwill figure that and/or build that for you.\n\n## Contribute\n\nAny PR is very welcomed!\n\n## License\n\nLicensed under the [Apache-2.0 license](https://www.apache.org/licenses/LICENSE-2.0).\nFor more information see the [LICENSE](/LICENSE) file.\n",
        "createdAt": "2023-06-08T20:43:44.000Z",
        "updatedAt": "2025-10-21T08:39:48.000Z",
        "language": "Rust",
        "homepage": "https://docs.rs/mseed",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/damb/mseed/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "NeptuneProjects/RISProcess",
        "url": "https://github.com/NeptuneProjects/RISProcess",
        "description": "Package to process seismic data recorded on the Ross Ice Shelf, Antarctica from 2014-2017.",
        "stars": 1,
        "forks": 2,
        "readme": "# RISProcess\nRISProcess is a Python package designed to download, process, and save seismic\ndata that were collected on the Ross Ice Shelf (RIS), Antarctica, from 2014-\n2017. The package is principally built using [Obspy](https://docs.obspy.org)\nand [h5py](https://www.h5py.org). The package is used to build the data set\nrequired for the deep clustering project,\n[RISCluster](https://github.com/NeptuneProjects/RISCluster). Details on the\nclustering project are available in\n[Jenkins et al.](https://doi.org/10.1029/2021JB021716).\n\nInformation about the seismic data set can be found at\n[FDSN](https://www.fdsn.org/networks/detail/XH_2014/).  The project for which\nthe data were collected is documented in\n[Bromirski et al.](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1002/2015GL065284).\n\n## Installation\nPre-requisites:\n[Anaconda](https://anaconda.org) or\n[Miniconda](https://docs.conda.io/en/latest/miniconda.html)\n\nTested on MacOS 11.1 and Red Hat Enterprise Linux 7.9.\n\nThe following steps will set up a Conda environment and install RISProcess.\n1. Open a terminal and navigate to the directory you would like to download the\n **RISProcess.yml** environment file.\n2. Save **RISProcess.yml** to your computer by running the following:\n  <br>a. **Mac**:\n  <br>`curl -LJO https://raw.githubusercontent.com/NeptuneProjects/RISProcess/master/RISProcess.yml`\n  <br>b. **Linux**:\n  <br>`wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/NeptuneProjects/RISProcess/master/RISProcess.yml`\n3. In terminal, run: `conda env create -f RISProcess.yml`\n4. Once the environment is set up and the package is installed, activate your\nenvironment by running `conda activate RISProcess` in terminal.\n\n## Usage\nThe Jupyter notebook,\n**[RISProcess.ipynb](https://github.com/NeptuneProjects/RISProcess/blob/master/RISProcess.ipynb)**,\nprovides an outline of general usage and workflow.  There are two components to\nthe worfklow: setting up configuration files, and executing scripts. The\nconfiguration files can be set up manually (not recommended), or using the\nprovided notebook. Scripts are executed from the terminal, with recommended\ncommands printed within the notebook. Copy and paste the commands from the\nnotebook into terminal, taking care to ensure the working directories and path\nnames are consistent. Due to irregularities that can arise from executing\ncommand line functions from within the iPython kernel, I chose to avoid calling\ncommands from within the notebook.\n\n## Author\nWilliam Jenkins\n<br>wjenkins [@] ucsd [dot] edu\n<br>Scripps Institution of Oceanography\n<br>University of California San Diego\n<br>La Jolla, California, USA\n",
        "createdAt": "2021-01-23T00:48:40.000Z",
        "updatedAt": "2022-01-05T14:38:54.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/NeptuneProjects/RISProcess/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "weijias-opensource/acc",
        "url": "https://github.com/weijias-opensource/acc",
        "description": "Auto-Correlogram Calculation in seismology",
        "stars": 11,
        "forks": 8,
        "readme": "[![DOI](https://zenodo.org/badge/doi/10.5281/zenodo.3674643.svg)](http://dx.doi.org/10.5281/zenodo.3674643)\n[![PyPI version](https://badge.fury.io/py/seis-acc.svg)](https://badge.fury.io/py/seis-acc)\n[![GitHub license](https://img.shields.io/github/license/weijias-opensource/acc)](https://github.com/weijias-opensource/acc/blob/master/LICENSE)\n[![GitHub issues](https://img.shields.io/github/issues/weijias-opensource/acc)](https://github.com/weijias-opensource/acc/issues)\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/seis-acc)\n![PyPI - Downloads](https://img.shields.io/pypi/dm/seis-acc)\n\n\n# ACC: Auto-Correlogram Calculation in seismology\n\nExtracting P-wave reflections between the free surface and the lithospheric discontinuities to image the subsurface structures.\n\n## Requirements\n\n* Python 3 \n* python packages including: 'click', 'commentjson', 'geographiclib', 'matplotlib>=2', 'numpy', 'obspy>=1.0.3', 'pandas', 'setuptools', 'shapely', 'scipy>=0.19.0', 'tqdm'\n\n## Installation\n\nHere I offer two conventional ways to install the package. The first is downloading the code via git clone command.\n\n```\n>>> git clone https://github.com/weijias-opensource/acc.git\n```\n\nand enter the main directory of the package where the `setup.py` file is, then execute\n\n```\n>>> python setup.py install\n```\n. The second is just simply to execute the command of \n\n```\n>>> pip install seis-acc\n```\n\nI strongly suggest you install [Anaconda3](https://docs.anaconda.com/anaconda/install/) first, since \n\n\n>Anaconda Distribution is a free, easy-to-install package manager, environment manager, and Python distribution with a collection of 1,500+ open source packages with free community support. Anaconda is platform-agnostic, so you can use it whether you are on Windows, macOS, or Linux.\n\n\nThis allow you to install the acc package using the second way above easily.\n\n\n## Tutorials\n\nPlease go the the example directory and run \n\n```\n>>> sh run.sh\n``` \n\nfor a simple example of the Warramungga array data. \n\nMore information can be found at https://acc.readthedocs.io/en/latest/index.html.\n\n\n## Deployment\n\nThe package could be run on all operating systems, including Windows, Mac and Linux. But the package is well-tested on Ubuntu Linux (19.10) with Anaconda3 at now.\n\n## Authors\n\n* **Weijia Sun**\n\nIf you have any suggestions to help improve the package, please let me know and I will try to carry them out as soon.\n\n## Contributors\n\n* **B. L. N. Kennett**\n* **Huaiyu Yuan**\n\n## Acknowledgments\n\nThe author learned to write a flexible, practical and modern software for friendly usage from other packages. More, a small part of code in this package is also reproduced from other projects.\n\n* [rf](https://github.com/trichter/rf)\n* [seispy](https://github.com/xumi1993/seispy)\n* [obspy](https://github.com/obspy/obspy)\n\n",
        "createdAt": "2020-02-11T03:00:03.000Z",
        "updatedAt": "2025-07-01T01:41:45.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.3674643",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.3674643",
            "dataCite": "10.5281/zenodo.3674643",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/weijias-opensource/acc/master/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.3674643",
            "title": "weijias-opensource/acc: Update version",
            "journal": "Zenodo",
            "dateReleased": "2020-04-13T00:00:00.000Z",
            "abstract": "Auto-Correlogram Calculation in seismology",
            "citationsArray": []
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "YuanYusung/Double-Beamforming-Analysis",
        "url": "https://github.com/YuanYusung/Double-Beamforming-Analysis",
        "description": "Double beamforming code for analyzing seismic ambient noise cross-correlations (ANCs) in dense arrays. It is designed for rapid phase velocity extraction and surface wave mode analysis at a specified period beneath the array.",
        "stars": 0,
        "forks": 0,
        "readme": "# Double-Beamforming-Analysis\nDouble beamforming code for analyzing ambient noise cross-correlations (ANCs) in dense arrays. It is designed for rapid phase velocity extraction and surface wave mode analysis at a specified period beneath the array.\n<img src=\"https://github.com/user-attachments/assets/50d5b44f-690d-4b9f-8804-a06079711df1\" width=\"600\" />\n\n",
        "createdAt": "2025-11-15T13:24:23.000Z",
        "updatedAt": "2025-11-15T13:35:34.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/YuanYusung/Double-Beamforming-Analysis/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "sohan23/2015-Nepal-Earthquake---An-analysis-of-the-earthquake-and-it-s-effects",
        "url": "https://github.com/sohan23/2015-Nepal-Earthquake---An-analysis-of-the-earthquake-and-it-s-effects",
        "description": "An in-depth analysis of the 2015 Nepal earthquake, integrating seismic data from the IRIS network (processed with ObsPy) and GPS displacement data from the Nevada Geodetic Laboratory (visualized with GMT). This research explores seismicity, ground deformation, and tectonic stress release.",
        "stars": 1,
        "forks": 0,
        "readme": "## **2015-Nepal Earthquake: Analysis**\n\nThis repository contains the `ipynb` file called `2015-Nepal.ipynb` and other supporting files including the Seismic and GPS data used for the analysis purposes.\n\n### **Unpacking the Earthquake’s Effects**\n\n* The 2015 Nepal earthquake was a seismic event that greatly shaped the Himalayan region, and I wanted to understand both its seismicity and the ground deformations it caused. My goal was to explore how tectonic stress was released in this geologically active region. To do so, I turned to seismic data from five global stations, using the resources provided by the IRIS network. Processing this data with the ObsPy Python package allowed me to dive into the earthquake's inner workings, focusing on P-wave arrival times and travel paths. This analysis gave me insights into how the earth's crust responded, showing clear differences between continental and oceanic crusts.\n* However, seismic data alone wasn’t enough. To fully understand the earthquake's impact, I also gathered GPS data from the Nevada Geodetic Laboratory, focusing on two key stations. I used GMT to process the data and visualize the eastward, northward, and vertical ground displacements. The uplift I observed was particularly striking, providing geodetic evidence of the earthquake's direct impact on the region's tectonic structures.\n\n### **What's Next for This Research**\n\n* Going forward, I'm excited to integrate SAR data into my analysis. This will allow for even more detailed geomorphological insights, and I hope to draw stronger correlations between seismic wave amplitudes and GPS data. By understanding how the lithospheric setting influences earthquake impacts, I aim to shed new light on the tectonic activity in this critical region.\n",
        "createdAt": "2024-10-09T15:53:12.000Z",
        "updatedAt": "2025-07-19T05:39:29.000Z",
        "language": "Roff",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/sohan23/2015-Nepal-Earthquake---An-analysis-of-the-earthquake-and-it-s-effects/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "niyiyu/PNW-ML",
        "url": "https://github.com/niyiyu/PNW-ML",
        "description": "A ML-ready curated data set for a wide range of seismic signals from Pacific Northwest",
        "stars": 25,
        "forks": 4,
        "readme": "# Pacific Northwest Curated Seismic Dataset\n[![DOI](https://zenodo.org/badge/470042054.svg)](https://zenodo.org/badge/latestdoi/470042054) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n## A curated dataset for a wide range of sources from the Pacific Northwest\n\n![map](./figures/README_overview.png)\n\n## Overview\nEach dataset is made by two files: waveform (in HDF5 format) and metadata (in CSV format). All follow the structure of [seisbench format](https://seisbench.readthedocs.io/en/latest/). See [here](https://seisbench.readthedocs.io/en/latest/pages/data_format.html) to learn more about the file structure.\n\n## Datasets\nWe are hosting two copies of the dataset: one on Google Drive, another on UW-ESS server. All datasets are also available through [SeisBench](https://seisbench.readthedocs.io/en/latest/pages/benchmark_datasets.html#pnw).\n\n### 1. ComCat Events\n- EH, BH, and HH channel (velocity)\n  - waveform (62.7 GB): [[GDrive](https://drive.google.com/file/d/10UCLyJSRibvhon9CuUTfns3fObNFKDer/view?usp=sharing)] | [[UW-ESS](https://dasway.ess.washington.edu/shared/niyiyu/PNW-ML/comcat_waveforms.hdf5)]\n  - metadata (47.7 MB): [[GDrive](https://drive.google.com/file/d/1bKDITx8KiDGZUaUoWQSZilpo7GhdWxKv/view?usp=sharing)] | [[UW-ESS](https://dasway.ess.washington.edu/shared/niyiyu/PNW-ML/comcat_metadata.csv)]\n\n- EN (accelerometer)\n  - waveform (2.1 GB): [[GDrive](https://drive.google.com/file/d/1I16psU3YJ7CFFNWZiaAGPlw1M3BmvuT8/view?usp=sharing)] | [[UW-ESS](https://dasway.ess.washington.edu/shared/niyiyu/PNW-ML/accelerometer_waveforms.hdf5)]\n  - metadata (1.7 MB): [[GDrive](https://drive.google.com/file/d/1xpeaoC3NsZqyICIbNHF2J46WsfZwwF6K/view?usp=sharing)] | [[UW-ESS](https://dasway.ess.washington.edu/shared/niyiyu/PNW-ML/accelerometer_metadata.csv)]\n\n### 2. Noise Waveform (EH, BH, and HH)\n  - waveform (~18 GB): [[GDrive](https://drive.google.com/file/d/1Z55WTcoyy-bR-WwWbedlZJrSo6tkRLlJ/view?usp=sharing)] | [[UW-ESS](https://dasway.ess.washington.edu/shared/niyiyu/PNW-ML/noise_waveforms.hdf5)]\n  - metadata (4.8 MB): [[GDrive](https://drive.google.com/file/d/1Ou5AKRczEqnNRsSEUSafIRlGcXTvLLUW/view?usp=sharing)] | [[UW-ESS](https://dasway.ess.washington.edu/shared/niyiyu/PNW-ML/noise_metadata.csv)]\n  \n### 3. Exotic Events (EH, BH, and HH)\n  - waveform (3.9 GB): [[GDrive](https://drive.google.com/file/d/1pxGQnLnAwXf9Zhc8xfh1HXEOsXjga2sG/view?usp=sharing)] | [[UW-ESS](https://dasway.ess.washington.edu/shared/niyiyu/PNW-ML/exotic_waveforms.hdf5)]\n  - metadata (1.4 MB): [[GDrive](https://drive.google.com/file/d/1brCZkrKjRtToLxBX5ob7qHX6EBq00nAM/view?usp=sharing)] | [[UW-ESS](https://dasway.ess.washington.edu/shared/niyiyu/PNW-ML/exotic_metadata.csv)]\n\n### 4. Northern California Sequence (December 2022)\n  - waveform (346 MB): [[GDrive](https://drive.google.com/file/d/15UxIbxacloPlY2DUTDBEnBaMYvh2eXVI/view?usp=sharing)] | [[UW-ESS](https://dasway.ess.washington.edu/shared/niyiyu/PNW-ML/norcal_waveforms.hdf5)]\n  - metadata (126 KB): [[GDrive](https://drive.google.com/file/d/1BhLVODzlu407JDZ0OteoPgZlTE-o469O/view?usp=sharing)] | [[UW-ESS](https://dasway.ess.washington.edu/shared/niyiyu/PNW-ML/norcal_metadata.csv)]\n\n### 5. ML-enhanced catalog\n  - CSV (~93 MB): [[GDrive](https://drive.google.com/file/d/16qUT_3-duVuKwfmPmvtH5EifL4eeyRvv/view?usp=sharing)] \n\n## Access\n### Quick tour to the dataset\nHere are several ways to use the PNW dataset.\n\n1. Jupyter Notebook\n   \n  A jupyter notebook is available to load and plot PNW dataset at [here](./notebooks/inspect_pnw_dataset.ipynb). Download and run it on a local machine to enable the interactive plotting (e.g., zoom in/out for checking the picks).\n\n2. A notebook is available [here](./notebooks/curated_pnw_dataset_seisbench.ipynb) on accessing the dataset with SeisBench APIs.\n\n3. Google Colab [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/17Qu54ZI_HxJjIgLgo9K18-vwpXWoIeYM?usp=sharing)\n\n  If you are more familiar with Google Colab, go to the link above. Note that interactive plotting is not available on Colab.\n\n### Demo sets\n1. A micro version of the dataset, which contains 10 earthquake streams, 10 explosion streams, 10 sonic boom streams, 10 thunder streams, and 10 surface event streams. See [data/microPNW](https://github.com/niyiyu/PNW-ML/tree/main/data/microPNW).\n   \n2. A mini version of the dataset, which contains 500 earthquake streams, 500 explosion streams, 500 surface event streams, 126 sonic boom streams, and 94 thunder quake streams.\n  - waveform (640 MB): [[Google Drive](https://drive.google.com/file/d/1Yq6n8R0sb338OaT0KTwW2XFDb9x_LG6g/view?usp=share_link)] | [[UW-ESS](https://dasway.ess.washington.edu/shared/niyiyu/PNW-ML/miniPNW_waveforms.hdf5)]\n  - metadata (424 KB): [[Google Drive](https://drive.google.com/file/d/1Y0nK6ObBVABuoTopaWRNg2lPqjQsXa7e/view?usp=share_link)] | [[UW-ESS](https://dasway.ess.washington.edu/shared/niyiyu/PNW-ML/miniPNW_metadata.csv)]\n\n3. A meso version with 10% of the full ComCat dataset (only earthquake + explosion).\n  - waveform (6.3 GB): [[Google Drive](https://drive.google.com/file/d/1SrbiQpBpU6mPq5Un_lJpPcfBekwczLzp/view?usp=share_link)] | [[UW-ESS](https://dasway.ess.washington.edu/shared/niyiyu/PNW-ML/mesoPNW_waveforms.hdf5)]\n  - metadata (4.7 MB): [[Google Drive](https://drive.google.com/file/d/1HK2AuWPQj3dCdKShYcJ7a5E577XASrab/view?usp=share_link)] | [[UW-ESS](https://dasway.ess.washington.edu/shared/niyiyu/PNW-ML/mesoPNW_metadata.csv)]\n\n## Metadata\n| Attribute | Description | Example |\n| ----------- | ----------- |-------|\n| event_id | Event identifier | uw10564613 |\n| source_origin_time | Source origin time in UTC | 2002-10-03T01:56:49.530000Z |\n| source_latitude_deg | - | 48.553 |\n| source_longitude_deg | - | -122.52 |\n| source_type | - | earthquake |\n| source_type_pnsn_label | PNSN AQMS event type | eq |\n| source_depth_km | - | 14.907 |\n| source_magnitude_preferred | - | 2.1 |\n| source_magnitude_type_preferred | - | Md |\n| source_magnitude_uncertainty_preferred | - | 0.03 |\n| source_local/duration/hand_magnitude | Ml, Md, and Mh if available | 1.32 |\n| source_local/duration_magnitude_uncertainty | magnitude uncertainty if available | 0.15 |\n| source_depth_uncertainty_km | - | 1.69 |\n| source_horizontal_uncertainty_km | - |0.694 |\n| station_network_code | FDSN network code | UW |\n| station_code | FDSN station code | GNW |\n| station_location_code | FDSN location code | 01 |\n| station_channel_code | FDSN channel code (first two digits) | BH |\n| station_latitude_deg | - | 47.5641 |\n| station_longitude_deg | - | -122.825 |\n| station_elevation_m | - | 220.0 |\n| trace_name | Bucket and array index | bucket1\\$0,:3:15001 |\n| trace_sampling_rate_hz | All traces resampled to 100 Hz | 100 |\n| trace_start_time |  Trace start time in UTC | 2002-10-03T01:55:59.530000Z |\n| trace_P/S_arrival_sample | Closest sample index of arrival  | 8097 |\n| trace_P/S_arrival_uncertainty_s | Picking uncertainty in second |  0.02 |\n| trace_P/S_onset | - |  emergent |\n| trace_P_polarity | P-wave onset polarity | positive, negative, or undecidable |\n| trace_has_offset | Any visible offset in the trace | 1 |\n| trace_missing_channel | Number of missing channel of the trace | 2 |\n| trace_snr_db | SNR for each component |  6.135|3.065|11.766 |\n\n## Reference\nNi, Y., Hutko, A., Skene, F., Denolle, M., Malone, S., Bodin, P., Hartog, R., & Wright, A. (2023). Curated Pacific Northwest AI-ready Seismic Dataset. *Seismica*, 2(1). https://doi.org/10.26443/seismica.v2i1.368\n\nBiBTex:\n```bibtex\n@article{ni2023pnw, \n  title={Curated Pacific Northwest AI-ready Seismic Dataset}, \n  volume={2}, \n  url={https://seismica.library.mcgill.ca/article/view/368}, \n  number={1}, \n  journal={Seismica}, \n  author={Ni, Yiyu and Hutko, Alexander and Skene, Francesca and Denolle, Marine and Malone, Stephen and Bodin, Paul and Hartog, Renate and Wright, Amy}, \n  year={2023}, \n  month={05},\n  doi={10.26443/seismica.v2i1.368}\n}\n```\n\n## Known issues and updates\n* [August 2023] Very few events (~15) in the ComCat dataset may have inconsistent `event_type_pnsn_label` and `event_type`. This issue comes from the outdated ComCat event metadata. Please prioritize PNSN label when such inconsistent occurs.\n* [June 2025] The `trace_start_time` field in the exotic metadata was delayed by 50 seconds. The metadata has now been corrected for all affected files.\n* [November 2025] 4970 (~2.7%) and 445 (~0.8%) data samples are removed from the comcat and noise dataset, respectively. These samples contain one or more undocumented events (see [Suarez and Beroza (2025)](https://arxiv.org/abs/2511.09805) for more information). Note that these smaples are only removed in the metadata (csv indexing file).\n\n## Report bugs\nIf you find any issue in the dataset, please report through [GitHub Issue](https://github.com/niyiyu/PNW-ML/issues) or [Email](mailto:niyiyu@uw.edu). \n",
        "createdAt": "2022-03-15T07:01:48.000Z",
        "updatedAt": "2025-11-28T14:59:23.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/niyiyu/PNW-ML/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "phyrros/seismology_permafrost",
        "url": "https://github.com/phyrros/seismology_permafrost",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2024-08-20T20:11:31.000Z",
        "updatedAt": "2024-08-21T16:23:14.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "JoseMariaGarciaMarquez/sismolab",
        "url": "https://github.com/JoseMariaGarciaMarquez/sismolab",
        "description": "SISMOLAB is an interactive application for analyzing seismological data using Tkinter.",
        "stars": 2,
        "forks": 0,
        "readme": "### SISMOLAB\n\n**SISMOLAB** is an interactive application for analyzing seismological data using Tkinter.\n\n#### Features\n\n- **Load Data** 📂: Import seismogram files from your computer.\n- **Interactive Visualization** 📊: Display Z, N, and E components with zoom and pan capabilities.\n- **Record Amplitudes and Times** 🖱️: Use the picking mode to register clicks on graphs and save amplitudes and times to a CSV file.\n- **Distance Calculation** 📏: Calculate the distance between the earthquake origin and the station using P and S wave times.\n- **Magnitude Calculation** 📐: Compute Moment Magnitude (Mw) and Energy Magnitude (Me).\n- **Integrated Terminal** 💻: Display messages and results within the application interface.\n\nSISMOLAB simplifies the analysis and visualization of seismological data, aiding in the study of earthquakes and interpretation of seismographic data.\n\n![_eef4fcc7-f5b5-4f95-9d9f-d8b7f90878df](https://github.com/JoseMariaGarciaMarquez/sismolab/assets/30852961/fc80f1ba-020a-48e5-95de-c184f4f27e02)\n\n#### Usage\n\n##### Main window:\nThe main window has buttons:\n- **Load Data**: Opens a file dialog to select a seismogram file (MiniSEED format) from your computer and loads it for analysis.\n- **Picking**: Toggles the picking mode, allowing you to click on the graph to record amplitude and time values.\n- **Distance**: Calculates the distance between the earthquake origin and the station based on the P and S wave arrival times.\n- **Mw**: Computes the Moment Magnitude (Mw) based on the recorded amplitudes.\n- **ME**: Computes the Energy Magnitude (Me) based on the recorded amplitudes.\n- **Maps**: Generates a map with the locations of the seismic stations and the estimated epicenter.\n- **+ Gain**: Increases the gain (amplification) of the seismogram plot.\n- **- Gain**: Decreases the gain (amplification) of the seismogram plot.\n- **Reset Gain**: Resets the gain to the default value.\n\n![main_window](https://github.com/JoseMariaGarciaMarquez/sismolab/assets/30852961/b38f9565-3245-4918-8754-443c261b36b9)\n\n#### Load Data\n\nClick the **Load Data** button to open a file dialog. Select a seismogram file in MiniSEED format. The application will load the file and display the Z, N, and E components of the seismogram in separate plots.\n\n![load](https://github.com/JoseMariaGarciaMarquez/sismolab/assets/30852961/71cd4b37-4788-40f1-b1e7-2714cfd57a74)\n\n#### Picking Mode\n\nToggle the picking mode by clicking the **Picking** button. In this mode, you can click on the seismogram plots to record the amplitude and time of specific points. The recorded data is saved to a CSV file.\n\n![pick_p](https://github.com/JoseMariaGarciaMarquez/sismolab/assets/30852961/609d0638-6c92-4887-b952-0c2ff17cefeb)\n![pick_ps](https://github.com/JoseMariaGarciaMarquez/sismolab/assets/30852961/7ca1d808-7447-412d-b719-28de94309fd0)\n\n#### Distance Calculation\n\nClick the **Distance** button to calculate the distance between the earthquake origin and the station. This is done by using the recorded P and S wave arrival times. The distance is calculated using the formula:\n\n\\[ \\text{Distance} = \\frac{\\Delta t}{\\left(\\frac{1}{V_s} - \\frac{1}{V_p}\\right)} \\]\n\nwhere \\(\\Delta t\\) is the difference between the P and S wave arrival times, \\(V_s\\) is the S wave velocity, and \\(V_p\\) is the P wave velocity.\n\n#### Magnitude Calculation\n\n- **Mw (Moment Magnitude)**: Click the **Mw** button to calculate the Moment Magnitude. The formula used is:\n\n\\[ M_w = \\frac{2}{3} \\left( \\log_{10}(M_0) - 9.1 \\right) \\]\n\nwhere \\(M_0\\) is the seismic moment calculated from the recorded amplitudes.\n\n- **ME (Energy Magnitude)**: Click the **ME** button to calculate the Energy Magnitude. The formula used is:\n\n\\[ M_E = \\frac{2}{3} \\log_{10}(E_s) - 8.45 \\]\n\nwhere \\(E_s\\) is the seismic energy calculated from the recorded amplitudes.\n\n![me](https://github.com/JoseMariaGarciaMarquez/sismolab/assets/30852961/6a5306be-5d68-4375-8d34-6fdbcb3b24ad)\n\n#### Map Generation\n\nClick the **Maps** button to generate a map showing the locations of the seismic stations and the estimated epicenter. The map is created using the Folium library and saved as an HTML file.\n\n![mapgenerator](https://github.com/JoseMariaGarciaMarquez/sismolab/assets/30852961/180d840b-2799-40af-b18b-6a0b83dd1aa0)\n\n![mapshot](https://github.com/JoseMariaGarciaMarquez/sismolab/assets/30852961/0a35b7fb-a093-4a8f-9579-6c2dcb35577f)\n\n\n",
        "createdAt": "2024-07-03T16:31:35.000Z",
        "updatedAt": "2024-10-11T17:30:19.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/JoseMariaGarciaMarquez/sismolab/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "fabian-kutschera/WP5_Seismology",
        "url": "https://github.com/fabian-kutschera/WP5_Seismology",
        "description": "Theoretical Seismology",
        "stars": 0,
        "forks": 0,
        "readme": "# WP5 Seismology\n## Theoretical Seismology\n\nThis course is part of the International Masters Programme in Geophysics at LMU and TUM.\n\nTo start Binder, click on the botton below:\n\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/fabian-kutschera/WP5_Seismology/main)\n\nTo access class material go to [Nextcloud](https://wolke.geophysik.uni-muenchen.de/s/FLn68bw4y4T9d3x).\n\nLearn how to deal with submodules: https://git-scm.com/book/de/v2/Git-Tools-Submodule\n",
        "createdAt": "2021-11-10T13:19:01.000Z",
        "updatedAt": "2021-12-17T11:42:06.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/fabian-kutschera/WP5_Seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "KonovDS/neural_earthquake",
        "url": "https://github.com/KonovDS/neural_earthquake",
        "description": "Neural network approach to seismological inverse problem.",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2022-04-22T01:02:25.000Z",
        "updatedAt": "2022-04-22T01:21:29.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ispingos/pytheas-splitting",
        "url": "https://github.com/ispingos/pytheas-splitting",
        "description": "Home of the Pytheas software for local shear-wave splitting analysis",
        "stars": 45,
        "forks": 8,
        "readme": "Pytheas: an open-source software solution for local shear-wave splitting studies\n================================================================================\n\n.. image:: https://zenodo.org/badge/181635686.svg\n   :target: https://zenodo.org/badge/latestdoi/181635686\n\n***IMPORTANT*** \n\n   You can find a pre-release which includes: \n\n   * stabler Catalogue Cluster Analysis, including parallelization\n   * fix for the 3D Rotation bug \n   * Updated installation\n\n   The updated code can be found in the `devel <https://github.com/ispingos/pytheas-splitting/tree/devel>`_ branch and related `pre-release <https://github.com/ispingos/pytheas-splitting/releases/tag/v.0.3.0%2Bd14>`_.\n\nPytheas is a tool that aims to introduce a new mentality in shear-wave splitting analysis from local recordings, incorporating manual, semi- and fully- automatic methods under one Graphical User Interface. Integrating databases and offering compatibility with popular data and metadata file formats, Pytheas is designed with the simplification of analysis in mind, while, at the same time, enhanching the effectiveness of processing and quality control of results.\n\nAlong with the program itself, you can find the following files:\n\n* *docs:* Contains a detailed User's Guide as well as other relevant documentation.\n* *acquisition_Kscripts:* Scripts for acquiring event data and metadata, as well as station metadata from EIDA and IRIS FDSN services.\n* *example:* Sample waveforms, catalogues, station file and velocity model for trying out the program.\n\nPytheas is released under the GNU GPLv3 license.\n\nAuthors: Spingos I. & Kaviris G. (c) 2019-2024\n\nSpecial thanks to Millas C. for testing the software and providing valuable feedback from the very early stages of this endeavor!\n\nFor any issues, comments or suggestions please contact us at pytheas.splitting@gmail.com or through `GitHub <https://www.github.com/ispingos/pytheas-splitting>`_.\n\n\n**Demonstrations**\n\nPytheas on `YouTube <https://www.youtube.com/channel/UC7USfZT9PfnNTNqMiY1AgTg>`_\n\nShort hands-on lecture about Pytheas in `CRL School 2020 <https://www.youtube.com/watch?v=cUB5qNdUFh0>`_.\n\n**How to cite**\n\n   Spingos, I., Kaviris, G., Millas, C., Papadimitriou, P., Voulgaris, N., 2020. \n   Pytheas: An open-source software solution for local shear-wave splitting studies. Comput. Geosci. 134, 104346. \n   doi: 10.1016/j.cageo.2019.104346\n\nThe published article can be accessed through `Elsevier <https://www.sciencedirect.com/science/article/pii/S0098300419303784>`_.\n",
        "createdAt": "2019-04-16T07:16:15.000Z",
        "updatedAt": "2025-10-28T01:49:46.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ispingos/pytheas-splitting/master/README.rst",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "tomgarth/ISC_Earthquake_Toolbox",
        "url": "https://github.com/tomgarth/ISC_Earthquake_Toolbox",
        "description": "A set of MATLAB functions for downloading and handling earthquake data from the International Seismological Centre (ISC)",
        "stars": 13,
        "forks": 0,
        "readme": "[![View the ISC Earthquake Toolbox on File Exchange](https://www.mathworks.com/matlabcentral/images/matlab-file-exchange.svg)](https://www.mathworks.com/matlabcentral/fileexchange/167786-isc-earthquake-toolbox?s_tid=srchtitle)\n\n# ISC Earthquake Toolbox for MATLAB\nA set of MATLAB functions for downloading and handling earthquake data from the [International Seismological Centre (ISC)](http://www.isc.ac.uk/).\nThe toolbox contains a number of MATLAB Live Scripts to get you started. It requires [MATLAB](https://uk.mathworks.com/products/matlab.html) and [Mapping Toolbox](https://uk.mathworks.com/products/mapping.html?requestedDomain=).\n\nA brief overview of the functionality is given in the MATLAB Live Script **'Introduction_ISCToolboxForMATLAB.mlx'**. [![Open in MATLAB Online](https://www.mathworks.com/images/responsive/global/open-in-matlab-online.svg)](https://matlab.mathworks.com/open/github/v1?repo=tomgarth/ISC_Earthquake_Toolbox&file=Introduction_ISCToolboxForMATLAB.mlx)\n\nMore detailed examples are then given in 3 further Live Scripts detailed below.\n\n- The first **(ISCtoolbox_WB1_Download_ISC_Data.mlx)** is a MATLAB Live Script that allows you to interactively search the ISC \ndatabase, and build a data set based on the area, time span and magnitude range you are interested in. [![Open in MATLAB Online](https://www.mathworks.com/images/responsive/global/open-in-matlab-online.svg)](https://matlab.mathworks.com/open/github/v1?repo=tomgarth/ISC_Earthquake_Toolbox&file=ISCtoolbox_WB1_Download_ISC_Data.mlx)\n\n- The second **(ISCtoolbox_WB2_Exploring_Earthquake_Locations.mlx)** gives an overview of the data that can be downloaded (either \nby you in the previous workbook, or one of the distributed example datasets) with a focus on plotting earthquake locations. [![Open in MATLAB Online](https://www.mathworks.com/images/responsive/global/open-in-matlab-online.svg)](https://matlab.mathworks.com/open/github/v1?repo=tomgarth/ISC_Earthquake_Toolbox&file=ISCtoolbox_WB2_Exploring_Earthquake_Locations.mlx)\n\n- The third **(ISCtoolbox_WB3_Exploring_Earthquake_Magnitudes.mlx)** introduces and compares different earthquake magnitude types, \nand enables a simple interactive Mc and b-value calculations. [![Open in MATLAB Online](https://www.mathworks.com/images/responsive/global/open-in-matlab-online.svg)](https://matlab.mathworks.com/open/github/v1?repo=tomgarth/ISC_Earthquake_Toolbox&file=ISCtoolbox_WB3_Exploring_Earthquake_Magnitudes.mlx)\n\nFor more information on the ISC Toolbox for MATLAB please visit www.isc.ac.uk/projects/matlab\n\nOr email earthquake-toolbox@isc.ac.uk with questions or to be added to the mailing list.\n",
        "createdAt": "2023-11-30T12:31:04.000Z",
        "updatedAt": "2025-11-17T14:29:01.000Z",
        "language": "MATLAB",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/tomgarth/ISC_Earthquake_Toolbox/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "stephenhernandez/paramo-matlab",
        "url": "https://github.com/stephenhernandez/paramo-matlab",
        "description": "Various Matlab routines for Stephen's daily seismological work flow",
        "stars": 0,
        "forks": 0,
        "readme": "# paramo-matlab\nVarious Matlab routines, dubbed Páramo-Matlab, essential to Stephen's daily \nseismological workflow.\n\nThe name \"páramo\" refers to the tropical alpine tundra common in the \nEcuadorian Andes. Since most of this code was written during my tenure at the\nInstituto Geofísico in Quito, Ecuador, I find/found inspiration from the spare \nbut efficient ecosystem characteristic of the páramo. It is my hope that the \nPáramo code will live up to the properties of its namesake.\n",
        "createdAt": "2019-07-23T13:16:24.000Z",
        "updatedAt": "2019-07-23T22:09:56.000Z",
        "language": null,
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/stephenhernandez/paramo-matlab/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ScaDS/ai4seismology-2025",
        "url": "https://github.com/ScaDS/ai4seismology-2025",
        "description": "Resources and materials for the AI4Seismology Training School (see: https://scads.ai/event/international-training-school-ai-4-seismology/)",
        "stars": 8,
        "forks": 7,
        "readme": "The contents of this repository can be viewed at this URL:\r\n\r\nhttps://scads.github.io/ai4seismology-2025/\r\n",
        "createdAt": "2025-04-27T13:46:58.000Z",
        "updatedAt": "2025-07-28T20:46:23.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ScaDS/ai4seismology-2025/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "yasuit21/AzimPy",
        "url": "https://github.com/yasuit21/AzimPy",
        "description": "Estimate horizontal orientation of ocean-bottom seismograph",
        "stars": 1,
        "forks": 0,
        "readme": "# AzimPy\nEstimate horizontal orientation of ocean-bottom seismograph\n\n![PyPI](https://img.shields.io/pypi/v/AzimPy)\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/AzimPy)\n![PyPI - Format](https://img.shields.io/pypi/format/AzimPy)\n![PyPI - Status](https://img.shields.io/pypi/status/AzimPy)\n![GitHub tag (latest by date)](https://img.shields.io/github/v/tag/yasuit21/AzimPy)\n![GitHub release (latest by date)](https://img.shields.io/github/v/release/yasuit21/AzimPy)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7184189.svg)](https://doi.org/10.5281/zenodo.7184189)\n\nCopyright (c) 2022–2024 Yasunori Sawaki[![ORCID](https://orcid.org/sites/default/files/images/orcid_16x16.png)](https://orcid.org/0000-0002-4043-3391) All rights reserved.\n\n[`AzimPy`](https://github.com/yasuit21/AzimPy) is an open-source Python package for estimating the horizontal orientation of ocean-bottom seismographs. \nThis package performs the Rayleigh-wave polarization method (e.g., Stachnik+ 2012; Doran & Laske 2017). \nOne of the main classes `OrientOBS`, inherited from [`obspy.clients.fdsn.Client`](https://docs.obspy.org/packages/autogen/obspy.clients.fdsn.client.Client.html), searches an earthquake catalog for teleseismic events as a web client, and computes Rayleigh-wave polarizations for each event–station pair. \nThis package also provides other classes (e.g., `OrientSingle`, `OrientAnalysis`) and functions for statistical analysis of circular data and plotting the estimated azimuths with uncertainties.\n\n## Terms of use\n- Cite an article below (Sawaki et al., 2023, GJI) and a Zenodo DOI for the specific version of `AzimPy` when you publish your reseach or make a presentation. The DOI representing the specific version is probably found through [the Zenodo page for the latest version](https://doi.org/10.5281/zenodo.6972713).\n- This package is under development, so any bug reports and suggestions are welcome! \n\n#### Use cases\n- Sawaki, Y., Yamashita, Y., Ohyanagi, S., Garcia, E.S.M., Ito, A., Sugioka, H., Takahashi, T., Shinohara, M., & Ito, Y. (2023), Seafloor depth controls seismograph orientation uncertainty, *Geophys. J. Int.*, 232(2), 1376–1392, https://doi.org/10.1093/gji/ggac397\n\n## How to install\n\n### [Recommended] Install `AzimPy` from `PyPI` in a new `conda` environment \n\nYou may replace `mamba` with `conda`.\n\n```\n$ mamba create -n azimpy-test python=3.10 jupyter astropy \"matplotlib>=3.5\" \"scipy>=1.4\" pandas numpy tqdm\n$ mamba activate azimpy-test\n(azimpy-test) $ mamba install -c conda-forge \"obspy>=1.3\" \n(azimpy-test) $ python -m pip install AzimPy\n```\n\n- [Alternative] `pip install` locally in the environment \n\n```\n$ mamba create -n azimpy-test python=3.10 jupyter\n$ mamba activate azimpy-test\n(azimpy-test) $ git clone -b v0.3.0 https://github.com/yasuit21/AzimPy.git\n(azimpy-test) $ cd AzimPy\n(azimpy-test) $ python -m pip install .\n```\n\n### Optional installation : [`rpy2`](https://rpy2.github.io/)\n\n#### Installation of `R`\n\n```\n(azimpy-test) $ mamba install r-essentials r-base r-circular\n```\nNote that this installation will take time.\n\nThen, set environent variables.\n```\nexport R_HOME=/path/to/envs/azimpy-test/lib/R\nexport R_USER=/path/to/envs/azimpy-test/lib/python3.9/site-packages/rpy2\n```\n\n#### Installation of `rpy2` in `PyPI`\n```\n(azimpy-test) $ python -m pip install rpy2\n(azimpy-test) $ python -c \"import azimpy\"\n```\nIf no warning or error is returned, the installation has been completed.\n\n\n## Usage\n\n### Compute Rayleigh-wave polarization\n\n```python\nimport obspy as ob\nfrom azimpy import OrientOBS\n\n## Initialize web client\n## Specity the timezone of recording data\nobs = OrientOBS(base_url='USGS', timezone=9)\n\n## Query earthquake event catalog\nobs.query_events(\n    starttime=ob.UTCDateTime('20200401000000'),\n    endtime=ob.UTCDateTime('20201001000000'),\n    minmagnitude=6.0,\n    maxdepth=100,\n    orderby='time-asc',\n)\n\n## Compute Rayleigh-wave polarization for each event\n## Raw SAC data should be located in '/path/to/datadir'\nobs.find_stream(\n    '/path/to/datadir',\n    output_path='/path/to/output/stationA1',\n    polezero_fpath='/path/to/polezero/hoge.paz',\n    fileformat=\"sac\",\n    filenameformat=f'*.*.%y%m%d%H%M.sac',\n    freqmin=1./40, freqmax=1./20,\n    max_workers=4,\n    vel_surface=4.0,\n    time_before_arrival=-20.0,\n    time_after_arrival=600.0,\n    distmin=5., distmax=120.,\n    read_func=ob.read\n)\n```\nNote that `fileformat` was renamed as `filenameformat` in `v0.3.0`. `fileformat` denotes the data format of the records. \nAlso, a user-defined read function can be incorpolated in `v0.3.0`. Specify the function in `read_func` argument. This would allow us to read data recorded by local formats such as `WIN/WIN32`, which are not supported by the `ObsPy`'s read function. \n\nThen, the output dataframe will be pickled as `stationA1_020_040.pickle` under `/path/to/output/stationA1` directory. The pickled dataframe can be loaded by `pd.read_pickle()`.\n\n### Perform circular statistics and make a plot\n\n#### Single station\n\nThe example uses a single station `stationA1`.\n\n1. Perform analysis and save as pickled data\n    ```python\n    import pandas as pd\n    from azimpy import OrientSingle, read_chtbl\n\n    ## Init params\n    min_CC = 0.5\n    alpha_CI = 0.05  ## 100(1-a)% CI\n    bootstrap_iteration = 5000\n\n    ## The output dataframe of orientations\n    df_orient = pd.read_pickle(\n        '/path/to/output/stationA1/stationA1_020_040.pickle'\n    )\n\n    ## Init OrientSingle for circular statistics\n    orientsingle_raw = OrientSingle(\n        df_orient, 'stationA1', \n        if_selection=False,  # w/o bootstrap analysis\n        min_CC=min_CC, weight_CC=True,\n    )\n    orientsingle_boot = OrientSingle(\n        df_orient, 'stationA1', \n        if_selection=True,  # perform bootstrap analysis\n        min_CC=min_CC, weight_CC=True, K=5.0,\n        bootstrap_iteration=bootstrap_iteration, alpha_CI=alpha_CI\n    )\n    ## Save orientsingle objects as pickled data\n    orientsingle_raw.write_obj(\n        '/path/to/output/orientsingle/raw/stationA1_020_040.pickle'\n    )\n    orientsingle_boot.write_obj(\n        '/path/to/output/orientsingle/boot/stationA1_020_040.pickle'\n    )\n    ```\n1. Plot the result\n    ```py\n    ## Load orientsingle objects\n    ## You may skip this part\n    orientsingle_raw = OrientSingle.load_obj(\n        '/path/to/output/orientsingle/raw/stationA1_020_040.pickle'\n    )\n    orientsingle_boot = OrientSingle.load_obj(\n        '/path/to/output/orientsingle/boot/stationA1_020_040.pickle'\n    )\n\n    ## Init a figure with subfigures\n    fig = plt.figure(figsize=[8,4])\n    subfigs = fig.subfigures(nrows=1, ncols=2).flatten()\n\n    ## Plot for `orientsingle_raw`\n    orientsingle_raw.plot(\n        polar=True, \n        fig=subfigs[0], in_parentheses='BB',\n        add_cbar=True\n    )\n    subfigs[0].legend(loc=1, bbox_to_anchor=(1,1.15), fontsize='small')\n\n    ## Plot for `orientsingle_boot`\n    orientsingle_boot.plot(\n        fig=subfigs[1], in_parentheses='BB',\n    )\n    subfigs[1].legend(loc=1, bbox_to_anchor=(1,1.15), fontsize='small')\n\n    ## Show or save the figure\n    fig.savefig('/path/to/fig/stationA1_020_040.png')\n    plt.show()\n    ```\n![](./images/sample/single.png)\n\n#### Multiple stations\nThe example uses multiple stations whose names are `stationAX`.\n\n1. Initialize `OrientAnalysis`\n    ```python\n    from azimpy import OrientAnalysis\n\n    stationList = ['stationA1','stationA2','stationA3','stationA4']\n\n    ## Channeltable including above stations' info\n    df_chtbl = read_chtbl('/path/to/channeltable.txt')\n    df_chtbl = df_chtbl.query('comp.str.endswith(\"U\")')\n\n    ## Init OrientAnalysis for circular statistics\n    oa_raw = OrientAnalysis(\n        if_selection=False,  # w/o bootstrap analysis\n        df_chtbl=df_chtbl, \n        min_CC=min_CC, \n    )\n    oa_boot = OrientAnalysis(\n        if_selection=True,  # perform bootstrap analysis\n        df_chtbl=df_chtbl, \n        min_CC=min_CC, alpha_CI=alpha_CI, \n        bootstrap_iteration=bootstrap_iteration, \n    )\n    ```\n1. Store the analyzed data or perform analysis\n    - If storing the orientation data by `OrientSingle` \n        ```py\n        for stationName in stationList:\n            period = df_chtbl.at[stationName,'period']\n\n            ## Add the dataframe in `oa_raw`\n            oa_raw.add_station(\n                orientsingle_path=f'/path/to/output/orientsingle/raw/{stationName}_020_040.pickle',\n                period=period,\n            )\n            oa_boot.add_station(\n                orientsingle_path=f'/path/to/output/orientsingle/boot/{stationName}_020_040.pickle',\n                period=period,\n            )\n        ```\n    - If performing analysis\n        ```py\n        for stationName in stationList:\n            period = df_chtbl.at[stationName,'period']\n            df_orient = pd.read_pickle(\n                f'/path/to/output/{stationName}/{stationName}_020_040.pickle'\n            )\n            ## Add the dataframe in `oa_raw`\n            ## This is actually passed to `OrientSingle`\n            oa_raw.add_station(\n                df_orient, stationName, \n                period=period\n            )\n            ## Add the dataframe in `oa_boot`\n            oa_boot.add_station(\n                df_orient, stationName, \n                period=period\n            )\n        ```\n1. Plot the results using `matplotlib.pyplot`\n    - Original results w/o bootstrap resampling\n    ```python\n    fig = oa_raw.plot()\n    ```\n    - Results of bootstrap analysis\n    ```python\n    fig = oa_boot.plot()\n    ```\n1. Save the results\n    ```py\n    ## Write dataframe as csv, json, or pickle\n    df_analysis = oa_boot.write(\n        \"/path/to/output/orientation/StationAX_020_040.csv\",\n        networkname='StationAX',\n        format='csv'\n    )\n    ```\n\n### How to read the result CSV file\n\n- Saved dataframe can be loaded as\n    ```py\n    from azimpy import read_result\n\n    df_analysis = read_result(\n        \"/path/to/output/orientation/StationAX_020_040.csv\"\n    )\n    ```\n- The column `station` is indexed\n- The estimated orientation is in the column `circular mean`. `circular_mean` and `h1azimuth` are aliases for `circular mean`.\n    ```py\n    df_analysis.h1azimuth\n    ```\n- The uncertainty is in the column `Half 95%CI`. `uncertainty` is the alias for `Half 95%CI`.\n    ```py\n    df_analysis.uncertainty\n    ```\n\n\n## Note\n- `SAC` format is only supported, but you may use some other formats. Specify the function in `read_func` argument for `OrientOBS..find_stream()`.\n- The observed data files must be located in one directory, where `OrientOBS.find_stream()` will try to search for necessary input files. No waveform data in websites and repository are available in this package at this moment.\n- The author has tested this package in `Linux` environments (`CentOS 7` and `WSL Ubuntu 20.04`), so it might be incompatible when installed in `Windows`.\n- `rpy2` is an optional wrapper to run [`circular`](https://www.rdocumentation.org/packages/circular) in `R` language, which performs the Kuiper test.\n\n\n### References\n- Sawaki, Y., Yamashita, Y., Ohyanagi, S., Garcia, E.S.M., Ito, A., Sugioka, H., Takahashi, T., Shinohara, M., & Ito, Y., 2023, Seafloor Depth Controls Seismograph Orientation Uncertainty, *Geophys. J. Int.*, 232(2), 1376–1392, https://doi.org/10.1093/gji/ggac397\n- Stachnik, J.C., Sheehan, A.F., Zietlow, D.W., et al., 2012, Determination of New Zealand ocean bottom seismometer orientation via Rayleigh-wave polarization. *Seismol. Res. Lett.*, 83, 704–713. https://doi.org/10.1785/0220110128 \n- Doran, A.K. & Laske, G., 2017, Ocean‐bottom deismometer instrument orientations via automated Rayleigh‐wave arrival‐angle measurements. *Bull. Seismol. Soc. Am.*, 107, 691–708. https://doi.org/10.1785/0120160165 \n- Takagi, R., Uchida, N., Nakayama, T., et al., 2019, Estimation of the orientations of the S‐net cabled ocean‐bottom sensors. *Seismol. Res. Lett.*, 90, 2175–2187. https://doi.org/10.1785/0220190093\n- [Concept DOI for the latest `AzimPy`: `10.5281/zenodo.6972713`](https://doi.org/10.5281/zenodo.6972713)\n    \n## Acknowledgments\n\nThis package makes use of [`ObsPy>=1.3.0`](https://github.com/obspy/obspy) for [FDSN web client services](https://www.fdsn.org/webservices/) and processing seismograms.\n\n\n## License\n\nThis project is licensed under the MIT License, see the `LICENSE` for details.\n",
        "createdAt": "2022-08-05T06:53:07.000Z",
        "updatedAt": "2025-02-05T12:45:30.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.7184189",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.7184189",
            "dataCite": "10.5281/zenodo.7184189",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/yasuit21/AzimPy/main/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.7184189",
            "title": "yasuit21/AzimPy: v0.2.0",
            "journal": "Zenodo",
            "dateReleased": "2022-10-11T00:00:00.000Z",
            "abstract": "Plotting methods are highly improved in this release. From this release, installing using <code>pip</code> is also available. Copyright (c) 2022 Yasunori Sawaki All rights reserved. <code>AzimPy</code> is an open-source Python package for estimating the horizontal orientation of ocean-bottom seismographs.<br> This package performs the Rayleigh-wave polarization method (e.g., Stachnik+ 2012; Doran &amp; Laske 2017).<br> One of the main classes <code>OrientOBS</code>, inherited from <code>obspy.clients.fdsn.Client</code>, searches an earthquake catalog for teleseismic events as a web client and computes Rayleigh-wave polarizations for each event–station pair.<br> This package also provides other classes (e.g., <code>OrientSingle</code>, <code>OrientAnalysis</code>) and functions for statistical analysis of circular data and plotting the estimated azimuths with uncertainties. Terms of use Cite an article below (Sawaki et al., in press) and a Zenodo DOI for the specific version of <code>AzimPy</code> when you publish your reseach or make a presentation. The DOI representing the specific version is probably found through the Zenodo page for the latest version. This package is under development, so any bug reports and suggestions are welcome! Use cases Sawaki, Y., Yamashita, Y., Ohyanagi, S., Garcia, E.S.M., Ito, A., Sugioka, H., Takahashi, T., Shinohara, M., &amp; Ito, Y., Seafloor Depth Controls Seismograph Orientation Uncertainty, <em>Geophys. J. Int.</em>, in press, https://doi.org/10.1093/gji/ggac397",
            "citationsArray": []
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Shengyan-Pan/utaipei_seismology",
        "url": "https://github.com/Shengyan-Pan/utaipei_seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# utaipei_seismology",
        "createdAt": "2020-08-18T16:43:04.000Z",
        "updatedAt": "2020-08-18T16:54:46.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Shengyan-Pan/utaipei_seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "trgy0/Application_of_deepdenoiser_to_global_seismology",
        "url": "https://github.com/trgy0/Application_of_deepdenoiser_to_global_seismology",
        "description": "This study is part of an internship in LGL-TPE lab. ",
        "stars": 1,
        "forks": 0,
        "readme": "# Application_of_deepdenoiser_to_global_seismology\nThis study is part of an internship in LGL-TPE lab. \n",
        "createdAt": "2022-05-30T13:05:00.000Z",
        "updatedAt": "2023-10-25T07:40:51.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/trgy0/Application_of_deepdenoiser_to_global_seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "fmagrini/seislib",
        "url": "https://github.com/fmagrini/seislib",
        "description": "Seismic imaging at local, regional, and global scale",
        "stars": 93,
        "forks": 26,
        "readme": "<p align=\"center\">\n  <img width=\"90%\" src=\"https://github.com/fmagrini/seislib/raw/master/docs/source/_static/seislib_logo.png\">\n</p>\n\n[![PyPI version](https://img.shields.io/pypi/v/seislib?logo=pypi&style=flat-square&color=cae9ff&labelColor=f8f9fa)](https://pypi.org/project/seislib/)\n[![Documentation Status](https://img.shields.io/readthedocs/seislib?logo=readthedocs&style=flat-square&color=fed9b7&labelColor=f8f9fa&logoColor=eaac8b)](https://seislib.readthedocs.io/en/latest/?badge=latest)\n\nSeisLib is a Python package that allows for obtaining seismic images of the sub-surface from the local to the global scale. It is the result of a long-term effort of our team to make efficient and open source some of the Python codes behind our seismological publications over the last few years. The library is in rapid expansion and, at present, includes:\n\n\n## **Seismic Ambient Noise Interferometry**\n*  Automated download of continuous seismograms\n* Fast cross-correlation of continuous seismograms in the frequency domain\n* Extraction of frequency-dependent phase velocities for both Rayleigh and Love waves based on pairs of receivers\n* Retrieval of frequency-dependent Rayleigh-wave attenuation coefficient based on dense seismic arrays\n\n## **Surface-Wave Tomography based on Teleseismic Earthquakes**\n* Automated download of seismograms recording strong earthquakes\n* Retrieval of frequency-dependent Rayleigh and Love phase velocities based on pairs of receivers lying on the same great-circle path as the epicentre (Two-Station Method)\n\n## **Least-Squares Imaging of Lateral Variations in Surface-Wave Velocity**\n* Equal-area and regular parameterizations, suited for data sets collected at local, regional, and global scale\n* Adaptive parameterizations, with finer resolution in the areas characterized by relatively high density of measurements\n* Linearized inversion of velocity measurements based on ray theory\n* Computational speed optimized (via Cython) for very large data sets\n* Possibility to perform L-curve analyses and resolution tests (e.g., spike, checkerboard)\n\n<p align=\"center\">\n  <img width=\"100%\" src=\"https://github.com/fmagrini/seislib/raw/master/docs/source/_static/lib_diagram.png\">\n</p>\n\n<p>&nbsp;</p>\n\n## **Documentation**\n\nFor more information on SeisLib, make sure to visit our [wiki page](https://seislib.readthedocs.io/en/latest/)!\n\n<p>&nbsp;</p>\n\n## **Installation**\n\nFirst, make sure you have all the dependencies installed, i.e., ``obspy``, ``cartopy``, ``cython``, and ``cmcrameri``. We recommend installing such dependences using conda (see below). You will also need ``gcc`` or equivalent, to compile the cython parts of the library.\n\n```bash\nconda create -n seislib python=3.9 numpy=1.20\nconda activate seislib\nconda install -c conda-forge obspy\nconda install -c conda-forge cartopy\nconda install -c anaconda cython\n```\n\n\nOnce the above dependences have been installed, you can proceed with the installation of ``seislib``: \n\n```\npip install seislib\n```\n\nIf you run into troubles with the above, you can try the following approach:\n```\ngit clone https://github.com/fmagrini/seislib.git\ncd seislib/seislib/tomography/_ray_theory\npython setup_all.py build_ext --inplace\n```\nThe last command will compile the Cython files. If you work on an anaconda environment, you might need to replace \"python\" with, e.g., \"/home/your_name/anaconda3/bin/python\". (You can retrieve the path to your python executable by typing \"import sys; print(sys.executable)\" in your Python GUI. Make sure to then add ~/seislib to your path to being able to import its modules in your Python codes.\n\n\n<p>&nbsp;</p>\n\n## **References**\nSpecific to the Python package:\n- Magrini, F., Lauro, S., Kästle, E. & Boschi, L., 2022. Surface-wave tomography using SeisLib: a Python package for multi-scale seismic imaging. *Geophys. J. Int.*, ggac236, https://doi.org/10.1093/gji/ggac236\n\nAdditional references depending on the use you made of SeisLib:\n- Boschi, L. & Dziewonski, A.M., 1999. High- and low-resolution images of the Earth's mantle: Implications of different approaches to tomographic modeling. *J. Geophys. Res.*, 104(B11)\n- Boschi, L., Magrini, F., Cammarano, F., & van der Meijde, M. 2019. On seismic ambient noise cross-correlation and surface-wave attenuation. *Geophys. J. Int.*, 219(3), 1568-1589\n- Kästle, E., Soomro, R., Weemstra, C., Boschi, L. & Meier, T., 2016. Two-receiver measurements of phase velocity: cross-validation of ambient-noise and earthquake-based observations. *Geophys. J. Int.*, 207, 1493-1512\n- Magrini, F., Diaferia, G., Boschi, L. & Cammarano, F., 2020. Arrival-angle effects on two-receiver measurements of phase velocity. *Geophys. J. Int.*, 220, 1838-1844\n- Magrini, F. & Boschi, L., 2021. Surface-wave attenuation from seismic ambient noise: numerical validation and application. *J. Geophys. Res.*, 126, e2020JB019865\n- Magrini, F., Boschi, L., Gualtieri, L., Lekić, V. & Cammarano, F., 2021. Rayleigh‑wave attenuation across the conterminous United States in the microseism frequency band. *Scientific Reports*, 11, 1-9\n\n",
        "createdAt": "2022-01-19T21:01:04.000Z",
        "updatedAt": "2025-12-04T00:13:36.000Z",
        "language": "Jupyter Notebook",
        "homepage": "https://seislib.readthedocs.io/en/latest/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/fmagrini/seislib/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "jhzhang711/OBS_Trans_Inv",
        "url": "https://github.com/jhzhang711/OBS_Trans_Inv",
        "description": "Seismological Bayesian Inversion Tools in Marine Environments.",
        "stars": 1,
        "forks": 0,
        "readme": "# OBS_Trans_Inv\nSeismological Transdimensional Bayesian Inversion Tools for OBS in Marine Environments.\n\n## 📌 \n\nThis repository provides a transdimensional Bayesian inversion tool designed for marine environments. \nThe implementation largely follows the methodology of Bodin et al. (2012), but extends it by accounting for the effect of the overlying water layer on ocean bottom seismometers (OBS). Specifically, the water-layer thickness is incorporated as a prior in the inversion. The treatment of water-layer effects draws on the insights of Akuhara et al. (2023). This extended version has been packaged and archived with a DOI (Zhang, 2025).\n\n\n## 📦 \n\n```\n\n├── codes-inv/         # codes\n├── RFx.obs            # RF Observations\n├── SWD_xxxxx.obs      # Dispersion  Observations\n├── REF_in.mod         # Initial Model for Inversion\n├── inputmodel.txt     # Model Used to Generate Observations\n├── plotResult.sh      # Script to Plot Results \n├── .gitignore         # Git 忽略文件\n├── README.md          # 说明文档\n└── LICENSE            # 许可证\n```\n\n## 🚀 Installation \nType make in the codes-inv directory. Please edit the Makefile in accordance with your environment (i.e., compiler type and libarary paths).\n\n### 🔧 Requirements\n\n```sh\n# for example\nmodule load mpich/3.1.4-gcc4.9.2  \nmodule load fftw/3.3.8-mpi\nmodule load lapack\n```\n\n### 🏃 How to run\n\n```sh\n# for example\nmkdir -p ./OUT\nmpirun ./codes-inv/JointINV > ${STA}.out\n```\n\n## 📜 Quick Guidance\n\nYou can modify the prior information of the water layer in Joint.f90, as well as specify the inversion type, number of data points, weights, iterations, and other parameters. \nAdditionally, you can adjust the prior velocity and velocity ratio information in priorvalue.f90 and priorvpvs.f90.\n\n\n\n## 😄 Reference 😁\n**1. Original Methodology**\n\nAkuhara, T., Yamashita, Y., Ohyanagi,S., Sawaki, Y., Yamada, T., & Shinohara,M. (2023). Shallow low-velocity layer in the Hyuga-nada accretionary prism and its hydrological implications: Insights from a passive seismic array. Journal of Geophysical Research: Solid Earth, 128, e2022JB026298. https://doi.org/10.1029/2022JB026298\n\nBodin, T., M. Sambridge, H. Tkalčić, P. Arroucau, K. Gallagher, and N. Rawlinson (2012), Transdimensional inversion of receiver functions and surface wave dispersion, J. Geophys. Res., 117, B02301, doi:10.1029/2011JB008560.\n\n**2. Modified and Extended Implementation (This Repository)**\n\nFor details of the modified algorithm and its applications, please cite:\n\nZhang, J. (2025). OBS_Trans_Inv: Seismological Transdimensional Bayesian Inversion Tools for OBS in Marine Environments [Computer Program]. Zenodo. https://doi.org/10.5281/zenodo.17614189\n\n**3. Software DOI**\n   \nA permanent archived version of this software is available on Zenodo:\n\nZhang, J. (2025). OBS_Trans_Inv (Version X.X). Zenodo.\nhttps://doi.org/10.5281/zenodo.17614189\n\nPlease cite this DOI when using the code.\n\n\n## 📄 \n\nGNU General Public License \n\n\n\n",
        "createdAt": "2025-03-25T02:07:04.000Z",
        "updatedAt": "2025-11-23T04:19:09.000Z",
        "language": "Fortran",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.17614189",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.17614189",
            "dataCite": "10.5281/zenodo.17614189",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/jhzhang711/OBS_Trans_Inv/main/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.17614189",
            "title": "jhzhang711/OBS_Trans_Inv: OBS_Trans_Inv v1.0.0",
            "journal": "Zenodo",
            "dateReleased": "2025-11-15T00:00:00.000Z",
            "abstract": "The joint inversion code for OBS.",
            "citationsArray": []
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "bgreer/CSSS",
        "url": "https://github.com/bgreer/CSSS",
        "description": "CSS-Seismology",
        "stars": 0,
        "forks": 0,
        "readme": "\nDoes seismology-like things on CSS simulations. Not useful if you don't have a CSS simulation or don't know what helioseismology is.\n",
        "createdAt": "2014-03-26T17:18:06.000Z",
        "updatedAt": "2014-03-27T18:40:30.000Z",
        "language": "FORTRAN",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/bgreer/CSSS/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "tso1257771/RED-PAN",
        "url": "https://github.com/tso1257771/RED-PAN",
        "description": "This is the official implementation of Real-time Earthquake Detection and Phase-picking with multi-task Attention Network",
        "stars": 17,
        "forks": 2,
        "readme": "# RED-PAN\nThis is the official implementation of **Real-time Earthquake Detection and Phase-picking with multi-task Attention Network**<br />\n\nhttps://user-images.githubusercontent.com/30610646/166941015-921d6ba1-f77e-4413-a532-e3e5af6d658f.mp4\n\n## Summary\n\n* [Installation](#installation)\n* [Project Architecture](#project-architecture)\n\n### Installation\nTo run this repository, we suggest to install packages with Anaconda.\n\nClone this repository:\n\n```bash\ngit clone https://github.com/tso1257771/RED-PAN.git\ncd RED-PAN\n```\n\nCreate a new environment via pip (suggested)\n\n```bash\nconda create --name redpan python==3.10.0 \nconda activate redpan\npip install -r requirements.txt\n```\nor via environment.yml \n\n```bash\nconda env create --file environment.yml\nconda activate redpan\n```\n",
        "createdAt": "2022-04-18T05:18:17.000Z",
        "updatedAt": "2025-11-15T05:53:13.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/tso1257771/RED-PAN/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "RensHofman/SeismicMatch",
        "url": "https://github.com/RensHofman/SeismicMatch",
        "description": "Cross-correlation based template matching for seismology",
        "stars": 3,
        "forks": 0,
        "readme": "# SeismicMatch documentation\n```\n                          _                       __\n                         / \\               _   __/  \\       __\n          /\\            /   \\       _     / \\_/      \\     /  \\       _\n_____/\\  /  \\_/\\_______/     \\     / \\   /            \\   /    \\     / \\___\n       \\/                     \\   /   \\_/              \\_/      \\   /\n                               \\_/                               \\_/\n```\n- [Installation](#installation)\n- [Workflow](#workflow)\n  1. [Project configuration](#1-project-configuration)\n  2. [Template extraction](#2-template-extraction)\n  3. [Template matching](#3-template-matching)\n  4. [Event families](#4-event-families)\n- [File formats](#file-formats)\n  -  [Configuration file](#configuration-file)\n  -  [Continuous data](#continuous-data)\n  -  [Station metadata](#station-metadata)\n  -  [Starting catalog](#starting-catalog)\n  -  [Event files](#event-files)\n  -  [Match files](#match-files)\n  -  [Event family files](#event-family-files)\n- [Data example (tutorial)](#example-data-tutorial)\n- [References](#references)\n\n## Installation\n\nThis software is designed to run on an NVIDIA GPU and makes use of the CUDA\ntoolkit. If no NVIDIA graphics card is available, or if the user whishes so,\nSeismicMatch can also run in full CPU mode.\n\n### From source\n\nDownload the SeismicMatch zip archive by clicking the green `<> Code` button\nat the top of this page, or clone the repository using git:\n```console\n>>> git clone https://github.com/RensHofman/SeismicMatch.git\n```\n\nCreate a conda environment with python3 installed:\n```console\n>>> conda create -n SM python=3\n```\n#### Install dependencies\n\n```console\n>>> conda install numpy==1.23.5 scipy pyyaml\n>>> conda install -c conda-forge obspy\n```\nIf you have an NVIDIA GPU connected to your machine and whish to use the\nGPU mode, install the CuPy package using the command below. Do not install\nCuPy if you do not have an NVIDIA GPU available. The CPU mode will always\nbe available, whether CuPy is installed or not.\n```console\n>>> conda install -c conda-forge cupy\n```\nNote that the installation of cupy requires the CUDA toolkit, and the CuPy\nversion depends on the CUDA driver. Conda should automatically pick the right\nversion, but a table is also provided on the [CuPy website](https://docs.cupy.dev/en/stable/install.html).\n\nSee [performance settings](#performance-settings) for information on how to use\nthe full CPU mode.\n\n#### Install SeismicMatch\n\nMove inside the SeismicMatch project folder that contains \"setup.py\" and run the\ninstallation script (requires pip and setuptools).\n```console\n>>> cd SeismicMatch\n>>> pip install .\n```\n\n## Workflow\n\nThis is an overview of the normal SeismicMatch workflow. Before starting you own\nproject, it is also possible to play around with the [example data](#example-data-tutorial) described below.\n\nThe prerequisites for starting a new project are:\n- continuous data obervations in a format readable by ObsPy\n- a starting catalog for which templates should be extracted\n- a station metadata file (channel level) for the selection of stations to be used\n  in StationXML format\n\nBefore you start, make sure that SeismicMatch is installed and the virtual\nenvironment in which it is installed is activated. Next, choose a location for\nyour new project and create a project folder:\n```console\n>>> mkdir my_tm_project\n>>> cd my_tm_project\n```\n\n### 1. Project configuration\nTo start a new project, we need to create project configuration file. This file\nmust be located in the root of our project folder and must be called `config.yaml`.\nWe can create a new configuration file with default settings using the command-\nline tool `create_config`:\n```console\n>>> create_config\n```\n\nThis creates `config.yaml` in the current working directory. Open the file using\nyour preferred editor and change the parameters to your needs. A description of\nall fields is provided at the bottom of the file. The performance settings are\nautomatically detected when a new configuration file is created and are therefore\nspecific to your machine.\n\nNote that changes to this file after extracting the template waveforms in the next\nstep may cause problems, since preprocessing is applied permanently to the template\nwaveforms. Changing the preprocessing values after template extraction will cause\nthe continuous data to be preprocessed differently.\n\n### 2. Template extraction\nThe next step is to extract template waveforms for the starting catalog of events\nusing the command-line tool `create_templates`:\n```console\n>>> create_templates my_starting_catalog.xml\n```\n\nNote that the starting catalog should be in a format readable by ObsPy. This will\ncreate a folder where template waveforms will be stored. The location of this folder\nas well as the (pre-processing) settings for the template waveforms are defined in\nthe configuration file. Additionally, individual event files are created in the events\nfolder.\n\nIf no templates are being created, please check that the continuous data is provided\nin the data folder, that the data structure and filenames are correctly defined\nin the configuration file, and data is available for the events in the starting\ncatalogue. Add the command-line argument `-vvv` to increase the verbosity for debugging.\n\n### 3. Template matching\nNow, we can start the main template matching script using the command-line\n`match_templates`. It is possible to specify specific template waveforms as command-\nline arguments:\n```console\n>>> match_templates *template_waveform_files\n```\n\nIf no specific template waveforms are provided as command-line arguments, all template\nwaveforms in the template folder are used by default:\n```console\n>>> match_templates\n```\n\nThe template waveforms will now be cross-correlated with the continuous data in\nthe data folder, for the time-span defined in the configuration file.\n\n### 4. Event families\nThe command-line tool `create_event_families` combines the matches from each individual\ntemplate waveform and applies the selection criteria defined in the configuration file.\nIf a set of simultaneous matches meets these criteria, an event is appended to the\ntemplate family of the template event. It is possible to specify specific match files as\ncommand-line arguments:\n```console\n>>> create_event_families *match_files\n```\n\nIf no specific match files are provided as command-line arguments, all match files in the\nmatches folder are used by default:\n```console\ncreate_event_families\n```\n\nNote that if an event is detected using multiple template events, it will appear in multiple\nevent families. This creates a list of event detections for each template event in the event\nfolder. The filenames of the event family files match the filenames of the template event\nfiles in the event folder.\n\n## File formats\n\n### Configuration file\nThe configuration file is called `config.yaml` and is located in the root of the current project\nfolder. Settings are read from this file by all command-line tools in the SeismicMatch package.\nA configuration file with default (example) setting can be created automatically as described in\nsection [1. Project configuration](#1-project-configuration) of the [Workflow](#workflow) section.\n\nThe project parameters that can be adjusted are listed below.\n\n#### performance settings:\nThese settings control the performance of SeismicMatch. When the configuration  \nfile is created, the optimal settings are automatically detected from your system.  \nIf you wish to use full CPU mode, set `n_gpu` to 0. This is the default setting  \nif no graphics card is available.\n\n- **n_cpu** *(int, optional)*: maximum number of parallel processes to be  \n      used. Defaults to the number of cpu cores.  \n- **n_gpu** *(int, optional)*: maximum number of graphics cards to use. By  \n      default, all graphics cards will be used.  \n- **cuda_devices** *(list, optional)*: specify which CUDA devices should be  \n      used. By default, the first `n_gpu` devices will be used.\n\n#### template settings:\n- **n_stations** *(int, required)*: the number of stations for which to extract  \n    templatewaveforms. These will be the closest available stations  \n    to the event hypocenter.\n- **channel** *(str, optional)*: channel code to be used. The use of multiple  \n    channels is currently not supported. Defaults to 'HHZ'.\n- **prepick** *(float, required)*: starttime of the template windows relative  \n    to the estimated P-wave arrival in seconds.\n- **min_len** *(float, required)*: minimum length of the template waveforms in  \n    sec. The templates will be lengthened with 5 second increments for  \n    increasing hypocentral distance.\n- **length_fixed** *(bool, required)*: if set to True, all template waveforms  \n     will have the same length determined by `min_len`.\n- **template_data_path** *(str, optional)*: path to the folder that holds the  \n     continuous data from which templates should be extracted in case this  \n     path is required to be different from the general data path. Defaults to  \n     the `data_path` under 'folders and file structure'.  \n- **template_data_structure** *(str, optional)*: description of the data  \n     structure (folders & filenames) within `template_data_path`.  \n     Placeholders can (and should) be used to include the data folder, year,  \n     network code, station code, location code, channel code and the julian  \n     day in curly brackets: {data_path}, {year}, {net}, {sta}, {loc}, {cha},  \n     {loc}, {julday}. The tag {quality} can be used optionally to include a  \n     data quality flag. Defaults to the `data_structure` as defined under  \n     'folders and file structure'.\n\n#### pre-processing settings:\n- **highpass** *(float, required)*: lower frequency in Hz of the bandpass  \n    filter applied to both the templates (permanent) and continuous  \n    data (upon loading).\n- **lowpass** *(float, required)*: upper frequency in Hz of the bandpass filter  \n    applied to both the templates (permanent) and continuous data (upon  \n    loading).\n- **decimate** *(int, required)*: the factor by which the sampling rate of the  \n    data should be lowered (for faster computation). Decimation will be  \n    applied permanently to the templates and dynamically to the  \n    continuous data upon loading.\n\n#### cross-correlation settings:\n- **data_start** *(yyyy-mm-dd, required)*: starting date for cross-correlation  \n    of the templates with the continous data.\n- **data_stop** *(yyyy-mm-dd, required)*: last date to be included in the  \n    cross-correlation.\n- **cc_threshold** *(float, required)*: threshold of the absolute normalized  \n    cross-correlation value.\n- **mad_threshold** *(float, required)*: threshold of the normalized cross-  \n    correlation value as a factor of the daily median absolute\n    deviation (MAD).\n- **combine_thresholds** *(bool, required)*: if True, both thresholds need to  \n    be passed. If False, only one threshold needs to be passed.\n\n#### folders and file structure:\nPath names are defined either absolute, or relative to the project folder  \ncontaining this configuration file.\n- **meta_dir** *(str, required)*: path to the metadata folder that holds the station  \n    xml file `stations.xml` containing the station information.\n- **event_dir** *(str, required)*: path to the folder where event information is  \n    stored. Single event catalog files will be created here to which the template  \n    files and event families can be traces back to through their filenames.\n- **template_dir** *(str, required)*: path to the folder where template waveforms  \n    will be stored.\n- **matches_dir** *(str, required)*: path to the folder where all matches to each  \n    individual template waveform are stored.\n- **family_dir** *(str, required)*: path to the folder where event detections that   \n    exceed the selection criteria are stored for each template event. Note that the  \n    same event can occur within multiple event families.\n- **data_path** *(str, required)*: path to the folder that holds the continuous data.\n- **data_structure** *(str, required)*: description of the data structure (folders &  \n    filenames) within `data_path`. Placeholders can (and should) be used to include  \n    the data folder, year, netowrk code, station code, location code, channel code  \n    and the julian day in curly brackets: {data_path}, {year}, {net}, {sta}, {loc},  \n    {cha}, {loc}, {julday}. The tag {quality} can be used optionally to include a  \n    data quality flag.\n\n#### event selection criteria:\n- **cc_criteria** *(list of floats, required)*: selection criteria to define an event  \n    in terms of the absolute normalized cross-correlation value.  \n    For example: [0.7, 0.5] would mean that two simultaneous matches with absolute  \n    cross-correlation values >= 0.7 & >= 0.5 on two different stations are required  \n    within the time range `max_t_diff`. Use an empty list `[]` if no cc-criteria  \n    should be applied.\n- **mad_criteria** *(list of floats, required)*: selection criteria to define an  \n    event in terms of a factor of the daily median absolute deviation (MAD) of the  \n    normalized cross-correlation function. For example: [10, 8] would mean that two  \n    simultaneous matches with 10x and 8x the daily MAD value are required within the  \n    time range `max_t_diff`. Use an empty list `[]` if no MAD-criteria should be  \n    applied.\n- **max_t_diff** *(float, required)*: maximum time difference in seconds between  \n    individual detections that allows them to be called simultaneous for the purpose  \n    of the selection criteria defined above. Note that the time difference relates to  \n    the estimated origin time of the detections and not the actual occurrence of  \n    the template waveforms, since these would depend on the stations hypocentral  \n    distance.\n- **combine_criteria** *(bool, required)*: if True, both the `cc_criteria` as well as  \n    the `mad_criteria` need to be met. If set to False an event is defined when either  \n    of both criteria are met.\n\n### Continuous data\nThe continuous data must be in a [format readable by ObsPy](https://docs.obspy.org/packages/autogen/obspy.core.stream.read.html#supported-formats),\nwhere each file represents data from a single channel on a single day. The data should be\nstructured in a way that the filename (including the path) contains the network code, station\ncode, location code, channel code and the julian day as a 3-digit number. This data structure\nneeds to be defined in the configuration file.\n\n### Station metadata\nThe station metadata needs to be provided in StationXML format at the channel level. This format\nis provided by all [FDSN webservices](https://www.fdsn.org/xml/station/).\n\n### Starting catalog\nThe starting catalog should be in a [format readable by ObsPy](https://docs.obspy.org/packages/autogen/obspy.core.event.read_events.html#supported-formats).\n\n### Event files\nIndividual event files are created in the event folder for each template event. The filenames\nare a representation of the event origin time as defined in the starting catalog. This identifier\nis also used in the template waveform files. This way, matches can easily be recombined with\ntemplate information to construct event detections. The event files are written in QuakeML format\nto support all event information contained in the starting catalog.\n\n### Match files\nThe filenames of the match files are a combination of the waveform identifier, the template event\nidentifier, and the number of samples in the template waveform: `{waveform-id}_{event-id}_{n-samples}`.\nEach file contains the instances where a single template waveform passes the cross-correlation\nthreshold with the corresponding continuous data. The files consist of 4 columns, separated by\nspaces. Each line represents a single detection. The columns represent the detection time (time\nthat aligns with the starttime of the template waveform), the maximum normalized cross-correlation\ncoefficient, the cross-correlation coefficient expressed as a factor of the mean absolute deviation\n(MAD) of the daily cross-correlation funcion, and the amplitude ratio of the detected event compared\nto the template event waveform.\n\nExample match file `example_data/matches/CX.PB01..HHZ_2021005T032907.3800Z_1076`:\n```\n2021005T004454.6183Z 0.755 20.582 5.327E-02\n2021005T032928.7783Z 1.000 27.262 9.999E-01\n2021005T033907.2983Z 0.869 23.685 2.634E-01\n2021005T035238.8983Z 0.916 24.971 1.086E-01\n2021005T115025.3783Z 0.814 22.190 5.558E-02\n```\n\n### Event family files\nThe filenames of the event family files correspond to the template event files in the event folder,\nand are a representation of the template events origin time. The files consist of 5 columns, separated\nby spaces. Each line represents an event detection. The columns represent the estimated event origin\ntime, a comma separated list of channels contributing to the detection, a comma separated list of the\nnormalized cross-correlation values for each channel, a comma separated list of the MAD values for\neach channel, and a comma separated list of the amplitude ratios for each channel.\n\nExample event family file `example_data/event_families/2021005T032907.3800Z`:\n```\n2021-01-05T00:44:33.220000Z CX.PB02..HHZ,CX.PB11..HHZ,CX.PSGCX..HHZ,CX.PB01..HHZ 0.899,0.940,0.926,0.755 25.288,29.297,23.823,20.582 5.557E-02,5.364E-02,5.179E-02,5.327E-02\n2021-01-05T03:29:07.380000Z CX.PSGCX..HHZ,CX.PB11..HHZ,CX.PB01..HHZ,CX.PB02..HHZ 1.000,1.000,1.000,1.000 25.722,31.176,27.262,28.137 1.000E+00,1.000E+00,9.999E-01,1.000E+00\n2021-01-05T03:38:45.900000Z CX.PSGCX..HHZ,CX.PB11..HHZ,CX.PB02..HHZ,CX.PB01..HHZ 0.885,0.880,0.852,0.869 22.774,27.425,23.973,23.685 2.220E-01,2.035E-01,2.332E-01,2.634E-01\n2021-01-05T03:41:44.819907Z CX.PSGCX..HHZ,CX.PB02..HHZ 0.764,0.779 19.657,21.924 3.183E-02,3.567E-02\n2021-01-05T03:52:17.540000Z CX.PB11..HHZ,CX.PSGCX..HHZ,CX.PB02..HHZ,CX.PB01..HHZ 0.913,0.950,0.934,0.916 28.448,24.448,26.289,24.971 1.159E-01,1.109E-01,1.144E-01,1.086E-01\n2021-01-05T11:50:03.979907Z CX.PSGCX..HHZ,CX.PB11..HHZ,CX.PB01..HHZ,CX.PB02..HHZ 0.896,0.890,0.814,0.914 23.035,27.741,22.190,25.711 5.267E-02,6.075E-02,5.558E-02,5.405E-02\n\n```\n\n## Example data (tutorial)\n\nThe SeismicMatch installation folder contains a directory `example_data` that\ncan be used to follow allong with this example. First, move to this directory.\n\n```console\n>>> cd ~/Downloads/SeismicMatch\n>>> cd example_data\n```\n\nNext, we need to create a configuration file.\n\n```console\n>>> create_config\n```\n\nThis creates the project configuration file (`config.yaml`) containing all \nparameters and data descriptors. This configuration file is read by all scripts\nthat are executed from within this folder. The default settings in the configuration\nfile are already suitable for the example data, and do not need to be changed.\nThe performance settings at the top of the configuration file are automatically\ndetected from your machine.\n\nThis folder contains sample data from the [*CX*](#references) network in northern Chile\nfor a single day in the folder `data_CX`. The `metadata` folder contains\nstation metadata (`stations.xml`) for each of the stations. The file\n`sippl_catalog_sample.xml` contains a sample from the IPOC seismicity catalog\nfor northern Chile ([Sippl et al., 2023](https://doi.org/10.5880/GFZ.4.1.2023.004)).\n\nThe first step is to extract template waveforms for the events in the sample\ncatalog using the script `create_templates`.\n\n```console\n>>> create_templates sippl_catalog_sample.xml\n```\n\nThis creates a two folders `templates` and `events` within the project\ndirectory. The config file `config.yaml` allows you to define different names\nand locations, as well as to set all the relevant parameter settings.\n- preprocessed template waveforms will be written to the `templates` folder\n  in MSEED format.\n- individual event files will be written to the `events` folder.\n\nThe template waveforms can now be cross-correlated with the continuous data in\nthe data folder. The data folder contains data for a single day, which is also\nreflected in the the time-span defined in the configuration file.\n\n```console\n>>> match_templates\n```\n\nThis starts the template matching. Note that SeismicMatch is designed to handle\nlarge data volumes efficiently. Small projects such as in this example do not\nreflect its performance very well, especially using GPUs. The example is merely\nintended to demonstrate the workflow and test different settings.\n\nA file is created for each template waveform in the folder `matches`, containing\na list of all instances where the cross-correlation threshold is passed.\n\nFinally, we can combine the matches from the individual template waveforms into\nevent families. An event family represents a set of event detections that are\nrelated to a single template event using the criteria defined in the configuration\nfile.\n\n```console\n>>> create_event_families\n```\n\nThis creates a list of event detections for each template event in the folder\n`event_families`. The filenames of the event family files match the filenames\nof the template event files in the `events` folder.\n\n### References\n> [Sippl et al., 2023] Sippl, C., Schurr, B., Münchmeyer, J., Barrientos, S., and Oncken, O. (2023). The\nnorthern chile forearc constrained by 15 years of permanent seismic monitoring. Journal of South American\nEarth Sciences, 126:104326.\n> \n> GFZ German Research Centre for Geosciences; Institut des Sciences de l’Univers-Centre National de la\nRecherche CNRS-INSU (2006): IPOC Seismic Network. Integrated Plate boundary Observatory Chile - IPOC.\nDataset/Seismic Network. doi:10.14470/PK615318.\n",
        "createdAt": "2024-12-24T14:40:47.000Z",
        "updatedAt": "2025-10-27T09:04:32.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/RensHofman/SeismicMatch/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "barsch/seishub.core",
        "url": "https://github.com/barsch/seishub.core",
        "description": "SeisHub - a seismological XML/SQL database hybrid",
        "stars": 23,
        "forks": 9,
        "readme": "[![Build Status](https://secure.travis-ci.org/barsch/seishub.core.png?branch=master)](https://travis-ci.org/barsch/seishub.core)\n\nWelcome to SeisHub\n==================\n\nSummary\n-------\n\nSeismic databases and processing tools currently available are mainly limited to classic three-component seismic recordings and cannot handle collocated multi-component, multi-disciplinary datasets easily. Further, these seismological databases depend on event-related data and are not able to manage state of the art continuous waveform data input as well. None of them allows for automated request of data available at seismic data centers or to share specific data to users outside one institute. Some seismic databases even depend on licensed database engines, which contradicts the open source character of most software packages used in seismology.\n\nThis study intends to provide a suitable answer to the deficiencies of existing seismic databases. SeisHub is a novel web-based database approach created for archiving, processing, and sharing geophysical data and meta data (data describing data), particularly adapted for seismic data. The implemented database prototype offers the full functionality of a native XML database combined with the versatility of a RESTful Web service. The XML database itself uses a standard relational database as back-end, which is currently tested with [PostgreSQL](http://www.postgres.org/) and [SQLite](http://www.sqlite.org/). This sophisticated structure allows for the usage of both worlds: on the one hand the power of the SQL for querying and manipulating data, and one the other hand the freedom to use any standard connected to XML, e.g. document conversion via XSLT (Extensible Stylesheet Language Transformations) or resource validation via XSD (XML Schema). The actual resources and any additional services are available via fixed Uniform Resource Identifiers (URIs), where as the database back-end stores the original XML documents and all related indexed values. Indexes are generated using the XPath language and may be added at any time during runtime. This flexibility of the XML/SQL mixture introduced above enables the user to include parameters or results as well as meta data from additional or yet unknown monitoring techniques at any time. SeisHub also comprises features of a “classical seismic database” providing direct access to continuous seismic waveform data and associated meta data. Additionally, SeisHub offers various access protocols (HTTP/HTTPS, SFTP, SSH), an extensible plug-in system, user management, and a sophisticated web-based administration front-end. The SeisHub database is an open source project and can be freely downloaded via the project home page https://github.com/barsch/seishub.core/.\n\nThe SeisHub database has already been deployed as central database component within two scientific projects: [Exupéry](http://www.exupery-vfrs.de/), a mobile Volcano Fast Response System (VFRS), and BayernNetz, the seismological network of the Bavarian Seismological Service [Erdbebendienst Bayern](http://www.erdbeben-in-bayern.de/).\nAcknowledgements\n\nThis project was funded by the German Science Foundation (DFG) via grant DFG IG 16/9-1.\n\nDocumentation\n-------------\n\n* Barsch, Robert (2009): [Web-based technology for storage and processing of multi-component data in seismology: First steps towards a new design](http://edoc.ub.uni-muenchen.de/11043/). Dissertation, LMU München: Fakultät für Geowissenschaften.\n* A tutorial for writing SeisHub Plugins: [seishub.plugins.how_to_extend_seishub](https://github.com/krischer/seishub.plugins.how_to_extend_seishub)\n\n###### Related Papers ######\n\n* Bernsdorf, S., Barsch, R., Beyreuther, M., Zakšek, K., Hort, M., Wassermann, J. (2010), [Decision support system for the mobile volcano fast response system](http://www.tandfonline.com/toc/tjde20/3/3). International Journal of Digital Earth. 3 (3), 280-291.\n* Beyreuther, M., Barsch, R., Krischer, L., Megies, T., Behr, Y., and Wassermann, J. (2010), [ObsPy: A Python Toolbox for Seismology](http://www.seismosoc.org/publications/SRL/SRL_81/srl_81-3_es/), Seismological Research Letters, 81 (3), 530-533.\n* Megies, T., Beyreuther, M., Barsch, R., Krischer, L., and Wassermann, J. (2011), [ObsPy - What can it do for data centers and observatories?](http://www.annalsofgeophysics.eu/index.php/annals/article/view/4838), Annals of Geophysics, 54 (1).\n\nInstalling SeisHub\n------------------\n\nThe following section shows how to install SeisHub and associated components. It will not cover the installation of a relational database back-end, like [PostgreSQL](http://www.postgresql.org/). Please refer to the manual of the preferred database.\n\nFor Linux and UNIX systems the author suggests to install !SeisHub as a non-administrative user applying a new, separate, local Python >= 2.6.x instance.\n\nInstalling Python on a Windows operating system is more complicated because development tools like a C compiler are not part of a standard Windows distribution. Therefore many Python modules using C extensions have to be delivered as binary package with an executable installer. The fastest, most unproblematic way is to install Python and all extensions as the administrative system user.\n\n\n### Python ###\n\n1. Download and uncompress the latest stable Python 2.6.x package for the used operating system from http://www.python.org/download/. Windows user may just use the executable installer and skip to the next subsection.\n2. Run \n\n        ./configure --prefix=$HOME\n        make\n        make install\n\n3. Add `$HOME/bin` to the `PATH` environmental variable, e.g. in bash:\n\n        export PATH=\"$HOME/bin:$PATH\"\n\n4. Call `python` in command line. It should show the correct version number.\n\n\n### Easy Install ###\n\nEasy Install is a powerful command-line based package management tool for Python. Like CPAN for Perl, it automates the download, build, installation and update process of Python packages.\n\n1. Download http://python-distribute.org/distribute_setup.py.\n2. Run \n\n        python distribute_setup.py\n\n### Required Python extensions ###\n\n    easy_install SQLAlchemy\n    easy_install Cheetah\n    easy_install pycrypto\n    easy_install Twisted\n    easy_install pyparsing\n    easy_install pyasn1\n    easy_install lxml               # Linux requires libxml2-dev and libxslt-dev\n    easy_install pyOpenSSL          # Linux requires libssl-dev\n    easy_install numpy              # see link in Notes\n    easy_install obspy\n\nThe [seismology plug-in](https://github.com/barsch/seishub.plugins.seismology) requires the following additional modules:\n\n    easy_install matplotlib\n\n###### Notes ######\n\n* More details (especially for *matplotlib* and *numpy* installation on linux) can be found at https://github.com/obspy/obspy/wiki/Installation-on-Linux:-Dependencies\n* Windows users need to install *pywin32* (Python for Windows extension). Download and install from http://sourceforge.net/projects/pywin32/.\n* Package *lxml* requires the *libxml2-dev* and *libxslt-dev* packages. Compiling takes a while. This does not apply to an installation on Windows - here are binaries delivered.\n* Binary packages for *pyOpenSSL* can be found at http://www.egenix.com/products/python/pyOpenSSL/ or http://sourceforge.net/projects/pyopenssl/\n\n\n### Additional database bindings (optional) ###\n\nSeisHub uses as default data back-end [SQLite](http://www.sqlite.org/), which comes with Python 2.6.x. \n\nFor [PostgreSQL](http://www.postgresql.org/) additional database bindings are required. Those bindings can be installed via:\n\n    easy_install psycopg2           # Linux requires libpq-dev\n\n###### Notes ######\n* On Debian/Ubuntu install *python-psycopg2* via package management\n* Windows binary packages for *psycopg2* can be found at http://www.stickpeople.com/projects/python/win-psycopg/\n\n### SeisHub ###\n\n1. Get the latest SeisHub version via PyPI\n        \n        easy_install seishub.core==dev\n\n2. Optionally get any plug-in you need, e.g.:\n\n        easy_install seishub.plugins.seismology==dev\n\n3. Go into your Python script directory and initialize a new local instance:\n        \n        seishub-admin initenv /path/to/instance\n\n4. Change to the bin directory within the instance path and start the server\n\n        cd /path/to/instance/bin\n        ./start.sh\n\n5. Open http://localhost:8080/manage in your browser. Default user name and password are both set to \"admin\".\n \nYou probably want to stop the server after the first run and adjust the\nsettings within the configuration file `seishub.ini` within the `conf`\ndirectory or use the web interface for that.\n\n### PostgreSQL (optional) ###\n\nUsing PostgreSQL as default database backend requires a few more additional steps:\n\n1. Login as postgres super user\n\n        su - postgres\n\n2. Create a new database user \"username\" using [`createuser`](http://developer.postgresql.org/pgdocs/postgres/app-createuser.html). It will prompt for a password.\n\n        createuser -D -P -S -R username\n\n3. Create a new database \"databasename\" for the user \"username\" using [`createdb`](http://developer.postgresql.org/pgdocs/postgres/app-createdb.html).\n\n        createdb -O username databasename\n\n4. Logout\n\nAfter creating the user and database you may use the connection string `postgresql://username:password@host:port/databasename` (postgres default port is `5432`).\n",
        "createdAt": "2012-07-18T19:19:20.000Z",
        "updatedAt": "2023-01-28T07:50:21.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/barsch/seishub.core/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "saifulkhan/isc_vbas",
        "url": "https://github.com/saifulkhan/isc_vbas",
        "description": "VBAS : Visual Bulletin Analytics System developed for International Seismological Centre",
        "stars": 1,
        "forks": 0,
        "readme": "This software is actively maintained by [ISC](https://www.isc.ac.uk/) internally. For up-to-date information, please reach out to them.\n\n\n## VBAS : Visual Bulletin Analytics System\n\nAn example screenshoot of [2011 Tōhoku earthquake and tsunami](https://en.wikipedia.org/wiki/2011_T%C5%8Dhoku_earthquake_and_tsunami)\n\n![alt tag](figures/vbas-screen.png)\n\nThe GUI is designed to view on two-monitor screen. Please download and zoom to see the name the graphs and to get a more clear view of the graphs.  A brief description of some of the key graphs: \n\n**Phase Travel-time:** The grey curves are travel time curves for AK135 velocity model.  The various shapes represent the reported phase times.  The pale turquoise shapes are crustal phases, the pink shapes are mantle phases, the red shapes are core phases, the dark blue shapes are depth phases, the grey and black shapes are amplitude times and unknown phase types. \n\n**Hypocentre Seismicity:**  The coloured circles represent historical events from the ISC Bulletin from 1993-2013. The colours represent the prime hypocentre depths of the individual events and they are not scaled to magnitude. The large open diamond is the ISC prime (or preferred) hypocentre for this event and the large open squares indicate all the reported hypocentres from different institutes.\n\n**Hypocentre Magnitudes:** The graph on the left displays the reported magnitudes. They are separated into simplified magnitude types and the colour scheme is based on logo-logical colour (ML=lime (local), MS=strawberry (surface), mb=blueberry(body), MW=wine(moment magnitude), Other=orange). The graph on the right displays deviations from the overall mean (or median or preferred??) magnitude for the event. \n\n**Station Geometry:** This 360-degree projection is centred on the ISC prime (or preferred) hypocentre for the event (indicated by the black x). The blue circles are the stations that reported phases and/or amplitudes for this event. The pink arc on the outside of the grey circle shows the primary azimuthal gap and the orange arc next to the pink arc indicates the secondary azimuthal gap. The green curved histograms on the inside of the grey circle indicate the number of reported stations in that direction in 30 degree bins. The numbers in the grey dashed arcs show the proportion of stations and the number of stations reported in that 30-degree arc.\n\n**Hypocentre Depths:** Reported hypocentre depths (in km) are shown by the vertical bars.  A fixed depth is indicated with a small open circle and reported depth errors are indicated by the short horizontal lines. The 3-5 letter codes represent the different institutes who calculated a hypocentre which was reported to the ISC. The ISC is highlighted in red and is the preferred depth.\n\n\n\n## Background \n\nhttps://www.oerc.ox.ac.uk/projects/seismic-change\n",
        "createdAt": "2016-10-27T11:44:54.000Z",
        "updatedAt": "2025-04-28T15:20:11.000Z",
        "language": "Java",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/saifulkhan/isc_vbas/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "jowillem/Mars-Seismology-Undergraduate-Research-Project",
        "url": "https://github.com/jowillem/Mars-Seismology-Undergraduate-Research-Project",
        "description": "Repository containing all of the work from my undergraduate final research project. Elaborate project description is located in the README.md file.",
        "stars": 1,
        "forks": 0,
        "readme": "# Mars-Seismology-Undergraduate-Research\nMy undergraduate research encompasses the use of seismic data from the NASA InSight mission to refine our understanding of the structure of the martian core. The primary research question I addressed is “What are the uncertainties on seismically determined estimates of the core radius of Mars?”. I learned how we can use the most up to date marsquake data recorded by InSight to help us confirm/refine the measurement of the core radius of Mars for this project. I used marsquake data given to me from previous research done to create seismological models to determine the proper time for the ScS window being measured from that data. ScS waves are core reflected S waves from the marsquakes that can give us distance approximations since they bounce off of the core. Their travel times can be used to constrain the depth of the mantle when combined with accurate seismic velocity models of the martian interior. From the ScS window, there are calculations that can be done using seismic velocity models to measure the distance of how far the S waves traveled. Previous detections of ScS phases from 6 marsquakes have indicated that the martian core radius is 1830 ± 40 km (Stahler et al., 2021), suggesting an enrichment in light elements. This is the current value that we have for the radius of the liquid metal core of Mars. Since the time of publication, there have been many more high quality marsquakes recorded that may contain ScS phases. In this project, I analyzed low-frequency marsquakes that were not used in previous investigations for signs of ScS. Increasing the number of ScS detections will improve our knowledge of the core radius and associated measurement uncertainties. The InSight mission ended December 15th, 2022, and the data from the full catalog of marsquakes is openly available for public use. This project was advised by professor Ross Maguire, and took place over two semesters.\n",
        "createdAt": "2023-10-17T19:36:30.000Z",
        "updatedAt": "2024-07-27T17:59:44.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/jowillem/Mars-Seismology-Undergraduate-Research-Project/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "angystallone/Seismology_Stuff",
        "url": "https://github.com/angystallone/Seismology_Stuff",
        "description": null,
        "stars": 1,
        "forks": 0,
        "readme": "**Download_Large_Catalogs.py**\n\nFor several web services based on FDSN specification, limits exist on the maximum number of objects per query.\nThis simple code allows you to overcome this limitation.\n\nBased on ObsPy.\n\n**from_gdf_to_shp.py**\n\nConvert .gdf files to .shp files\n",
        "createdAt": "2020-11-24T11:19:10.000Z",
        "updatedAt": "2024-06-13T10:15:16.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/angystallone/Seismology_Stuff/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "devitocodes/seigen",
        "url": "https://github.com/devitocodes/seigen",
        "description": "An elastic wave equation solver for seismological problems based on the finite element method",
        "stars": 18,
        "forks": 6,
        "readme": "# Seigen: Seismology through code generation\n\n## Overview\n\n[Seigen](http://www.opesci.org) is an elastic wave\nequation solver for seimological problems based on the\n[Firedrake](http://www.firedrakeproject.org) finite element\nframework. It forms part of the [OPESCI](http://www.opesci.org)\nseismic imaging project.\n\n## Quickstart\n\nSeigen requires the installation of Firedrake and must be run from\nwithin the Firedrake virtual environment. To first install Firedrake\nplease follow these [instructions](http://www.firedrakeproject.org/download.html#).\n\nOnce Firedrake is installed and the virtual environment is activated, you can install\nSeigen using the following commands:\n\n```\ngit clone https://github.com/opesci/seigen.git\npip install -e seigen\n```\n\n## Licence\n\nSeigen is open-source software and is released under the [MIT License](https://opensource.org/licenses/MIT). See the file ``LICENSE.md`` for more information.\n\n## Contact\n\nComments and feedback may be sent to opesci@imperial.ac.uk.\n",
        "createdAt": "2017-08-03T19:27:06.000Z",
        "updatedAt": "2025-11-25T09:28:30.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/devitocodes/seigen/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "rpsinghcodes/earthquake-store-1",
        "url": "https://github.com/rpsinghcodes/earthquake-store-1",
        "description": "Seismological data of India from 2015-2018",
        "stars": 0,
        "forks": 0,
        "readme": "# earthquake-store\nSeismological data of India from 2015-2018\n",
        "createdAt": "2021-02-07T09:34:34.000Z",
        "updatedAt": "2021-12-20T11:05:26.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/rpsinghcodes/earthquake-store-1/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "IESDMC/TAPSClient",
        "url": "https://github.com/IESDMC/TAPSClient",
        "description": "A python toolbox for request data from Taiwan Archive Platform for Seismology(TAPS).",
        "stars": 4,
        "forks": 0,
        "readme": "# TAPSClient\nA python toolbox for request data from Taiwan Archive Platform for Seismology ([TAPS](https://taps.earth.sinica.edu.tw)).\n\n## Install\n```shell\npip install -r requirement.txt\n```\n\n### get_stations\nref: [obspy.clients.fdsn - FDSN web service client for ObsPy](https://docs.obspy.org/packages/obspy.clients.fdsn.html#obspy-clients-fdsn-fdsn-web-service-client-for-obspy)\n```python\n>>> from client import Client\n>>> client = Client('TAPS')\n>>> from obspy import UTCDateTime\n>>> starttime = UTCDateTime(\"2008-01-01\")\n>>> endtime = UTCDateTime(\"2008-12-31\")\n>>> inv = client.get_stations(network=\"TW\", station=\"NSE*\",starttime=starttime,endtime=endtime, level=\"response\")\n>>> print(inv)\nInventory created at 2021-05-31T04:12:36.000000Z\n        Created by: TECDC WEB SERVICE\n                    /fdsnws/station/0/query?starttime=2008-01-01T00:00:00.000000&endtim...\n        Sending institution: TECDC (TECDC)\n        Contains:\n                Networks (1):\n                        TW\n                Stations (27):\n                        TW.NSE01 (E.Taiwan Range, NSE01)\n                        TW.NSE02 (E.Taiwan Range, NSE02)\n                        TW.NSE03 (E.Taiwan Range, NSE03)\n                        TW.NSE04 (E.Taiwan Range, NSE04)\n                        TW.NSE05 (E.Taiwan Range, NSE05)\n                        TW.NSE06 (E.Taiwan Range, NSE06)\n                        TW.NSE07 (E.Taiwan Range, NSE07)\n                        TW.NSE08 (E.Taiwan Range, NSE08)\n                        TW.NSE09 (E.Taiwan Range, NSE09)\n                        TW.NSE10 (E.Taiwan Range, NSE10)\n                        TW.NSE11 (E.Taiwan Range, NSE11)\n                        TW.NSE12 (E.Taiwan Range, NSE12)\n                        TW.NSE13 (E.Taiwan Range, NSE13)\n                        TW.NSE14 (E.Taiwan Range, NSE14)\n                        TW.NSE15 (E.Taiwan Range, NSE15)\n                        TW.NSE16 (E.Taiwan Range, NSE16)\n                        TW.NSE17 (E.Taiwan Range, NSE17)\n                        TW.NSE18 (E.Taiwan Range, NSE18)\n                        TW.NSE19 (E.Taiwan Range, NSE19)\n                        TW.NSE20 (E.Taiwan Range, NSE20)\n                        TW.NSE21 (E.Taiwan Range, NSE21)\n                        TW.NSE22 (E.Taiwan Range, NSE22)\n                        TW.NSE23 (E.Taiwan Range, NSE23)\n                        TW.NSE24 (E.Taiwan Range, NSE24)\n                        TW.NSE25 (E.Taiwan Range, NSE25)\n                        TW.NSE26 (E.Taiwan Range, NSE26)\n                        TW.NSE27 (E.Taiwan Range, NSE27)\n                Channels (81):\n                        TW.NSE01..EHZ, TW.NSE01..EHN, TW.NSE01..EHE, TW.NSE02..EHZ, \n                        TW.NSE02..EHN, TW.NSE02..EHE, TW.NSE03..EHZ, TW.NSE03..EHN, \n                        TW.NSE03..EHE, TW.NSE04..EHZ, TW.NSE04..EHN, TW.NSE04..EHE, \n                        TW.NSE05..EHZ, TW.NSE05..EHN, TW.NSE05..EHE, TW.NSE06..EHZ, \n                        TW.NSE06..EHN, TW.NSE06..EHE, TW.NSE07..EHZ, TW.NSE07..EHN, \n                        TW.NSE07..EHE, TW.NSE08..EHZ, TW.NSE08..EHN, TW.NSE08..EHE, \n                        TW.NSE09..EHZ, TW.NSE09..EHN, TW.NSE09..EHE, TW.NSE10..EHZ, \n                        TW.NSE10..EHN, TW.NSE10..EHE, TW.NSE11..EHZ, TW.NSE11..EHN, \n                        TW.NSE11..EHE, TW.NSE12..EHZ, TW.NSE12..EHN, TW.NSE12..EHE, \n                        TW.NSE13..EHZ, TW.NSE13..EHN, TW.NSE13..EHE, TW.NSE14..EHZ, \n                        TW.NSE14..EHN, TW.NSE14..EHE, TW.NSE15..EHZ, TW.NSE15..EHN, \n                        TW.NSE15..EHE, TW.NSE16..EHZ, TW.NSE16..EHN, TW.NSE16..EHE, \n                        TW.NSE17..EHZ, TW.NSE17..EHN, TW.NSE17..EHE, TW.NSE18..EHZ, \n                        TW.NSE18..EHN, TW.NSE18..EHE, TW.NSE19..EHZ, TW.NSE19..EHN, \n                        TW.NSE19..EHE, TW.NSE20..EHZ, TW.NSE20..EHN, TW.NSE20..EHE, \n                        TW.NSE21..EHZ, TW.NSE21..EHN, TW.NSE21..EHE, TW.NSE22..EHZ, \n                        TW.NSE22..EHN, TW.NSE22..EHE, TW.NSE23..EHZ, TW.NSE23..EHN, \n                        TW.NSE23..EHE, TW.NSE24..EHZ, TW.NSE24..EHN, TW.NSE24..EHE, \n                        TW.NSE25..EHZ, TW.NSE25..EHN, TW.NSE25..EHE, TW.NSE26..EHZ, \n                        TW.NSE26..EHN, TW.NSE26..EHE, TW.NSE27..EHZ, TW.NSE27..EHN, \n                        TW.NSE27..EHE\n```\n\n```python\n>>> net = inv[0]\n>>> print(net)\nNetwork TW (TAIGER Active Source Experiment March 2008)\n        Station Count: 27/27 (Selected/Total)\n        2008-02-01T00:00:00.000000Z - 2008-07-31T00:00:00.000000Z\n        Access: UNKNOWN\n        Contains:\n                Stations (27):\n                        TW.NSE01 (E.Taiwan Range, NSE01)\n                        TW.NSE02 (E.Taiwan Range, NSE02)\n                        TW.NSE03 (E.Taiwan Range, NSE03)\n                        TW.NSE04 (E.Taiwan Range, NSE04)\n                        TW.NSE05 (E.Taiwan Range, NSE05)\n                        TW.NSE06 (E.Taiwan Range, NSE06)\n                        TW.NSE07 (E.Taiwan Range, NSE07)\n                        TW.NSE08 (E.Taiwan Range, NSE08)\n                        TW.NSE09 (E.Taiwan Range, NSE09)\n                        TW.NSE10 (E.Taiwan Range, NSE10)\n                        TW.NSE11 (E.Taiwan Range, NSE11)\n                        TW.NSE12 (E.Taiwan Range, NSE12)\n                        TW.NSE13 (E.Taiwan Range, NSE13)\n                        TW.NSE14 (E.Taiwan Range, NSE14)\n                        TW.NSE15 (E.Taiwan Range, NSE15)\n                        TW.NSE16 (E.Taiwan Range, NSE16)\n                        TW.NSE17 (E.Taiwan Range, NSE17)\n                        TW.NSE18 (E.Taiwan Range, NSE18)\n                        TW.NSE19 (E.Taiwan Range, NSE19)\n                        TW.NSE20 (E.Taiwan Range, NSE20)\n                        TW.NSE21 (E.Taiwan Range, NSE21)\n                        TW.NSE22 (E.Taiwan Range, NSE22)\n                        TW.NSE23 (E.Taiwan Range, NSE23)\n                        TW.NSE24 (E.Taiwan Range, NSE24)\n                        TW.NSE25 (E.Taiwan Range, NSE25)\n                        TW.NSE26 (E.Taiwan Range, NSE26)\n                        TW.NSE27 (E.Taiwan Range, NSE27)\n                Channels (81):\n                        TW.NSE01..EHZ, TW.NSE01..EHN, TW.NSE01..EHE, TW.NSE02..EHZ, \n                        TW.NSE02..EHN, TW.NSE02..EHE, TW.NSE03..EHZ, TW.NSE03..EHN, \n                        TW.NSE03..EHE, TW.NSE04..EHZ, TW.NSE04..EHN, TW.NSE04..EHE, \n                        TW.NSE05..EHZ, TW.NSE05..EHN, TW.NSE05..EHE, TW.NSE06..EHZ, \n                        TW.NSE06..EHN, TW.NSE06..EHE, TW.NSE07..EHZ, TW.NSE07..EHN, \n                        TW.NSE07..EHE, TW.NSE08..EHZ, TW.NSE08..EHN, TW.NSE08..EHE, \n                        TW.NSE09..EHZ, TW.NSE09..EHN, TW.NSE09..EHE, TW.NSE10..EHZ, \n                        TW.NSE10..EHN, TW.NSE10..EHE, TW.NSE11..EHZ, TW.NSE11..EHN, \n                        TW.NSE11..EHE, TW.NSE12..EHZ, TW.NSE12..EHN, TW.NSE12..EHE, \n                        TW.NSE13..EHZ, TW.NSE13..EHN, TW.NSE13..EHE, TW.NSE14..EHZ, \n                        TW.NSE14..EHN, TW.NSE14..EHE, TW.NSE15..EHZ, TW.NSE15..EHN, \n                        TW.NSE15..EHE, TW.NSE16..EHZ, TW.NSE16..EHN, TW.NSE16..EHE, \n                        TW.NSE17..EHZ, TW.NSE17..EHN, TW.NSE17..EHE, TW.NSE18..EHZ, \n                        TW.NSE18..EHN, TW.NSE18..EHE, TW.NSE19..EHZ, TW.NSE19..EHN, \n                        TW.NSE19..EHE, TW.NSE20..EHZ, TW.NSE20..EHN, TW.NSE20..EHE, \n                        TW.NSE21..EHZ, TW.NSE21..EHN, TW.NSE21..EHE, TW.NSE22..EHZ, \n                        TW.NSE22..EHN, TW.NSE22..EHE, TW.NSE23..EHZ, TW.NSE23..EHN, \n                        TW.NSE23..EHE, TW.NSE24..EHZ, TW.NSE24..EHN, TW.NSE24..EHE, \n                        TW.NSE25..EHZ, TW.NSE25..EHN, TW.NSE25..EHE, TW.NSE26..EHZ, \n                        TW.NSE26..EHN, TW.NSE26..EHE, TW.NSE27..EHZ, TW.NSE27..EHN, \n                        TW.NSE27..EHE\n```\n\n```python\n>>> sta = net[0]\n>>> print(sta)\nStation NSE01 (E.Taiwan Range, NSE01)\n        Station Code: NSE01\n        Channel Count: 3/3 (Selected/Total)\n        2008-02-01T00:00:00.000000Z - 2008-07-31T00:00:00.000000Z\n        Access: None \n        Latitude: 24.08, Longitude: 121.61, Elevation: 30.0 m\n        Available Channels:\n                NSE01..EHZ, NSE01..EHN, NSE01..EHE\n```\n\n```python\n>>> cha = sta[0]\n>>> print(cha)\nChannel 'EHE', Location '' \n        Time range: 2008-02-01T00:00:00.000000Z - 2008-07-30T23:59:59.000000Z\n        Latitude: 24.08, Longitude: 121.61, Elevation: 30.0 m, Local Depth: 0.0 m\n        Azimuth: 90.00 degrees from north, clockwise\n        Dip: 0.00 degrees down from horizontal\n        Channel types: GEOPHYSICAL\n        Sampling Rate: 100.00 Hz\n        Sensor (Description): None (Lennartz Products LE-xD/SAMTAC-801H Datalogger)\n        Response information available\n>>> print(cha.response)\nChannel Response\n        From m/s (velocity in meters per second) to counts (digital counts)\n        Overall Sensitivity: 2.05008e+08 defined at 5.000 Hz\n        2 stages:\n                Stage 1: PolesZerosResponseStage from m/s to V, gain: 400.407\n                Stage 2: CoefficientsTypeResponseStage from V to counts, gain: 512000\n>>>\n```\n\n### get_waveforms\n```python\n>>> from client import Client\n>>> client = Client('TAPS')\n>>> user = 'user'\n>>> password = 'password'\n>>> client.set_credentials(user, password)\n>>> from obspy import UTCDateTime\n>>> t = UTCDateTime(\"2008-04-16T00:00:00.000\")\n>>> st = client.get_waveforms(\"TW\", \"NSE01\", \"--\", \"*\", t, t + 60 * 60)\n>>> print(st)\n1 Trace(s) in Stream:\nTW.NSE01..EHZ | 2008-04-16T00:00:00.000000Z - 2008-04-16T00:59:59.990000Z | 100.0 Hz, 360000 samples\n>>> st.plot(outfile='singlechannel.png')\n```\n\n### Remove response\nref: [obspy.core.trace.Trace.remove_response](https://docs.obspy.org/packages/autogen/obspy.core.trace.Trace.remove_response.html#obspy-core-trace-trace-remove-response)\n```python\n>>> tr = st[0]\n>>> pre_filt = [0.001, 0.005, 45, 50]\n>>> tr.remove_response(inventory=inv, pre_filt=pre_filt, output=\"DISP\",\n                   water_level=60, plot='outfile.png')\n<...Trace object at 0x...>\n```\n",
        "createdAt": "2021-05-28T02:15:51.000Z",
        "updatedAt": "2025-09-11T01:14:23.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/IESDMC/TAPSClient/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "jakewalter/intro_seismology",
        "url": "https://github.com/jakewalter/intro_seismology",
        "description": null,
        "stars": 4,
        "forks": 0,
        "readme": "# Introduction to Seismology - GPHY 4553\n\nThis was taught at University of Oklahoma in Spring 2024 and more or less follows Shearer's Introduction to Seismology\n\n**  \nLectures**\n\n| Number | Date (approx.) | Content | Assignment |\n| --- | --- | --- | --- |\n| 1   | T Jan 16 | CLASS CANCELLED |     |\n| 2   | Th Jan 18 | Course overview, wide world of seismology | #1 assigned |\n| 3   | T Jan 23 | Tensors/linear algebra, strain and stress |     |\n| 4   | Th Jan 25 | Computer: Matrices, data | #1 due/#2 assigned |\n| 5   | T Jan 30 | Waves: distance, motion, phase |     |\n| 6   | Th Feb 1 | Wave equation, P and S phases | #2 due |\n| 7   | T Feb 6 | Computer: Wave propagation, Fourier transforms | #3 assigned |\n| 8   | Th Feb 8 | Seismic waves and velocity models |     |\n| 9   | T Feb 13 | Ray tracing in 1D | #3 due |\n| 10  | Th Feb 15 | Ray tracing, velocity models | #4 assigned |\n| 11  | T Feb 20 | Inversion |     |\n| 12  | Th Feb 22 | Velocity model inversion | #4 due |\n| 13  | T Feb 27 | Tomography inversion in 3D |     |\n| 14  | Th Feb 29 (Leap Day) | Computer: Model resolution and misfit | #5 assigned |\n| 15  | T Mar 5 | Earthquake location inversion |     |\n| 16  | Th Mar 7 | Computer: earthquake location |     |\n| 17  | T Mar 12 | Course review/midterm prep | #5 due |\n| Midterm | Th Mar 14 | Midterm Exam |     |\n| 18  | T Mar 19 - Spring Break | No class |     |\n| 19  | Th Mar 21 - Spring Break | No class |     |\n| 20  | T Mar 26 | Wave energy and amplitudes |     |\n| 21  | Th Mar 28 | Reflection, transmission, attenuation | #6 assigned |\n| 22  | T Apr 2 | Green’s functions |     |\n| 23  | Th Apr 4 | Moment tensors | #6 due/#7 assigned |\n| 24  | T Apr 9 | Faulting type from seismograms |     |\n| 25  | Th Apr 11 | Spectral analysis, stress drop |     |\n| 26  | T Apr 16 | Computer: Earthquake observations | #7 due/#8 assigned |\n| 27  | Th Apr 18 | Surface waves, large earthquakes |     |\n| 28  | T Apr 23 | Earthquake physics, induced seismicity | #8 due |\n| 29  | Th Apr 25 | Computer: Final project dataset and skills | Final project available, due May 2 |\n| 30  | T Apr 30 | Computer: Final project continued (Zoom) |     |\n| 31  | Th May 2 | Exam preparation (Zoom) |     |\n| Final exam | T May 7 1:30-3:30 PM |     |     |\n\nClass exercise 1/25 on Python basics: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jakewalter/intro_seismology/blob/main/class_python_basics.ipynb)\n\nClass exercise 2/1 on Fourier/time series analysis: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jakewalter/intro_seismology/blob/main/fourier_time_series.ipynb)\n\nClass exercise 2/8 on basic earthquake download and waves: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jakewalter/intro_seismology/blob/main/edmond_earthquake.ipynb)\n\nClass exercise 2/15 on a basic finite difference approach to model earthquake waves (and part of Assignment 3): [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jakewalter/intro_seismology/blob/main/wave_equation_finite_difference.ipynb)\n\nClass exercise 2/22 on ray tracing (and part of Assignment 4): [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jakewalter/intro_seismology/blob/main/ray_tracing.ipynb)\n\nClass exercise 2/27 on simple inversion: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jakewalter/intro_seismology/blob/main/linear_inversion.ipynb)\n\nClass exercise 2/29 on inversion applied to tomography: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jakewalter/intro_seismology/blob/main/tomography.ipynb)\n\nClass exercise 3/5 on inversion applied to the earthquake location problem: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jakewalter/intro_seismology/blob/main/earthquake_location.ipynb)\n\nClass exercise 3/28 on reflection coefficients and attenuation: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jakewalter/intro_seismology/blob/main/reflection_coefficient.ipynb)\n\nClass exercise 4/11 on earthquake moment and catalogs: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jakewalter/intro_seismology/blob/main/earthquake_moment.ipynb)\n\nFinal Project Part A: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jakewalter/intro_seismology/blob/main/final_project_A.ipynb)\n\nFinal Project Part B: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jakewalter/intro_seismology/blob/main/final_project_B.ipynb)\n\nFinal Project Part C: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jakewalter/intro_seismology/blob/main/final_project_C.ipynb)\n",
        "createdAt": "2024-01-17T17:57:52.000Z",
        "updatedAt": "2025-02-18T03:29:28.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/jakewalter/intro_seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ahotovec/REDPy",
        "url": "https://github.com/ahotovec/REDPy",
        "description": "Repeating Earthquake Detector (Python)",
        "stars": 86,
        "forks": 40,
        "readme": "# REDPy has moved!\n\nAfter over a year of concentrated development work and many hurdles overcome, REDPy has moved to its new home as official U.S. Geological Survey software at [**https://code.usgs.gov/vsc/REDPy/**](https://code.usgs.gov/vsc/REDPy/).\n\nI did a lot of work under the hood to make it more efficient, and I have many plans to add more features in the coming years.\n\n_**All future development will be hosted in the new repository and this repository will no longer be updated.**_\n\nNew users should download the [latest version](https://code.usgs.gov/vsc/REDPy/-/releases), and read the [updated Wiki](https://code.usgs.gov/vsc/REDPy/-/wikis/home) for documentation.\n\nA snapshot of the last commit in this repository with an intact `README.md` file is available [here](https://github.com/ahotovec/REDPy/releases/) for posterity as \"Version 0\" of the code. I will no longer be supporting users of Version 0, but I am happy to help you migrate to the new one.\n\n~ Alicia Hotovec-Ellis ([ahotovec-ellis@usgs.gov](mailto:ahotovec-ellis@usgs.gov))\n",
        "createdAt": "2014-05-21T21:49:52.000Z",
        "updatedAt": "2025-09-10T02:42:20.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ahotovec/REDPy/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "nimanzik/scoter",
        "url": "https://github.com/nimanzik/scoter",
        "description": "Multiple-earthquake localization by using static and source-specific station correction terms",
        "stars": 4,
        "forks": 1,
        "readme": "# SCOTER\n\n[![DOI](https://img.shields.io/badge/DOI-10.5880%2FGFZ.2.1.2019.002-blue.svg)](http://doi.org/10.5880/GFZ.2.1.2019.002)\n[![](https://img.shields.io/badge/licence-GPL--3.0-orange)](LICENSE)\n[![](https://img.shields.io/badge/python-2.7-blue)](https://docs.python.org/2.7/)\n\n**SCOTER** implements static and shrinking-box source-specific station terms\n(SSST) techniques to reduce the effect of spatially correlated residuals\ncaused by 3-D velocity structure and improve the relative location accuracy\namong nearby seismic events.\n\n## Citation\n\nThe recommended citation for SCOTER is:\n\n>  Nooshiri, Nima; Heimann, Sebastian; Tilmann, Frederik; Dahm, Torsten; Saul, \nJoachim (2019): SCOTER - Software package for multiple-earthquake relocation by \nusing static and source-specific station correction terms. V. 0.1. GFZ Data Services. http://doi.org/10.5880/GFZ.2.1.2019.002\n\n## License\n\nGNU General Public License, Version 3, 29 June 2007\n\nCopyright © 2019 Helmholtz Centre Potsdam - GFZ German Research Centre for\nGeosciences, Potsdam, Germany\n\nSCOTER is free software: you can redistribute it and/or modify it under the\nterms of the GNU General Public License as published by the Free Software\nFoundation, either version 3 of the License, or (at your option) any later\nversion. SCOTER is distributed in the hope that it will be useful, but WITHOUT\nANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\nFOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.\nYou should have received a copy of the GNU General Public License along with\nthis program. If not, see <http://www.gnu.org/licenses/>.\n\n## Contact\n* Nima Nooshiri - nima.nooshiri@gfz-potsdam.de\n\n```\nGFZ German Research Centre for Geosciences\nSection 2.1: Physics of Earthquakes and Volcanoes\nHelmholtzstr. 6/7\n14467 Potsdam, Germany\n```\n\n## Download and Installation\n\n1. Install [Pyrocko](https://pyrocko.org/):\n    See Pyrocko installation page [here](https://pyrocko.org/docs/current/install/).\n\n2. Install NonLinLoc:\n    See NonLinLoc repository that is modified and packaged as SCOTER backend [here](https://gitext.gfz-potsdam.de/nooshiri/scoter-nonlinloc.git).\n\n3. Install SCOTER and the rest of dependencies:\n\n    ```bash\n    cd ~/src/   # or wherever you keep your source packages\n    git clone https://gitext.gfz-potsdam.de/nooshiri/scoter.git\n    cd scoter\n    sudo python setup.py install\n    ```\n\n## SCOTER Overview Poster (EGU 2018)\n\n<p align=\"center\">\n  <img src=\"images/Nooshiri_EGU2018_15263.png\" width=\"1100\" />\n</p>\n\n## Documentation\n\nThe SCOTER download includes a users guide, including examples that describes\nrudimentary usage. Input and output file formats are specified.\n",
        "createdAt": "2018-05-02T09:58:16.000Z",
        "updatedAt": "2025-11-08T04:33:15.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/nimanzik/scoter/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "earthinversion/convert-mseed2mat",
        "url": "https://github.com/earthinversion/convert-mseed2mat",
        "description": "Python utility program to convert mseed file to the mat format",
        "stars": 1,
        "forks": 1,
        "readme": "# Python utility program to convert mseed file to mat \n- by Utpal Kumar, IESAS, 2021/04\n- matlab script to read the mat data and analyze\n\n## Installation\n\n### Using Anaconda/Miniconda\n```\nconda create -n earthinversion\nconda activate earthinversion\nconda install -c conda-forge obspy\n```\n\n### Using venv\n```\npython -m venv earthinversion\nsource earthinversion/bin/activate\npip install obspy\n```\n\n### Usage\ntype `python convert_mseed_mat.py -h`\n\n```\nusage: convert_mseed_mat.py [-h] -inp INPUT_MSEED [-out OUTPUT_MAT]\n\nPython utility program to convert mseed file to mat (by Utpal Kumar, IESAS, 2021/04)\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -inp INPUT_MSEED, --input_mseed INPUT_MSEED\n                        input mseed file, e.g. example_2020-05-01_IN.RAGD..BHZ.mseed\n  -out OUTPUT_MAT, --output_mat OUTPUT_MAT\n                        output mat file name, e.g. example_2020-05-01_IN.RAGD..BHZ.mat\n```\n\n### Output data structure\n- `stats` contains all the meta data information corresponding to each trace and \n- `data` contain the time series data\n\n```\nmat_file.mat -> stats, data\nstats -> stats_0, stats_1, ...\ndata -> data_0, data_1, ...\n```\n\n### Example\n```\npython convert_mseed_mat.py -inp example_2020-05-01_IN.RAGD..BHZ.mseed\n```\n\n#### Utility Program Plot using Obspy\n![Utility Program Plot using Obspy](docs/example_2020-05-01_IN.RAGD..BHZ.png)\n\n#### MATLAB time series plot\n![MATLAB time series plot](docs/example_2020-05-01_IN.RAGD..BHZ_ts.jpg)\n\n#### MATLAB spectrogram plot\n![MATLAB spectrogram plot](docs/example_2020-05-01_IN.RAGD..BHZ_spectrogram.jpg)",
        "createdAt": "2021-04-27T06:19:47.000Z",
        "updatedAt": "2022-01-05T14:34:53.000Z",
        "language": "MATLAB",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/earthinversion/convert-mseed2mat/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "dhsheen/AI_seismology",
        "url": "https://github.com/dhsheen/AI_seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# 인공지능을 이용한 지진 분석 실습\n\n## 실습을 위한 명령어 모음\n\n### 1. 구글 colab을 활용한 실습에 필요한 초기 명령어\n   \n```python\n!pip install obspy\n```\n   \n### 2. 지진파 위상도달시각 및 진원 결정 실습에 필요한 자료와 코드 다운로드 방법\n- 2024년 6월 12일 규모 4.8 부안지진의 지진자료에서 인공지능을 활용하여 지진파 위상도달시각을 결정\n- 단순화시킨 비선형 역산 함수를 사용해 지진의 진원을 결정하는 실습 예제\n    \n```python\n!wget https://github.com/dhsheen/KFpicker/raw/refs/heads/main/KFpicker_20230217.h5\n\n!wget https://github.com/dhsheen/AI_seismology/raw/refs/heads/main/PhasePicking/EQLocateDL.py\n\n!wget https://github.com/dhsheen/AI_seismology/raw/refs/heads/main/PhasePicking/buan2024_practice.pkl\n\n```\n\n\n",
        "createdAt": "2025-08-29T11:24:29.000Z",
        "updatedAt": "2025-09-24T00:32:21.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/dhsheen/AI_seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "cdefabry/Seismology_Research",
        "url": "https://github.com/cdefabry/Seismology_Research",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# Seismology_Research",
        "createdAt": "2017-01-25T23:14:46.000Z",
        "updatedAt": "2017-01-25T23:17:21.000Z",
        "language": "Java",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/cdefabry/Seismology_Research/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "manjunath5496/Seismology-Books",
        "url": "https://github.com/manjunath5496/Seismology-Books",
        "description": "\"Our problems started in Dallas, when the fire-breathing sheep destroyed the King Tut exhibit.\"― Rick Riordan",
        "stars": 6,
        "forks": 2,
        "readme": "<table class=\"vertical-navbox nowraplinks\">\n<tbody>\n<tr>\n<th class=\"navbox-title\"><a title=\"Earthquake\" href=\"https://en.wikipedia.org/wiki/Earthquake\">Earthquakes</a></th>\n</tr>\n<tr>\n<td>\n<div id=\"NavFrame1\" class=\"NavFrame collapsed\">\n<div class=\"NavHead\"><a title=\"Types of earthquake\" href=\"https://en.wikipedia.org/wiki/Types_of_earthquake\">Types</a></div>\n<div class=\"NavContent hlist\">\n<ul>\n<li><a title=\"Foreshock\" href=\"https://en.wikipedia.org/wiki/Foreshock\">Foreshock</a></li>\n<li><a title=\"Aftershock\" href=\"https://en.wikipedia.org/wiki/Aftershock\">Aftershock</a></li>\n<li><a title=\"Blind thrust earthquake\" href=\"https://en.wikipedia.org/wiki/Blind_thrust_earthquake\">Blind thrust</a></li>\n<li><a title=\"Doublet earthquake\" href=\"https://en.wikipedia.org/wiki/Doublet_earthquake\">Doublet</a></li>\n<li><a title=\"Interplate earthquake\" href=\"https://en.wikipedia.org/wiki/Interplate_earthquake\">Interplate</a></li>\n<li><a title=\"Intraplate earthquake\" href=\"https://en.wikipedia.org/wiki/Intraplate_earthquake\">Intraplate</a></li>\n<li><a title=\"Megathrust earthquake\" href=\"https://en.wikipedia.org/wiki/Megathrust_earthquake\">Megathrust</a></li>\n<li><a title=\"Remotely triggered earthquakes\" href=\"https://en.wikipedia.org/wiki/Remotely_triggered_earthquakes\">Remotely triggered</a></li>\n<li><a title=\"Slow earthquake\" href=\"https://en.wikipedia.org/wiki/Slow_earthquake\">Slow</a></li>\n<li><a title=\"Submarine earthquake\" href=\"https://en.wikipedia.org/wiki/Submarine_earthquake\">Submarine</a></li>\n<li><a title=\"Supershear earthquake\" href=\"https://en.wikipedia.org/wiki/Supershear_earthquake\">Supershear</a></li>\n<li><a title=\"Tsunami earthquake\" href=\"https://en.wikipedia.org/wiki/Tsunami_earthquake\">Tsunami</a></li>\n<li><a title=\"Earthquake swarm\" href=\"https://en.wikipedia.org/wiki/Earthquake_swarm\">Earthquake swarm</a></li>\n</ul>\n</div>\n</div>\n</td>\n</tr>\n<tr>\n<td>\n<div id=\"NavFrame2\" class=\"NavFrame collapsed\">\n<div class=\"NavHead\">Causes</div>\n<div class=\"NavContent hlist\">\n<ul>\n<li><a title=\"Fault (geology)\" href=\"https://en.wikipedia.org/wiki/Fault_(geology)\">Fault movement</a></li>\n<li><a title=\"Volcano tectonic earthquake\" href=\"https://en.wikipedia.org/wiki/Volcano_tectonic_earthquake\">Volcanism</a></li>\n<li><a title=\"Induced seismicity\" href=\"https://en.wikipedia.org/wiki/Induced_seismicity\">Induced seismicity</a></li>\n</ul>\n</div>\n</div>\n</td>\n</tr>\n<tr>\n<td>\n<div id=\"NavFrame3\" class=\"NavFrame collapsed\">\n<div class=\"NavHead\">Characteristics</div>\n<div class=\"NavContent hlist\">\n<ul>\n<li><a title=\"Epicenter\" href=\"https://en.wikipedia.org/wiki/Epicenter\">Epicenter</a></li>\n<li><a title=\"Hypocenter\" href=\"https://en.wikipedia.org/wiki/Hypocenter\">Hypocenter</a></li>\n<li><a title=\"Shadow zone\" href=\"https://en.wikipedia.org/wiki/Shadow_zone\">Shadow zone</a></li>\n<li><a title=\"Seismic wave\" href=\"https://en.wikipedia.org/wiki/Seismic_wave\">Seismic waves</a></li>\n<li><a title=\"P-wave\" href=\"https://en.wikipedia.org/wiki/P-wave\">P-wave</a></li>\n<li><a title=\"S-wave\" href=\"https://en.wikipedia.org/wiki/S-wave\">S-wave</a></li>\n</ul>\n</div>\n</div>\n</td>\n</tr>\n<tr>\n<td>\n<div id=\"NavFrame4\" class=\"NavFrame collapsed\">\n<div class=\"NavHead\">Measurement</div>\n<div class=\"NavContent hlist\">\n<ul>\n<li><a title=\"Seismometer\" href=\"https://en.wikipedia.org/wiki/Seismometer\">Seismometer</a></li>\n<li><a title=\"Seismic magnitude scales\" href=\"https://en.wikipedia.org/wiki/Seismic_magnitude_scales\">Seismic magnitude scales</a></li>\n<li><a title=\"Seismic intensity scales\" href=\"https://en.wikipedia.org/wiki/Seismic_intensity_scales\">Seismic intensity scales</a></li>\n</ul>\n</div>\n</div>\n</td>\n</tr>\n<tr>\n<td>\n<div id=\"NavFrame5\" class=\"NavFrame collapsed\">\n<div class=\"NavHead\"><a title=\"Earthquake prediction\" href=\"https://en.wikipedia.org/wiki/Earthquake_prediction\">Prediction</a></div>\n<div class=\"NavContent hlist\">\n<ul>\n<li>\n<div><a title=\"Coordinating Committee for Earthquake Prediction\" href=\"https://en.wikipedia.org/wiki/Coordinating_Committee_for_Earthquake_Prediction\">Coordinating Committee for<br />Earthquake Prediction</a></div>\n</li>\n<li><a title=\"Earthquake forecasting\" href=\"https://en.wikipedia.org/wiki/Earthquake_forecasting\">Forecasting</a></li>\n</ul>\n</div>\n</div>\n</td>\n</tr>\n<tr>\n<td>\n<div id=\"NavFrame6\" class=\"NavFrame collapsed\">\n<div class=\"NavHead\">Other topics</div>\n<div class=\"NavContent hlist\">\n<ul>\n<li><a title=\"Shear wave splitting\" href=\"https://en.wikipedia.org/wiki/Shear_wave_splitting\">Shear wave splitting</a></li>\n<li><a title=\"Adams&ndash;Williamson equation\" href=\"https://en.wikipedia.org/wiki/Adams%E2%80%93Williamson_equation\">Adams&ndash;Williamson equation</a></li>\n<li><a title=\"Flinn&ndash;Engdahl regions\" href=\"https://en.wikipedia.org/wiki/Flinn%E2%80%93Engdahl_regions\">Flinn&ndash;Engdahl regions</a></li>\n<li><a title=\"Earthquake engineering\" href=\"https://en.wikipedia.org/wiki/Earthquake_engineering\">Earthquake engineering</a></li>\n<li><a title=\"Seismite\" href=\"https://en.wikipedia.org/wiki/Seismite\">Seismite</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Seismology\">Seismology</a></li>\n</ul>\n</div>\n</div>\n</td>\n</tr>\n<tr>\n<td class=\"hlist\">\n<ul>\n<li><a title=\"Portal:Earth sciences\" href=\"https://en.wikipedia.org/wiki/Portal:Earth_sciences\">Earth Sciences Portal</a></li>\n</ul>\n<ul>\n<li><a title=\"Category:Earthquakes\" href=\"https://en.wikipedia.org/wiki/Category:Earthquakes\">Category</a></li>\n<li><a title=\"Index of geology articles\" href=\"https://en.wikipedia.org/wiki/Index_of_geology_articles\">Related topics</a></li>\n</ul>\n</td>\n</tr>\n</tbody>\n</table>\n</br>\n<p><strong>Seismology</strong>&nbsp;(<span class=\"rt-commentedText nowrap\"><small>&nbsp;</small><span class=\"IPA nopopups noexcerpt\"><a title=\"Help:IPA/English\" href=\"https://en.wikipedia.org/wiki/Help:IPA/English\">/<span title=\"'s' in 'sigh'\">s</span><span title=\"/aɪ/: 'i' in 'tide'\">aɪ</span><span title=\"'z' in 'zoom'\">z</span><span title=\"/ˈ/: primary stress follows\">ˈ</span><span title=\"'m' in 'my'\">m</span><span title=\"/ɒ/: 'o' in 'body'\">ɒ</span><span title=\"'l' in 'lie'\">l</span><span title=\"/ə/: 'a' in 'about'\">ə</span><span title=\"/dʒ/: 'j' in 'jam'\">dʒ</span><span title=\"/i/: 'y' in 'happy'\">i</span>/</a></span></span>; from&nbsp;<a title=\"Ancient Greek\" href=\"https://en.wikipedia.org/wiki/Ancient_Greek\">Ancient Greek</a>&nbsp;&sigma;&epsilon;&iota;&sigma;&mu;ό&sigmaf; (<em>seism&oacute;s</em>) meaning \"earthquake\" and -&lambda;&omicron;&gamma;ί&alpha; (<em>-log&iacute;a</em>) meaning \"study of\") is the scientific study of&nbsp;<a title=\"Earthquake\" href=\"https://en.wikipedia.org/wiki/Earthquake\">earthquakes</a>&nbsp;and the propagation of&nbsp;<a title=\"Linear elasticity\" href=\"https://en.wikipedia.org/wiki/Linear_elasticity#Elastic_wave\">elastic waves</a>&nbsp;through the&nbsp;<a title=\"Earth\" href=\"https://en.wikipedia.org/wiki/Earth\">Earth</a>&nbsp;or through other planet-like bodies. The field also includes studies of&nbsp;<a title=\"Earthquake environmental effects\" href=\"https://en.wikipedia.org/wiki/Earthquake_environmental_effects\">earthquake environmental effects</a>&nbsp;such as&nbsp;<a class=\"mw-redirect\" title=\"Tsunamis\" href=\"https://en.wikipedia.org/wiki/Tsunamis\">tsunamis</a>&nbsp;as well as diverse&nbsp;<a title=\"Seismic source\" href=\"https://en.wikipedia.org/wiki/Seismic_source\">seismic sources</a>&nbsp;such as volcanic, tectonic, oceanic, atmospheric, and artificial processes such as explosions. A related field that uses&nbsp;<a title=\"Geology\" href=\"https://en.wikipedia.org/wiki/Geology\">geology</a>&nbsp;to infer information regarding past earthquakes is&nbsp;<a title=\"Paleoseismology\" href=\"https://en.wikipedia.org/wiki/Paleoseismology\">paleoseismology</a>. A recording of earth motion as a function of time is called a&nbsp;<a title=\"Seismogram\" href=\"https://en.wikipedia.org/wiki/Seismogram\">seismogram</a>. A seismologist is a scientist who does research in seismology.</p>\n</br>\n<h2> Books </h2>\n\n\n</br>\n\n\n\n<ul>\n\n                             \n\n <li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(1).pdf\" style=\"text-decoration:none;\">Asymptotic ray method in seismology: A tutorial</a></li>\n\n <li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(2).pdf\" style=\"text-decoration:none;\">Microearthquake Seismology and\nSeismotectonics of South Asia</a></li>\n\n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(3).pdf\" style=\"text-decoration:none;\">Mathematical Geophysics: A Survey of Recent Developments in Seismology and Geodynamics</a></li>\n <li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(4).pdf\" style=\"text-decoration:none;\">Special Topics in Earthquake\nGeotechnical Engineering</a></li>                              \n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(5).pdf\" style=\"text-decoration:none;\"> Perspectives\nin Modern Seismology</a></li>\n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(6).pdf\" style=\"text-decoration:none;\">Mine Seismology: Data Analysis and Interpretation</a></li>\n <li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(7).pdf\" style=\"text-decoration:none;\">Computational Seismology: A Practical Introduction</a></li>\n\n <li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(8).pdf\" style=\"text-decoration:none;\"> Advances in Indian\nEarthquake Engineering and Seismology </a></li>\n   <li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(9).pdf\" style=\"text-decoration:none;\">Mine Seismology: Seismic Response to the Caving Process</a></li>\n  \n   \n <li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(10).pdf\" style=\"text-decoration:none;\">Strong Ground Motion Seismology</a></li>                              \n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(11).pdf\" style=\"text-decoration:none;\"> An Introduction to\nSeismology, Earthquakes, and Earth Structure</a></li>\n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(12).pdf\" style=\"text-decoration:none;\">Historical Seismology: Interdisciplinary Studies of Past and Recent Earthquakes</a></li>\n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(13).pdf\" style=\"text-decoration:none;\">Instrumentation in Earthquake Seismology</a></li>\n\n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(14).pdf\" style=\"text-decoration:none;\">SEISMOLOGY</a></li>\n                              \n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(15).pdf\" style=\"text-decoration:none;\">Reflection Seismology: The Continental Crust</a></li>\n\n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(16).pdf\" style=\"text-decoration:none;\">Rotational Motions in Seismology,\nTheory and Application</a></li>\n\n  <li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(17).pdf\" style=\"text-decoration:none;\">Applied Seismology - A Comprehensive Guide to Seismic Theory and Application</a></li>   \n  \n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(18).pdf\" style=\"text-decoration:none;\">Basic Earthquake Engineering</a></li> \n\n  \n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(19).pdf\" style=\"text-decoration:none;\">Physics of Tsunamis </a></li> \n\n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(20).pdf\" style=\"text-decoration:none;\">Mathematical Aspects of Seismology</a></li>\n\n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(21).pdf\" style=\"text-decoration:none;\">Seismological Structure of Slabs</a></li>\n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(22).pdf\" style=\"text-decoration:none;\">Methods in Computational Physics</a></li> \n <li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(23).pdf\" style=\"text-decoration:none;\">Principles of Seismology</a></li> \n \n\n   <li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(24).pdf\" style=\"text-decoration:none;\">Introduction to Seismology</a></li>\n\n\n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(25).pdf\" style=\"text-decoration:none;\">Routine Data Processing in Earthquake Seismology </a></li> \n\n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(26).pdf\" style=\"text-decoration:none;\">Giant star seismology</a></li>\n\n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(27).pdf\" style=\"text-decoration:none;\">Numerical Methods of Exploration Seismology\nwith algorithms in MATLAB</a></li>\n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(28).pdf\" style=\"text-decoration:none;\">Seismic Analysis of Structures</a></li> \n <li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(29).pdf\" style=\"text-decoration:none;\">The Finite-Difference Modelling of Earthquake Motions: Waves and Ruptures</a></li> \n \n\n   <li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(30).pdf\" style=\"text-decoration:none;\">Elastic Wave Propagation and Generation in Seismology</a></li>\n\n\n\n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(31).pdf\" style=\"text-decoration:none;\">Shocks and Rocks: Seismology in the Plate Tectonics Revolution </a></li> \n\n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(32).pdf\" style=\"text-decoration:none;\">Aspects of Observational Seismology</a></li>\n\n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(33).pdf\" style=\"text-decoration:none;\">Seismology for rockburst prediction</a></li>\n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(34).pdf\" style=\"text-decoration:none;\">Greek Seismology</a></li> \n <li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(35).pdf\" style=\"text-decoration:none;\">Facilitating New Discoveries in Seismology and Exploring the Earth: The Next Decade</a></li> \n \n\n   <li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(36).pdf\" style=\"text-decoration:none;\">Basic Theory of Exploration Seismology</a></li>\n\n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(37).pdf\" style=\"text-decoration:none;\">Digital Seismology and Fine Modeling\nof the lithosphere</a></li>\n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(38).pdf\" style=\"text-decoration:none;\">Perspectives on European Earthquake Engineering and Seismology</a></li> \n <li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(39).pdf\" style=\"text-decoration:none;\">Elements of 3D Seismology</a></li> \n \n\n   <li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(40).pdf\" style=\"text-decoration:none;\">Modern global seismology</a></li>\n\n <li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(41).pdf\" style=\"text-decoration:none;\">Earthquake Engineering\nand Structural Dynamics in Memory of Ragnar Sigbjörnsson</a></li>\n\n\n   <li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(42).pdf\" style=\"text-decoration:none;\">International Handbook of\nEarthquake and Engineering Seismology</a></li>\n\n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(43).pdf\" style=\"text-decoration:none;\"> List of Seismological Stations of the World</a></li>\n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(44).pdf\" style=\"text-decoration:none;\">Modern Earthquake Engineering</a></li> \n <li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(45).pdf\" style=\"text-decoration:none;\">Quantitative Seismology</a></li> \n \n\n   <li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(46).pdf\" style=\"text-decoration:none;\">Introduction to Volcanic Seismology</a></li>\n\n <li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(47).pdf\" style=\"text-decoration:none;\">Seismology and the Structure of the Earth</a></li>\n\n\n\n <li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(48).pdf\" style=\"text-decoration:none;\">The Illustrated History of Natural Disasters</a></li>\n\n\n\n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(49).pdf\" style=\"text-decoration:none;\">A Crack in the Edge of the World: America and the Great California Earthquake of 1906</a></li> \n\n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(50).pdf\" style=\"text-decoration:none;\">A Disastrous History Of The World: Chronicles Of War, Earthquake, Plague And Flood</a></li>\n\n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(51).pdf\" style=\"text-decoration:none;\">Apocalypse : earthquakes, archaeology, and the wrath of God</a></li>\n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(52).pdf\" style=\"text-decoration:none;\">Catastrophes! Earthquakes, Tsunamis,\nTornadoes, and Other Earth-Shattering Disasters</a></li> \n <li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(53).pdf\" style=\"text-decoration:none;\">Dynamics of Structures: Theory and applications to earthquake engineering</a></li> \n \n\n   <li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(54).pdf\" style=\"text-decoration:none;\">Earthquake and Volcano Deformation</a></li>\n\n\n\n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(55).pdf\" style=\"text-decoration:none;\">Earthquake Engineering for Nuclear Facilities </a></li> \n\n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(56).pdf\" style=\"text-decoration:none;\">Earthquake Engineering: Theory and Implementation</a></li>\n\n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(57).pdf\" style=\"text-decoration:none;\">Earthquake Geotechnical Engineering Design</a></li>\n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(58).pdf\" style=\"text-decoration:none;\">Earthquake Resistant Design of Structures</a></li> \n <li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(59).pdf\" style=\"text-decoration:none;\">Earthquakes: Plate Tectonics and Earthquake Hazards</a></li> \n \n\n   <li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(60).pdf\" style=\"text-decoration:none;\">Encyclopedia of Earthquakes and Volcanoes</a></li>\n\n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(61).pdf\" style=\"text-decoration:none;\">Fundamentals of Earthquake Engineering</a></li>\n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(62).pdf\" style=\"text-decoration:none;\">How the Earth Works: 60 Fun Activities for Exploring Volcanoes, Fossils, Earthquakes, and More</a></li> \n <li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(63).pdf\" style=\"text-decoration:none;\">Living Under the Threat of Earthquakes: Short and Long-term Management of Earthquake Risks and Damage Prevention in Nepal</a></li> \n \n\n   <li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(64).pdf\" style=\"text-decoration:none;\">Plate Tectonics, Volcanoes, and Earthquakes</a></li>\n\n <li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(65).pdf\" style=\"text-decoration:none;\">Predicting the Unpredictable: The Tumultuous Science of Earthquake Prediction</a></li>\n\n\n   <li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(66).pdf\" style=\"text-decoration:none;\">Preventing\nEarthquake Disasters</a></li>\n\n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(67).pdf\" style=\"text-decoration:none;\"> Slope Earthquake Stability</a></li>\n<li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(68).pdf\" style=\"text-decoration:none;\">Soil Dynamics and Earthquake Geotechnical Engineering</a></li> \n <li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(69).pdf\" style=\"text-decoration:none;\">Soil Liquefaction During Earthquakes</a></li> \n \n\n   <li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(70).pdf\" style=\"text-decoration:none;\">The Parting of the Sea: How Volcanoes, Earthquakes, and Plagues Shaped the Exodus Story</a></li>\n\n <li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(71).pdf\" style=\"text-decoration:none;\">The Physics of Rock Failure and Earthquakes</a></li>\n\n\n\n <li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(72).pdf\" style=\"text-decoration:none;\">The San Francisco Earthquake and Fire of 1906 (Great Historic Disasters)</a></li>\n\n <li><a target=\"_blank\" href=\"https://github.com/manjunath5496/Seismology-Books/blob/master/sei(73).pdf\" style=\"text-decoration:none;\">Earthquake Thermodynamics and Phase\nTransformations in the Earth's Interior</a></li>\n\n\n\n   \n   </ul>\n",
        "createdAt": "2020-07-12T01:55:15.000Z",
        "updatedAt": "2024-08-12T20:03:38.000Z",
        "language": null,
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/manjunath5496/Seismology-Books/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "arahenrique-dev/Seismology-and-Structure-of-the-Earth-TP1",
        "url": "https://github.com/arahenrique-dev/Seismology-and-Structure-of-the-Earth-TP1",
        "description": "A MATLAB-based project analyzing Earth’s internal structure using the PREM and IASP91 models. Includes data visualization, pressure calculations, and seismic wave interpretation.",
        "stars": 1,
        "forks": 0,
        "readme": "# 🌍 Seismology and Structure of the Earth – TP1\n\nA practical project exploring Earth's internal structure using seismic data and velocity models (PREM and IASP91).  \nIncludes data analysis in **OpenOffice Calc** and numerical simulations in **MATLAB** to study density, velocity, and pressure variations with depth.\n\n## 🧠 Features\n- Visualization of seismic velocities and density profiles  \n- Calculation of lithostatic pressure  \n- Identification of major internal discontinuities  \n- Data plots using MATLAB\n\n## 📂 Files\n- `PREM.ods` – Dataset of Earth model parameters  \n- `tp1.m` – MATLAB script for data analysis and plotting  \n- `report.pdf` – Lab report and discussion  \n\n## ▶️ How to Run\n1. Open **MATLAB**.  \n2. Place all files (`PREM.ods`, `tp1.m`) in the same folder.  \n3. Open `tp1.m` in MATLAB.  \n4. Run the script (press **Run** or type `tp1` in the command window).  \n5. The plots and calculations will be displayed automatically.\n\n## 🧑‍💻 Authors\n- Henrique Pires Aragão  \n- Sibylle Paulsen  \n- Binh Minh Tran  \n\n**Sorbonne University – LU2ST035: Seismology and Structure of the Globe (2025)**\n",
        "createdAt": "2025-10-07T16:56:52.000Z",
        "updatedAt": "2025-10-07T17:07:50.000Z",
        "language": "MATLAB",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/arahenrique-dev/Seismology-and-Structure-of-the-Earth-TP1/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "niosh-mining/obsplus",
        "url": "https://github.com/niosh-mining/obsplus",
        "description": "A Pandas-Centric ObsPy Expansion Pack",
        "stars": 41,
        "forks": 10,
        "readme": ".. image:: https://user-images.githubusercontent.com/11671536/51515070-22241c00-1dc7-11e9-90d5-832f3caa5c70.png\n\n\"A Pandas-Centric ObsPy_ Expansion Pack\"\n\n|Coverage| |Supported Versions| |PyPI| |Conda| |DOI| |Licence|\n\nDocumentation_\n\nCode_\n\nChange_Log_\n\nContributing_\n\nSupport_\n\nLicense_\n\nAbout_\n\nCode_of_Conduct_\n\nIf you find ObsPlus useful consider citing it:\n\nChambers, D. J., Boltz, M. S., & Chamberlain, C. J. (2021).\nObsPlus: A Pandas-centric ObsPy expansion pack.\nJournal of Open Source Software, 6(60), 2696.\n\n\n.. _About: https://github.com/niosh-mining/about\n.. _ObsPy: https://github.com/obspy/obspy\n.. _Documentation: https://niosh-mining.github.io/obsplus/versions/latest/index.html\n.. _Support: https://niosh-mining.github.io/obsplus/versions/latest/notebooks/support.html\n.. _Code: https://github.com/niosh-mining/obsplus\n.. _Change_Log: https://github.com/niosh-mining/obsplus/CHANGELOG.txt\n.. _License: https://choosealicense.com/licenses/lgpl-3.0/\n.. _Code_of_Conduct: https://github.com/niosh-mining/obsplus/blob/master/.github/CODE_OF_CONDUCT.md\n.. _Contributing: https://niosh-mining.github.io/obsplus/versions/latest/notebooks/contributing.html\n\n.. |Coverage| image:: https://codecov.io/gh/niosh-mining/obsplus/branch/master/graph/badge.svg\n   :target: https://codecov.io/gh/niosh-mining/obsplus\n\n.. |Supported Versions| image:: https://img.shields.io/pypi/pyversions/obsplus.svg\n   :target: https://pypi.python.org/pypi/obsplus\n\n.. |Licence| image:: https://www.gnu.org/graphics/lgplv3-88x31.png\n   :target: https://www.gnu.org/licenses/lgpl.html\n\n.. |PyPI| image:: https://pepy.tech/badge/obsplus\n   :target: https://pepy.tech/project/obsplus\n\n.. |Conda| image:: https://img.shields.io/conda/dn/conda-forge/obsplus?label=conda%20downloads\n   :target: https://github.com/conda-forge/obsplus-feedstock\n\n.. |DOI| image:: https://zenodo.org/badge/DOI/10.5281/zenodo.4544008.svg\n   :target: https://doi.org/10.5281/zenodo.4544008\n",
        "createdAt": "2018-10-25T20:13:13.000Z",
        "updatedAt": "2025-09-19T12:47:44.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.4544008",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.4544008",
            "dataCite": "10.5281/zenodo.4544008",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/niosh-mining/obsplus/master/README.rst",
        "mainPaper": {
            "doi": "10.5281/zenodo.4544008",
            "title": "niosh-mining/obsplus: v0.2.0",
            "journal": "Zenodo",
            "dateReleased": "2021-02-17T00:00:00.000Z",
            "abstract": "This release fixes a number of bugs and adds a few new features. Notably, PyDantic is now used for validation and json conversions.",
            "citationsArray": []
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "bclswl0827/mseedio",
        "url": "https://github.com/bclswl0827/mseedio",
        "description": "A simple Golang module that deals with miniSEED files.",
        "stars": 6,
        "forks": 2,
        "readme": "# mseedio\nA simple Golang module that deals with MiniSEED files\n",
        "createdAt": "2023-08-06T18:10:40.000Z",
        "updatedAt": "2025-10-09T16:02:43.000Z",
        "language": "Go",
        "homepage": "https://pkg.go.dev/github.com/bclswl0827/mseedio",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/bclswl0827/mseedio/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "mtuqorg/seisgen",
        "url": "https://github.com/mtuqorg/seisgen",
        "description": "SEISGEN is a python package to acquire and generate the Greens function and synthetic waveform from the stored receiver-side 3D database including the Strain Greens Tensor (SGT) database and the (displacement) Greens function (DGF) database.",
        "stars": 11,
        "forks": 0,
        "readme": "# SEISGEN\n\n[![DOI](https://zenodo.org/badge/430866912.svg)](https://zenodo.org/badge/latestdoi/430866912)\n\n\n![SEISGEN](https://github.com/Liang-Ding/seisgen/blob/main/doc/figs/seisgen.png)\n\n\nSEISGEN is a python package to acquire and generate the Greens function and synthetic waveform from the stored receiver-side 3D database including the Strain Greens Tensor (SGT) database and the (displacement) Greens function (DGF) database. \nThe Greens function and synthetic waveform could be utilized by inversion packages such as [pyCAPLunar](https://github.com/Liang-Ding/pyCAPLunar), MTUQ and gCAP-series packages to determine the parameters of single-point sources including the moment tensor and force and finite fault models. \n\nThe 3D Greens function database is created by using the [SPECFEM3D_Cartesian software](https://geodynamics.org/resources/specfem3dcartesian) and [the python script](https://github.com/Liang-Ding/pyCAPLunar/blob/master/DSEM_Utils/merge_strainfield.py) in the project [pyCAPLunar](https://github.com/Liang-Ding/pyCAPLunar).\n\n\n## Installation\nFor basic install:\n```shell\ngit clone https://github.com/Liang-Ding/seisgen.git\ncd seisgen\npip install -e .\n```\nor using pip \n```shell\npip install seisgen\n```\n\n## Package Structure\n![SEISGEN](https://github.com/Liang-Ding/seisgen/blob/main/doc/figs/seisgen_structure.png)\n \nThe seisgen package requires the following database to work:\n* The stored Greens function database, such as the SGT, DGF database\n* The 3D background model utilized to create the SGT database \n* The HDF5 file storing the locations and interpolation parameters of the points within the 3D model. \n\n## Documentation\n<a href=\"https://github.com/Liang-Ding/seisgen/tree/main/doc/usage/examples.md\">examples</a>\n",
        "createdAt": "2021-11-22T21:14:44.000Z",
        "updatedAt": "2025-10-24T15:49:53.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/mtuqorg/seisgen/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "pratyush1234/My-seismology-Page",
        "url": "https://github.com/pratyush1234/My-seismology-Page",
        "description": "Pushing HTML and CSS codes for webpage development. Matlab codes for inversion and optimization",
        "stars": 0,
        "forks": 0,
        "readme": "# My-seismology-Page\nPushing HTML and CSS codes for webpage development. Matlab codes for inversion and optimization\n",
        "createdAt": "2020-04-24T14:30:01.000Z",
        "updatedAt": "2020-04-24T14:30:05.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/pratyush1234/My-seismology-Page/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "flexie/-EAS4803-8803",
        "url": "https://github.com/flexie/-EAS4803-8803",
        "description": "Exploration Seismology",
        "stars": 0,
        "forks": 1,
        "readme": "# -EAS4803-8803\n",
        "createdAt": "2018-08-16T01:51:44.000Z",
        "updatedAt": "2021-05-06T19:26:20.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/flexie/-EAS4803-8803/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "s-schneider/BowPy",
        "url": "https://github.com/s-schneider/BowPy",
        "description": "Development of a f-k method toolbox applicable to seismological array data ",
        "stars": 18,
        "forks": 4,
        "readme": "# Seismic-Toolbox: bowPy\nDevelopment of a toolbox applicable to seismological array data using Body Waves, containing f-k methods for data reconstruction and filtering\n\n\nInstallation:\n\n- clone/download the repo\n\n- cd into bowPy\n\n- python3 -m venv env\n\n- source env/bin/activate\n\n- (env) pip install .\n\n- (env) pip install basemap\n\nstart ipython\n\nvoilá\n\n\nExample of FK-filter\n```\nfrom obspy.core.stream import read as read_stream\nfrom obspy.core.inventory import read_inventory\nfrom obspy.core.event import read_events\nfrom bowpy.filter.fk import fk_filter\nfrom bowpy.util.array_util import vespagram\n\n\nst = read_stream(\"PATH_TO_DATA/*.SAC\")\ninv = read_inventory(\"PATH_TO_INVENTORY_FILE\")\nevents = read_events(\"PATH_TO_EVENT_FILE\")\n\nvespa_data = vespagram(st, inv=inv, event=events[0])\n\nfitered_st = fk_filter(st, inv=inv, event=events[0])\n\nvespa_data_filtered = vespagram(fitered_st, inv=inv, event=events[0])\n\n```",
        "createdAt": "2015-11-10T08:12:09.000Z",
        "updatedAt": "2025-07-14T06:43:56.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/s-schneider/BowPy/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "hadrienmichel/SardineReborn",
        "url": "https://github.com/hadrienmichel/SardineReborn",
        "description": "Seismic refraction - New implementation of the Sardine software ",
        "stars": 18,
        "forks": 4,
        "readme": "![Sardine Reborn Logo](./images/SardineRebornLogo_100ppp.png)\n\n[![made-with-python](https://img.shields.io/badge/Made%20with-Python-1f425f.svg)](https://www.python.org/) [![GitHub license](https://img.shields.io/github/license/hadrienmichel/SardineReborn.svg)](https://github.com/hadrienmichel/SardineReborn) \n\n[![GitHub release](https://img.shields.io/github/release/hadrienmichel/SardineReborn.svg)](https://GitHub.com/hadrienmichel/SardineReborn/releases/) [![DOI](https://zenodo.org/badge/456904646.svg)](https://zenodo.org/badge/latestdoi/456904646)\n\n[![GitHub commits](https://img.shields.io/github/commits-since/hadrienmichel/SardineReborn/latest.svg)](https://GitHub.com/hadrienmichel/SardineReborn/commit/) [![GitHub latest commit](https://badgen.net/github/last-commit/hadrienmichel/SardineReborn)](https://GitHub.com/hadrienmichel/SardineReborn/commit/)\n# Sardine Reborn\n\nSardine reborn is a new implementation of the Sardine software (Sardine v 1.0, ULiège (2000)). Sardine originaly stands for Seismic And Radar data INtErpretation.\n\nThis new implementation is fully devellopped in Python with a pyQT GUI. Currently, the software has limited functionalities but can be easily expended for custom needs (filtering, automatic picking, etc.).\n\n## Functionalities\nSardine Reborn has several functionalities. All are oriented towards the more pedagogical aspect than for production-readyness. This means that very few actions are automated andthat the software is aimed as an hands-on approach to seismic refraction data.\n- [Sardine Reborn](#sardine-reborn)\n  - [Functionalities](#functionalities)\n  - [Installation](#installation)\n  - [Picking](#picking)\n  - [Inversion](#inversion)\n    - [Other options:](#other-options)\n  - [Modelling](#modelling)\n- [References:](#references)\n- [Troubleshooting:](#troubleshooting)\n- [Collaboration:](#collaboration)\n\nEach of those functionalities have a dedicated tab in the UI. The user can save picking and models, as well as figures that are produced. \n\n## Installation\nTo install the package and code, download the code using this link:\n[https://github.com/hadrienmichel/SardineReborn/archive/refs/tags/v0.5.1.zip](https://github.com/hadrienmichel/SardineReborn/archive/refs/tags/v0.5.1.zip). Un-zip the downloaded archive and locate the directory where it is placed (C:/my-directory)\n\nIn `Anaconda prompt` run the following command (the file `sardineEnv.yml` must be in the current directory):\n``` \ncd \"C:/my-directory\"\nconda env create -f sardineEnv.yml\n```\nThis will create a new environement (called `sardine`) with the dependencies for the project.\n\nThen, activate the environement:\n```\nconda activate sardine\n```\nOnce the enviroement created, you can run the code by typing in the command line:\n```\npython Interface.pyw\n```\n\nThe code will launch automatically.\n\n__Attention: the current implementation of the code is not stable to user errors. Save your progresses along the way!__\n\n__Above all, avoid accents and specific characters in the path that leads to your data and saving location as some dependencies of the code cannot work with those!__\n\n___Example files are available in the `example` folder.___\n\n## Picking\n![Picking window layout](./images/pickingTab.PNG)\n\nIn the picking tab, you can load data from `*.segy` or `*.seg2` data (read operations done through [obspy](https://github.com/obspy/obspy)). To read a file, you must use a custom formatted header file that contains the informations about the files (extension = `*.geometry`). To load a `*.geometry` file, go to the menu `File` and select the option `Open geometry file` or press `CTRL+O`.\n\nThe formatting of the geometry file with `2` sources and `n` receivers is given in the snippet below:\n```\nSOURCES\nFilename1.seg2   sourceX sourceY\nFilename2.seg2   sourceX sourceY\nRECEIVERS\nr1X r1Y\nr2X r2Y\n...\nrnX rnY\n```\n\nNote that for the current version, the geometry file only supports a single array of receivers with multiple sources. In the case of roll-alongs, you must use multiple geometry files and pick them separetly. It is then possible to merge them manually into a single *.sgt file and carry on with the inversion.\n\nIn the picking tab, two graphs will appear:\n- The full seismic traces for the current source file.\n- A zoomed-in visualization of the current trace.\n\nThe current trace is overlayed in red. The zoomed-in graph updates itself with the current X position of the mouse on the main graph. It helps to pick an accurate first-break of the trace. You can also use the zoom option on the main graph to zoom on a part of full traces and have a finer picking.\n\nTo select the first arrival, `left click` on the position. The position will be locked and represented in green on both graphs. By default, a 3% error is assigned (and showed) for all picking. You can select a custom error by `right clicking` on the upper (or lower) bound of the error.\n\nYou can change the source file by selecting the corresponding file in the drop-down menu at the top of the tab.\n\nOnce you are happy with the current picking, you can set the picking by selecting the `Set picking` button. To resume picking, click back on the same button. __It is strongly advised to set the picking to stop the animation process in the background an gain computing performances for the inversion!__ \n\nIn case you want to begin back from scratch, you can select the `Reset picking` button to remove all picking.\n\nOnce everything is ready, you can save the picking into a `*.sgt` file (in the menu `File` select `Save Current Picking` or `CTRL+P`). This file uses the unified data formatting from [pyGIMLI](https://www.pygimli.org/). An exemple of the formatting is given in the snippet below:\n```\n20 # shot/geophone points\n#x\ty\n0.00\t0.00\n3.00\t0.00\n6.00\t0.00\n9.00\t0.00\n12.00\t0.00\n15.00\t0.00\n18.00\t0.00\n21.00\t0.00\n24.00\t0.00\n27.00\t0.00\n33.00\t0.00\n36.00\t0.00\n39.00\t0.00\n42.00\t0.00\n48.00\t0.00\n51.00\t0.00\n54.00\t0.00\n57.00\t0.00\n60.00\t0.00\n63.00\t0.00\n19 # measurements\n#s\tg\tt\terr\n1\t2\t0.006738\t0.002257\n1\t3\t0.011867\t0.003385\n1\t4\t0.015765\t0.001436\n1\t5\t0.017304\t0.000821\n1\t6\t0.019253\t0.000923\n1\t7\t0.020997\t0.001539\n1\t8\t0.024177\t0.000616\n1\t9\t0.026229\t0.000923\n1\t10\t0.027665\t0.000821\n1\t11\t0.031871\t0.000616\n1\t12\t0.031974\t0.001436\n1\t13\t0.034846\t0.000821\n1\t14\t0.036180\t0.000718\n1\t15\t0.037718\t0.001334\n1\t16\t0.039155\t0.000616\n1\t17\t0.041309\t0.000051\n1\t18\t0.042950\t0.000718\n1\t19\t0.045207\t0.001539\n1\t20\t0.046746\t0.001128\n```\n\nYou can have topography for the position of the geophones as those will be considered for the inversion. However, modelling does only work without topography!\n\nLastely, you can also load an already existing `*.sgt` file. In case the geometry corresponds with the already loaded signals, the picking will be displayed on top. Loading a `*.sgt` file will also initialize the inversion and modelling windows if possible.\n\n## Inversion\nThe inversion of the first arrival is performed by interfacing the pyGIMLI API and giving access to the main inversion options. Travel-time tomography works better with mutliple sources and receivers but can still work with a limited number of sources. __Note that the results of the inversion might not result in an accurate estimation of the subsurface due to under-constraints on the model!__ \n\nTo load an existing `*.sgt` file, use either of those 3 options:\n- Menu `File` select `Load Picking File`\n- Press `CTRL+L`\n- Click on the `. . .` button\n\nA mesh will automatically appear with the default parameters (maximum cell size of 5 m² and depth of 50 meters). The data will also be plotted.\n\n![Initialized inversion tab](./images/inversionTab.PNG)\n\nYou can change the inversion parameters in the `Inversion options` part of the tab. The different options are:\n- `Lambda`: the regularization parameter. \n- `Z-weight`: the relative weight for vertical-to-horizontal regularization. \n- `V<sub>top</sub>` and `V<sub>bottom</sub>`: the top and bottom velocities for the start model (and by extension the reference model).\n- `Min. velocity` and `Max. velocity`: the minimum and maximum velocities accepted in the inversion process.\n- `Mesh max. cell size`: the maximum size of the cells in the created mesh.\n- `Mesh max. depth`: the maximum depth (relative to the surface) of the mesh.\n\nThe different parameters have different effects on the inversion. The regularization parameter will apply a higher or lower weight to the variations of the obtained model in the objective function. A high regularization parameter will penalize small structures in the model. On the other hand, an underregularized inversion might lead to numerous artefacts in the inversion due to overfitting of the data. The vertical-to-horizontal regularization will reward horizontal (value lower than 1.0) or vertical (value above 1.0) features in the inversion model.\n\nThe starting model can have a huge impact on the results of the inversion. Therefore, it is important to select a starting model that is close to the a-priori idea of the site investigated. You can also have a uniform starting model by putting the top and bottom velocities to the same value.\n\nIn the case you might already know the geology relatively well, you can set the limits for the inversion velcoities accordingly. Leave this interval large in doubt.\n\nFor the mesh parameters, they will have an impact on the final model that is obtained. The finer the mesh, the better, but the longer to computational time. The deeper the mesh, the lower the chance of interferences from the boundary conditions on the inversion results. However, a finner and bigger mesh will lead to more difficult computations.\n\nWhen all the parameters are selected, click the `Run inversion` button to run the inversion (using [pyGIMLI](https://www.pygimli.org/)).\n\n### Other options:\nIn the `Inversion` menu, you can save the inversion mesh, the inversion results and responses, and the inversion. Those are done using the options:\n- `Load Inversion Mesh`: Load an inversion mesh (GIMLi format)\n- `Load Initial Model`: Load *.vector file that contains the velocities for each cells of the mesh.\n- `Save Inversion Mesh`: Save the current inversion mesh (GIMLi format)\n- `Save the Inverse Response`: Save the last inversion response (travel times)\n- `Save the Inverse Results`: Save the last inversion model (velocities)\n- `Save Inversion as VTK`: Save the inversion results in *vtk to read in paraview for changes in the visualization/ localization at large scale.\n\n\n![Inversion results displayed](./images/inversionTabFinal.PNG)\n\nBy playing with the different parameters, you will notice that they have a strong impact on the obtained inverse model. Finde the ideal set of parameters that suits your needs by comparing the misfit to the error on each points until you reach a satisfactory estimation.\n## Modelling\n\nThe modelling of the data is perfomed using a simple Mota model (see: [Mota, 1954. Determination of dips and depths of geological layers by the seismic refraction method. Geophysics, 19 (2).](https://pubs.geoscienceworld.org/geophysics/article/19/2/242/67153/Determination-of-dips-and-depths-of-geological)). To load an existing `*.sgt` file, use either of those 3 options:\n- Menu `File` select `Load Picking File`\n- Press `CTRL+L`\n- Click on the `. . .` button\n\nOnce the data is loaded, a first guess model is proposed. A first guess based on the maximum curvature is proposed.\n\n![First guess of the modelling](./images/modellingTab.PNG)\n\nYou can change the model by moving the black points present in the hodochones graph. To do so, select a combination of source and orientation. Then, select `Move Points`. You can move freely all the points for this combination except from the first one (the source position). \n\n__Unselect `Move Points` when the selection is done to gain performances on the computer!__\n\nYou can also add a layer in the model by increasing the number in `Select the number of layers` box.\n\n\n# References:\nThis code relies on several python libraries appart from the common numpy, matplotlib, etc.:\n- ObsPy: used for the reading of the seismic traces\n\n    Moritz Beyreuther, Robert Barsch, Lion Krischer, Tobias Megies, Yannik Behr and Joachim Wassermann (2010), [ObsPy: A Python Toolbox for Seismology](http://www.seismosoc.org/publications/SRL/SRL_81/srl_81-3_es/), _SRL_, 81(3), 530-533,  doi:`10.1785/gssrl.81.3.530`.\n- pyGIMLI: used for the inversion of travel-time tomography\n\n    Rücker, C., Günther, T., Wagner, F.M., 2017. [pyGIMLi: An open-source library for modelling and inversion in geophysics](http://www.sciencedirect.com/science/article/pii/S0098300417300584/pdfft?md5=44253eaacd5490e3fb32210671672496&pid=1-s2.0-S0098300417300584-main.pdf), _Computers and Geosciences_, 109, 106-123, doi:`10.1016/j.cageo.2017.07.011`.\n\n# Troubleshooting:\nHere are some common issues that can arise and how to troubelshoot them.\n\n- I cannot read my seg2 files, even though they are correctly stated in the `*.geometry` file:\n\n    The reading operations are performed using obspy.read. In case the seg2 file was written on a computer that is not setup in English, this can lead to inconsistancies in the date-time reading as those can be written in your language and not in English. \n\n    You can change this behaviour by adding the abbreviations for your language in the `header.py` file of obspy (`...\\obspy\\io\\seg2\\header.py`). In case this does not resolve the issue, report back and create an issue with the `*.geometry` file, the `*.seg2` file and a log of the error message that was provided in your terminal.\n\n- More to be added... Please report issues you might enconter to help improve the code.\n\n# Collaboration:\nA huge thanks for @asuliege for the extensive testing of the code and all the feedback. ",
        "createdAt": "2022-02-08T11:29:51.000Z",
        "updatedAt": "2025-07-26T06:58:39.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.6833415",
            "openAlex": "10.5281/zenodo.6833415",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "10.5281/zenodo.6833415",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/hadrienmichel/SardineReborn/master/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.6833415",
            "title": "Sardine Reborn",
            "journal": "Zenodo",
            "dateReleased": "2021-08-11T00:00:00.000Z",
            "abstract": "",
            "citationsArray": []
        },
        "repoDoi": "10.5281/zenodo.6833415",
        "publications": [
            {
                "doi": "10.5281/zenodo.6833415",
                "name": "Sardine Reborn",
                "source": "",
                "authorNames": [],
                "abstract": "",
                "publicationDate": "2021-08-11T00:00:00.000Z"
            }
        ]
    },
    {
        "source": "GitHub",
        "name": "ib31iat/earthquake-monitoring",
        "url": "https://github.com/ib31iat/earthquake-monitoring",
        "description": "This is a projected conducted in the Data Science Lab during the Master of Data Science at ETH",
        "stars": 2,
        "forks": 0,
        "readme": "# EarthEarthquake Monitoring - Python Setup\n- Python version: 3.11.5\n- Install packages: `pip install -r requirements.txt` (first install seisbench from the submodule)\n\n## Seisbench\nSince we adapt EQTransformer to only use one channel, the `seisbench` project is added as a submodule.  To install the *local* version of `seisbench` as opposed to the one on `pypi`, run the following from the project root with your virtual env active:\n```bash\ncd external/seisbench\npip install .\n```\nThis installs the `seisbench` package inside `external/seisbench` into the current virtual environment.\n\nAfter changes to the local `seisbench` package, `pip install .` needs to rerun for those changes to propagate to any place where `import seisbench` or similar is used.\n\n## SWAG\nRun `ln -s external/swa_gaussian/swag .` in the project root such that the swag script can find the swag submodule.\n",
        "createdAt": "2023-10-11T07:50:12.000Z",
        "updatedAt": "2024-10-17T10:24:57.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ib31iat/earthquake-monitoring/master/README.md",
        "mainPaper": {
            "doi": "",
            "title": "Deep Learning Earthquake Monitoring",
            "dateReleased": "2023-12-22T00:00:00.000Z"
        },
        "repoDoi": "",
        "publications": [
            {
                "doi": "",
                "name": "Deep Learning Earthquake Monitoring",
                "source": "",
                "authorNames": [],
                "publicationDate": "2023-12-22T00:00:00.000Z"
            }
        ]
    },
    {
        "source": "GitHub",
        "name": "mpalssonur/SeismologyCourse",
        "url": "https://github.com/mpalssonur/SeismologyCourse",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2025-10-15T14:21:35.000Z",
        "updatedAt": "2025-10-22T23:56:52.000Z",
        "language": "TeX",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "javiquinte/owndc",
        "url": "https://github.com/javiquinte/owndc",
        "description": "FDSN Dataselect for seismologists which allows the access to any data centre",
        "stars": 4,
        "forks": 0,
        "readme": "Owndc\n~~~~~\n\n.. image:: https://img.shields.io/pypi/l/owndc.svg\n   :target: https://img.shields.io/pypi/l/owndc.svg\n\n.. image:: https://img.shields.io/pypi/v/owndc.svg\n   :target: https://img.shields.io/pypi/v/owndc.svg\n   \n.. image:: https://img.shields.io/pypi/pyversions/owndc.svg\n   :target: https://img.shields.io/pypi/pyversions/owndc.svg\n   \n.. image:: https://img.shields.io/pypi/format/owndc.svg\n   :target: https://img.shields.io/pypi/format/owndc.svg\n   \n.. image:: https://img.shields.io/pypi/status/owndc.svg\n   :target: https://img.shields.io/pypi/status/owndc.svg\n   \nFDSN-WS Dataselect for the seismological community which allows the access to any data centre.\n\nPurpose\n-------\n\nowndc was originally designed to give a SeisComP3 user access to waveforms in any data centre in the world.\nIt provides a fully complaint FDSN-WS Dataselect interface, which is compatible with any standard client\nusing this type of service.\n\nIn SeisComP3, the results from the FDSN-WS Station are not needed. Mainly because the metadata must be already\nloaded in the SC3 database. This is the main reason why we do not provide also support for the Station-WS service.\n\nIt provides an automatic way to configure it to access all data stored at EIDA nodes.\n\nVery, very quick installation instructions\n------------------------------------------\n\nUsing pip\n=========\nExecute the following commands to deploy [owndc from Pypi](https://pypi.python.org/pypi/owndc). ::\n\n  $ sudo pip install owndc\n  $ owndcupdate --reset\n  $ owndc\n\n\nFrom the Github repository\n==========================\nThe execution of the following commands will deploy and setup a ready-to-run\nowndc instance with access to all the data within\n[EIDA](http://www.orfeus-eu.org/eida/). ::\n\n  $ git clone https://github.com/javiquinte/owndc\n  $ cd owndc\n  $ cp owndc.cfg.sample owndc.cfg\n  $ python2 owndcupdate.py --reset\n  $ ./owndc.py\n\nCreate the owndc documentation\n------------------------------\n\nVery detailed instructions regarding installation as well as different aspects\nof the software can be found in the official documentation provided in this\nrepository. In order to generate it you should execute these commands:\n\n```\n$ cd doc\n$ make latexpdf\n$ evince _build_/latex/owndc.pdf\n```\n\n",
        "createdAt": "2015-09-24T20:08:21.000Z",
        "updatedAt": "2025-06-10T10:04:56.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/javiquinte/owndc/master/README.rst",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Patrick-Cole/pygmi",
        "url": "https://github.com/Patrick-Cole/pygmi",
        "description": "PyGMI - Python Geophysical Modelling and Interpretation",
        "stars": 99,
        "forks": 20,
        "readme": "PyGMI\n=====\n\n.. |pythonversion| image:: https://img.shields.io/pypi/pyversions/pygmi\n   :alt: PyPI - Python Version\n   :target: https://pypi.org/project/pygmi\n.. |pygmiversion| image:: https://img.shields.io/pypi/v/pygmi\n   :alt: PyPI - Version\n   :target: https://pypi.org/project/pygmi\n.. |pygmilicence| image:: https://img.shields.io/github/license/patrick-cole/pygmi\n   :alt: GitHub License\n   :target: https://github.com/Patrick-Cole/pygmi/blob/pygmi3/LICENSE.txt\n.. |pygmirelease| image:: https://img.shields.io/github/release/patrick-cole/pygmi\n   :alt: GitHub Release\n   :target: https://github.com/Patrick-Cole/pygmi/releases\n.. image:: https://joss.theoj.org/papers/10.21105/joss.07019/status.svg\n   :target: https://doi.org/10.21105/joss.07019\n\n\n|pythonversion| |pygmiversion| |pygmilicence| |pygmirelease|\n\nOverview\n--------\n\nPyGMI stands for Python Geoscience Modelling and Interpretation. It is a modelling and interpretation suite aimed at magnetic, gravity, remote sensing and other datasets. PyGMI has a graphical user interface, and is meant to be run as such.\n\nPyGMI is developed at the `Council for Geoscience <http://www.geoscience.org.za>`_ (Geological Survey of South Africa).\n\nIt includes:\n\n* Magnetic and Gravity 3D forward modelling.\n* Cluster Analysis, including use of scikit-learn libraries.\n* Routines for cutting, reprojecting and doing simple modifications to data.\n* Convenient display of data using pseudo-color, ternary and sunshaded representation.\n* MT processing and 1D inversion using MTpy.\n* Gravity processing.\n* Seismological functions for SEISAN data.\n* Remote sensing ratios and improved imports.\n\nIt is released under the `Gnu General Public License version 3.0 <http://www.gnu.org/copyleft/gpl.html>`_\n\nThe PyGMI `Wiki <http://patrick-cole.github.io/pygmi/index.html>`_ pages, include installation and full usage! Contributors can check this `link <https://github.com/Patrick-Cole/pygmi/blob/pygmi3/CONTRIBUTING.md>`_ for ways to contribute.\n\nThe latest release version (including windows installers) can be found `here <https://github.com/Patrick-Cole/pygmi/releases>`_.\n\nYou may need to install the `Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019 <https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads>`_.\n\nIf you have any comments or queries, you can contact the author either through `GitHub <https://github.com/Patrick-Cole/pygmi>`_ or via email at pcole@geoscience.org.za\n\n\nInstallation\n------------\nThe simplest installation of PyGMI is on Windows, using a pre-built installer at `64-bit <https://github.com/Patrick-Cole/pygmi/releases>`_.\n\nIf you prefer building from source, you can use PyPi or Conda.\n\nOnce built using PyPi, running pygmi can be done at the command prompt as follows:\n\n   pygmi\n\nIf you are in python, you can run PyGMI by using the following commands:\n\n   from pygmi.main import main\n\n   main()\n\nIf you prefer not to install pygmi as a library, download the source code and execute the following command to run it manually:\n\n   python quickstart.py\n\nRequirements\n^^^^^^^^^^^^\nPyGMI will run on both Windows and Linux. It should be noted that the main development is done in Python 3.13 on Windows.\n\nPyGMI should still work with Python 3.10.\n\nPyGMI is developed and has been tested with the following libraries in order to function:\n\n* fiona>=1.10.1\n* geopandas>=1.0.1\n* h5netcdf>=1.6.1\n* matplotlib>=3.10.1\n* natsort>=8.4.0\n* numba>=0.61.2\n* numexpr>=2.10.2\n* openpyxl>=3.1.5\n* psutil>=7.0.0\n* pyside6>=6.9.0\n* pytest>=8.3.5\n* pyvista>=0.45.0\n* pyvistaqt>0.11.2\n* rasterio>=1.4.3\n* rioxarray>=0.19.0\n* scikit-learn>=1.6.1\n* scikit-image>=0.25.2\n* shapelysmooth>=0.2.1\n* simpeg>=0.24.0\n* beautifulsoup4>=4.13.4\n* pwlf>=2.5.1\n\nPyPi - Windows\n^^^^^^^^^^^^^^\nWindows users can use the `WinPython <https://winpython.github.io/>`_ distribution as an alternative to Anaconda. It comes with most libraries preinstalled, so using pip should be sufficient.\n\nInstall with the following command.\n\n   pip install pygmi\n\nShould you wish to manually install binaries, related binaries can be obtained at the `website <https://github.com/cgohlke/geospatial-wheels/>`_ by Christoph Gohlke.\n\nIf you wish to update GDAL, you will need to download and install:\n\n* fiona\n* GDAL\n* pyproj\n* rasterio\n* Rtree\n* shapely\n\nAll these binaries should be downloaded since they have internal co-dependencies.\n\nPyPi - Linux\n^^^^^^^^^^^^\nLinux normally comes with python installed, but the additional libraries will still need to be installed.\n\nThe process is as follows:\n\n   sudo apt-get install pipx\n   \n   pipx ensurepath\n\n   pipx install pygmi\n\nOnce installed, running pygmi can be done at the command prompt as follows:\n\n   pygmi\n\nIf you get the following error: *qt.qpa.plugin: Could not load the Qt platform plugin \"xcb\" in \"\" even though it was found.*, then you can try the following command, since this is Linux issue:\n\n   sudo apt-get install libxcb-xinerama0\n\nAnaconda\n^^^^^^^^\nAnaconda users are advised not to use pip since it can break PyQt5. However, one package is installed only by pip, so a Conda environment should be created.\n\nThe process to install is as follows:\n\n   conda create -n pygmi python=3.13\n\n   conda activate pygmi\n\n   conda config --env --add channels conda-forge\n\n   conda install -c anaconda pyqt=6.7.1\n\n   conda install fiona\n\n   conda install matplotlib\n\n   conda install psutil\n\n   conda install numexpr\n\n   conda install rasterio\n\n   conda install geopandas\n\n   conda install natsort\n\n   conda install numba\n\n   conda install scikit-learn\n\n   conda install scikit-image\n\n   conda install pyvista\n\n   conda install pyvistaqt\n\n   conda install simpeg\n\n   conda install shapelysmooth\n\n   conda install openpyxl\n\n   conda install h5netcdf\n\n   conda install rioxarray\n\n   conda install pytest\n\n   conda install beautifulsoup4\n\n   conda install pyside6\n\n   conda install pyyaml\n\n   conda update --all\n\nOnce this is done, download pygmi, extract (unzip) it to a directory, and run it from its root directory with the following command:\n\n   python quickstart.py\n\nReferences\n----------\n\n* Cole, P. 2012, Development of a 3D Potential Field Forward Modelling System in Python, AGU fall meeting, 3-7 December, San Francisco, USA\n* Cole, P. 2013, PyGMI – The use of Python in geophysical modelling and interpretation. South African Geophysical Association, 13th Biennial Conference, Skukuza Rest Camp, Kruger National Park (7-9 October)\n* Cole, P. 2014, The history and design behind the Python Geophysical Modelling and Interpretation (PyGMI) package, SciPy 2014, Austin, Texas (6-12 July)\n* Cole, P. 2016, The continued evolution of the open source PyGMI project. 35th IGC, Cape Town.\n* Cole, P. 2025, PyGMI - a python package for geoscience modelling and interpretation. Journal of Open Source Software, 10(111), 7019, https://doi.org/10.21105/joss.07019",
        "createdAt": "2014-07-18T07:44:47.000Z",
        "updatedAt": "2025-12-04T12:34:10.000Z",
        "language": "HTML",
        "homepage": "http://patrick-cole.github.io/pygmi/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Patrick-Cole/pygmi/pygmi3/README.rst",
        "mainPaper": {
            "doi": "",
            "title": "The history and design behind the Python Geophysical Modelling and Interpretation (PyGMI) package",
            "dateReleased": null,
            "abstract": "PyGMI stands for Python Geoscience Modelling and Interpretation. It is a modelling and interpretation suite aimed at magnetic, gravity and other datasets."
        },
        "repoDoi": "",
        "publications": [
            {
                "doi": "",
                "name": "The history and design behind the Python Geophysical Modelling and Interpretation (PyGMI) package",
                "source": "",
                "authorNames": [],
                "abstract": "PyGMI stands for Python Geoscience Modelling and Interpretation. It is a modelling and interpretation suite aimed at magnetic, gravity and other datasets.",
                "publicationDate": null
            }
        ]
    },
    {
        "source": "GitHub",
        "name": "Z0207917/seismic-research",
        "url": "https://github.com/Z0207917/seismic-research",
        "description": "Seismology Dissertation",
        "stars": 0,
        "forks": 0,
        "readme": "# Seismic Noise Clustering Code\n\nProprietary dissertation code for clustering analysis in seismic research.  \n\n## Contents\n- `Dissertation code.py` – Python script implementing the clustering workflow  \n- `quake-catalogue.csv` – catalogued earthquake data (from IRIS Wilber 3)  \n- `latest-jma-wind-data.csv` – historical wind speed data (from JMA Past Weather Data Download)  \n",
        "createdAt": "2025-09-04T10:37:31.000Z",
        "updatedAt": "2025-09-05T08:20:44.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Z0207917/seismic-research/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Darkening-Silhouette/Advanced_Reflection_Seismology",
        "url": "https://github.com/Darkening-Silhouette/Advanced_Reflection_Seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Darkening-Silhouette/Advanced_Reflection_Seismology/blob/main/JMAG111_point_source_demo.ipynb)\n",
        "createdAt": "2025-09-17T10:03:09.000Z",
        "updatedAt": "2025-09-17T10:18:57.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Darkening-Silhouette/Advanced_Reflection_Seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "eguzmanv/Hypocenter-cross-sections",
        "url": "https://github.com/eguzmanv/Hypocenter-cross-sections",
        "description": "This repository provides a Python code for generating cross-section plots in seismology research. The method offers an easy and efficient way to create cross-sections, allowing you to analyze hypocenter distribution.",
        "stars": 7,
        "forks": 1,
        "readme": "# Cross-section generation with Python\n\nThis repository provides a Python code for generating cross-section plots in seismology research. The method offers an easy and efficient way to create cross-sections, allowing you to analyze hypocenter distribution.\n\n## Overview\n\n![Seismicity example](figs/readme_figs/0.jpg)\n![Cross-section example1](figs/readme_figs/0_A.jpg)\n![Cross-section example2](figs/readme_figs/0_B.jpg)\n\n## Features\n\n- **Easy cross-section generation:** Learn a step-by-step approach to project hypocenters onto section lines, obtain projected coordinates, and write projection data.\n- **Contiguous cross-sections**: Generate a set of n contiguous cross-sections with a desired width.\n- **Flexible visualization options**\n- **Alternative to Pygmt**: This repository provides an alternative solution for cross-section generation without relying on the Pygmt library.\n\n## Getting started\n\nFollow the instructions below to get started with using cross-section generation:\n\n1. **Installation**: Install the required Python libraries by running '*pip install -r requirements.txt'.\n2. **Data preparation**: Prepare your event catalog in the required format.\n\n## Dependencies\n\nThe following Python libraries are used in this project:\n\n- NumPy\n- Shapely\n- Matplotlib\n- Obspy\n\n## Colab\n\nOpen the Jupyter Notebook in Google Colab:\n\n[![Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eguzmanv/Hypocenter-cross-sections/blob/main/hypo_xsects_COLAB.ipynb)\n\n## Contributing\n\nContributions to this repository are welcome! If you have any suggestions/improvements/bug fixes, please feel free to submit a pull request.\n\n## License\n\nThis project is licensed under the **MIT License**.\n\n## Contact\n\nEmmanuel Guzman Vitola\nemguzmanvi@unal.edu.co\n",
        "createdAt": "2023-07-04T04:23:20.000Z",
        "updatedAt": "2025-03-16T05:56:28.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/eguzmanv/Hypocenter-cross-sections/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "schipp/eikonal_tomography",
        "url": "https://github.com/schipp/eikonal_tomography",
        "description": "Synthetic demonstration of Eikonal tomography",
        "stars": 9,
        "forks": 5,
        "readme": "# Eikonal Tomography - Synthetic Demonstration\n\n[![DOI](https://zenodo.org/badge/365721985.svg)](https://zenodo.org/badge/latestdoi/365721985)\n\n![example](img/example.png)\n\nThe Eikonal equation relates local phase velocity c with the local gradient of travel times ∇T: ∇T = 1/c.\n\nWhen a wave field is densely sampled, sufficiently dense maps of travel times and thus travel time gradients can be computed. This allows to retrieve phase velocity maps without the need for costly inversion algorithms. In seismology, this has been applied in some cases where spatially homogeneous and sufficiently dense spacing of stations was available. There, seismologists usually exploit the ambient seismic field to compute *estimated Green's Functions* between station-pairs from cross-correlations of the ambient seismic field to measure seismic wave velocities. This approach can provide the spatially and azimuthally dense sampling necessary for Eikonal tomography to function (e.g., Lin et al. 2009, de Ridder et al. 2015).\n\nThis repository is a synthetic demonstration of that principle. For this, a velocity model and synthetic station locations are defined. Travel times between those stations are estimated using the Fast Marching Method (FMM). From these measured travel times, interpolated maps of travel time are generated. Gradients are computed for these maps, and the final velocity model is the mean model from all virtual source stations. The error is the standard deviation of those models.\n\n## Requirements\n\n* python (tested on 3.9)\n* numpy, scipy, matplotlib\n* tqdm (*because the measurement of travel times can take a long time*)\n* [scikit-fmm](https://github.com/scikit-fmm/scikit-fmm)\n\n## References\n\nLin, F.C., Ritzwoller, M.H., Snieder, R. (2009) Eikonal tomography: surface wave tomography by phase front tracking across a regional broad-band seismic array. <i>Geophysical Journal International</i>, <b>177</b>, 1091–1110. doi:10.1111/j.1365-246X.2009.04105.x\n\nRidder, S.A.L. de, Biondi, B.L. &#38; Nichols, D. (2015) Elliptical-anisotropic eikonal phase velocity tomography. <i>Geophysical Research Letters</i>, <b>42</b>, 758–764. doi:10.1002/(ISSN)1944-8007",
        "createdAt": "2021-05-09T10:09:24.000Z",
        "updatedAt": "2025-09-22T09:06:51.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/schipp/eikonal_tomography/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "komec/EQE582_Lecture",
        "url": "https://github.com/komec/EQE582_Lecture",
        "description": "Seismology Tools for Civil Engineers",
        "stars": 5,
        "forks": 3,
        "readme": "<p align=\"left\">  <img src=\"./images/GTU_LOGO_1200X768_JPG_EN_Small.jpg\" width = 20% > </p>\n\n---\n\n<div style='background-image: \"intro_pic.png\" ; padding: 0px ; background-size: cover ; border-radius: 5px ; height: 500px'>\n    <div style=\"float: center ; margin: 50px ; padding: 30px ; background: rgba(255 , 255 , 255 , 0.7) ; width: 50% ; height: 150px\">\n        <div style=\"position: left ; top: 50% ; transform: translatey(-50%)\">\n            <div style=\"font-size: x-large ; font-weight: 600 ; color: rgba(0 , 0 ,10 , 0.6) ; line-height: 70%\">Department of Civil Engineering</div>\n            <div style=\"font-size: xx-large ; font-weight: 900 ; color: rgba(200 , 0 , 0 , 0.8) ; line-height: 200%\">EQE 582_ Introduction to SEISMOLOGY</div>\n            <div style=\"font-size: large ; padding-top: 10px ; color: rgba(0 , 0 , 0 , 0.7); line-height: 80%\">Python applications for Seismology</div>\n            <img src=\"images/intro_pic.png\" style=\"width:60%\"><img>\n        </div>\n    </div>\n</div>\n\nThis repository covers six weeks of **_\"EQE582_Introduction to Seismology\"_** lecture applications. It starts with introductory to Python and proceeds with the seismology tools for **Earhquake and Structure Engineering** program graduate students.\n\n\n---\nAttend to [Slack App](https://app.slack.com/client/T01FY4SLLSW/C01FV26R0J1);an informal platform for **Q/A** discussions about Python coding and more, with your collegues.\n\n---\n## Contact\n\nAhu Kömeç Mutlu :smiley: [@komec](https://twitter.com/ahukomecmutlu)\n\n---\n## Acknowledgements\n* [Python documenation](http://docs.python.org/)\n* Seismo-Live: http://seismo-live.org\n* [IPython/Jupyter notebook](http://www.nature.com/news/interactive-notebooks-sharing-the-code-1.16261)\n\n---\n",
        "createdAt": "2020-12-01T20:04:01.000Z",
        "updatedAt": "2023-11-07T07:30:59.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/komec/EQE582_Lecture/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "avillasenorh/OCR-bulletins",
        "url": "https://github.com/avillasenorh/OCR-bulletins",
        "description": "OCR, Proofreading and Processing Scanned Seismological Bulletins",
        "stars": 0,
        "forks": 0,
        "readme": "# OCR-bulletins\nOCR, Proofreading and Processing Scanned Seismological Bulletins\n\n1. Contents of this repository\n\n* `src`: software for processing and proof-reading OCR output from seismic bulletins\n* `stations`: files with list of valid station names used in the ISS bulletins (1918-1963)\n* `examples`: example files to test the processing codes\n\n\n2. Contents of the `src` repository\n\n* `fmt56`: C programs to proofread OCR output from ISS bulletins for the time period 1918-1956\n* `fmt57`: C programs to proofread OCR output from ISS bulletins for the time period 1957-1963\n* `baas`: C programs to reformat BAAS bulletins\n* `gr`: C programs to reformat Gutenberg's notebooks\n* `isc`: C programs to convert output of `fmt56` and `fmt57` to ISC's FFB format\n\n3. Usage\n\nProofread and reformat txt files:\n\n`$ fmt56 i53_0020.txt`\n`$ fmt57 i59_0020.txt`\n\n\n",
        "createdAt": "2020-08-03T12:57:16.000Z",
        "updatedAt": "2020-08-03T14:43:50.000Z",
        "language": "C",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/avillasenorh/OCR-bulletins/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "velgueta/Theoretical_seismology25",
        "url": "https://github.com/velgueta/Theoretical_seismology25",
        "description": "In this repo I will post my homeworks and projects for this class",
        "stars": 0,
        "forks": 0,
        "readme": "# Theoretical Seismology 2025 — Wavefield Gradients\n\nReproducible code and notebooks for synthesizing 3-C seismograms, rotating to LQT, and estimating **strain** and **rotation** from far-field theory and **finite differences**. Includes geometry studies (spacing/aperture), bandwidth effects, and accuracy metrics (NRMSE).\n\n## What’s inside\n- **`notebooks/Homework1_Theoretical_seismology2025.ipynb`** – Main workflow (Parts D–F, figures).\n- **Core functions (in the notebook)**:\n  - `sdr_to_mt(...)`, `add_clvd_iso(...)` (moment tensors: DC, DC+CLVD+ISO)\n  - `make_surface_array(...)`, `make_surface_array_from_lambdaS(...)` (station grids)\n  - `enz_to_lqt_array(...)` (ENZ → L/Q/T)\n  - Synthesis (far-field P+S via \\(\\dot M(t)\\)), Theory: `strain_th_from_displacement(...)`, `omega_th_from_displacement(...)`\n  - FD: `fd_strain(...)` (centered differences on surface arrays)\n  - Plots: `plot_LQT_all_stations(...)`\n  - Metrics: NRMSE vs. \\(\\Delta/\\lambda_S\\)\n\n## How to run\n```bash\npython -m venv .venv && source .venv/bin/activate\npip install -r requirements.txt        # numpy, scipy, matplotlib (extend if needed)\njupyter lab                            # open the notebook and run top→bottom\n\n",
        "createdAt": "2025-10-26T07:44:12.000Z",
        "updatedAt": "2025-10-26T07:57:13.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/velgueta/Theoretical_seismology25/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "openeew/openeew-alert-twitter",
        "url": "https://github.com/openeew/openeew-alert-twitter",
        "description": "Prototyping sending Tweets when an earthquake is detected by OpenEEW-Seismology",
        "stars": 2,
        "forks": 0,
        "readme": "# openeew-alert-twitter\n\nThis repository contains a Dockerfile that can be used for local development.\n\nTo build an a docker image, from the root directory run the following:\n\n```\ndocker build --tag alert-twitter:dev .\n```\n\nModify the .env file with relevant information.\n\nTo then run this docker image execute the following command:\n\n```\ndocker run \\\n  --interactive \\\n  --detach \\\n  --env-file .env \\\n  --name alert-twitter \\\n  alert-twitter:dev\n```\n\n**Formatter**\n\nThis repository is written in Python and runs Black on all Pull Request.\n\nTo install and run black formatter:\n\n```\npip install black\nblack /path/to/file\n```\n",
        "createdAt": "2021-05-06T19:46:29.000Z",
        "updatedAt": "2021-11-08T09:33:48.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/openeew/openeew-alert-twitter/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "pysmo/pysmo",
        "url": "https://github.com/pysmo/pysmo",
        "description": "Python types for Seismology",
        "stars": 22,
        "forks": 5,
        "readme": "<h1 align=\"center\">pysmo</h1>\n\n<div align=\"center\">\n<a href=\"https://github.com/pysmo/pysmo/actions/workflows/run-tests.yml\" target=\"_blank\">\n<img src=\"https://github.com/pysmo/pysmo/actions/workflows/run-tests.yml/badge.svg\" alt=\"Test Status\">\n</img></a>\n<a href=\"https://github.com/pysmo/pysmo/actions/workflows/build.yml\" target=\"_bank\">\n<img src= \"https://github.com/pysmo/pysmo/actions/workflows/build.yml/badge.svg\" alt=\"Build Status\">\n</img></a>\n<a href=\"https://pysmo.readthedocs.io/en/latest/?badge=latest\" target=\"_blank\">\n<img src=\"https://readthedocs.org/projects/pysmo/badge/?version=latest\" alt=\"Documentation Status\">\n</img></a>\n<a href=\"https://codecov.io/gh/pysmo/pysmo\" target=\"_blank\">\n<img src=\"https://codecov.io/gh/pysmo/pysmo/branch/master/graph/badge.svg?token=ZsHTBN4rxF\" alt=\"codecov\">\n</img></a>\n<a href=\"https://pypi.org/project/pysmo/\" target=\"_blank\">\n<img src=\"https://img.shields.io/pypi/v/pysmo\" alt=\"PyPI\">\n</img></a></div>\n\n<p align=\"center\">\n<em>Documentation:</em> <a href=\"https://docs.pysmo.org\" target=\"_blank\">https://docs.pysmo.org</a>\n</p>\n<p align=\"center\">\n<em>Source Code:</em> <a href=\"https://github.com/pysmo/pysmo\" target=\"_blank\">https://github.com/pysmo/pysmo</a>\n</p>\n\n---\nThe addition of type annotations to Python marked a significant step forward\nin user experience (e.g. intelligent autocompletion in modern editors) and type\nsafety (by catching errors before code is executed). With types thus becoming\nmore meaningful, it is worth revisiting what a type should *mean* to a\nseismologist.\n\nTraditionally a lot of data are stored together (e.g. in a seismogram file of\nsome format containing a time series and its metadata), only for most of that\ndata to be ignored during processing. While it makes sense to *store* the data\ntogether, it is better to split them up into far simpler types when writing\ncode for *processing*.\n\nPysmo offers simple data types for seismologists to write code with. Instead\nof working with one big class containing all kinds of data, psymo uses\nseparate, narrowly defined protocol classes that are not tied to any particular\nfile format or existing class.\n\nCode written with pysmo types is easy to understand and maintain. Most\nimportantly, it can often be reused in different projects, thus reducing the\nduplication of effort.\n\nPysmo itself is designed to be modular and easy to expand without interfering\nwith the existing code base, making it straightforward to incorporate user\ncontributions.\n",
        "createdAt": "2012-09-19T13:36:39.000Z",
        "updatedAt": "2025-10-20T10:53:03.000Z",
        "language": "Python",
        "homepage": "https://docs.pysmo.org",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/pysmo/pysmo/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Dengda98/PyFMM",
        "url": "https://github.com/Dengda98/PyFMM",
        "description": "PyFMM is a C/Python package for solving eikonal equation using Fast Marching/Sweeping Method, with examples and annotations online. 基于Fast Marching/Sweeping Method求解程函方程得到走时场的C/Python程序包，在线文档包括示例和注释。 ",
        "stars": 19,
        "forks": 1,
        "readme": "\n<h1 align=\"center\"><b><i>PyFMM</i></b></h1>\n\n<p align=\"center\">\n  <a href=\"https://pyfmm.readthedocs.io/zh-cn/latest/?badge=latest\">\n    <img src=\"https://readthedocs.org/projects/pyfmm/badge/?version=latest\" alt=\"Documentation Status\" />\n  </a>\n  <a href=\"https://zenodo.org/doi/10.5281/zenodo.13823187\">\n    <img src=\"https://zenodo.org/badge/860537381.svg\" alt=\"DOI\" />\n  </a>\n  <img alt=\"GitHub code size in bytes\" src=\"https://img.shields.io/github/languages/code-size/Dengda98/PyFMM\">\n  <img alt=\"GitHub License\" src=\"https://img.shields.io/github/license/Dengda98/PyFMM\">\n  <img alt=\"GitHub Actions Workflow Status\" src=\"https://img.shields.io/github/actions/workflow/status/Dengda98/PyFMM/build.yml\">\n\n</p>\n\n\n<p align=\"center\">\n  <img src=\"./figs/output2.png\" alt=\"Image 2\" width=\"55%\" />\n  <img src=\"./figs/output3.png\" alt=\"Image 3\" width=\"42%\" />\n</p>\n\n**欢迎Star！**\n\n[**PyFMM**](https://github.com/Dengda98/PyFMM) 是一个基于 **Fast Marching/Sweeping Method** 求解程函方程 $|\\nabla T|^2 = s^2$ 的C/Python程序包，包括示例和注释。  其中 **Fast Sweeping Method** 包括了并行版本，详见[**在线文档**](https://pyfmm.readthedocs.io/zh-cn/latest/)或文献 [(Zhao, 2007)](https://www.jstor.org/stable/43693378)。\n\n[**PyFMM**](https://github.com/Dengda98/PyFMM) is a C/Python package for solving eikonal equation using Fast Marching/Sweeping Method, with examples and annotations.  \n\nAt present, **PyFMM** can run on\n  - [x] Linux\n  - [x] macOS\n  - [x] Windows\n\n----\n\n我还制作了一个简易图形界面 [**PyFMM-GUI**](https://github.com/Dengda98/PyFMM-GUI) 计算二维走时场，初学者可更好的理解射线追踪，也可更方便、直观地看到不同速度场下射线的扭曲形态。\n\n![](https://github.com/Dengda98/PyFMM-GUI/blob/main/figs/example.gif)\n\n-------\n</br>\n\n我主要使用 **PyFMM** 计算地震波从震源出发在复杂介质中传播形成的初至波走时场，\n并使用梯度下降获得满足费马原理的射线路径，故代码中的一些术语偏专业性。\n类似的原理也可用于其它方面，如计算点到曲线/面的距离，或光学、电磁学等。\n\n\n+ **Python语言的便携、可扩展性与C语言的计算高效特点结合**。\n  C程序被编译链接成动态库 *libfmm.so* ，**PyFMM** 再基于Python的 [ctypes](https://docs.python.org/3/library/ctypes.html)\n  标准库实现对C库函数的调用。再基于第三方库 [NumPy](https://numpy.org/)、 \n  [SciPy](https://scipy.org/) 等可很方便地完成对C程序结果的数据整合；\n\n\n+ C代码采取模块化编写，各功能分在不同代码文件中，方便移植到其它程序；\n\n\n+ 支持二维和三维情况；2D and 3D\n\n\n+ 支持直角坐标系和球坐标系；Cartesian and Spherical Coordinate\n\n\n+ 中文注释及示例；\n\n<br>\n\n\n# 文档 Documents\n为方便使用，我建立了[**在线文档**](https://pyfmm.readthedocs.io/zh-cn/latest/)，包括简易安装、API的介绍以及使用示例。  \n\n<br>\n\n# 安装 Installation \n\n**新版本已添加预编译的C动态库**，无需本地再编译，支持`pip`命令一键安装：\n```bash\npip install pyfmm-kit\n```\n\n\n<br>\n\n\n# 使用示例 Usage Example \n更多使用示例详见[**在线文档**](https://pyfmm.readthedocs.io/zh-cn/latest/)。\n``` python \nimport pyfmm \nimport numpy as np \nimport matplotlib.pyplot as plt\nfrom scipy import interpolate\n\npyfmm.logger.myLogger.setLevel('ERROR')\n\n# 定义网格 \nnx, ny, nz = 401, 1, 101\nxarr = np.linspace(0, 200, nx)\nyarr = np.array([0.0])\nzarr = np.linspace(0, 50, nz)\n\n# 定义1D速度\nvel1d = np.array([\n    [0.0, 3.2],\n    [5.0, 5.8],\n    [15.0, 6.5],\n    [30.0, 6.8],\n    [35.0, 8.1],\n    [80.0, 8.2]\n])\n\n# 插值1d分层速度\n# _idxs = np.searchsorted(vel1d[:,0], zarr)\n# velocity = vel1d[_idxs, 1]\n# OR\n# 插值1d梯度速度 \nvelocity = interpolate.interpn((vel1d[:,0],), vel1d[:,1], zarr)\n\n# 慢度数组 \nslowness = np.empty((nx, ny, nz))\nslowness[...] = 1.0/velocity[None,None,:]\n\n# 定义震源位置\nsrcloc = [0.0, 0.0, 0.0]\n\n# 计算时间场\nTT = pyfmm.travel_time_source(\n    srcloc,\n    xarr, yarr, zarr, slowness)\n\n#====================================================================\n# 绘制走时场和射线\nfig, ax1 = plt.subplots(1, 1)\ncs = ax1.contour(xarr, zarr, TT[:, 0, :].T, levels=30, linewidths=0.5)\nax1.clabel(cs)\n\nfor x in np.arange(5, 200, 5):\n    # 射线追踪\n    rcvloc = [x, 0, 0]\n\n    travt, rays = pyfmm.raytracing(\n        TT, srcloc, rcvloc, xarr, yarr, zarr, 0.1)\n    ax1.plot(rays[:,0], rays[:,2], c='r', lw=0.8, ls='--')\n\nax1.set_aspect('equal')\nax1.set_xlim(0, 200)\nax1.set_ylim(0, 50)\nax1.yaxis.set_inverted(True)\n\n```\n![](https://github.com/Dengda98/PyFMM/blob/main/figs/example.png)\n\n\n# 其它\n代码是我在研二写的，如果遇到bug，欢迎联系我(zhudengda@mail.iggcas.ac.cn)，我会完善！\n也欢迎提出建议和更多示例！\n\n基于PyFMM的体波走时反演以及面波反演后续也会开源。\n",
        "createdAt": "2024-09-20T16:13:07.000Z",
        "updatedAt": "2025-09-16T06:23:24.000Z",
        "language": "C",
        "homepage": "https://pyfmm.readthedocs.io/zh-cn/latest/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.13823187",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.13823187",
            "dataCite": "10.5281/zenodo.13823187",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Dengda98/PyFMM/main/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.13823187",
            "title": "Dengda98/PyFMM: v0.5.0",
            "journal": "Zenodo",
            "dateReleased": "2025-09-03T00:00:00.000Z",
            "abstract": "Bug fixed\n\n\n\n修复分段计算射线走时bug。\n\n预编译阶段在ubuntu系统中使用docker在centos 7环境中编译代码,以使用较低版本的glibc。这一点与 PyGRT PR #50一致。\n\n\nWhat's Changed\n\n\n\nFIX: add last segemnt contribution to traveltime in raytracing by @Dengda98 in https://github.com/Dengda98/PyFMM/pull/2\n\n\nFull Changelog: https://github.com/Dengda98/PyFMM/compare/v0.4.0...v0.5.0",
            "citationsArray": []
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "DeepLearningORC/ConvNets_Seismology",
        "url": "https://github.com/DeepLearningORC/ConvNets_Seismology",
        "description": "Discrimination of distributed acoustic sensing strain data using convolutional neural networks",
        "stars": 0,
        "forks": 0,
        "readme": "# ConvNets_Seismology\nThis repository provides an overview of the results from a research study done on discriminating images generated from ocean bottom fibre optic cables.\n# Model specifics\nThe model is based on a convolutional neural network architecture that has been optimised using Gaussian Processes regression.\n",
        "createdAt": "2023-05-31T10:27:24.000Z",
        "updatedAt": "2023-05-31T10:49:19.000Z",
        "language": null,
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/DeepLearningORC/ConvNets_Seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "LindsayWorthington/lobomoho",
        "url": "https://github.com/LindsayWorthington/lobomoho",
        "description": "seismology codes from University of New Mexico",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2020-02-29T00:24:49.000Z",
        "updatedAt": "2020-02-29T00:24:49.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "hydrocoast/DC3D.f90",
        "url": "https://github.com/hydrocoast/DC3D.f90",
        "description": "Okada (1992) DC3D model in Fortran free-form",
        "stars": 13,
        "forks": 2,
        "readme": "<p align=\"center\">\n<img src=\"https://github.com/hydrocoast/DC3D.f90/blob/master/test/uxuyuz_strike-slip.png\", width=\"250\">\n<img src=\"https://github.com/hydrocoast/DC3D.f90/blob/master/test/uxuyuz_dip-slip.png\", width=\"250\">\n<img src=\"https://github.com/hydrocoast/DC3D.f90/blob/master/test/uxuyuz_tensile.png\", width=\"250\">\n</p>  \n\n# DC3D0/DC3D Fortran\n\n## Overview\nDC3D.f90 is a subroutine package in Fortran free-form for calculating deformation due to a fault model.  \nThis is just a file converted from the original code [DC3D0/DC3D](http://www.bosai.go.jp/study/application/dc3d/DC3Dhtml_E.html) in FORTRAN77 fixed-form\nand no additional function is given.\nAny bug report would be appreciated.\n\n## Usage\nTo compile [DC3D.f90](https://github.com/hydrocoast/DC3D.f90/blob/master/DC3D.f90) and generate an object file, clone this repository and run the following command on the top directory:\n```bash\nmake obj\n```\nThe shared object can also be generated using `make so`.\nNote that a Fortran compiler (e.g. `ifort` and `gfortran`) and an environment variable `FC` are required to run these commands.  \n\n## Tests\nYou can quickly test the codes as follows:\n```bash\nmake test\n```\nFiles in the directory `test` can help you for further testing.\nSome of them need to be run with the Julia language.\n```bash\nmake so\ncd test\njulia -q\njulia> include(\"./plot_strike-dip-tensile.jl\")\n```\n\n\n## Validation\n`test/validation_DC3D.jl` compares return values of the original code (FORTRAN77) and the converted one (Fortran90/95).  \n<p align=\"center\">\n<img src=\"https://github.com/hydrocoast/DC3D.f90/blob/master/test/diffu.svg\", width=\"900\">\n</p>  \n\n\n## License\nMIT  \n\nThe author got permission in an e-mail massage to upload this repository from National Research Institute for Earth Science and Disaster Resilience (NIED), the copyright holder of the original file.\nNIED also takes no responsibility for any damage and loss.\n\n## Author\nTakuya Miyashita  \nDisaster Prevention Research Institute, Kyoto University  \nmiyashita@hydrocoast.jp  \nhttp://hydrocoast.jp   \n",
        "createdAt": "2020-01-14T13:00:35.000Z",
        "updatedAt": "2025-09-11T09:11:12.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/hydrocoast/DC3D.f90/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "mpross/Single-Station-Seismology",
        "url": "https://github.com/mpross/Single-Station-Seismology",
        "description": "Analysis code for extracting density profiles from Rayleigh wave phase velocities which are measured using precision tilt sensors and a traditional seismometer.",
        "stars": 6,
        "forks": 2,
        "readme": "Analysis code for extracting density profiles from Rayleigh wave phase velocities which are measured using precision tilt sensors and a traditional seismometer. In addition to using seismic tilt signals in a new way, this is also a \"single station\" measurement meaning that the density profile would be measured for a single location. The traditional way of obtaining these profiles is to use an array of seismometers which effectively average over the spatial extent of the array.\n\nPrevious publication using these devices: https://arxiv.org/abs/1707.03084\n\nDescription of methods: https://github.com/mpross/Single-Station-Seismology/blob/master/velocityProfileDescription.pdf\n",
        "createdAt": "2018-09-27T17:01:21.000Z",
        "updatedAt": "2025-05-14T05:35:22.000Z",
        "language": "MATLAB",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/mpross/Single-Station-Seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "UT-GlobalSeismology/UT-GlobalSeismology.github.io",
        "url": "https://github.com/UT-GlobalSeismology/UT-GlobalSeismology.github.io",
        "description": "Repository for GitHub Pages",
        "stars": 2,
        "forks": 0,
        "readme": "# Global Seismology Group of the University of Tokyo\n\nThe Global Seismology Group is engaged in exciting interdisciplinary research topics in solid earth science, from the exploration of Earth's interior using waveform inversion to planetary geology.\n\nWebsite: https://utglobalseismology.org\n\nOrganization page on GitHub: https://github.com/UT-GlobalSeismology\n\n\n## Softwares\n\n### 1. Direct Solution Method (DSM)\n\nSoftware for computing synthetic seismograms in a spherically symmetric, transversely isotropic (TI) model using the Direct Solution Method (DSM).\n\nhttps://github.com/UT-GlobalSeismology/DSMsynTI-mpi\n\n\n### 2. ANISOtime\n\nSoftware for computing travel times of seismic waves in a spherically symmetric, transversely isotropic (TI) medium.\n\nhttps://github.com/UT-GlobalSeismology/anisotime\n",
        "createdAt": "2020-05-02T07:57:07.000Z",
        "updatedAt": "2024-06-03T03:09:34.000Z",
        "language": null,
        "homepage": "https://ut-globalseismology.github.io/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/UT-GlobalSeismology/UT-GlobalSeismology.github.io/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "pyrocko/fomosto-psgrn-pscmp",
        "url": "https://github.com/pyrocko/fomosto-psgrn-pscmp",
        "description": "An official read-only mirror of https://git.pyrocko.org/pyrocko/fomosto-psgrn-pscmp. Fomosto backend for psgrn and pscmp",
        "stars": 30,
        "forks": 13,
        "readme": "# PSGRN and PSCMP (packaged as fomosto backend)\n\n[![Build Status](https://travis-ci.org/pyrocko/fomosto-psgrn-pscmp.svg?branch=master)](https://travis-ci.org/pyrocko/fomosto-psgrn-pscmp)\n\nCode to calculate synthetic stress/strain/tilt/gravitational fields on a\nlayered viscoelastic halfspace.\n\nPSGRN and PSCMP have been written by Rongjiang Wang.\n\nPackaging has been done by Hannes Vasyura-Bathke.\n\n## References\n\n- Wang, R., F. Lorenzo-Martín and F. Roth (2003), Computation of deformation\n  induced by earthquakes in a multi-layered elastic crust - FORTRAN programs\n  EDGRN/EDCMP, Computer and Geosciences, 29(2), 195-207.\n- Wang, R., F. Lorenzo-Martin and F. Roth (2006), PSGRN/PSCMP - a new code for\n  calculating co- and post-seismic deformation, geoid and gravity changes\n  based on the viscoelastic-gravitational dislocation theory, Computers and\n  Geosciences, 32, 527-541. DOI:10.1016/j.cageo.2005.08.006.\n- Wang, R. (2005), The dislocation theory: a consistent way for including the\n  gravity effect in (visco)elastic plane-earth models, Geophysical Journal\n  International, 161, 191-196.\n\n# Compile and install PSGRN and PSCMP\n```sh\nautoreconf -i   # only if 'configure' script is missing\n./configure\nmake\nsudo make install\n```\n\n# To run PSGRN/PSCMP with more receivers/source points\n\n1. Choose larger values in `src/psgrn/psgglob.h`,\n   e.g.:\n   ```fortran\n   parameter(lmax=1000)\n   parameter(nrmax=4001)\n   ```\n\n2. Choose larger values in `src/pscmp/pscglob.h`,\n   e.g.:\n   ```fortran\n   parameter(NZSMAX=5000,NRMAX=10000)\n   parameter(NPSMAX=40000)\n   parameter(NRECMAX=40000)\n   ```\n\n3. Recompile:\n   ```sh\n   make clean\n   make\n   sudo make install\n   ```\n",
        "createdAt": "2017-01-25T12:15:59.000Z",
        "updatedAt": "2025-10-29T00:27:02.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/pyrocko/fomosto-psgrn-pscmp/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Dengda98/PyGRT",
        "url": "https://github.com/Dengda98/PyGRT",
        "description": "An Efficient and Integrated C/Python Package for Computing Synthetic Seismograms, Strain, Rotation and Stress Tensor in a Layered Half-Space Model (Dynamic & Static Cases)",
        "stars": 27,
        "forks": 1,
        "readme": "<p align=\"center\">\n  <img src=\"./figs/logo.png\" alt=\"Image 2\" width=\"45%\" />\n</p>\n\n<p align=\"center\">\n  <img alt=\"GitHub code size in bytes\" src=\"https://img.shields.io/github/languages/code-size/Dengda98/PyGRT\">\n  <img alt=\"GitHub Actions Workflow Status\" src=\"https://img.shields.io/github/actions/workflow/status/Dengda98/PyGRT/build.yml\">\n  <img alt=\"Github Tag\" src=\"https://img.shields.io/github/v/tag/Dengda98/PyGRT\">\n  <img alt=\"GitHub License\" src=\"https://img.shields.io/github/license/Dengda98/PyGRT\">\n</p>\n\n<p align=\"center\">\n  <img src=\"./figs/example_ZRT.png\" alt=\"Image 1\" width=\"80%\" />\n</p>\n\n\n> ⭐ **Like this project? Give it a Star!** ⭐\n\n[**Chinese Document**](https://pygrt.readthedocs.io/zh-cn/)  |  ~~English Document (no longer maintained)~~\n\n**[PyGRT](https://github.com/Dengda98/PyGRT)** : An Efficient and Integrated C/Python Package for Computing Synthetic Seismograms, Strain, Rotation and Stress Tensor in a Layered Half-Space Model (Dynamic & Static Cases)\n\n + **PyGRT** now can compute following properties in **both dynamic and static case.**  \n    ✔️ **Displacements and its spatial derivatives**  \n    ✔️ **Strain Tensor**  \n    ✔️ **Rotation Tensor**  \n    ✔️ **Stress Tensor**\n\n\n + **At present, PyGRT can run on**  \n    ✔️ **Linux**  \n    ✔️ **MacOS**  \n    ✔️ **Windows**\n\n + **PyGRT is extremely easy to install** by distributing pre-built binary files.\n\n + **PyGRT now supports the model with liquid layers.**\n\n + **PyGRT is still evolving**, and more features will be released in the future.\n\n + **Surface wave** modules will be released soon.\n\n\n\n<p align=\"center\">\n  <img src=\"./figs/diagram_cut.png\" alt=\"Image 2\" width=\"80%\" />\n</p>\n\n\n# Contact\nIf you have any questions or suggestions, feel free to reach out:\n- **Email**: zhudengda@mail.iggcas.ac.cn\n- **GitHub Issues**: You can also raise an issue directly on GitHub.\n\n# Citation\n\nSince PyGRT has been under continuous maintenance and extension during the peer review, **its functions have exceeded the scope described in this paper.** For detailed usage of each function, please refer to the [**documentation**](https://pygrt.readthedocs.io/zh-cn/).\n\n> Zhu D., J. Wang, J. Hao, S. Yao, Y. Xu, T. Xu and Z. Yao (2026). PyGRT: An Efficient and Integrated Python Package for Computing Synthetic Seismograms in a Layered Half-Space Model. Seismological Research Letters. (accepted)\n\n> Zhu, D., T. Xu, J. Hao, and Z. Yao (2026). A Direct Convergence Method for Computing Synthetic Seismograms for a Layered Half-space with Sources and Receivers at Close Depths, Bulletin of the Seismological Society of America. (accepted)\n\n",
        "createdAt": "2024-07-19T13:50:53.000Z",
        "updatedAt": "2025-12-05T11:01:12.000Z",
        "language": "C",
        "homepage": "https://pygrt.readthedocs.io",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Dengda98/PyGRT/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "adhithya9945/matlab_code",
        "url": "https://github.com/adhithya9945/matlab_code",
        "description": "This MATLAB program estimates earthquake source parameters from seismological data",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2024-06-15T15:18:31.000Z",
        "updatedAt": "2024-06-15T15:21:10.000Z",
        "language": "MATLAB",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "lyc11776611/SGY2TXT",
        "url": "https://github.com/lyc11776611/SGY2TXT",
        "description": "A Python script to translate SEG-Y seismic record file into txt files. ",
        "stars": 6,
        "forks": 1,
        "readme": "# SGY2TXT\nA Python script to translate SEG-Y seismic record file into txt files. \nIf there's any bug, please contact me at: YC_Lee@WHU.edu.cn\n\nThis script helps to translate SEG-Y file into two txt files. \nOne of the files, named 'outputparfilename' in the script, stores some important (or may not be so important) parameters of the SGY file including its 3200-bytes textual file header, some parameters in binary file header and trace headers.\nAnother file, named 'outputdatafilename' in the script, stores seismic data of the SGY file using numpy.savetxt()\n\nThe script uses numpy and binascii. While numpy only serves to save the data, binascii uses to translate binary into hexadecimal bytes.\n\nHope it can help.(～￣▽￣)～\n",
        "createdAt": "2018-11-08T08:34:44.000Z",
        "updatedAt": "2025-08-13T14:30:15.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/lyc11776611/SGY2TXT/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "fmingsl/Seismology-Learning-Materials",
        "url": "https://github.com/fmingsl/Seismology-Learning-Materials",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# Seismology-Learning-Materials\nThere are a part of code refering to Introduction to Seismology.\n",
        "createdAt": "2022-05-11T13:09:09.000Z",
        "updatedAt": "2022-05-11T13:14:37.000Z",
        "language": "MATLAB",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/fmingsl/Seismology-Learning-Materials/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ghfbsd/FERC",
        "url": "https://github.com/ghfbsd/FERC",
        "description": "Translate (latitude,longitude) points into a Flinn-Engdahl region code",
        "stars": 1,
        "forks": 0,
        "readme": "# FERC -- Translate a (lon,lat) point to a Flinn-Engdahl region code.\n\nThis is code transliterated by G. Helffrich 24 July 2022, from perl into\nPython based on the version at\n[ftp://hazards.cr.usgs.gov/feregion/fe_1995/feregion.pl](ftp://hazards.cr.usgs.gov/feregion/fe_1995/feregion.pl)\n```\n   Perl Version 0.2 - Bob Simpson January, 2003 <simpson<at>usgs.gov>\n   With fix supplied by George Randall <ger<at>geophysics.lanl.gov> 2003-02-03\n```\n\nIt uses the 1995 revision of F-E region definitions.\n\n## Usage\n\n```\nclass FERC():\n    '''\n    ## FERC class - Convert a lat/lon pair to a region code name and/or number.\n\n    Public methods:\n       code(lon=, lat=):  Return region code (integer) for given (lat,lon).\n       name(lon=, lat=):  Return region name (string) for given (lat,lon).\n       codename(lon=, lat=):  Return region (code,name) as tuple for given\n          (lat,lon).\n\n    Usage:\n       from ferc import FERC\n\n       reg = FERC()\n       code = reg.code(...)\n       name = reg.name(...)\n       (code,name) = reg.codename(...)\n    '''\n```\n",
        "createdAt": "2022-08-17T07:11:55.000Z",
        "updatedAt": "2024-10-16T13:43:24.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ghfbsd/FERC/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "SCECcode/ts-process",
        "url": "https://github.com/SCECcode/ts-process",
        "description": "Ground motion time series processing tools",
        "stars": 10,
        "forks": 4,
        "readme": "# ts-process\n[![Python](https://img.shields.io/badge/python-%3E%3D3.7-blue)](https://www.python.org)\n[![License](https://img.shields.io/badge/License-BSD_3--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause)\n![GitHub repo size](https://img.shields.io/github/repo-size/sceccode/ts-process)\n[![ts-process-ci Actions Status](https://github.com/SCECcode/ts-process/workflows/ts-process-ci/badge.svg)](https://github.com/SCECcode/ts-process/actions)\n\nGround motion time series processing tools\n\nThis is a collection of Python3-based software programs that are ground motions time series processing utilities designed to integrate 3D ground motion simulation seismograms from AWP-ODC and Hercules with Broadband Platform time series. The ts-process library also provides codes for calculating ROTD50 so that a common implementation is used to process both 3D simulation seismograms and 1D broadband platform seismograms.\n\nThese codes have been developed as part of earthquake ground motion research performed by the Southern California Earthquake Center (SCEC) www.scec.org.\n\n## Primary Developers of ts-process library:\n\n* Ricardo Taborda - Universidad EAFIT Medellín Colombia\n* Christine Goulet - University of Southern California\n* Fabio Silva - Southern California Earthquake Center\n\n## Software support:\n* software @ scec.org\n",
        "createdAt": "2018-03-18T20:31:03.000Z",
        "updatedAt": "2025-08-21T17:09:35.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/SCECcode/ts-process/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "earthinversion/earthquake_info_extractor",
        "url": "https://github.com/earthinversion/earthquake_info_extractor",
        "description": "These programs can be used to download the earthquake catalog as well as to extract the event informations such as latitude, longitude etc.",
        "stars": 4,
        "forks": 4,
        "readme": "This program is helpful in extracting the earthquake informations from the Global CMT website. It can be used to get the event latitude, longitude, depth, magnitude, name etc by just calling the EQinfo_extractor.py program. \nRequirements:\npython 3\n",
        "createdAt": "2017-08-27T12:59:32.000Z",
        "updatedAt": "2024-02-07T17:33:05.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/earthinversion/earthquake_info_extractor/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "rcounsell/GRCowling",
        "url": "https://github.com/rcounsell/GRCowling",
        "description": "Implementation of the general relativistic Cowling approximation in neutron-star seismology.",
        "stars": 0,
        "forks": 2,
        "readme": "# GR Cowling: Gravity and Interface Modes\n\nThis project represents an implementation of the general relativistic Cowling approximation in neutron-star seismology. It was developed to support <a href=\"https://academic.oup.com/mnras/article/536/2/1967/7920787?login=false\"> Mon. Not. R. Astron. Soc. 536, 1967 (2025)</a> and other work in preparation.\n\n## Installation\n\nThe software is developed using the python programming language.\n\n## Getting Started\n\nThe general functions are found in the ModeSolver.py file. Examples of how to use them are in the Notebooks and can be run using python.\n\n## Citations\n\nIf you found this project to be useful in academic work, please cite it using the following references:\n\n```bibtex\n@misc{counsell2025interfacemodesinspirallingneutron,\n      title={Interface modes in inspiralling neutron stars: A gravitational-wave probe of first-order phase transitions}, \n      author={A. R. Counsell and F. Gittins and N. Andersson and I. Tews},\n      year={2025},\n      eprint={2504.06181},\n      archivePrefix={arXiv},\n      primaryClass={gr-qc},\n      url={https://arxiv.org/abs/2504.06181}, \n}\n@ARTICLE{2025MNRAS.536.1967C,\n       author = {{Counsell}, A.~R. and {Gittins}, F. and {Andersson}, N. and {Pnigouras}, P.},\n        title = \"{Neutron star g modes in the relativistic Cowling approximation}\",\n      journal = {\\mnras},\n     keywords = {General Relativity and Quantum Cosmology, Astrophysics - High Energy Astrophysical Phenomena},\n         year = 2025,\n        month = jan,\n       volume = {536},\n       number = {2},\n        pages = {1967-1979},\n          doi = {10.1093/mnras/stae2721},\narchivePrefix = {arXiv},\n       eprint = {2409.20178},\n primaryClass = {gr-qc},\n       adsurl = {https://ui.adsabs.harvard.edu/abs/2025MNRAS.536.1967C},\n      adsnote = {Provided by the SAO/NASA Astrophysics Data System}\n}\n```\n",
        "createdAt": "2025-03-21T13:48:25.000Z",
        "updatedAt": "2025-06-30T11:00:58.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/rcounsell/GRCowling/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "LANL-Seismoacoustics/pisces",
        "url": "https://github.com/LANL-Seismoacoustics/pisces",
        "description": "Pisces: A practical seismological database library in Python.",
        "stars": 17,
        "forks": 10,
        "readme": "# Pisces\n\nPisces is a Python library that connects your geophysical analysis environment\nto a SQL database that uses the Center for Seismic Studies (CSS) 3.0 or NNSA KB\nCore table schema.\n\nDocumentation: <https://lanl-seismoacoustics.github.io/pisces>\n\nRepository: <https://github.com/lanl-seismoacoustics/pisces/>\n\n![Build Status](https://github.com/LANL-Seismoacoustics/pisces/workflows/Python%20package/badge.svg?branch=master)\n\n\n## Features\n\n* Import/export waveforms directly to/from your database.  \n* Build database queries using Python objects and methods\n    ([SQLAlchemy](http:/www.sqlalchemy.org)), not by concatenating SQL strings.\n* Integration with [ObsPy](http://www.obspy.org).\n* Geographic filtering of results.\n\n\n## Installation\n\nRequires:\n\n* ObsPy\n* Click\n* C compiler (for optional `e1` dependency)\n\nInstall from [PyPI](https://pypi.python.org/pypi):\n\n```\npip install pisces\n```\n\nIf you use \"e1\" format data, you also need to install the `e1` package:\n\n```\npip install e1\n```\n\nYou can install them both at the same time with:\n\n```\npip install pisces[e1]\n```\n\n\nInstall current master from GitHub:\n\n```\npip install git+https://github.com/LANL-Seismoacoustics/pisces\n```\n",
        "createdAt": "2014-01-22T20:45:37.000Z",
        "updatedAt": "2025-12-01T22:24:49.000Z",
        "language": "Python",
        "homepage": "http://lanl-seismoacoustics.github.io/pisces/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/LANL-Seismoacoustics/pisces/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "grillo/openeew",
        "url": "https://github.com/grillo/openeew",
        "description": "Introduction to OpenEEW, and open-source Earthquake Early-Warning toolkit",
        "stars": 8,
        "forks": 1,
        "readme": "## We have moved 👨‍💻\nThe OpenEEW project is now hosted by the Linux Foundation. This repo has moved to its own [OpenEEW home on Github](https://github.com/openeew/openeew/). \n",
        "createdAt": "2019-02-21T17:49:03.000Z",
        "updatedAt": "2025-01-29T07:25:36.000Z",
        "language": null,
        "homepage": "https://openeew.com",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/grillo/openeew/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "crowboydoudou/crowboydoudou.github.io",
        "url": "https://github.com/crowboydoudou/crowboydoudou.github.io",
        "description": "seismology/codes/memory",
        "stars": 0,
        "forks": 0,
        "readme": "## Welcome to GitHub Pages\n\nYou can use the [editor on GitHub](https://github.com/crowboydoudou/crowboydoudou.github.io/edit/master/README.md) to maintain and preview the content for your website in Markdown files.\n\nWhenever you commit to this repository, GitHub Pages will run [Jekyll](https://jekyllrb.com/) to rebuild the pages in your site, from the content in your Markdown files.\n\n### Markdown\n\nMarkdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for\n\n```markdown\nSyntax highlighted code block\n\n# Header 1\n## Header 2\n### Header 3\n\n- Bulleted\n- List\n\n1. Numbered\n2. List\n\n**Bold** and _Italic_ and `Code` text\n\n[Link](url) and ![Image](src)\n```\n\nFor more details see [GitHub Flavored Markdown](https://guides.github.com/features/mastering-markdown/).\n\n### Jekyll Themes\n\nYour Pages site will use the layout and styles from the Jekyll theme you have selected in your [repository settings](https://github.com/crowboydoudou/crowboydoudou.github.io/settings). The name of this theme is saved in the Jekyll `_config.yml` configuration file.\n\n### Support or Contact\n\nHaving trouble with Pages? Check out our [documentation](https://help.github.com/categories/github-pages-basics/) or [contact support](https://github.com/contact) and we’ll help you sort it out.\n",
        "createdAt": "2018-11-19T08:01:44.000Z",
        "updatedAt": "2018-11-19T08:21:46.000Z",
        "language": null,
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/crowboydoudou/crowboydoudou.github.io/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "gthompson/KSCRocketSeismology",
        "url": "https://github.com/gthompson/KSCRocketSeismology",
        "description": "Codes for the 2022 pilot project that acquired seismo-acoustic and well data near SLC 39A",
        "stars": 0,
        "forks": 0,
        "readme": "Codes for KSC rocket seismo-acoustics Pilot Project\n\n\n",
        "createdAt": "2024-03-23T02:24:45.000Z",
        "updatedAt": "2025-09-05T21:14:19.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/gthompson/KSCRocketSeismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "earthinversion/Determine-Mohorovicic-Discontinuity-in-1D-Earth-Model",
        "url": "https://github.com/earthinversion/Determine-Mohorovicic-Discontinuity-in-1D-Earth-Model",
        "description": "To determine the moho layer for the PREM and SEMUCB 1D layered model",
        "stars": 3,
        "forks": 0,
        "readme": "## Determine the Mohorovičić discontinuity layer in 1D layered Earth Model\n\nThe Mohorovičić discontinuity, usually referred to as the Moho discontinuity or the Moho, is the boundary between the Earth's crust and the mantle. It is defined by the distinct change in velocity of seismological waves as they pass through changing densities of rock. This distinctive discontinuous layer extends from 30 km to 50 km below the continents and about 10 km below sea-level in the ocean basins.\n\n## Compile the code\nRequires the installation of `gfortran` or add your compiler to the `makefile`.\n```\nmake clean\nmake\n```\n\n## Run the code\n1. Edit the 1D Earth model file name in the script `determineMohoLayer.f90`, line 5\n```\ncharacter (len=256) :: model_file = 'model1D_prem.dat' !PREM\n```\n\n1. Compile the script\n1. Run the script `./determineMohoLayer`\n1. This will output the total layers in the model file, total number of discontinuities, and the layer number for the Moho discontinuity.\n1. Edit the script according to your needs.\n\n## References\n1. https://www-nature-com.libproxy.berkeley.edu/articles/2071082a0\n1. https://geology.com/articles/mohorovicic-discontinuity.shtml\n1. https://en.wikipedia.org/wiki/Mohorovičić_discontinuity\n",
        "createdAt": "2022-01-28T00:53:08.000Z",
        "updatedAt": "2022-12-30T23:07:45.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/earthinversion/Determine-Mohorovicic-Discontinuity-in-1D-Earth-Model/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "INGV/qquake",
        "url": "https://github.com/INGV/qquake",
        "description": "A plugin for QGIS 3.x that relies on web services for loading seismological data",
        "stars": 9,
        "forks": 2,
        "readme": "# QQuake, a plugin for loading seismological data in QGIS\n\nQQuake plugin is a plugin that facilitates the download of seismological data\nfrom different sources and load them directly into QGIS.\n\nMore information at the QQuake homepage at<br>\nhttps://www.emidius.eu/qquake/\n\nThe following is a scientific paper describing the plugin and the context that led to its development\n\nLocati M., Vallone R., Ghetta M. and Dawson N. (2021). QQuake, a QGIS Plugin for Loading Seismological Data From Web Services. Front. Earth Sci. 9:614663. https://doi.org/10.3389/feart.2021.614663\n\nIf you want to try the latest, unstable version, you have to download the source code from this repository, create a zip file with the content of the \"qquake\" folder, and install the plugin from the zip file in QQuake.\nFor example, in the Linux terminal, you may follow this procedure to create the zip file\n```\n$ mkdir qquake_test\n$ cd qquake_test\n$ wget https://github.com/INGV/qquake/archive/refs/heads/master.zip\n$ unzip master.zip\n$ cd qquake-master\n$ zip -r qquake_test.zip qquake\n```\nAnd then, install the plugin selecting \"qquale_test.zip\" in QQGIS.\n",
        "createdAt": "2020-10-26T10:27:56.000Z",
        "updatedAt": "2025-05-23T14:50:29.000Z",
        "language": "Python",
        "homepage": "https://www.emidius.eu/qquake/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/INGV/qquake/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "AlbertSeismo/SeismologyCourses",
        "url": "https://github.com/AlbertSeismo/SeismologyCourses",
        "description": null,
        "stars": 1,
        "forks": 0,
        "readme": "![Seismology](https://github.com/DIG-Kaust/Seismology/blob/main/logo.png)\n\nTeaching material for ErSE 210 Seismology course to be held at KAUST during the Fall semester.\n\n## Material\n\nThe repository is organized as follows:\n\n- **Slides**: deck of slides summarizing the key concept introduced in each class. Some of the figures in these slides are taken from the reference textbook (Shearer, P., Introduction to Seismology). \n- **Data**: input data used in the practical sessions:\n- All of the other folders in this repository contains Python codes and Jupyter Notebooks used in the practical sessions:\n\n   - [**PlaneWave**](https://github.com/DIG-Kaust/Seismology/blob/main/PlaneWave/PlaneWave.ipynb): create and display plane waves in time-space and wavenumber domain.\n   - [**GassmannFluidSub**](https://github.com/DIG-Kaust/Seismology/blob/main/GassmannFluidSub/Gassmann.ipynb): implement basic rock physics equations and Gassmann substitution and apply it to the Smehaia well log.\n   - [**SeismicModelling**](https://github.com/DIG-Kaust/Seismology/blob/main/SeismicModelling/SeismicModellingInversion.ipynb): perform convolutional and AVO modelling, and apply pre-stack inversion\n   - [**RayTrace**](https://github.com/DIG-Kaust/Seismology/blob/main/RayTrace/RayTrace.ipynb): implement 2D raytracing by solving the associated ODE\n   - [**SeismicTomography**](https://github.com/DIG-Kaust/Seismology/blob/main/SeismicTomography/SeismicTomography.ipynb): create the 2D tomographic matrix and solve the associated inverse problem\n   - [**Obspy**](https://github.com/DIG-Kaust/Seismology/blob/main/Obspy/ObspyIntro.ipynb): a short introduction to Obspy and its usage for epicenter localization of earthquakes\n   - [**ReflectionSeismic**](https://github.com/DIG-Kaust/Seismology/blob/main/ReflectionSeismic): implement basic NMO processing and learn how to work with SEGY files using *segyio* and the Volve dataset.\n\n## Environment\n\nTo ensure reproducibility of the results, we have provided an `environment.yml` file. Ensure to have installed Anaconda or Miniconda on your computer. If you are not familiar with it, we suggest using the \n[KAUST Miniconda Install recipe](https://github.com/kaust-rccl/ibex-miniconda-install). This has been tested both on macOS and Unix operative systems.\n\nAfter that simply run:\n```\n./install_env.sh\n```\nIt will take some time, if at the end you see the work `Done!` on your terminal you are ready to go!\n\n## Binder\n\nAlternatively, you can work directly on Binder. Simply click this button and access\nthe material from your web browser without the need for any local installation\n\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/DIG-Kaust/Seismology/HEAD)",
        "createdAt": "2023-02-03T18:05:05.000Z",
        "updatedAt": "2025-02-22T08:53:41.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/AlbertSeismo/SeismologyCourses/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Caltech-geoquake/PySeismoSoil",
        "url": "https://github.com/Caltech-geoquake/PySeismoSoil",
        "description": "A Python library for performing 1D seismic site response analysis",
        "stars": 87,
        "forks": 30,
        "readme": "# PySeismoSoil\n\n[![PyPI](https://img.shields.io/pypi/v/pyseismosoil)](https://pypi.org/project/PySeismoSoil/)\n[![Docs](https://github.com/Caltech-geoquake/PySeismoSoil/actions/workflows/build-and-deploy-docs.yml/badge.svg?branch=main)](https://github.com/Caltech-geoquake/PySeismoSoil/actions/workflows/build-and-deploy-docs.yml)\n[![Downloads](https://static.pepy.tech/badge/pyseismosoil)](https://pepy.tech/project/pyseismosoil)\n[![Downloads](https://static.pepy.tech/badge/pyseismosoil/month)](https://pepy.tech/project/pyseismosoil)\n[![Downloads](https://static.pepy.tech/badge/pyseismosoil/week)](https://pepy.tech/project/pyseismosoil)\n\nPySeismoSoil is a Python library for performing 1D seismic site response analysis.\n\n## Copyright and license\n\nCopyright (c) 2024, California Institute of Technology, based on research supported by the National Science Foundation (NSF) Cooperative Agreement EAR-1033462 and the U.S. Geological Survey (USGS) Cooperative Agreement G12AC20038. All rights reserved.\n\nPlease carefully read the license [here](https://caltech-geoquake.github.io/PySeismoSoil/blob/master/LICENSE) for the terms and conditions of using this library.\n\n## Authors\n\nThe authors of this library are the current and past members of the [Geoquake Research Group](http://asimaki.caltech.edu/) of the [California Institute of Technology](https://www.caltech.edu/): Jian Shi, Domniki Asimaki, Wei Li, and Flora Xia.\n\n## Installation\n\nInstall most recent stable version:\n\n```bash\npip install PySeismoSoil\n```\n\nIf you already have an older version installed and want to upgrade to the newest version, use `pip install --upgrade PySeismoSoil`.\n\n## Supported Python versions\n\nPySeismoSoil currently support these Python versions:\n\n- 3.10\n- 3.11\n- 3.12\n- 3.13\n\n## API Documentation\n\nhttps://caltech-geoquake.github.io/PySeismoSoil/\n\n## Examples\n\nGo to the \"examples\" folder from the root directory. Those examples help you quickly get familiar with the usage of this library.\n\n## Knowledge base\n\nThe models and algorithms used in this library mainly come from these research papers:\n\n1. J. Shi (2019) \"Improving Site Response Analysis for Earthquake Ground Motion Modeling.\" _Dissertation (Ph.D.), California Institute of Technology_. doi:10.7907/X5NZ-DQ21. [[URL](http://resolver.caltech.edu/CaltechTHESIS:05302019-150220368)]\n\n2. J. Shi, D. Asimaki (2018) \"A Generic Velocity Profile for Basin Sediments in California Conditioned on Vs30.\" _Seismological Research Letters_, 89 (4), 1397-1409. [[URL](http://resolver.caltech.edu/CaltechAUTHORS:20180523-153705346)]\n\n3. J. Shi, D. Asimaki (2017) \"From stiffness to strength: Formulation and validation of a hybrid hyperbolic nonlinear soil model for site-response analyses.\" _Bulletin of the Seismological Society of America_, 107 (3), 1336-1355. [[URL](http://resolver.caltech.edu/CaltechAUTHORS:20170404-150827374)]\n\n4. W. Li, D. Assimaki (2010) \"Site- and motion-dependent parametric uncertainty of site-response analyses in earthquake simulations.\" _Bulletin of the Seismological Society of America_, 100 (3), 954-968. [[URL](http://resolver.caltech.edu/CaltechAUTHORS:20140904-160952252)]\n\n5. D. Asimaki, W. Li, J. Steidl, J. Schmedes (2008) \"Quantifying nonlinearity susceptibility via site-response modeling uncertainty at three sites in the Los Angeles Basin.\" _Bulletin of the Seismological Society of America_, 98 (5), 2364-2390. [[URL](http://resolver.caltech.edu/CaltechAUTHORS:20140828-163417572)]\n\n## Report issues\n\nTo report bugs and submit suggestions, please use the [\"Issues\" section](https://github.com/jsh9/PySeismoSoil/issues) of this GitHub repository.\n\n## How to cite this library\n\nTo cite this library, please include this DOI in your publication: [![DOI](https://zenodo.org/badge/169386936.svg)](https://zenodo.org/badge/latestdoi/169386936).\n\n## How to contribute to this library\n\nPlease read the [contributing instructions](https://caltech-geoquake.github.io/PySeismoSoil/blob/master/CONTRIBUTING.md) to get started.\n",
        "createdAt": "2019-02-06T10:14:38.000Z",
        "updatedAt": "2025-11-27T05:25:38.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Caltech-geoquake/PySeismoSoil/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "holishing/freebsd-ports-obspy",
        "url": "https://github.com/holishing/freebsd-ports-obspy",
        "description": "FreeBSD ports:  Python framework for seismological observatories (add Python3 depends)",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2019-05-06T12:45:13.000Z",
        "updatedAt": "2019-05-06T13:16:10.000Z",
        "language": "Makefile",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "gianca1994/seismology-api",
        "url": "https://github.com/gianca1994/seismology-api",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# seismology-api",
        "createdAt": "2022-04-30T20:29:56.000Z",
        "updatedAt": "2022-08-01T19:50:27.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/gianca1994/seismology-api/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "DomagojHrvojevic/format_seismo",
        "url": "https://github.com/DomagojHrvojevic/format_seismo",
        "description": "Preprocessing seismological data",
        "stars": 1,
        "forks": 0,
        "readme": "# format_seismo\n`format_seismo` is a package for preprocessing seismological data. It utilizes functions from the [ObsPy](https://docs.obspy.org/) and [NumPy](https://numpy.org/) libraries.\n\n## Instalation\nInstall from PyPI:\n```bash\npip install format_seizmo\n```\n\n## Import\nImport the `format_seismo` library in python code:\n```python\nimport format_seismo as fs\n```\n\n## Functions\n### solohr\nFormats seismological data gathered by a SmartSolo portable seismograph into an hourly time-step format.\nSmartSolo instruments store seismological data in MiniSEED format (_.mseed_ extension).\nCall the function `solohr()` with the following arguments:\n```python\nfs.solohr(IN_path, OUT_path=None, station_code='CODE', station_net='NET', language='cro', silent=False)\n```\n\n* `IN_path:`   Path to a folder containing all _.mseed_ files of a single smartsolo station.\n\n* `OUT_path:`  _(Optional)_ Path to the folder where formatted _.mseed_ files will be saved. If set to `None` (default value), output will be stored in a newly created subdirectory `/hourly_formatted_data` inside `IN_path`.\n\n* `station_code:`  _(Optional)_ Deault is `'CODE'`. Used in the filename and metadata of the formatted _.mseed_ files.\n\n* `staton_net:` _(Optional)_ Default is `'NET'`. Used in metadata of the formatted _.mseed_ files.\n\n* `language:`  _(optional)_ Default is `'cro'` (Croatian). Language used in naming folders in the output path. Supported languages are:\n  * `deu` German\n  * `spa` Spanish\n  * `fra` French\n  * `zho` Chinese (Simplified)\n  * `eng` English\n\n* `silent:` _(optional)_ If set to True, suppresses terminal output. Default is False.\n<br>\n\n__Note__ <br>\nThis function processes data for __one station__ at a time.\nIf you have multiple stations, wrap this function in a loop.\n\n__Example__ <br>\nImporting and calling the `solohr()` function:\n```python\nimport format_seismo as fs\nfs.solohr('/mnt/raw_solo_data', '/mnt/formatted_solo_data', station_code='STAT', station_net='NET', language='eng')\n```\nPart of the function output:\n```bash\nUserWarning: Warning: Default network code 'NET' is used. Please provide a valid network code.\nWorking on station: STAT, network: NET, component: Z\nDirectory  mnt/formatted_solo_data/year_2025/month_07/day_04/hour_17  created.\nFormatted file stat_z_250_20250704_1700.mseed\nDirectory  mnt/formatted_solo_data/year_2025/month_07/day_04/hour_18  created.\nFormatted file stat_z_250_20250704_1800.mseed\nDirectory  mnt/formatted_solo_data/year_2025/month_07/day_04/hour_19  created.\nFormatted file stat_z_250_20250704_1900.mseed\nDirectory  mnt/formatted_solo_data/year_2025/month_07/day_04/hour_20  created.\nFormatted file stat_z_250_20250704_2000.mseed\nDirectory  mnt/formatted_solo_data/year_2025/month_07/day_04/hour_21  created.\nFormatted file stat_z_250_20250704_2100.mseed\n...\n```\n\nPart of the output directory tree:\n```bash\nuser@DESKTOP:/mnt/formatted_solo_data$ tree\n.\n└── year_2025\n    └── month_07\n        ├── day_04\n        │   ├── hour_17\n        │   │   ├── stat_e_250_20250704_1700.mseed\n        │   │   ├── stat_n_250_20250704_1700.mseed\n        │   │   └── stat_z_250_20250704_1700.mseed\n        │   ├── hour_18\n        │   │   ├── stat_e_250_20250704_1800.mseed\n        │   │   ├── stat_n_250_20250704_1800.mseed\n        │   │   └── stat_z_250_20250704_1800.mseed\n        │   ├── hour_19\n        │   │   ├── stat_e_250_20250704_1900.mseed\n        │   │   ├── stat_n_250_20250704_1900.mseed\n        │   │   └── stat_z_250_20250704_1900.mseed\n        │   ├── hour_20\n        │   │   ├── stat_e_250_20250704_2000.mseed\n        │   │   ├── stat_n_250_20250704_2000.mseed\n        │   │   └── stat_z_250_20250704_2000.mseed\n        │   ├── hour_21\n        │   │   ├── stat_e_250_20250704_2100.mseed\n        │   │   ├── stat_n_250_20250704_2100.mseed\n        │   │   └── stat_z_250_20250704_2100.mseed\n        │   ├── hour_22\n        │   │   ├── stat_e_250_20250704_2200.mseed\n        │   │   ├── stat_n_250_20250704_2200.mseed\n        │   │   └── stat_z_250_20250704_2200.mseed\n        │   └── hour_23\n        │       ├── stat_e_250_20250704_2300.mseed\n        │       ├── stat_n_250_20250704_2300.mseed\n        │       └── stat_z_250_20250704_2300.mseed\n        ├── day_05\n        │   ├── hour_00\n        │   │   ├── stat_e_250_20250705_0000.mseed\n        │   │   ├── stat_n_250_20250705_0000.mseed\n        │   │   └── stat_z_250_20250705_0000.mseed\n...\n```\n",
        "createdAt": "2025-07-29T08:31:20.000Z",
        "updatedAt": "2025-08-22T05:57:44.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/DomagojHrvojevic/format_seismo/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "nfsi-canada/EarthquakeSeisToolsMATLAB",
        "url": "https://github.com/nfsi-canada/EarthquakeSeisToolsMATLAB",
        "description": "MATLAB codes for various problems in earthquake seismology, such as estimating hypocenters, focal mechanisms, stress, etc.",
        "stars": 30,
        "forks": 5,
        "readme": "",
        "createdAt": "2020-11-18T21:00:38.000Z",
        "updatedAt": "2025-11-17T02:54:39.000Z",
        "language": "MATLAB",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "armstrong06/atucus",
        "url": "https://github.com/armstrong06/atucus",
        "description": "Arrival Time Uncertainties Created Using SWAG",
        "stars": 2,
        "forks": 1,
        "readme": "# Arrival Time Uncertainties Created Using SWAG\n\nFrom the paper **A Deep-learning Phase Picker with Calibrated Bayesian-derived Uncertainties for Earthquakes in the Yellowstone Volcanic Region** by Alysha Armstrong, Zachary Claerhout, Ben Baker, and Keith D. Koper\n\n## Overview\n\nTrain and evaluate deep-learining P- and S-phase arrival time pickers using Stochastic Weight Averaging - Gaussian (SWAG; Maddox *et al.*, 2019) and Multiple SWAG (MultiSWAG; Wilson and Izmailov, 2020). Using SWAG, these models produce Bayesian-derived uncertainity estimates for the phase arrival times. The uncertainties are calibrated using the method from Kuleshov *et al.* (2018). The deep-learning model architecture is from Ross *et al.* (2018). \n  \n[<img src=\"https://github.com/armstrong06/atucus/assets/46799601/8fa421a6-a737-4f62-aff4-7c912ba8481b\" width=\"450\"/>](figure8.jpeg)  \n*Examples of MultiSWAG and calibration applied to two waveforms from the University of Utah Seismograph Stations (UUSS) catalog. The vertical-component waveform (Z) is shown with a histogram of predicted picks from three separate SWAG models (m1, m2, and m3), the UUSS analyst pick (y_act), the predicted pick from MultiSWAG (y_pred), the standard deviation of the MultiSWAG predictions (1 st. dev.) and the calibrated 90% credible interval (C.I.). The x-axis is relative to the analyst pick (0.0 s). (a) A P arrival labeled as high quality (0) by a seismic analyst. (b) A P arrival labeled as low quality (2) by a seismic analyst. The signal-to-noise (SNR) value for the arrival is in the top right corner.*\n\n## Directories\n- swa_gaussian-master: [SWAG code](https://github.com/wjmaddox/swa_gaussian), downloaded on May 3, 2022\n- swag_modified: Modified SWAG code used in Armstrong *et al.* (2023) to produce uncertainty estimates on phase arrival time estimates\n- uuss: Hyperparameter tuning, training, evaluation, and uncertainty calibration of models presented in Armstrong *et al.* (2023) \n\n## References\n- Armstrong, A. D., Z. Claerhout, B. Baker, and K. D. Koper (2023). A deep-learning phase picker with calibrated bayesian-derived uncertainties for earthquakes in the Yellowstone Volcanic Region, Bull. Seismol. Soc. Am. XX, 1–22\n- Kuleshov, V., N. Fenner, and S. Ermon (2018). Accurate uncertainties for deep learning using calibrated regression, in Proceedings of the 35th International Conference on Machine Learning Stockholm, Sweden, PMRL.\n- Maddox, W., T. Garipov, P. Izmailov, D. Vetrov, and A. G. Wilson (2019). A simple baseline for Bayesian uncertainty in deep learning, Advances in Neural Information Processing Systems 32, 13153–13164.\n- Ross, Z. E., M. Meier, and E. Hauksson (2018). P wave arrival picking and first‐motion polarity determination with deep learning, J. Geophys. Res. Solid Earth 123, 5120–5129.\n- Wilson, A. G., and P. Izmailov (2020). Bayesian deep learning and a probabilistic perspective of generalization, in Advances in Neural Information Processing Systems, 4697–4708.\n",
        "createdAt": "2023-08-03T03:32:37.000Z",
        "updatedAt": "2024-08-30T08:10:44.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/armstrong06/atucus/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "williameaton/Theoretical_and_Computational_Seismology",
        "url": "https://github.com/williameaton/Theoretical_and_Computational_Seismology",
        "description": "Answers to Chapters 6 and 7 of Theoretical and Computational Seismology by Jeroen Tromp",
        "stars": 1,
        "forks": 2,
        "readme": "# Online solution reference for the book \"Theoretical & Computational Seismology\"\n\n\n## Change of ownership edits\n\n- [ ] Update `.github/workflows/deploy_gh_pages.yml` update github repo link\n- [ ] Update `tcs/_config.yml` link to repo.\n\n## Other TO-DO: \n\n- [ ] Rename Chapter 6 files with buffer 0 for single digits \n\n## Chapter 6      \n| Problem | Assigned | Completed |\n|---------|----------|-----------|\n| 6.1     | WE       | ✓         |\n| 6.2     | WE       | ✓         |\n| 6.3     | WE       | ✓         |\n| 6.4     | WE       | ✓         | \n| 6.5     | WE       | ✓         |\n| 6.6     | WE       | ✓         |\n| 6.7     | WE       | ✓         |\n| 6.8     | WE       | ✓         |\n| 6.9     | WE       | ✓         |\n| 6.10     | WE       | ✓         |\n| 6.11     | WE       | ✓         |\n| 6.12     | WE       | Needs updates          |\n| 6.13     | WE       | ✓         |\n| 6.14     | WE       | ✓         |\n\n\n## Chapter 7\n| Problem | Assigned | Completed |\n|---------|----------|-----------|\n| 7.1     | WE       | ✓         |\n| 7.2     |          |           |\n| 7.3     | WE       | ✓         |\n| 7.4     | WE       | ✓         |\n| 7.5     |          |           |\n| 7.6     |          |           |\n| 7.7     |          |           |\n| 7.8     |          |           |\n| 7.9     |          |           |\n| 7.10     |          |           |\n| 7.11     |          |           |\n| 7.12     |          |           |\n| 7.13     |          |           |\n| 7.14     |          | * HW6 of my Geo441 (kinda) but I went a bit overkill with classes  |\n| 7.15     |          |           |\n| 7.16     |          |           |\n| 7.17     |          |           |\n| 7.18     |          |           |\n| 7.19     |          |           |\n| 7.20     |          |           |\n| 7.21     |          |           |\n| 7.22     |          |           |\n\n\n",
        "createdAt": "2025-01-16T20:14:01.000Z",
        "updatedAt": "2025-04-12T19:00:17.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/williameaton/Theoretical_and_Computational_Seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "seismotologist/ETHZ_geophysicsIII",
        "url": "https://github.com/seismotologist/ETHZ_geophysicsIII",
        "description": "Exercises and class material for earthquake seismology segment of Geophysics III class at ETH Zurich ",
        "stars": 3,
        "forks": 0,
        "readme": "",
        "createdAt": "2022-10-13T10:49:37.000Z",
        "updatedAt": "2023-01-25T09:00:47.000Z",
        "language": "MATLAB",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "rezaeimh7/SeismologySystem",
        "url": "https://github.com/rezaeimh7/SeismologySystem",
        "description": "Java implementation of seismology system using Unfolding Map Library",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2018-06-17T04:25:10.000Z",
        "updatedAt": "2018-06-17T04:25:10.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "NimaDolatabadi/Earthquake-Seismology",
        "url": "https://github.com/NimaDolatabadi/Earthquake-Seismology",
        "description": "A full repository containing all useful scripts and other seismological programs.",
        "stars": 13,
        "forks": 4,
        "readme": "# Earthquake Seismology Repository\n\nThis repository is a comprehensive collection of scripts and programs designed for seismological analysis. It serves as a resource for researchers and practitioners in the field of earthquake seismology, providing tools for data processing, analysis, and visualization.\n\n## Table of Contents\n\n- [Introduction](#introduction)\n- [Repository Structure](#repository-structure)\n- [Installation](#installation)\n- [Usage](#usage)\n- [Contributing](#contributing)\n- [License](#license)\n\n## Introduction\n\nThe Earthquake Seismology Repository aims to centralize useful scripts and programs that facilitate various tasks in seismology. Whether you're analyzing seismic data, generating visualizations, or automating workflows, this repository offers tools to support your work.\n\n## Repository Structure\n\nThe repository is organized into the following directories and files:\n\n- `data_processing/` - Scripts for processing raw seismic data.\n- `visualization/` - Tools for creating visual representations of seismic events.\n- `analysis/` - Programs for statistical and qualitative analysis of seismic data.\n- `utilities/` - Miscellaneous helper scripts and utilities.\n- `GitHub Cheat Sheet` - A guide for using GitHub under Ubuntu command line.\n\n## Branches\n\nThis repository utilizes branches to manage different features, experiments, or versions. Below is an overview of the existing branches:\n\n- **master**: The default branch containing the stable version of the codebase. All major updates are merged here after thorough testing.\n\n- **SAC-101.6a**: SAC software configuration on linux: [https://www.youtube.com/watch?v=3yQ0UulFkl4](url)\n\n- **SAC-102.0----For-Windows**: SAC software installation on windows using Cygwin (Not Rcommended): [https://www.youtube.com/watch?v=4htSqxYzu60](url)\n- **Matlab-Functions**: Usefull matlab functions in case of need for seismic processing\n- **Seisan**: Seisan Installation on Linux: [https://www.youtube.com/watch?v=dINPfp_tPsY](url)\n- **GMT**: GMT 4/5 compilation and installation on linux: [https://www.youtube.com/watch?v=InVvSzD0BCw](url) , [https://www.youtube.com/watch?v=dYFhZtU4_Uk](url)\n\n## Installation\n\n## Usage\nEach script and program comes with its own documentation or help message. To understand how to use a specific tool, navigate to its directory and refer to the README file or execute the script with the --help flag.\n\n## Contributing\nContributions to this repository are welcome. If you have scripts or programs that could benefit the seismological community, please consider contributing.\n\n**To Contribute:**\nFork the repository.\nCreate a new branch for your feature or bug fix.\nCommit your changes with clear and descriptive messages.\nPush your changes to your forked repository.\nSubmit a pull request to the main repository.\nFor detailed guidelines, refer to the CONTRIBUTING.md file.\n\n## Clone the Repository\ngit clone https://github.com/NimaDolatabadi/Earthquake-Seismology.git\ncd Earthquake-Seismology\n\n## License\nThis repository is licensed under the MIT License. You are free to use, modify, and distribute the code, provided you include proper attribution. For more information, see the LICENSE file.\n",
        "createdAt": "2018-04-29T10:28:49.000Z",
        "updatedAt": "2025-06-12T05:59:22.000Z",
        "language": null,
        "homepage": "https://github.com/NimaDolatabadi/Earthquake-Seismology",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/NimaDolatabadi/Earthquake-Seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "EsraaHasanQabbaha/JSO",
        "url": "https://github.com/EsraaHasanQabbaha/JSO",
        "description": "Jordan Seismological Observatory",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2019-11-26T10:09:49.000Z",
        "updatedAt": "2019-11-26T10:09:49.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "rcakir/seismology_washington",
        "url": "https://github.com/rcakir/seismology_washington",
        "description": "Work Environment for Portable Seismographs and Data Processing",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2016-12-06T17:42:17.000Z",
        "updatedAt": "2016-12-06T17:42:17.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "geoign/cmt-catalog-qgis",
        "url": "https://github.com/geoign/cmt-catalog-qgis",
        "description": "The implementation of GlobalCMT catalog on QGIS.",
        "stars": 2,
        "forks": 0,
        "readme": "# cmt-catalog-qgis\nA hastely-made implementation of GlobalCMT catalog on QGIS for your convenience.\n![Sample image](https://github.com/geoign/cmt-catalog-qgis/blob/main/samplescreenshot.jpg)\n\n## Product detail\nAll data is based on GlobalCMT https://www.globalcmt.org/ .\nObspy https://docs.obspy.org/ is used for extracting GlobalCMT data.\n~~In this implementation, earthquakes larger than M4 between the period of 1991-2020 (30 years) are included.~~\nNow it covers all events in GlobalCMT catalog as of Dec. 2022.\nSorry, I set the wrong link to the Google Drive ZIP file. Now it is fixed. (2025/02/01)\n\n## Installation\nNote: The following steps show how-to in QGIS 3.28.1. \n1. Download ![GlobalCMT-All.gpkg](https://github.com/geoign/cmt-catalog-qgis/blob/main/GlobalCMT-All.gpkg). This file contains the table (id, latitude, longitude, magnitude, depth, image file name). \n2. Download [a ZIP file containing SVG images (Google Drive)](https://drive.google.com/file/d/1eyFN9rPi6tNDi8nPYqlECI6mo2zioISg/view?usp=share_link).\n3. Download ![QGIS Style file](https://github.com/geoign/cmt-catalog-qgis/blob/main/globalcmt-style-M7-30km.qml). This file allows to do the basic setup for the gpkg layer.\n4. Extract the ZIP file. You will find 57k SVG images in CMTimages folder. Remember where you place them.\n5. Open GlobalCMT-All.gpkg on QGIS. Proceed **layer properties -> Style -> Import style**. Choose **globalcmt-style-M7-30km.qml** which was downloaded earlier and apply it to the gpkg layer.\n8. Next, choose **Symbology** and click the marker. The marker setting window will pop up. Scroll to the bottom. \n7. Find textbox to choose the path to SVG files. Click \"ε\" and choose edit to open a textbox.\n   ![showing steps 2](https://github.com/geoign/cmt-catalog-qgis/blob/main/screengrab3.jpg)\n8. In the textbox, you will find the text in place. \n   You need to change it accodringly to the path to the SVG files which you extracted from the ZIP file.\n9. Click OK to close the all pop-ups. You will see several beachballs around the world.\n   By default, only the events with Depth<30 km and Mag>7 are shown. \n   You may change it through modifying the filter in **Layer properties -> Symbology -> Value**. \n\n## Disclaimer\nThese datasets are provided for conveniently view regional-scale trend in CMT solutions.\nUncertainty, differences in magnitude types, etc. are not considered in this implementation.\n\n## Developer information\nFumihiko Ikegami (Twitter:@geoign) of Ikegami Georesearch. \n",
        "createdAt": "2022-12-19T15:00:11.000Z",
        "updatedAt": "2025-02-01T03:02:17.000Z",
        "language": "QML",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/geoign/cmt-catalog-qgis/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "fgallovic/LinSlipInv",
        "url": "https://github.com/fgallovic/LinSlipInv",
        "description": "Linear multi time-window earthquake slip inversion with k^-2 smoothing",
        "stars": 22,
        "forks": 11,
        "readme": "#LinSlipInv\n-----------\n\nLinear multi time-window earthquake slip inversion with *k*<sup>-2</sup> smoothing\n\nSuite of codes for linear slip inversions and resolution analysis.\n\n####Capabilities of the codes:\n - Inversion of provided data for a given (possibly segmented) fault geometry\n - Resolution analysis by means of synthetic tests with prescribed target rupture model or slip-rate pulse model\n - Inversion in-depth analysis by means of spectral analysis of the forward matrix **G**\n - Can take advantage of Intel MKL library and/or CULA (GPU) for faster performance\n - Plotting the results\n\n####Possible data types for inversions:\n - Seismic waveforms including processed HR-GPS\n - Static GPS vectors\n\n####Implemented regularizations of the inversion:\n - Truncated SVD\n - Spatial *k*<sup>-2</sup> prior covariance function\n - Positivity constraint on the slip rate functions\n\n####Included codes for evaluation of Green's functions:\n - Axitra (full-wavefield in 1D layered media)\n - Okada (static displacements in homogeneous halfspace)\n\n------------\n\n###Content of directories:\n - `src` - Inversion codes\n - `src-stations` - Converts stations locations from lat,long to X,Y (X points towards north, Y towards east)\n - `src-dwn` - Axitra code for Green's function calculations\n - `src-graphics` - Codes for generating graphics (requires Gnuplot)\n - `examples` - Several examples for testing the code\n - `papers` - Papers related to the inversion codes, explaining basics of the SVD and NNLS approaches, resolution analysis, etc.\n ",
        "createdAt": "2014-10-03T20:12:42.000Z",
        "updatedAt": "2025-04-08T11:02:10.000Z",
        "language": "Fortran",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/fgallovic/LinSlipInv/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ifountoul/SSA2py",
        "url": "https://github.com/ifountoul/SSA2py",
        "description": "SSA2py: Source-Scanning Algorithm in Python",
        "stars": 25,
        "forks": 4,
        "readme": "<p align=\"center\">\n  <a href=\"\">\n    <img src=\"logo.jpg\" width=\"400\" alt=\"SSA2py logo\">\n  </a>\n</p>\n\n<h3 align=\"center\">SSA2py</h3>\n\n<p align=\"center\">\n   Unleashing Source Scanning at the Speed of Python\n  <br>\n  <a href=\"https://ssa2py.readthedocs.io/en/latest/\"><strong>Explore SSA2py docs »</strong></a>\n  <br>\n  <br>\n  <a href=\"https://github.com/ifountoul/SSA2py/issues\">Report bug</a>\n</p>\n\n\n# SSA2py 1.0.0\n\nWelcome to the public repo for SSA2py.\n\n## What is SSA2py?\n\n[SSA2py]() **is an open-source python project that follows the Source-Scanning Algorithm (SSA)**.\nIt provides interconnection with FDSN Compliant Web Services and it is adapted to run in GPU and CPU multiprocessing architectures. \nThe aim of SSA2py is to provide rapid and accurate calculations of SSA method in near-realtime conditions.\n\nThe official documentation is hosted on [Read the Docs](https://ssa2py.readthedocs.io/en/latest/).\n\nThe SSA2py Publication in [SRL](https://pubs.geoscienceworld.org/ssa/srl/article-abstract/doi/10.1785/0220230335/637097/SSA2py-A-High-Performance-Python-Implementation-of?redirectedFrom=fulltext).\n\n## Status\n\n[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)\n![Python](https://img.shields.io/badge/python-3.10-blue.svg)\n![Repo Size](https://img.shields.io/github/repo-size/Sulstice/global-chem)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)\n[![Maturity badge - level 2](https://img.shields.io/badge/Maturity-Level%202%20--%20First%20Release-yellowgreen.svg)](https://github.com/tophat/getting-started/blob/master/scorecard.md)\n\n## Table of contents\n\n- [Quick start](#quick-start)\n- [Supported Platforms](#supported-platforms)\n- [Community](#community)\n- [Contribution](#contribution)\n- [Creators](#creators)\n- [Thanks](#thanks)\n- [Copyright and license](#copyright-and-license)\n\n\n## Quick Start\n- `git clone https://github.com/ifountoul/SSA2py.git`\n- `cd SSA2py`\n- `conda env create -f environment.yml`\n- `conda activate SSA2PY`\n-  Create the events directory (Events Dir) and the traveltimes directory (Traveltimes/Save) declared in the configuration file.\n- `python3 SSA2py.py --download`\n- `python3 SSA2py.py` **(Everything OK? Ready to go!)**\n\n### Prerequisites\n- Install conda\n- If you install SSA2py on a brand new system install the C and C++ compilers before installing Anaconda.\n- Make sure that you have conda-forge in your channels (`conda config --show channels`). You can add it by executing `conda config --add channels conda-forge`.\n- To use GPU install the cudatoolkit throught anaconda. Please check the CUDA and NVIDIA driver versions.\n\nTo learn more about using SSA2py, follow our [guide here](https://ssa2py.readthedocs.io/en/latest/) or see the [examples](https://ssa2py.readthedocs.io/en/latest/applications.html).\n\n## Supported Platforms\n\nOur software is developed on Ubuntu 22.04.2 LTS and tested on the following platforms:\n- Linux (Ubuntu 22.04 LTS)\n- Windows (Windows 11 Pro - WSL 2)\n\n**Note:** SSA2py is not vigorously tested on macOS at the moment. Contributions and feedback related to macOS testing are welcome.\n\n## Community\n\nHave questions, comments or feedback? Start a [discussion](https://github.com/ifountoul/SSA2py/discussions).\n\n## Contribution\n\nFound a bug? Please submit an [issue](https://github.com/ifountoul/SSA2py/issues).\n\n## Creators\n\n**Ioannis Fountoulakis**\n\n:email: ifountoul@noa.gr\n\n**Christos Evangelidis**\n\n:email: cevan@noa.gr\n\n## Cite us!\n\nIf you used SSA2py in your research, please use the following BibTeX entry:\n\n```\n@article{10.1785/0220230335,\n    author = {Fountoulakis, Ioannis and Evangelidis, Christos P.},\n    title = \"{SSA2py: A High‐Performance Python Implementation of the Source‐Scanning Algorithm for Spatiotemporal Seismic Source Imaging}\",\n    journal = {Seismological Research Letters},\n    year = {2024},\n    issn = {0895-0695},\n    doi = {10.1785/0220230335},\n    url = {https://doi.org/10.1785/0220230335}\n}`\n```\n\n## Thanks \n\n<a href=\"https://www.elidek.gr/en/homepage/\">\n  <img src=\"https://www.elidek.gr/wp-content/themes/elidek/images/elidek_logo_en.png\" alt=\"H.F.R.I\" width=\"310\" height=\"90\">\n</a>\n\nThe research was supported by the Hellenic Foundation for Research and Innovation ([H.F.R.I.](https://www.elidek.gr/en/homepage/)) under the \n“First Call for H.F.R.I. Research Projects to support Faculty members and Researchers and the procurement of high-cost research equipment grant” (SIREN, Project Number: 910).\n\n<a href=\"https://www.noa.gr/en/\">\n  <img src=\"https://www.noa.gr/wp-content/uploads/2019/12/noa_logo.svg\" alt=\"NOA\" width=\"110\" height=\"110\">\n</a>\n\nThanks to [NOA](https://www.noa.gr/en/) for providing the infrastructure to develop this program!\n\n## Copyright and license\n\nCode released under the GNU [GENERAL PUBLIC LICENSE Version 3](https://github.com/ifountoul/SSA2py-Ghost/blob/master/LICENSE)\n",
        "createdAt": "2023-01-17T10:11:34.000Z",
        "updatedAt": "2025-11-08T05:48:18.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ifountoul/SSA2py/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ishiikurisu/ObSis",
        "url": "https://github.com/ishiikurisu/ObSis",
        "description": "Code made while developing for Seismologic Observatory",
        "stars": 0,
        "forks": 0,
        "readme": "# Seismologic Observatory\n\nCode made by [me](https://github.com/ishiikurisu) while studying at the Seismologic Observatory from the University of Brasilia.\n\n## Index ##\n\n### [\"Real\" Data](real-data)\n\nThis folder is an experiment for generating and analysing a \"real\" dataset, as if I were obtaining data from the real world.\n\n### [Synthetic Data](synth-data)\n\nThe idea of this part of the code is to analyze the \"real\" data with a known geophysical analysis process.\n\n### [Amplitude Picking](amps)\n\nHere, I want to analyze the collected data myself, opening the relevant files and performing the computations on my own instead of relying on 3rd party software.\n\n### Stuff\n\nThe [example](example) and the [stuff](stuff) folders have some random code that help me complete my tasks.\n",
        "createdAt": "2017-05-26T16:03:34.000Z",
        "updatedAt": "2017-08-18T18:58:40.000Z",
        "language": "PostScript",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ishiikurisu/ObSis/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "hvasbath/beat",
        "url": "https://github.com/hvasbath/beat",
        "description": "Bayesian Earthquake Analysis Tool",
        "stars": 151,
        "forks": 41,
        "readme": "\n![BEAT logo](https://pyrocko.org/beat/docs/current/_images/LOGO_BEAT.png)\n\n# Bayesian Earthquake Analysis Tool\n\nDocumentation of the current version moved to the pyrocko server can be found here:\nhttps://pyrocko.org/beat/\n\nBased on pyrocko, pytensor and pymc\n\n## News\nThe new BEAT v2.0.0 is not backward compatible with project setups from BEAT v1.x.x!\n\n## Tutorials\nStep by step points on how to use the tool for several scenarios can be found here:\n[Examples](https://pyrocko.org/beat/docs/current/examples/index.html#)\n\n## Citation\nIf your work results in an publication where you used BEAT we kindly ask you to consider citing the BEAT software package and the related article.:\n\n > Vasyura-Bathke, Hannes; Dettmer, Jan; Steinberg, Andreas; Heimann, Sebastian; Isken, Marius; Zielke, Olaf; Mai, Paul Martin; Sudhaus, Henriette; Jónsson, Sigurjón: The Bayesian Earthquake Analysis Tool. Seismological Research Letters. https://doi.org/10.1785/0220190075\n\n > Vasyura-Bathke, Hannes; Dettmer, Jan; Steinberg, Andreas; Heimann, Sebastian; Isken, Marius; Zielke, Olaf; Mai, Paul Martin; Sudhaus, Henriette; Jónsson, Sigurjón (2019): BEAT - Bayesian Earthquake Analysis Tool. V. 1.0. GFZ Data Services. http://doi.org/10.5880/fidgeo.2019.024\n\n\n## Support\nFor substantial issues please use and check the \"issues\" tab here in the repository.\nFor common issues please check out the BEAT [FAQ](https://pyrocko.org/beat/docs/current/faq.html).\nFor smaller issues or short clarifications there is a support [chat](https://hive.pyrocko.org/pyrocko-support/channels/beat). This is provided by the pyrocko project and is accessible after a short account creation.\n\nFinally, there is the option to write an email to:\n\nHannes Vasyura-Bathke\nhvasbath@uni-potsdam.de\n\nAndreas Steinberg\nandreas.steinberg@ifg.uni-kiel.de\n\n**HELP wanted!**\n\nThe new release contains a lot of undocumented features a list of these can be found here:\nhttps://github.com/hvasbath/beat/issues/69\nHowever, as this project is mostly the work of a single author it is becoming increasingly difficult to also\nwrite extended pages of tutorials that take days and days of writing. However, as this work is not acknowledged by the\ncurrent academic system, I had to decide for a delayed release of the documentation, whenever it will\nbe available provided by someone. Thus, in case you are willing to contribute I would be more than happy to guide/ support\nyou in writing parts of the documentation for a particular feature-if you want to try it out.\n\n## License\nGNU General Public License, Version 3, 29 June 2007\n\nCopyright © 2017 Hannes Vasyura-Bathke\n\nBEAT is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\nBEAT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.\nYou should have received a copy of the GNU General Public License along with this program. If not, see <http://www.gnu.org/licenses/>.\n\n\n## Data import\n### Geodetic\nWe recommend to prepare the SAR data (subsampling, data covariance estimation) using KITE (www.pyrocko.org).\nkite supports import of ISCE, GAMMA, ROI_Pac and GMTSAR processed interferograms. BEAT then supports import of the native KITE format.\n\n### Seismic\nTo see a list of the supported data types please see: [Trace Handling](https://pyrocko.org/docs/current/library/examples/trace_handling.html)\nIn addition to these an ascii text file with the station information is needed of the format::\n\n    #network_name.station_name.location_name latitude[deg] longitude[deg] elevation[m] depth[m]\n    IU.TSUM.10            -19.20220       17.58380         1260.0            0.0\n      BHE             90              0              1   # channel name azimuth[deg] dip[deg] gain \\n\n      BHN              0              0              1\n      BHZ              0            -90              1\n    IU.RCBR.00             -5.82740      -35.90140          291.0          109.0\n      BH1             48              0              1\n      BH2            138              0              1\n      BHZ              0            -90              1\n    ...\n\nTo ease the creation of this textfile we refer the user to investigate the pyrocko module: model (Function: dump_stations)\n\nIn addition to these, BEAT supports the output of the autokiwi package.\n\nWork is in progress to support obspy saved stream and inventory files, as well as stationxml stay tuned ...\n\nAlternatively the seismic data may be saved using the package \"pickle\" as a file \"seismic_data.pkl\"\ncontaining a list of 2 lists:\n1. list of \"pyrocko.trace.Trace\" objects alternating for (Z / T / R) rotated traces.\n2. list of \"pyrocko.model.Station\" objects in the same order like the data traces.\n\nWe invite the users to propose data formats or outputs of specific programs that they would\nlike to see implemented.\n\nP.S.: If you came looking for the beat package calculating internet time you can find it [here](https://github.com/tonyskapunk/beat).\n\n## Contributions\nThis is an open source project and contributions to the repository are welcome!\n",
        "createdAt": "2016-07-17T11:21:34.000Z",
        "updatedAt": "2025-11-08T23:06:40.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/hvasbath/beat/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "RyanJamesSchultz/Bval",
        "url": "https://github.com/RyanJamesSchultz/Bval",
        "description": "We present a Matlab-based routine, Bval that calculates the fit to the Gutenberg-Richter frequency-magnitude distribution (GR-FMD). This code could be readily adapted or modified for a broad range of seismological applications.",
        "stars": 6,
        "forks": 4,
        "readme": "# Bval\n\nWe present a Matlab-based routine, Bval that calculates the fit to the Gutenberg-Richter frequency-magnitude distribution (GR-FMD).  This code could be readily adapted or modified for a broad range of seismological applications.\n\nFor sample use see \"script_plot.m\" and \"script_Mc\" to recreate the supplementary figure 8 within the corresponding manuscript (Schultz et al., 2018).\n\nReferences: \n            \n            Schultz, R., Atkinson, G., Eaton, D. W., Gu, Y. J., & Kao, H. (2018). \n            Hydraulic fracturing volume is associated with induced earthquake productivity in the Duvernay play\n            Science, 359(6373), 304-308, doi: 10.1126/science.aao0159.\n\nThis program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version.\n\nThis program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details: http://www.gnu.org/licenses/\n",
        "createdAt": "2018-05-07T17:28:59.000Z",
        "updatedAt": "2023-11-10T11:59:23.000Z",
        "language": "MATLAB",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/RyanJamesSchultz/Bval/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "CNCA-CeNAT/computational_seismology",
        "url": "https://github.com/CNCA-CeNAT/computational_seismology",
        "description": null,
        "stars": 1,
        "forks": 0,
        "readme": "# Framework para la simulación en paralelo de fenómenos sísmicos y volcánicos\n\n## Pre-simulación\n\n*order\\_xyz.py* Toma un archivo xyz, tal como lo exporta QGIS y lo transforma al formato adecuado para simular con specfem3D.\n\n*cmp_online_synth.py* un archivo de configuración con el path a un directorio que contiene los sismogramas sintéticos y los parámetros de la simulación. Se conecta con IRIS para obtener las formas de onda de los sismogramas observados, los pre-procesa y guarda un gráfico con ambos sismogramas, sintéticos y observados. \n\n*?\\_stations* Lista de estaciones para cada red \n",
        "createdAt": "2017-07-20T20:32:57.000Z",
        "updatedAt": "2019-11-26T20:44:48.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/CNCA-CeNAT/computational_seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "panzhengyang/seismology_lesson_site",
        "url": "https://github.com/panzhengyang/seismology_lesson_site",
        "description": "summary  seismology lesson website",
        "stars": 0,
        "forks": 0,
        "readme": "# seismology_lesson_site\n\nsome lessons of learning seismology \n\nlist some personal homepage and some course website\n",
        "createdAt": "2015-10-10T06:43:09.000Z",
        "updatedAt": "2015-10-13T07:14:50.000Z",
        "language": null,
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/panzhengyang/seismology_lesson_site/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Sohan-Pandit/Download-Earthquake-Data-",
        "url": "https://github.com/Sohan-Pandit/Download-Earthquake-Data-",
        "description": "For Southeast Tibet",
        "stars": 4,
        "forks": 1,
        "readme": "# Download-Earthquake-Data-\nFor Southeast Tibet\n\n\n\n# Explaination of For the Getting Event Data\n1. The necessary libraries are imported, including os, obspy, numpy, pandas, matplotlib.pyplot, Normalize from matplotlib.colors, and Basemap from mpl_toolkits.basemap. These libraries are used for many different things, including processing files, manipulating data, charting, and visualizing maps. \n2. Folder_output specifies the folder path where the output files should be stored, and os.makedirs() creates the folder if it doesn't already exist.\n3. The excel_filename and excel_tab variables each contain the name of the Excel file and tab.\n4. Obspy.UTCDateTime is used to set the beginning and ending times for data retrieval. These show the time frame for which seismic events will be fetched.\n5. The \"USGS\" value, which designates the data source, is used to define the FDSN client.\n6. The minimum, and maximum latitude and longitude values (minlatitude, maxlatitude, minlongitude, and maxlongitude.) are set for the map's extent.\n7. The terms \"minmagnitude\" and \"maxmagnitude\" describe the earthquake events' minimum and highest magnitudes.\n8. Obspy.clients.fdsn.Client(client) is used to create the FDSN client.\n9. The FDSN client's get_events() method is used to retrieve the seismic events. The latitude and longitude range, start and end times, and lowest and maximum magnitudes are among the method parameters.\n10. The console prints information about the events that were fetched.\n11. The events are kept in df, a Pandas DataFrame. The columns listed in feature_list and an empty structure are used to initialise the DataFrame.\n12. Each event that is fetched is iterated over in a loop. The df.append() method is used inside the loop to extract pertinent data from each event, such as the origin time, latitude, longitude, depth, event type, magnitude, magnitude type, creation information, and event description, and add it as a new row to the DataFrame.\n13. The DataFrame is then saved using df.to_excel() to an Excel file that is defined by excel_output. One sheet with the name excel_tab will be present in the Excel file.\n14. The events are then plotted on a map using Basemap and Matplotlib in the code. The plot title, text size, tick intervals, and other settings and parameters are all defined.\n15. Variables x, y, and z are each given values from the DataFrame for longitude, latitude, and magnitude.\n16. The projection and map extent that are supplied when creating the Basemap instance.\n17. The Basemap techniques are used to draw the map's details, including the coasts, boundaries, continents, countries, parallels, and meridian lines.\n18. The scatter spots on the map represent the earthquake incidents. The magnitude values dictate the size and colour of the points, resulting in a color-coded representation of the earthquake magnitudes.\n19. The magnitude scale is indicated on the plot by a colorbar.\n20. Plot.savefig() is then used to save the plot as a PNG picture.\n\n\n\n\n",
        "createdAt": "2023-06-11T20:29:38.000Z",
        "updatedAt": "2025-08-16T13:05:56.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Sohan-Pandit/Download-Earthquake-Data-/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "schipp/awesome-seismology",
        "url": "https://github.com/schipp/awesome-seismology",
        "description": "Awesome Seismology - Earthquakes, Earth's structure, and related methodology",
        "stars": 25,
        "forks": 2,
        "readme": "# Awesome Seismology [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)\n\nEarthquakes, Earth's structure, and related methodology.\n\n## Contents\n\n- [Array seismology](#array-seismology)\n- [Earthquake bulletins/catalogues](#earthquake-bulletinscatalogues)\n- [Educational resources](#educational-resources)\n- [Fibre optic sensing](#fibre-optic-sensing)\n- [Imaging](#imaging)\n- [Inversion \\& Inference](#inversion--inference)\n- [Machine learning](#machine-learning)\n- [Marine seismology](#marine-seismology)\n- [Observatory operation](#observatory-operation)\n- [Phase picking and association](#phase-picking-and-association)\n- [Raytracing](#raytracing)\n- [Seismic data access](#seismic-data-access)\n- [Seismic data handling](#seismic-data-handling)\n- [Seismic interferometry and ambient noise](#seismic-interferometry-and-ambient-noise)\n- [Source parameter estimation](#source-parameter-estimation)\n- [Synthetic seismograms](#synthetic-seismograms)\n\n## Array seismology\n\n- [acoular](https://www.acoular.org) - Acoustic testing and source mapping software. ![python](assets/python.png)\n- [B3AM](https://github.com/katrinloer/B3AM) & [B3Ampy](https://github.com/cl-finger/B3Ampy) - Toolbox for easy and fast beamforming analysis of three-component array data. ![matlab](assets/matlab.png) ![python](assets/python.png)\n- [beampower](https://github.com/ebeauce/beampower) - Beamforming (or backprojection) of seismic signal for event detection and location. ![python](assets/python.png)\n- [covseisnet](https://github.com/leonard-seydoux/covseisnet) - Array covariance matrix analysis. ![python](assets/python.png)\n- [fast_beamforming](https://github.com/schipp/fast_beamforming) - Fast and efficient beamforming in Python. ![python](assets/python.png)\n- [TwistPy](https://twistpy.org) - Toolbox for wavefield inertial sensing techniques. ![python](assets/python.png)\n\n## Earthquake bulletins/catalogues\n\n- [BGR](https://www.szgrf.bgr.de) - Bulletin of the Federal Institute for Geosciences and Natural Resources, Germany.\n- [EMSC](https://emsc-csem.org) - Bulletin of the Euro-Mediterranean Seismological Centre.\n- [GEOFON](https://geofon.gfz-potsdam.de) - Bulletin of the GeoForschungsZentrum Potsdam, Germany.\n- [Geosphere Austria](https://beta.geosphere.at/de/karten/aktuelle-erdbeben#tab=tablemode) - Bulletin of the Geosphere Austria.\n- [Global CMT](https://www.globalcmt.org) - Bulletin of the Global Centroid-Moment-Tensor (CMT) Project.\n- [IGN](https://www.ign.es/web/en/ign/portal/sis-catalogo-terremotos) - Bulletin of the Instituto Geografico Nacional, Spain.\n- [INGV](https://www.ingv.it/en/resources-and-services/environment-earthquakes-and-volcanoes/newsletters) - Bulletin of the Instituto Nazionale Di Geofisicia e Vulcanologia, Italy.\n- [ISC](https://www.isc.ac.uk/iscbulletin/) - Bulletin of the International Seismological Centre, UK.\n- [ROB](http://seismology.be/en) - Bulletin of the Royal Observatory of Belgium.\n- [SED](http://www.seismo.ethz.ch/en/home/) - Bulletin of the Swiss Seismological Service, Switzerland.\n- [USGS](https://earthquake.usgs.gov/earthquakes/map/) - Bulletin of the US Geological Survey, USA.\n\n## Educational resources\n\n- [seismo-live](https://seismo-live.github.io) - Live jupyter notebooks for seismology. ![python](assets/python.png)\n\n## Fibre optic sensing\n\n- [awesome-das](https://github.com/DAS-RCN/awesome-das) - Curated list of awesome resources for Distributed Acoustic Sensing (DAS).\n- [DASCore](https://dascore.netlify.app) - Python library for distributed fiber optic sensing. ![python](assets/python.png)\n- [DASPack](https://github.com/asleix/daspack) - Controlled data compression for Distributed Acoustic Sensing. ![python](assets/python.png) ![rust](assets/rust.png)\n- [dastools](https://git.gfz-potsdam.de/javier/dastools) - Tools to work with data generated by DAS systems. ![python](assets/python.png)\n- [Lightguide](https://github.com/pyrocko/lightguide) - Package for handling, filtering and modelling Distributed Acoustic Sensing (DAS) data. ![python](assets/python.png) ![rust](assets/rust.png)\n- [Xdas](https://github.com/xdas-dev/xdas) - Python library for managing, processing and visualizing Distributed Acoustic Sensing (DAS) data. ![python](assets/python.png)\n\n## Imaging\n\n- [MSNoise-Tomo](https://github.com/ThomasLecocq/msnoise-tomo) - Plugin to the MSNoise framework for 2D tomography. ![python](assets/python.png)\n- [PyGLImER](https://github.com/PyGLImER/PyGLImER) - Workflow to create a global database for Ps and Sp receiver function imaging. ![python](assets/python.png)\n- [SeisLib](https://github.com/fmagrini/seislib) - Python package that allows for obtaining seismic images of the sub-surface. ![python](assets/python.png)\n\n## Inversion & Inference\n\n- [BayesBay](https://github.com/fmagrini/bayes-bay) - Generalised trans-dimensional and hierarchical Bayesian inference. ![python](assets/python.png)\n- [GeoBED](https://github.com/dominik-strutz/GeoBED) - Optimal experimental design tailored to geoscientific applications. ![python](assets/python.png)\n- [pyGIMLi](https://www.pygimli.org) - Multi-method modelling and inversion in geophysics. ![python](assets/python.png)\n\n## Machine learning\n\n- [DeepDenoiser](https://github.com/AI4EPS/DeepDenoiser) - Seismic signal denoising and decomposition using deep neural networks. ![python](assets/python.png)\n- [SeisBench](https://github.com/seisbench/seisbench) - Python toolbox for machine learning in seismology. ![python](assets/python.png)\n- [scatseisnet](https://github.com/scatseisnet/scatseisnet) - Transform time series into scattering coefficients with a scattering network. ![python](assets/python.png)\n\n## Marine seismology\n\n- [OBSTools](https://github.com/nfsi-canada/OBStools) - Processing broadband ocean-bottom seismic data. ![python](assets/python.png)\n- [OCloC (OBS Clock Correction)](https://ocloc.readthedocs.io/en/latest/index.html) - Detect and correct timing errors when using passive seismic records. ![python](assets/python.png)\n\n## Observatory operation\n\n- [Antelope](https://brtt.com) - Real time Earth monitoring for a dynamic world.\n- [Earthworm](https://gitlab.com/seismic-software/earthworm/) - Waveform and automatic earthquake processing software. ![c](assets/c.png)\n- [SEISAN](https://seisan.info) - Earthquake analysis software. ![fortran](assets/fortran.png)\n- [SeisComP](https://www.seiscomp.de) - Seismological software for data acquisition, processing, distribution and interactive analysis. ![cpp](assets/cpp.png)\n\n## Phase picking and association\n\n- [EQTransformer](https://github.com/smousavi05/EQTransformer) - AI-based earthquake signal detector and phase picker. ![python](assets/python.png)\n- [GaMMA](https://github.com/AI4EPS/GaMMA) - Gaussian Mixture Model Associator. ![python](assets/python.png)\n- [OBSTransformer](https://github.com/alirezaniki/OBSTransformer) - A Deep Learning Seismic Phase Picker for OBS Data. ![python](assets/python.png)\n- [PhaseNet](https://github.com/AI4EPS/PhaseNet) - Deep-neural-network-based seismic arrival time picking method. ![python](assets/python.png)\n- [PyOcto](https://github.com/yetinam/pyocto) - High-throughput seismic phase associator. ![python](assets/python.png)\n\n## Raytracing\n\n- [TauP](http://www.seis.sc.edu/taup/) - Seismic travel time calculator. ![java](assets/java.png)\n- [Cake (pyrocko)](https://pyrocko.org/docs/current/apps/cake/) - 1D travel-time and ray-path computations. ![python](assets/python.png)\n- [scikit-fmm](https://github.com/scikit-fmm/scikit-fmm) - Fast Marching Method for Python. ![python](assets/python.png)\n- [pykonal](https://github.com/malcolmw/pykonal) - Fast Marching Method in cartesian or spherical coordinates in 2 or 3 dimensions. ![python](assets/python.png)\n\n## Seismic data access\n\n- [EarthScope](https://ds.iris.edu/ds/nodes/dmc/data/) (formerly IRIS) - Waveform and event data access, US based.\n- [EIDA (ORFEUS)](https://orfeus-eu.org/data/eida/) - Waveform and event data access, EU based.\n- [FDSN network codes](https://www.fdsn.org/networks/) - List of all seismic network codes assigned by the FDSN.\n- [STEAD](https://github.com/smousavi05/STEAD) - STanford EArthquake Dataset (STEAD):A Global Data Set of Seismic Signals for AI.\n\n## Seismic data handling\n\n- [ObsPy](https://github.com/obspy/obspy/wiki/) - Python framework for processing seismological data. ![python](assets/python.png)\n- [Pyrocko](https://pyrocko.org) - Open source seismology toolbox and library. ![python](assets/python.png)\n- [SeisGo](https://github.com/xtyangpsp/SeisGo) - Ready-to-go Python toolbox for seismic data analysis. ![python](assets/python.png)\n- [Seismic Unix](https://github.com/JohnWStockwellJr/SeisUnix) - Seismic processing, research, and educational software package.\n- [Seis.jl](https://github.com/anowacki/Seis.jl) - Open, fast and flexible framework for analysing seismic data in Julia. ![julia](assets/julia.png)\n\n## Seismic interferometry and ambient noise\n\n- [MSNoise](http://www.msnoise.org) - Python package for monitoring using ambient seismic noise. ![python](assets/python.png)\n- [NoisePy](https://github.com/noisepy/NoisePy) - Fast and easy computation of ambient noise cross-correlation functions. ![python](assets/python.png)\n- [noisi](https://github.com/lermert/noisi) - Ambient noise cross-correlation modeling and inversion. ![python](assets/python.png)\n- [SANS](https://sans.ethz.ch) - Seismic ambient noise source maps.\n- [SeisNoise.jl](https://github.com/JuliaSeismo/SeisNoise.jl) - Fast and easy ambient noise cross-correlation. ![julia](assets/julia.png)\n- [SeisMIC](https://github.com/PeterMakus/SeisMIC) - Seismological monitoring using interferometric concepts. ![python](assets/python.png)\n- [WMSAN](https://github.com/lystom/WMSAN) - Wave model sources of ambient noise. ![python](assets/python.png)\n\n## Source parameter estimation\n\n- [Grond (pyrocko)](https://pyrocko.org/grond) - Probabilistic source optimization. ![python](assets/python.png)\n- [BEAT (pyrocko)](https://pyrocko.org/beat) - Bayesian Earthquake Analysis Tool. ![python](assets/python.png)\n- [BPMF](https://github.com/ebeauce/Seismic_BPMF) - Earthquake detection and location with GPU-accelerated processing. ![python](assets/python.png)\n- [focmec](https://seiscode.iris.washington.edu/projects/focmec) - Package for determining and displaying earthquake focal mechanisms.\n- [GrowClust3D](https://github.com/dttrugman/GrowClust3D.jl) - Relative relocation of earthquake hypocenters based on waveform cross-correlation data. ![julia](assets/julia.png)\n- [HypoDD](https://www.ldeo.columbia.edu/~felixw/hypoDD.html) - Double-difference earthquake location algorithm ![fortran](assets/fortran.png)\n- [NonLinLoc](https://github.com/alomax/NonLinLoc) - Non-Linear locations. ![c](assets/c.png)\n- [Qseek (pyrocko)](https://github.com/pyrocko/qseek) - The friendly earthquake detector. ![python](assets/python.png)\n- [Simul2023](https://zenodo.org/records/10695070) - Inversion of earthquake data for 3-D velocity and hypocenters or 3-D Q. ![fortran](assets/fortran.png)\n- [SKHASH](https://code.usgs.gov/esc/SKHASH) - Earthquake focal mechanism inversions. ![python](assets/python.png)\n\n## Synthetic seismograms\n\n- [AxiSEM](https://github.com/geodynamics/axisem) - Axially symmetric Spectral Element Method (2.5D). ![fortran](assets/fortran.png)\n- [AxiSEM3D](https://github.com/AxiSEMunity/AxiSEM3D) - Axially symmetric Spectral Element Method (3D). ![cpp](assets/cpp.png)\n- [Axitra](https://github.com/coutanto/axitra) - Seismograms in 3D plane layered medium. ![fortran](assets/fortran.png) ![python](assets/python.png)\n- [Computer Programs in Seismology](https://github.com/rbherrmann/ComputerProgramsSeismology) - Package of programs for making synthetic seismograms.\n- [disba](https://github.com/keurfonluu/disba) - Surface wave dispersion modeling. ![python](assets/python.png)\n- [Fomosto (pyrocko)](https://pyrocko.org/docs/current/apps/fomosto/) - Calculate and manage Green's function databases. ![python](assets/python.png)\n- [instaseis](https://instaseis.net) - Instant global seismograms based on a broadband waveform database. ![python](assets/python.png)\n- [Mineos](https://github.com/geodynamics/mineos) - Compute synthetic seismograms in a spherically symmetric non-rotating Earth by summing normal modes. ![fortran](assets/fortran.png)\n- [PyFK](https://github.com/ziyixi/pyfk) - Python port of FK used to calculate the Green's function and the synthetic waveforms for the 1D Earth model. ![python](assets/python.png)\n- [SeisSol](https://seissol.org) - Software package for simulating wave propagation and dynamic rupture. ![cpp](assets/cpp.png)\n- [SPECFEM](https://specfem.org) - Open-source spectral-element method software codes for computational seismology. ![fortran](assets/fortran.png) ![c](assets/c.png)\n- [SW4](https://github.com/geodynamics/sw4) - Seismic waves, 4th order accuracy. ![fortran](assets/fortran.png) ![cpp](assets/cpp.png)\n",
        "createdAt": "2024-12-19T19:13:17.000Z",
        "updatedAt": "2025-10-29T22:11:55.000Z",
        "language": null,
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/schipp/awesome-seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Jollyfant/node-seedlink-data-proxy",
        "url": "https://github.com/Jollyfant/node-seedlink-data-proxy",
        "description": "NodeJS proxy connecting to a remote Seedlink server broadcasting unpacked data samples",
        "stars": 10,
        "forks": 5,
        "readme": "# nodejs-seedlink-data-proxy\nLightweight NodeJS websocket server capable of communicating over the Seedlink protocol. The server broadcasts unpacked mSEED data samples through HTML5 websockets. When a client connects to the proxy server, a connection is relayed to a configured seedlink protocol. The server supports data streaming from multiple remote Seedlink servers & channels to different subscribers.\n\n\n## Installation\n\n    npm install\n\n## Configuration\n\n  - `__DEBUG__` Sets application in debug mode.\n  - `__NAME__` - Application name.\n  - `HOST` - Hostname exposing the Seedlink proxy server.\n  - `PORT` - Port the Seedlink proxy server is exposed on.\n  - `HEARTBEAT_INTERVAL_MS` - Number of miliseconds before checking the socket a ping\n\n## Channel Configuration\nA channel describes a configured data stream that can be subscribed to and is identified by a name. Each channel will open a single Seedlink connection when users are subscribed. Users that are subscribed to a channel will receive data packets attributed to that particular channel.\n\n## Testing\n\n    npm test\n\n## Running\n\n    npm start\n\n## Docker\n\n    docker build -t seedlink-proxy:1.0 .\n    docker run -p 8087:8087 [--rm] [-d] [-e \"SERVICE_PORT=8087\"] [-e \"SERVICE_HOST=0.0.0.0\"] seedlink-proxy:1.0\n\nTwo envrionment variables can passed to Docker run to modify settings at runtime. Otherwise information is read from the built configuration file.\n\n  * SERVICE\\_HOST\n  * SERVICE\\_PORT\n\n## Client Example\nFor an example of the client websocket look for `index.html`.\n\n## Websocket API\nTo communicate with the websocket server you will need to write an operation (e.g. (un)subscription) to the socket:\n\n    // Subscribe and unsubscribe from a channel\n    {\"subscribe\": \"NL.HGN\"}\n    {\"unsubscribe\": \"NL.HGN\"}\n\n    // Get a list of the available channels\n    {\"channels\": true} \n\nOnce a subscription is accepted, the server will start writing over the websocket. An unlimited number of subscriptions can be active per user.\n\n## Unpacked mSEED structure\nThe unpacked mSEED will be formatted as JSON with the following data (samples) & metadata.\n\n    {\n        \"start\": 1531139771269,\n        \"end\": 1531139780894,\n        \"data\": [...],\n        \"network\": \"NL\",\n        \"station\": \"HGN\",\n        \"location\": \"02\",\n        \"channel\": \"BHN\",\n        \"sampleRate\": 40,\n        \"id\": \"NL.HGN.02.BHN\"\n    }\n",
        "createdAt": "2018-07-09T15:12:11.000Z",
        "updatedAt": "2024-12-07T03:21:09.000Z",
        "language": "JavaScript",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Jollyfant/node-seedlink-data-proxy/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "earthinversion/Geospatial-Data-Visualization-using-PyGMT",
        "url": "https://github.com/earthinversion/Geospatial-Data-Visualization-using-PyGMT",
        "description": "Example script to visualize topographic data, earthquake data, and tomographic data on a map",
        "stars": 6,
        "forks": 1,
        "readme": "## Geospatial Data Visualization using PyGMT\nThis notebook has been created for the BSL talk on Feb 02, 2022 at McCone Hall, University of California, Berkeley\n\n## You can also open the notebooks in the Binder application online...\n\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/earthinversion/Geospatial-Data-Visualization-using-PyGMT/master)\n\n\n## Install libraries\n\n### Using python env\n```\npython -m venv geoviz\nsource geoviz/bin/activate\npip install pygmt\n```\n\n- Try:\n```\npython -c \"import pygmt\"\n```\nif there's no `ImportError`, then you are good to go.\n\n__NOTE:__\nIf there's any pygmt import problem, install GMT separately and link the `libgmt.dylib` file to the file python is looking for!\n\n- One way to install GMT is `conda install gmt -c conda-forge`\n\n```\nln -s ~/miniconda3/envs/boxgmt/lib/libgmt.dylib ~/miniconda3/envs/geoviz/lib/libgmt.dylib\n```\n\n### Using conda env (recommended)\n```\nconda create --name geoviz --channel conda-forge pandas pygmt jupyter notebook\n```\n\n\n\n\n```\nconda env export > environment.yml\n```\n",
        "createdAt": "2022-02-03T17:42:11.000Z",
        "updatedAt": "2023-11-20T06:41:13.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/earthinversion/Geospatial-Data-Visualization-using-PyGMT/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "xumi1993/docs.post",
        "url": "https://github.com/xumi1993/docs.post",
        "description": "Documents to seismology and programing ",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2022-02-04T13:14:27.000Z",
        "updatedAt": "2022-03-08T01:58:59.000Z",
        "language": "Python",
        "homepage": "https://docs.xumijian.me/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "paudetseis/RfPy",
        "url": "https://github.com/paudetseis/RfPy",
        "description": "Teleseismic receiver function calculation and post-processing",
        "stars": 49,
        "forks": 29,
        "readme": "\n![](./rfpy/examples/picture/RfPy_logo.png)\n\n## Teleseismic receiver function calculation and post-processing \n\nRfPy is a software to calculate single event-station receiver functions from the spectral deconvolution technique. Methods are available to post-process the receiver function data to calculate H-k stacks, back-azimuth harmonics and common-conversion-point (CCP) imaging. The code uses the ``StDb`` package for querying and building a station database and can be used through command-line scripts.\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3905414.svg)](https://doi.org/10.5281/zenodo.3905414)\n[![build](https://github.com/paudetseis/RfPy/workflows/Build/badge.svg)](https://github.com/paudetseis/RfPy/actions)\n[![codecov](https://codecov.io/gh/paudetseis/RfPy/branch/master/graph/badge.svg)](https://codecov.io/gh/paudetseis/RfPy)\n\nInstallation, Usage, API documentation and scripts are described at \nhttps://paudetseis.github.io/RfPy/.\n\nAuthors: [`Pascal Audet`](https://www.uogeophysics.com/authors/admin/) (Developer and Maintainer) and [`Jeremy Gosselin`](https://www.uogeophysics.com/authors/gosselin/) (Contributor)\n<!-- #### Citing\n\nIf you use `SplitPy` in your work, please cite the \n[`Zenodo DOI`](https://zenodo.org/badge/latestdoi/211722700).\n -->\n#### Contributing\n\nAll constructive contributions are welcome, e.g. bug reports, discussions or suggestions for new features. You can either [open an issue on GitHub](https://github.com/paudetseis/RfPy/issues) or make a pull request with your proposed changes. Before making a pull request, check if there is a corresponding issue opened and reference it in the pull request. If there isn't one, it is recommended to open one with your rationale for the change. New functionality or significant changes to the code that alter its behavior should come with corresponding tests and documentation. If you are new to contributing, you can open a work-in-progress pull request and have it iteratively reviewed.\n\nExamples of straightforward contributions include editing the documentation or adding notebooks that describe published examples of teleseismic receiver functions. Suggestions for improvements (speed, accuracy, flexibility, etc.) are also welcome.\n\n#### References\n\n- Audet, P. (2010) Temporal Variations in Crustal Scattering Structure near Parkfield, California, Using Receiver Functions, Bulletin of the Seismological Society of America (2010) 100 (3): 1356-1362. https://doi.org/10.1785/0120090299\n\n- Audet, P. (2015) Layered crustal anisotropy around the San Andreas Fault near Parkfield, California, J. Geophys. Res. Solid Earth, 120, 3527-3543, https://doi.org/10.1002/2014JB011821\n\n- Cossette, E., Audet, P., and Schneider, D.A. (2016) Structure and anisotropy of the crust in the Cyclades, Greece, using receiver functions constrained by in situ rock textural data, J. Geophys. Res. Solid Earth, 121, 2661-2678, https://doi.org/10.1002/2015JB012460\n\n- Tarayoun, A., P. Audet, S. Mazzotti, and A. Ashoori (2017) Architecture of the crust and uppermost mantle in the northern Canadian Cordillera from receiver functions, J. Geophys. Res. Solid Earth, 122, 5268–5287, https://doi.org/10.1002/2017JB014284.\n\n#### Use cases\n\n- Audet, P., Schutt, D., Schaeffer, A.J., Estève, C., Aster, R., and Cubley, J. (2020). Moho variations across the northern Canadian Cordillera, Seism. Res. Lett., accepted.",
        "createdAt": "2019-12-09T19:52:54.000Z",
        "updatedAt": "2025-11-14T12:08:26.000Z",
        "language": "Python",
        "homepage": "https://paudetseis.github.io/RfPy/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/paudetseis/RfPy/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "rintr-suzuki/WIN2PhaseNet",
        "url": "https://github.com/rintr-suzuki/WIN2PhaseNet",
        "description": "Tool to make data for prediction and training of PhaseNet (Zhu and Beroza, 2019) from WIN/WIN32 format waveform file and pick list.",
        "stars": 4,
        "forks": 0,
        "readme": "# WIN2PhaseNet\n## Related Publication\n\n[![DOI](https://img.shields.io/badge/DOI-10.1126%2Fscience.adt6389-blue)](https://doi.org/10.1126/science.adt6389)\n\nIf you use this code or data, please cite the following paper: <br>\nRintaroh Suzuki _et al._, The forearc seismic belt: A fluid pathway constraining down-dip megathrust earthquake rupture. _Science_ **389**, 190-194 (2025). https://doi.org/10.1126/science.adt6389\n\n## Summary\n\n![](docs/assets/WIN2NPZ_overview.png)\n \n* Tool to make data for prediction and training of PhaseNet (Zhu and Beroza, 2019) from WIN/WIN32 (hereafter just 'WIN') format waveform file and pick list.\n* High-speed processing is possible through the use of fwin module (Maeda, 2019) written in **fortran** and **multi-thread processing**.\n* Easy to run on various OS by using **docker**.\n* Provides the simplified operating procedure for PhaseNet and a docker environment to run PhaseNet.\n\n## Requirements\n* OS <br>\n  Support Windows, macOS and Linux\n\n* (Only required for Windows) Git Bash <br>\n  https://gitforwindows.org/ <br>\n  For Windows, run \"Git Bash\" and use it to execute commands for following steps.\n\n* docker <br>\n  * Installation <br>\n  For Windows and macOS, install \"Docker Desktop\" and run it to activate docker. <br>\n  https://docs.docker.com/get-docker/ <br>\n  For Linux, install \"Docker Engine\". <br>\n  https://docs.docker.com/engine/install/ <br>\n\n  * (Only required for Linux) Create the docker group and add your user <br>\n  https://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user <br>\n\n  * Verify installation <br>\n    ```\n    $ docker run hello-world\n    ...\n    Hello from Docker!\n    This message shows that your installation appears to be working correctly.\n    ...\n    ```\n\n## Usage\n* Installation\n  ```\n  $ git clone https://github.com/rintr-suzuki/WIN2PhaseNet.git\n  $ cd WIN2PhaseNet\n  ```\n\n* Execution\n  ### WIN2PhaseNet\n  ```\n  $ ./WIN2PhaseNet.bash -m cont --tbl2lst\n  # See 'out' directory for the result.\n  ```\n\n  ### PhaseNet prediction\n  ```\n  $ ./PhaseNet.bash --model_dir=src/PhaseNet/model/190703-214543 --data_dir=out/npz --data_list=out/npz.csv --amplitude --plot_figure\n  # See 'results' directory for the result.\n  ```\n\n* See following documents for the detailed information. <br>\n  For PhaseNet prediction, see [here](docs/README-prediction.md). <br>\n  For PhaseNet training, see [here](docs/README-training.md). <br>\n\n* See [here](docs/Tips.md) for the tips of this tool.\n\n## Acknowledgements\nA part of this program was created by Uchida, N and Matsuzawa, T.\n\n## References\n* Maeda, T (2019), Development of a WIN/WIN32 format seismic waveform data reader. The 2019 SSJ Fall Meeting. (In Japanese)\n* Saito, M (1978), An automatic design algorithm for band selective recursive digital filters, Geophysical exploration, 31, 240-263. (In Japanese)\n* Takagi, R., Uchida, N., Nakayama, T., Azuma, R., Ishigami, A., Okada, T., Nakamura, T., & Shiomi, K. (2019), Estimation of the orientations of the S-net cabled ocean-bottom sensors. Seismological Research Letters, 90(6), 2175–2187. https://doi.org/10.1785/0220190093\n* Zhu, W., & Beroza, G. C. (2019), PhaseNet: A deep-neural-network-based seismic arrival-time picking method. Geophysical Journal International, 216(1), 261–273. https://doi.org/10.1093/gji/ggy423\n",
        "createdAt": "2023-12-26T15:28:39.000Z",
        "updatedAt": "2025-07-22T07:11:28.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/rintr-suzuki/WIN2PhaseNet/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "dothaogiang/dothaogiang-Observational-Seismology",
        "url": "https://github.com/dothaogiang/dothaogiang-Observational-Seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "��#\u0000 \u0000I\u0000n\u0000t\u0000r\u0000o\u0000d\u0000u\u0000c\u0000t\u0000i\u0000o\u0000n\u0000 \u0000t\u0000o\u0000 \u0000O\u0000b\u0000s\u0000e\u0000r\u0000v\u0000a\u0000t\u0000i\u0000o\u0000n\u0000a\u0000l\u0000 \u0000S\u0000e\u0000i\u0000s\u0000m\u0000o\u0000l\u0000o\u0000g\u0000y\u0000 \u0000W\u0000o\u0000r\u0000k\u0000s\u0000h\u0000o\u0000p\u0000\r\u0000\n\u0000\r\u0000\n\u0000*\u0000*\u0000=��� \u0000T\u0000h\u0000�\u001ei\u0000 \u0000g\u0000i\u0000a\u0000n\u0000:\u0000*\u0000*\u0000 \u0000A\u0000p\u0000r\u0000i\u0000l\u0000 \u00002\u00001\u0000\u0013 2\u00005\u0000,\u0000 \u00002\u00000\u00002\u00005\u0000 \u0000 \u0000\r\u0000\n\u0000*\u0000*\u0000=��� \u0000\u0010\u0001�\u001ea\u0000 \u0000\u0011\u0001i\u0000�\u001em\u0000:\u0000*\u0000*\u0000 \u0000V\u0000N\u0000U\u0000 \u0000H\u0000a\u0000n\u0000o\u0000i\u0000 \u0000U\u0000n\u0000i\u0000v\u0000e\u0000r\u0000s\u0000i\u0000t\u0000y\u0000 \u0000o\u0000f\u0000 \u0000S\u0000c\u0000i\u0000e\u0000n\u0000c\u0000e\u0000,\u0000 \u00003\u00003\u00004\u0000 \u0000N\u0000g\u0000u\u0000y\u0000�\u001en\u0000 \u0000T\u0000r\u0000�\u0000i\u0000,\u0000 \u0000T\u0000h\u0000a\u0000n\u0000h\u0000 \u0000X\u0000u\u0000�\u0000n\u0000,\u0000 \u0000H\u0000�\u0000 \u0000N\u0000�\u001ei\u0000 \u0000 \u0000\r\u0000\n\u0000*\u0000*\u0000=�h�\r <��� \u0000G\u0000i\u0000�\u001en\u0000g\u0000 \u0000v\u0000i\u0000�\u0000n\u0000 \u0000c\u0000h\u0000�\u0000n\u0000h\u0000:\u0000*\u0000*\u0000 \u0000D\u0000r\u0000.\u0000 \u0000P\u0000h\u0000�\u001em\u0000 \u0000T\u0000h\u0000�\u0000n\u0000h\u0000 \u0000S\u0000�\u0001n\u0000 \u0000(\u0000A\u0000N\u0000U\u0000)\u0000\r\u0000\n\u0000\r\u0000\n\u0000-\u0000-\u0000-\u0000\r\u0000\n\u0000\r\u0000\n\u0000#\u0000#\u0000 \u0000<د� \u0000W\u0000o\u0000r\u0000k\u0000s\u0000h\u0000o\u0000p\u0000 \u0000D\u0000e\u0000s\u0000c\u0000r\u0000i\u0000p\u0000t\u0000i\u0000o\u0000n\u0000\r\u0000\n\u0000\r\u0000\n\u0000K\u0000h\u0000�\u0000a\u0000 \u0000h\u0000�\u001ec\u0000 \u0000k\u0000�\u0000o\u0000 \u0000d\u0000�\u0000i\u0000 \u0000m\u0000�\u001et\u0000 \u0000t\u0000u\u0000�\u001en\u0000 \u0000n\u0000h\u0000�\u001em\u0000 \u0000c\u0000u\u0000n\u0000g\u0000 \u0000c\u0000�\u001ep\u0000 \u0000k\u0000i\u0000�\u001en\u0000 \u0000t\u0000h\u0000�\u001ec\u0000 \u0000c\u0000�\u0001 \u0000b\u0000�\u001en\u0000 \u0000v\u0000�\u0000 \u0000t\u0000h\u0000�\u001ec\u0000 \u0000h\u0000�\u0000n\u0000h\u0000 \u0000v\u0000�\u001e \u0000\u0011\u0001�\u001ea\u0000 \u0000c\u0000h\u0000�\u001en\u0000 \u0000h\u0000�\u001ec\u0000 \u0000q\u0000u\u0000a\u0000n\u0000 \u0000s\u0000�\u0000t\u0000 \u0000(\u0000O\u0000b\u0000s\u0000e\u0000r\u0000v\u0000a\u0000t\u0000i\u0000o\u0000n\u0000a\u0000l\u0000 \u0000S\u0000e\u0000i\u0000s\u0000m\u0000o\u0000l\u0000o\u0000g\u0000y\u0000)\u0000.\u0000 \u0000N\u0000g\u0000�\u0001�\u001ei\u0000 \u0000h\u0000�\u001ec\u0000 \u0000s\u0000�\u001e \u0000\u0011\u0001�\u0001�\u001ec\u0000 \u0000h\u0000�\u0001�\u001en\u0000g\u0000 \u0000d\u0000�\u001en\u0000 \u0000s\u0000�\u001e \u0000d\u0000�\u001en\u0000g\u0000 \u0000c\u0000�\u0000c\u0000 \u0000c\u0000�\u0000n\u0000g\u0000 \u0000c\u0000�\u001e \u0000x\u0000�\u001e \u0000l\u0000�\u0000 \u0000d\u0000�\u001e \u0000l\u0000i\u0000�\u001eu\u0000 \u0000\u0011\u0001�\u001ea\u0000 \u0000c\u0000h\u0000�\u001en\u0000 \u0000h\u0000i\u0000�\u001en\u0000 \u0000\u0011\u0001�\u001ei\u0000,\u0000 \u0000l\u0000�\u001ep\u0000 \u0000b\u0000�\u001en\u0000 \u0000\u0011\u0001�\u001e \u0000k\u0000h\u0000o\u0000a\u0000 \u0000h\u0000�\u001ec\u0000,\u0000 \u0000v\u0000�\u0000 \u0000t\u0000i\u0000�\u001ep\u0000 \u0000c\u0000�\u001en\u0000 \u0000s\u0000�\u0001 \u0000b\u0000�\u001e \u0000v\u0000�\u001ei\u0000 \u0000h\u0000�\u001ec\u0000 \u0000m\u0000�\u0000y\u0000 \u0000t\u0000r\u0000o\u0000n\u0000g\u0000 \u0000\u0011\u0001�\u001ea\u0000 \u0000c\u0000h\u0000�\u001en\u0000.\u0000\r\u0000\n\u0000\r\u0000\n\u0000-\u0000-\u0000-\u0000\r\u0000\n\u0000\r\u0000\n\u0000#\u0000#\u0000 \u0000>��� \u0000A\u0000i\u0000m\u0000s\u0000\r\u0000\n\u0000\r\u0000\n\u0000-\u0000 \u0000G\u0000i\u0000�\u001ei\u0000 \u0000t\u0000h\u0000i\u0000�\u001eu\u0000 \u0000\u0011\u0001�\u001ea\u0000 \u0000c\u0000h\u0000�\u001en\u0000 \u0000h\u0000�\u001ec\u0000 \u0000n\u0000h\u0000�\u0001 \u0000m\u0000�\u001et\u0000 \u0000k\u0000h\u0000o\u0000a\u0000 \u0000h\u0000�\u001ec\u0000 \u0000t\u0000�\u0000n\u0000h\u0000 \u0000t\u0000o\u0000�\u0000n\u0000 \u0000v\u0000�\u001ei\u0000 \u0000d\u0000�\u001e \u0000l\u0000i\u0000�\u001eu\u0000 \u0000\u0011\u0001�\u001ea\u0000 \u0000c\u0000h\u0000�\u001en\u0000 \u0000t\u0000h\u0000�\u001e \u0000\u0011\u0001�\u001en\u0000g\u0000.\u0000\r\u0000\n\u0000-\u0000 \u0000L\u0000�\u0000m\u0000 \u0000q\u0000u\u0000e\u0000n\u0000 \u0000v\u0000�\u001ei\u0000 \u0000c\u0000�\u0000c\u0000 \u0000c\u0000�\u0000n\u0000g\u0000 \u0000c\u0000�\u001e:\u0000 \u0000P\u0000y\u0000t\u0000h\u0000o\u0000n\u0000,\u0000 \u0000J\u0000u\u0000p\u0000y\u0000t\u0000e\u0000r\u0000 \u0000N\u0000o\u0000t\u0000e\u0000b\u0000o\u0000o\u0000k\u0000,\u0000 \u0000G\u0000o\u0000o\u0000g\u0000l\u0000e\u0000 \u0000C\u0000o\u0000l\u0000a\u0000b\u0000,\u0000 \u0000O\u0000b\u0000s\u0000P\u0000y\u0000,\u0000 \u0000S\u0000e\u0000i\u0000s\u0000B\u0000e\u0000n\u0000c\u0000h\u0000.\u0000\r\u0000\n\u0000-\u0000 \u0000T\u0000h\u0000�\u001ec\u0000 \u0000h\u0000�\u0000n\u0000h\u0000 \u0000x\u0000�\u001e \u0000l\u0000�\u0000 \u0000d\u0000�\u001e \u0000l\u0000i\u0000�\u001eu\u0000 \u0000\u0011\u0001�\u001ea\u0000 \u0000c\u0000h\u0000�\u001en\u0000,\u0000 \u0000m\u0000�\u0000 \u0000h\u0000�\u0000n\u0000h\u0000 \u0000n\u0000g\u0000h\u0000�\u001ec\u0000h\u0000 \u0000\u0011\u0001�\u001eo\u0000 \u0000v\u0000�\u0000 \u0000h\u0000�\u001ec\u0000 \u0000m\u0000�\u0000y\u0000.\u0000\r\u0000\n\u0000\r\u0000\n\u0000-\u0000-\u0000-\u0000\r\u0000\n\u0000\r\u0000\n\u0000#\u0000#\u0000 \u0000=��� \u0000F\u0000o\u0000l\u0000d\u0000e\u0000r\u0000 \u0000S\u0000t\u0000r\u0000u\u0000c\u0000t\u0000u\u0000r\u0000e\u0000\r\u0000\n\u0000\r\u0000\n\u0000`\u0000`\u0000`\u0000\r\u0000\n\u0000O\u0000b\u0000s\u0000e\u0000r\u0000v\u0000a\u0000t\u0000i\u0000o\u0000n\u0000a\u0000l\u0000 \u0000S\u0000e\u0000i\u0000s\u0000m\u0000o\u0000l\u0000o\u0000g\u0000y\u0000/\u0000\r\u0000\n\u0000\u0002%\r\u0000\n\u0000\u001c%\u0000%\u0000% \u0000D\u0000a\u0000y\u0000 \u00001\u0000/\u0000\r\u0000\n\u0000\u0002% \u0000\u001c%\u0000%\u0000% \u0000D\u00001\u0000_\u0000L\u0000a\u0000b\u0000.\u0000i\u0000p\u0000y\u0000n\u0000b\u0000\r\u0000\n\u0000\u0002% \u0000\u0014%\u0000%\u0000% \u0000D\u00001\u0000_\u0000P\u0000r\u0000a\u0000c\u0000.\u0000i\u0000p\u0000y\u0000n\u0000b\u0000\r\u0000\n\u0000\u0002%\r\u0000\n\u0000\u001c%\u0000%\u0000% \u0000D\u0000a\u0000y\u0000 \u00002\u0000/\u0000\r\u0000\n\u0000\u0002% \u0000\u001c%\u0000%\u0000% \u0000D\u00002\u0000_\u0000L\u0000a\u0000b\u0000.\u0000i\u0000p\u0000y\u0000n\u0000b\u0000\r\u0000\n\u0000\u0002% \u0000\u001c%\u0000%\u0000% \u0000D\u00002\u0000_\u0000P\u0000r\u0000a\u0000c\u0000.\u0000i\u0000p\u0000y\u0000n\u0000b\u0000\r\u0000\n\u0000\u0002% \u0000\u001c%\u0000%\u0000% \u0000s\u0000t\u0000a\u0000t\u0000i\u0000o\u0000n\u0000s\u0000/\u0000\r\u0000\n\u0000\u0002% \u0000\u0014%\u0000%\u0000% \u0000w\u0000a\u0000v\u0000e\u0000f\u0000o\u0000r\u0000m\u0000s\u0000/\u0000\r\u0000\n\u0000\u0002%\r\u0000\n\u0000\u001c%\u0000%\u0000% \u0000D\u0000a\u0000y\u0000 \u00003\u0000/\u0000\r\u0000\n\u0000\u0002% \u0000\u001c%\u0000%\u0000% \u0000D\u00003\u0000_\u0000L\u0000a\u0000b\u0000.\u0000i\u0000p\u0000y\u0000n\u0000b\u0000\r\u0000\n\u0000\u0002% \u0000\u001c%\u0000%\u0000% \u0000D\u00003\u0000_\u0000P\u0000r\u0000a\u0000c\u0000.\u0000i\u0000p\u0000y\u0000n\u0000b\u0000\r\u0000\n\u0000\u0002% \u0000\u0014%\u0000%\u0000% \u0000G\u0000r\u0000a\u0000d\u0000e\u0000A\u0000_\u0000e\u0000v\u0000e\u0000n\u0000t\u0000s\u0000.\u0000c\u0000s\u0000v\u0000\r\u0000\n\u0000\u0002%\r\u0000\n\u0000\u001c%\u0000%\u0000% \u0000D\u0000a\u0000y\u0000 \u00004\u0000/\u0000\r\u0000\n\u0000\u0002% \u0000\u001c%\u0000%\u0000% \u0000D\u00004\u0000_\u0000L\u0000a\u0000b\u0000.\u0000i\u0000p\u0000y\u0000n\u0000b\u0000\r\u0000\n\u0000\u0002% \u0000\u001c%\u0000%\u0000% \u0000D\u00004\u0000_\u0000P\u0000r\u0000a\u0000c\u0000.\u0000i\u0000p\u0000y\u0000n\u0000b\u0000\r\u0000\n\u0000\u0002% \u0000\u001c%\u0000%\u0000% \u0000X\u0000C\u0000.\u00002\u00000\u00001\u00007\u00000\u00001\u00002\u00002\u0000.\u0000p\u0000k\u0000l\u0000\r\u0000\n\u0000\u0002%\r\u0000\n\u0000\u001c%\u0000%\u0000% \u0000D\u0000a\u0000y\u0000 \u00005\u0000/\u0000\r\u0000\n\u0000\u0002% \u0000\u0014%\u0000%\u0000% \u0000D\u00005\u0000_\u0000L\u0000a\u0000b\u0000.\u0000i\u0000p\u0000y\u0000n\u0000b\u0000\r\u0000\n\u0000\u0002%\r\u0000\n\u0000\u001c%\u0000%\u0000% \u0000R\u0000E\u0000A\u0000D\u0000M\u0000E\u0000.\u0000m\u0000d\u0000\r\u0000\n\u0000\u0014%\u0000%\u0000% \u0000T\u0000C\u0000T\u0000_\u0000T\u0000H\u0000�\u0000N\u0000G\u0000 \u0000B\u0000�\u0000O\u0000 \u0000S\u0000I\u0000N\u0000H\u0000 \u0000V\u0000I\u0000�\u0000N\u0000.\u0000p\u0000d\u0000f\u0000\r\u0000\n\u0000`\u0000`\u0000`\u0000\r\u0000\n\u0000\r\u0000\n\u0000-\u0000-\u0000-\u0000\r\u0000\n\u0000\r\u0000\n\u0000#\u0000#\u0000 \u0000=��� \u0000M\u0000o\u0000d\u0000u\u0000l\u0000e\u0000s\u0000 \u0000O\u0000v\u0000e\u0000r\u0000v\u0000i\u0000e\u0000w\u0000\r\u0000\n\u0000\r\u0000\n\u0000#\u0000#\u0000#\u0000 \u0000=��� \u0000M\u0000o\u0000d\u0000u\u0000l\u0000e\u0000 \u00001\u0000:\u0000 \u0000M\u0000a\u0000p\u0000p\u0000i\u0000n\u0000g\u0000 \u0000a\u0000n\u0000d\u0000 \u0000I\u0000n\u0000t\u0000r\u0000o\u0000d\u0000u\u0000c\u0000t\u0000i\u0000o\u0000n\u0000\r\u0000\n\u0000\r\u0000\n\u0000-\u0000 \u0000S\u0000e\u0000i\u0000s\u0000m\u0000o\u0000g\u0000r\u0000a\u0000p\u0000h\u0000 \u0000&\u0000 \u0000S\u0000e\u0000i\u0000s\u0000m\u0000i\u0000c\u0000i\u0000t\u0000y\u0000 \u0000i\u0000n\u0000 \u0000V\u0000i\u0000e\u0000t\u0000n\u0000a\u0000m\u0000\r\u0000\n\u0000-\u0000 \u0000A\u0000n\u0000t\u0000a\u0000r\u0000c\u0000t\u0000i\u0000c\u0000 \u0000s\u0000t\u0000a\u0000t\u0000i\u0000o\u0000n\u0000s\u0000 \u0000v\u0000i\u0000s\u0000u\u0000a\u0000l\u0000i\u0000z\u0000a\u0000t\u0000i\u0000o\u0000n\u0000\r\u0000\n\u0000\r\u0000\n\u0000#\u0000#\u0000#\u0000 \u0000=��� \u0000M\u0000o\u0000d\u0000u\u0000l\u0000e\u0000 \u00002\u0000:\u0000 \u0000R\u0000a\u0000y\u0000 \u0000T\u0000h\u0000e\u0000o\u0000r\u0000y\u0000 \u0000&\u0000 \u0000S\u0000e\u0000i\u0000s\u0000m\u0000o\u0000m\u0000e\u0000t\u0000r\u0000y\u0000\r\u0000\n\u0000\r\u0000\n\u0000-\u0000 \u0000T\u0000r\u0000a\u0000v\u0000e\u0000l\u0000 \u0000t\u0000i\u0000m\u0000e\u0000 \u0000m\u0000o\u0000d\u0000e\u0000l\u0000i\u0000n\u0000g\u0000\r\u0000\n\u0000-\u0000 \u0000T\u0000r\u0000i\u0000a\u0000n\u0000g\u0000u\u0000l\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000e\u0000x\u0000e\u0000r\u0000c\u0000i\u0000s\u0000e\u0000 \u0000(\u0000K\u0000o\u0000n\u0000 \u0000T\u0000u\u0000m\u0000 \u0000M\u00005\u0000.\u00002\u0000)\u0000\r\u0000\n\u0000\r\u0000\n\u0000#\u0000#\u0000#\u0000 \u0000=��� \u0000M\u0000o\u0000d\u0000u\u0000l\u0000e\u0000 \u00003\u0000:\u0000 \u0000I\u0000n\u0000v\u0000e\u0000r\u0000s\u0000e\u0000 \u0000P\u0000r\u0000o\u0000b\u0000l\u0000e\u0000m\u0000s\u0000\r\u0000\n\u0000\r\u0000\n\u0000-\u0000 \u0000L\u0000i\u0000n\u0000e\u0000a\u0000r\u0000 \u0000R\u0000e\u0000g\u0000r\u0000e\u0000s\u0000s\u0000i\u0000o\u0000n\u0000\r\u0000\n\u0000-\u0000 \u0000E\u0000a\u0000r\u0000t\u0000h\u0000q\u0000u\u0000a\u0000k\u0000e\u0000 \u0000L\u0000o\u0000c\u0000a\u0000t\u0000i\u0000o\u0000n\u0000\r\u0000\n\u0000-\u0000 \u0000M\u0000o\u0000m\u0000e\u0000n\u0000t\u0000 \u0000T\u0000e\u0000n\u0000s\u0000o\u0000r\u0000 \u0000I\u0000n\u0000v\u0000e\u0000r\u0000s\u0000i\u0000o\u0000n\u0000\r\u0000\n\u0000\r\u0000\n\u0000#\u0000#\u0000#\u0000 \u0000=��� \u0000M\u0000o\u0000d\u0000u\u0000l\u0000e\u0000 \u00004\u0000:\u0000 \u0000I\u0000n\u0000t\u0000e\u0000r\u0000f\u0000e\u0000r\u0000o\u0000m\u0000e\u0000t\u0000r\u0000y\u0000\r\u0000\n\u0000\r\u0000\n\u0000-\u0000 \u0000A\u0000u\u0000t\u0000o\u0000c\u0000o\u0000r\u0000r\u0000e\u0000l\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000o\u0000f\u0000 \u0000P\u0000-\u0000w\u0000a\u0000v\u0000e\u0000 \u0000c\u0000o\u0000d\u0000a\u0000\r\u0000\n\u0000-\u0000 \u0000G\u0000l\u0000o\u0000b\u0000a\u0000l\u0000 \u0000c\u0000o\u0000r\u0000r\u0000e\u0000l\u0000o\u0000g\u0000r\u0000a\u0000m\u0000\r\u0000\n\u0000\r\u0000\n\u0000#\u0000#\u0000#\u0000 \u0000=��� \u0000M\u0000o\u0000d\u0000u\u0000l\u0000e\u0000 \u00005\u0000:\u0000 \u0000M\u0000a\u0000c\u0000h\u0000i\u0000n\u0000e\u0000 \u0000L\u0000e\u0000a\u0000r\u0000n\u0000i\u0000n\u0000g\u0000 \u0000i\u0000n\u0000 \u0000S\u0000e\u0000i\u0000s\u0000m\u0000o\u0000l\u0000o\u0000g\u0000y\u0000\r\u0000\n\u0000\r\u0000\n\u0000-\u0000 \u0000C\u0000N\u0000N\u0000 \u0000f\u0000o\u0000r\u0000 \u0000P\u0000K\u0000I\u0000K\u0000P\u0000 \u0000P\u0000h\u0000a\u0000s\u0000e\u0000 \u0000P\u0000i\u0000c\u0000k\u0000i\u0000n\u0000g\u0000\r\u0000\n\u0000-\u0000 \u0000S\u0000e\u0000i\u0000s\u0000B\u0000e\u0000n\u0000c\u0000h\u0000 \u0000I\u0000n\u0000t\u0000r\u0000o\u0000d\u0000u\u0000c\u0000t\u0000i\u0000o\u0000n\u0000\r\u0000\n\u0000-\u0000 \u0000B\u0000o\u0000n\u0000u\u0000s\u0000:\u0000 \u0000M\u0000i\u0000c\u0000r\u0000o\u0000 \u0000i\u0000c\u0000e\u0000q\u0000u\u0000a\u0000k\u0000e\u0000 \u0000p\u0000i\u0000c\u0000k\u0000e\u0000r\u0000\r\u0000\n\u0000\r\u0000\n\u0000-\u0000-\u0000-\u0000\r\u0000\n\u0000\r\u0000\n\u0000#\u0000#\u0000 \u0000=�h�\r =�,� \u0000A\u0000b\u0000o\u0000u\u0000t\u0000 \u0000t\u0000h\u0000e\u0000 \u0000F\u0000a\u0000c\u0000i\u0000l\u0000i\u0000t\u0000a\u0000t\u0000o\u0000r\u0000\r\u0000\n\u0000\r\u0000\n\u0000*\u0000*\u0000D\u0000r\u0000.\u0000 \u0000P\u0000h\u0000�\u001em\u0000 \u0000T\u0000h\u0000�\u0000n\u0000h\u0000 \u0000S\u0000�\u0001n\u0000*\u0000*\u0000 \u0000l\u0000�\u0000 \u0000n\u0000h\u0000�\u0000 \u0000\u0011\u0001�\u001ea\u0000 \u0000c\u0000h\u0000�\u001en\u0000 \u0000h\u0000�\u001ec\u0000 \u0000c\u0000h\u0000u\u0000y\u0000�\u0000n\u0000 \u0000n\u0000g\u0000h\u0000i\u0000�\u0000n\u0000 \u0000c\u0000�\u001eu\u0000 \u0000c\u0000�\u001eu\u0000 \u0000t\u0000r\u0000�\u0000c\u0000 \u0000b\u0000�\u0000n\u0000 \u0000t\u0000r\u0000o\u0000n\u0000g\u0000 \u0000T\u0000r\u0000�\u0000i\u0000 \u0000\u0010\u0001�\u001et\u0000 \u0000b\u0000�\u001en\u0000g\u0000 \u0000s\u0000�\u0000n\u0000g\u0000 \u0000\u0011\u0001�\u001ea\u0000 \u0000c\u0000h\u0000�\u001en\u0000,\u0000 \u0000v\u0000�\u001ei\u0000 \u0000c\u0000�\u0000c\u0000 \u0000c\u0000�\u0000n\u0000g\u0000 \u0000c\u0000�\u001e \u0000t\u0000o\u0000�\u0000n\u0000 \u0000h\u0000�\u001ec\u0000 \u0000h\u0000i\u0000�\u001en\u0000 \u0000\u0011\u0001�\u001ei\u0000 \u0000n\u0000h\u0000�\u0001 \u0000x\u0000�\u001e \u0000l\u0000�\u0000 \u0000t\u0000�\u0000n\u0000 \u0000h\u0000i\u0000�\u001eu\u0000,\u0000 \u0000m\u0000�\u0000 \u0000h\u0000�\u0000n\u0000h\u0000 \u0000s\u0000�\u001e \u0000v\u0000�\u0000 \u0000s\u0000u\u0000y\u0000 \u0000l\u0000u\u0000�\u001en\u0000 \u0000\u0011\u0001�\u001ea\u0000 \u0000v\u0000�\u001et\u0000 \u0000l\u0000�\u0000.\u0000 \u0000N\u0000g\u0000h\u0000i\u0000�\u0000n\u0000 \u0000c\u0000�\u001eu\u0000 \u0000g\u0000�\u001en\u0000 \u0000\u0011\u0001�\u0000y\u0000 \u0000t\u0000�\u001ep\u0000 \u0000t\u0000r\u0000u\u0000n\u0000g\u0000 \u0000v\u0000�\u0000o\u0000 \u0000c\u0000�\u0000c\u0000 \u0000t\u0000�\u001en\u0000g\u0000 \u0000b\u0000\u0003\u0001n\u0000g\u0000 \u0000�\u001e \u0000h\u0000a\u0000i\u0000 \u0000c\u0000�\u001ec\u0000 \u0000t\u0000r\u0000o\u0000n\u0000g\u0000 \u0000b\u0000�\u001ei\u0000 \u0000c\u0000�\u001en\u0000h\u0000 \u0000b\u0000i\u0000�\u001en\u0000 \u0000\u0011\u0001�\u001ei\u0000 \u0000k\u0000h\u0000�\u0000 \u0000h\u0000�\u001eu\u0000.\u0000\r\u0000\n\u0000\r\u0000\n\u0000-\u0000-\u0000-\u0000\r\u0000\n\u0000",
        "createdAt": "2025-06-12T15:48:39.000Z",
        "updatedAt": "2025-06-12T15:59:10.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/dothaogiang/dothaogiang-Observational-Seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "johnowusuduah/K-NET-LSTM-PGA",
        "url": "https://github.com/johnowusuduah/K-NET-LSTM-PGA",
        "description": "Code for Seismological Research Letters Submission SRL-D-23-00427",
        "stars": 0,
        "forks": 0,
        "readme": "# K-NET-LSTM-PGA (Full title ommitted to forestall erroneous plagiarism detection by crawlers)\nCode for Seismological Research Letters Submission SRL-D-23-00427\n\n## Data\nData is sourced from the Japan National Research Institute for Earth Science and Disaster Resilience (NIED) website. Download the data from this website - https://www.kyoshin.bosai.go.jp/. Specifically we utilize Kyoshin Network (K-NET), which constitutes a nationwide network of strong-motion seismographs, comprising approximately 1,000 observation stations that are uniformly distributed across Japan at 20-kilometer intervals. Our data set used the version of the data accessed on May 14, 2023. In compressed format, the size of the data set is 2.8GB.\n\n**The path of the downloaded data set is required for data preprocessing, training and evaluation.**\n\n\n## Instructions\n### Phase Picking\nPhaseNet was employed to pick the arrival or P ans S-waves with code specified in this repository: https://github.com/AI4EPS/PhaseNet\n\n### Data Wrangling\nThe script used to wrangle seismograms is titled `preprocess.py`. Script includes detailed instructions of defining path to data in comments.\n\n### Training and Evaluation\nThe script used to train and evaluate experiments is titled `train_eval.py`. Here too, the script includes detailed instructions in comments.\n",
        "createdAt": "2024-01-16T21:50:46.000Z",
        "updatedAt": "2024-01-16T21:55:54.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/johnowusuduah/K-NET-LSTM-PGA/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "lwsspy/lwsspy.seismo",
        "url": "https://github.com/lwsspy/lwsspy.seismo",
        "description": "Seismology functions",
        "stars": 1,
        "forks": 0,
        "readme": "# Seismology part of LWSSPY \n\n---\n\n## Installation\n\n### Installation using Pip\n\nHowever, \n\n```bash\ngit clone git@github.com:lsawade/lwsspy.seismo.git\ncd lwsspy.seismo\npip install .\n```\n\nShould work. Use `pip install -e .` for development mode.\n",
        "createdAt": "2021-10-05T00:53:39.000Z",
        "updatedAt": "2021-12-22T19:15:20.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/lwsspy/lwsspy.seismo/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "heinerigel/SPIN-SC1",
        "url": "https://github.com/heinerigel/SPIN-SC1",
        "description": "SPIN Short Course 1 - Computational Seismology",
        "stars": 3,
        "forks": 4,
        "readme": "# SPIN-SC1\nSPIN Short Course 1 - Computational Seismology\n\nJupyter notebooks and slides or the SPIN Short Course in Computational Seismology\n\nYou can run all the Jupyter notebooks here by clicking on the *mybinder* link below:\n\nThe preparation of the notebooks on mybinder may take up to a few minutes. \n\nIf there are problems with the notebooks please get in touch. \n\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/heinerigel/SPIN-SC1/main)\n",
        "createdAt": "2022-03-29T13:43:03.000Z",
        "updatedAt": "2023-10-09T08:51:06.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/heinerigel/SPIN-SC1/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "PIC-IRIS/PH5",
        "url": "https://github.com/PIC-IRIS/PH5",
        "description": "Library of PH5 clients, apis, and utilities",
        "stars": 16,
        "forks": 8,
        "readme": "# PH5 \n\n[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT) [![DOI](https://zenodo.org/badge/66882151.svg)](https://zenodo.org/badge/latestdoi/66882151)\n\n\nPH5 is the [IRIS PASSCAL Instrument Center](https://www.passcal.nmt.edu/) recommended archival format for active, passive, and mixed source seismological data sets. This package contains command line utilities and APIs for building and interacting with PH5 datasets.\n\nTo get started using PH5 see the page on [Creating, Validating and Archiving PH5 Experiments](https://github.com/PIC-IRIS/PH5/wiki/PH5-Creating-Validating-and-Archiving).\n\n[**Read more on the PH5 Wiki**](https://github.com/PIC-IRIS/PH5/wiki)\n",
        "createdAt": "2016-08-29T21:29:43.000Z",
        "updatedAt": "2025-10-06T09:00:47.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/PIC-IRIS/PH5/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "RongjiangWang/QSSPPEGS_2020",
        "url": "https://github.com/RongjiangWang/QSSPPEGS_2020",
        "description": "A variant of FORTRAN code QSSP for calculating prompt elasto-gravity signals (PEGS) of an earthquake in a spherically symmetric and elastic earth",
        "stars": 3,
        "forks": 0,
        "readme": "A variant of FORTRAN code QSSP for calculating prompt elasto-gravity signals (PEGS) of an earthquake in a spherically symmetric and elastic earth.\n\nFor Windows user, the executable file is provided under folder \"WindowsEXE\". Linux user may compile the source codes with \"gfortran\" via a single command like, e.g.,\n\n~>cd .../SourceCode\n\n~>gfortran -o qssppegs *.f -O3\n\nto get the excutable code qssppegs.\n\nAfter start the executable code, the program ask for an input file in the ASCII format. An example input file is provided under folder \"InputFile\". You may change the input data included in this file for your own applications.\n\nReferences\n\nWang, R., and H. Wang (2007), A fast converging and anti-aliasing algorithm for Green’s functions in terms of spherical or cylindrical harmonics, Geophysical Journal International, doi: 10.1111/j.1365-246X.2007.03385.x.\n\nWang, R., S. Heimann, Y. Zhang, H. Wang, and T. Dahm (2017). Complete synthetic seismograms based on a spherical self-gravitating Earth model with an atmos-phere-ocean-mantle-core structure. Geophysical Journal International, doi: 10.1093/gji/ggx259.\n\nZhang, S., R. Wang, T. Dahm, S. Zhou, and S. Heimann (2020). Prompt elasto-gravity signals (PEGS) and their potential use in modern seismology. Earth and Planetary Science Letters, 36: 116150. doi:10.1016/j.epsl.2020.116150.\n\nZhang, S., R. Wang and X. Chen (2023). Seismic prompt gravity strain signals in a layered spherical Earth. Earthquake Science 36.  https://doi.org/10.1016/j.eqs.2023.09.002.\n",
        "createdAt": "2025-04-11T08:08:56.000Z",
        "updatedAt": "2025-07-15T14:02:42.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/RongjiangWang/QSSPPEGS_2020/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "OceeDiTT/2025-Seismology-Skill-Building-Workshop",
        "url": "https://github.com/OceeDiTT/2025-Seismology-Skill-Building-Workshop",
        "description": "This repository documents the skills and knowledge gained from the 2025 Seismology Skill Building Workshop. It also includes the scripts, notebooks and figures developed during the program.",
        "stars": 1,
        "forks": 0,
        "readme": "# 2025 Seismology Skill Building Workshop – Learning Report\n\n## Overview\nThis repository documents the skills and knowledge gained from the 2025 Seismology Skill Building Workshop. It also includes the scripts, notebooks and figures developed during the program. The program focused on hands-on training in scientific computing, data analysis, and seismological applications, equipping participants with both technical and research-oriented skills.\n\n## Key Learning Areas\n### Scientific Computing Foundations\n- Introduction to **Linux** and **command-line tools** for workflow management.  \n- Use of **GMT (Generic Mapping Tools)** to create maps, plots, and visualize seismic wave travel times.  \n- Application of scripting for reproducible scientific tasks.  \n\n### Seismic Data Analysis\n- Processing and analyzing seismograms with **SAC, Python, and ObsPy**.  \n- Signal filtering, spectrogram generation, and instrument response removal.  \n- Accessing and utilizing seismic datasets from **IRIS** and other SAGE data services.  \n- Documenting workflows and visualizations in **Jupyter Notebooks**.  \n\n### Advanced Topics & Professional Skills\n- Introduction to **GNSS time series analysis** for ground deformation monitoring.  \n- Integration of seismic and geodetic datasets for comprehensive Earth system studies.  \n- Development of good research practices: reproducibility, data literacy, and critical analysis.  \n\n## Summary\nThe workshop provided an end-to-end learning experience in modern seismology. I built a solid computing foundation, gained proficiency in key seismological software tools, and practiced integrating seismic and geodetic data for scientific interpretation. The skills acquired are directly applicable to academic research, graduate studies, and professional careers in the geosciences.  \n\n",
        "createdAt": "2025-08-31T01:00:09.000Z",
        "updatedAt": "2025-09-09T08:16:27.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/OceeDiTT/2025-Seismology-Skill-Building-Workshop/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "shaharkadmiel/CTG_Course",
        "url": "https://github.com/shaharkadmiel/CTG_Course",
        "description": "Introduction to seismo-acoustic waves in the Earth’s spheres - TU Delft",
        "stars": 4,
        "forks": 2,
        "readme": "# Introduction to seismo-acoustic waves in the Earth’s spheres, CEG, TU Delft\nCourse by Läslo Evers, Shahar Shani-Kadmiel, Pieter Smets\n\n## When and where\n\n- **Date:** 6, 7 & 8 March 2019\n- **Time:** 13.30-17.30hr\n- **Place:** Faculty of Civil Engineering & Geosciences (building 23)\n- **6 March (Wednesday):** room 02.110\n- **7 March (Thursday):** room 2.62\n- **8 March (Friday):** room 03.270\n\n# [Download the notebooks](http://tinyurl.com/y4aehjc5) http://tinyurl.com/y4aehjc5\n\n## Geting ready\n\nThe majority of the course will be interactive with a lot of hands-on\ndemonstrations and exercises purposed to enrich the learning experience.\nWe will do this by using [Jupyter Notebooks](https://jupyter.org/), which\nyou should install (or already have) on your laptops that you bring to the course.\n\nWe have prepared some instructions for setting up your environment so that you come\nprepared and so that we don’t spend precious time on these preparations during the course.\nPlease follow the instructions at your earliest convenience and send us an email if you run\ninto any difficulties that you are not able to solve yourself. These instructions work\non Linux and macOS but should work for Windows machines just as well.\n\n---\n**Step 1:**\n\nIf you do not already have Anaconda or Miniconda installed on your machine (Hint: If you are unsure, go ahead and install a fresh copy), follow this link to download Miniconda and install the right package for your OS: \n\nhttps://conda.io/en/latest/miniconda.html\n\nWe recommend the 64-bit Python 3.7 version regardless of your OS.\n\nFollow the Installation instructions (https://conda.io/projects/conda/en/latest/user-guide/install/index.html).\n\n---\n**Step 2:**\n\nWhether you just installed Miniconda or are using an already existing conda environment, create a new environment by typing the following command in you terminal ('Anaconda Prompt' on Windows):\n\n```shell\nconda create -n ctg -c conda-forge -y python=3.7\n```\n\nAnd activate this new environment with:\n\n```shell\nconda activate ctg\n```\n\n---\n**Step 3:**\n\nNow install the packages needed to run the course notebooks with the following command:\n\n```shell\nconda config --add channels conda-forge && conda install -y ipython \\\n    tornado=5.1.1 jupyter notebook ipywidgets numpy scipy numba pandas \\\n    gdal netcdf4 matplotlib basemap basemap-data-hires pillow obspy\n```\n\n---\n\n*See you Wednesday,*\n\nLäslo, Pieter, and Shahar\n",
        "createdAt": "2019-03-05T16:07:50.000Z",
        "updatedAt": "2024-07-16T13:46:27.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/shaharkadmiel/CTG_Course/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "agilescientific/in-bruges",
        "url": "https://github.com/agilescientific/in-bruges",
        "description": "A walking tour of bruges",
        "stars": 15,
        "forks": 18,
        "readme": "# in-bruges\nA walking tour of bruges\n\nFor now, there's only one notebook here, but we will add more.\n\n- [Welcome to bruges](Welcome_to_bruges.ipynb) ... [![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/agile-geoscience/in-bruges/master?filepath=notebooks%2FWelcome_to_bruges.ipynb)\n\n-----\n\n© 2018 Agile Scientific, licensed CC-BY and Apache 2.0\n",
        "createdAt": "2018-02-07T15:25:40.000Z",
        "updatedAt": "2025-10-24T20:09:53.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/agilescientific/in-bruges/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "RobertoGuardo/QcAnalysisDeceptionIsland",
        "url": "https://github.com/RobertoGuardo/QcAnalysisDeceptionIsland",
        "description": "Qc analysis for volcano attenuation tomography",
        "stars": 0,
        "forks": 0,
        "readme": "# QcAnalysisDeceptionIsland\n\nThis repositor contains:\n- the code for multifrequencies Qc analysis (fork from the [MuRAT code](https://github.com/LucaDeSiena/MuRAT));\n- the ArcPy code for the import and map creation from the MATLAB output.\n\nThe datasets used for the Qc analysis is the last one obtained from the [Qc data cleaning](https://github.com/RobertoGuardo/QcDataCleaning) process and they are stored in the Zenodo repository [at this link](https://zenodo.org/deposit/6561124).\n \n",
        "createdAt": "2022-05-18T16:35:45.000Z",
        "updatedAt": "2022-05-18T17:51:49.000Z",
        "language": "MATLAB",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/RobertoGuardo/QcAnalysisDeceptionIsland/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Miivanttu/Seismology-Web-App",
        "url": "https://github.com/Miivanttu/Seismology-Web-App",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# Seismology Web App\n",
        "createdAt": "2024-10-21T17:56:27.000Z",
        "updatedAt": "2024-12-16T17:03:32.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Miivanttu/Seismology-Web-App/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "jamesjara/toolkit-exploit-hacking-seismologic-networks",
        "url": "https://github.com/jamesjara/toolkit-exploit-hacking-seismologic-networks",
        "description": "toolkit for exploiting your own seismological networks",
        "stars": 8,
        "forks": 0,
        "readme": "# toolkit-exploit-hacking-seismologic-networks\ntoolkit for exploiting your own seismological networks\n",
        "createdAt": "2016-10-20T12:10:45.000Z",
        "updatedAt": "2023-11-10T06:48:56.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/jamesjara/toolkit-exploit-hacking-seismologic-networks/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "LucaDeSiena/MuRAT",
        "url": "https://github.com/LucaDeSiena/MuRAT",
        "description": "A multi-resolution seismic attenuation tomography code - currently in its 3.0 release",
        "stars": 55,
        "forks": 31,
        "readme": "MuRAT - Multi-Resolution seismic Attenuation Tomography\n=======\n\n![MuRAT is a code for attenuation, scattering and absorption tomography.](./img/muratlogo.jpg)\n\n[![test](https://github.com/deconvolution/MuRAT/actions/workflows/run_test.yml/badge.svg?branch=with_test)](https://github.com/deconvolution/MuRAT/actions/workflows/run_test.yml)\n\nMuRAT is a Matlab Package for seismic Attenuation, Scattering and Absorption Tomography using Body and Coda Waves at multiple frequencies.\n\nMuRAT measures seismic attenuation, scattering, and absorption from passive and active data and models 3D variations of these parameters in space.\n\nThe group of active users (providing questions, feedback, and snippets of code) is the [Volcano Earth Imaging group](https://www.lucadesiena.com).\n\nThe last release of the code is:\n\nLuca De Siena, mreissuf, Yi Zhang, GeoSeisUtilities, Aqeel Abbas, WMZ, Cheng Qingyang, Ferdinando Napolitano, & SimonaGabrielli. (2024). LucaDeSiena/MuRAT: 3.24.10.26 (v3.24.10.26). Zenodo. https://doi.org/10.5281/zenodo.13996752\n\n*Documentation*\n-------------\n\nThe file Documentation.pdf in this folder serves as complete documentation for MuRAT3.0. This README file and the *Input_.mlx* files in this folder act as additional documentation.\n\nThe Wiki for MuRAT is under construction, but you can already check a bit of the history of the code.\n\nWe recorded a Video Tutorial! Just go to the [Volcano Earth Imaging group page](https://www.lucadesiena.com), scroll down click on the video under the Murat Suite link.\n\n*System*\n------------\n\nThe program works on Mac, Linux and Windows systems with Matlab version R2017a or higher.\n\nNecessary Toolboxes: Signal Processing, Curve Fitting, Image Processing and Mapping. The Parallel Computing Toolbox is recommended for speed.\n\nCustom toolboxes not included in standard Matlab installations are also provided with the package. These are:\n\n1. Routines to read SAC files created by Zhigang Peng and available from [his SAC tutorial page](http://geophysics.eas.gatech.edu/classes/SAC/).\n2. The [Regularization Toolbox](https://www.mathworks.com/matlabcentral/fileexchange/52-regtools?s_tid=prof_contriblnk) was created by Per Christian Hansen and available from Matlab File Exchange.\n3. The [IRTools](https://github.com/jnagy1/IRtools/tree/ebd70d4036c3cd8c82fc1e17033351491fddf11f), included in MuRAT as a zipped folder.\n4. Functions from the [Geometry and Image-Based Bioengineering add-On for MATLAB](https://github.com/gibbonCode/GIBBON).\n\nThree sample datasets (Mount St. Helens, Romania, and Toba) are included and allow the user to obtain sample models. The datasets work with the three corresponding *input.mlx* files that show examples of what the user can get with the code.\n\n*Instructions in a nutshell*\n------------\n\nThe current version works following these steps:\n\n1. Download or clone the package at <https://github.com/LucaDeSiena/MuRAT>.\n\n2. Work in the downloaded folder after moving it to an appropriate location on your system.\n\n3. Check that the IRTools have been downloaded as a zipped folder in the corresponding folder in the working directory. Otherwise, download them from <https://github.com/jnagy1/IRtools/tree/ebd70d4036c3cd8c82fc1e17033351491fddf11f>.\n\n4. Open one of the three input .mlx files, providing a step-by-step explanation of all inputs (*Murat_inputMSH.mlx*, *Murat_inputRomania.mlx*, or *Murat_inputToba.mlx*) and create your own.\n\n5. Use a velocity model, storing it in the corresponding folder. The format is [Latitude, Longitude, Altitude (meters)]\n\n6. MuRAT works with [SAC files](https://ds.iris.edu/files/sac-manual/) that must be stored in a single folder and corrected for the instrument function. The files must have populated headers. Your SAC headers get tested anyway; the result is shown in an Excel file. The code takes from the header the following fields:\n***a)*** The P-wave picking in the reference time of the waveform (in seconds);\n***b)*** The coordinates of the event in degrees - beware, *the earthquake depth must be in kilometers*;\n***c)*** The coordinates of the station - beware, *the station elevation must be in meters*;\n***d)*** The origin time of the event (optional) in seconds.\n\n7. Run MuRAT3 and select the name of the input file desired.\n\n*Workflow*\n--------\n\nA. ***Start from the Murat_input..mlx files***\n\nThe input files are self-explanatory and provide detailed descriptions of every input and references to papers you can use to set them. If you have a 3D velocity model, use *MuRAT_InputMSH.mlx* otherwise start from either *MuRAT_InputRomania.mlx* or *MuRAT_InputToba.mlx*, the examples for 3-component data.\n\nB. ***Read the Documentation***\n\nThe Documentation includes a summary of the theory underlying attenuation imaging: read it to understand the approximations used to process data, forward model kernels, and invert observations.\n\nC. ***Understand the output text files***\n\nAll the output files (.mat, .txt and .xlsx) and figures are stored in the **TXT** sub-directories in the **Label** folder, created in the working directory. A list of the output files and what they contain is provided in the following. Ascii files contain the models in degrees and UTM. We strongly suggest imaging the TXT files using the [GeophysicalModelGenerator](https://github.com/JuliaGeodynamics/GeophysicalModelGenerator.jl).\n\nD. **Understand the output figure files**\n\nBeware, *.fig* figures are created with the invisible option in Matlab. There are two ways to open them:\n\n(1) Use the function *openfig(..,'visible')* to open them from the command window.\n\n(2) Click twice on the figure file and write *shg* in the command window.\n\nAll the figures are stored in subdirectories in the **Label** folder, created in the working directory:\n\n*Structure of the Label Folder*\n--------\n\n------------\n\n* **TXT directory**\n\n------------\n\n*peakdelay__.txt*, *Qc__.txt* and *Q__.txt*:  The 3D models of the parameters at different frequencies. The first three columns of all text files correspond to Latitude, Longitude, and altitude. The fourth column is the mapped parameter. They contain a minimum of five columns (for *Peak Delay*) that can be imported to show the locations of the anomalies in a simple (x,y,z) reference system. The fifth column shows blocks hit by at least one ray. Qc and Q are solved with an inversion and thus have (1) the sixth and seventh columns that correspond to the input and output of the checkerboard test; (2) the eighth and ninth columns that correspond to the input and output of the spike test.\n\n*Murat.mat*: A Matlab structure containing all inputs and data the code produces.\n\n*DataHeaders.xls*: A file containing all header variables of the SAC files used for the mapping, useful for data selection.\n\n------------\n\n* ***Checkerboard directory***\n\n------------\n\n      Qc subdirectory\n\n*Qc-Checkerboard__.tif* and *Qc-Checkerboard__.fig*: These figures show the input and output of the Qc checkerboard test in the 3D space (*.fig*) and through cross-sections (*.tif*).\n\n      Q subdirectory\n\n*Q-Checkerboard__.tif* and *Q-Checkerboard__.fig*: These figures show the input and output of the Q checkerboard test in the 3D space (*.fig*) and through cross-sections (*.tif*).\n\n------------\n\n* ***RaysKernels directory***\n\n------------\n\n*Clustering.tif*: This figure shows all rays used on the map (black, discarded) with those after declustering (red). \n\n*Rays__.tif*: These figures show how rays develop in 3D for the Peak Delay and Q measurements. It plots them on three slices (WE, SN, Z). The fourth panel shows the location of the area on the Earth.\n\n*Kernel__.tif* and *Kernel__.fig*: Each *.fig* figure has two panels showing the sensitivity kernels in the entire 3D space (left) and the normalised kernels in the chosen inversion grid (right). This reduction implies several hypotheses: the most important is that most energy is still in the grid (the difference is generally < 1% if all sources and stations are in the inversion grid). The *.tif* figures are sections in the WE, SN, and Z directions. Figures are produced for all frequencies.\n\n------------\n\n* ***Results directory***\n\n------------\n\n      Parameter subdirectory\n\n*Parameter__.tif* and *Parameter__.fig*: Parameter maps in 3D (*.fig*) and across sections (*.tif*).\n\n      PeakDelay subdirectory\n\n*Peak-Delay__.tif* and *Peak-Delay__.fig*: Peak delay maps in 3D (*.fig*) and across sections (*.tif*).\n\n      Q subdirectory\n\n*Q__.tif* and *Q__.fig*: Total attenuation maps in 3D (*.fig*) and across sections (*.tif*).\n\n      Qc subdirectory\n\n*Qc__.tif* and *Qc__.fig*: Coda attenuation maps in 3D (*.fig*) and across sections (*.tif*).\n\n  ------------\n\n* ***Spike directory***\n\n------------\n      Qc subdirectory\n\n*Qc-Spike__.tif* and *Qc-Spike__.fig*: These figures show the input and output of the Qc spike test in the 3D space (*.fig*) and through cross sections (*.tif*).\n\n      Q subdirectory\n\n*Q-Spike__.tif* and *Q-Spike__.fig*: These figures show the input and output of the Q spike test in the 3D space (*.fig*) and through cross sections (*.tif*).\n\n------------\n\n* ***Tests directory***\n\n------------\n\n*Qc_Analysis__.tif*, *PD_Analysis__.tif*, and *CN_Analysis__.tif*\n\nThree figures to evaluate the appropriate peak-delay and coda inputs. Read the documentation for further clarifications.\n\n*L_curve__.fig*: L-curves and cost functions (depending on the inversion method) for the Qc and Q inversions necessary to set the damping parameters. The user can ask for a prompt or set the damping parameters.\n\n*Qc_analysis__*: Relationship between coda attenuation and frequency.\n\n*Velocity_model.fig*: The 3D velocity model is also available as a figure in Matlab format. They can be loaded in Matlab and show the vertical and horizontal slices defined in *Figures Sections*.\n\n------------\n\n*Citing MuRAT*\n------------\n\nThe last release of the code is:\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.13996752.svg)](https://doi.org/10.5281/zenodo.13996752)\n\nIf you use MuRAT for your research and publications, please consider mentioning the GitHub internet site and citing the following papers, depending on the techniques you are going to use\n\n**Q (Total attenuation)**:\n\n1. De Siena, L., C. Thomas, and R. Aster. \"Multi-scale reasonable attenuation tomography analysis (MuRAT): An imaging algorithm designed for volcanic regions.\" Journal of Volcanology and Geothermal Research 277 (2014): 22-35. - *Older release that discusses the code for coda-normalisation, also used in the early works of Prudencio et al. 2015, a,b, GJI*\n\n2. De Siena, L., G. Chiodini, G. Vilardo, E. Del Pezzo, M. Castellano, S. Colombelli, N. Tisato, and G. Ventura, 2017. Source and dynamics of a volcanic caldera unrest: Campi Flegrei, 1983–84. Scientific reports: Nature Journals 7, 8099. - *Recent implementation of the Coda Normalization method with correction for coda attenuation variations*\n\n3. Sketsiou P., L. De Siena, S. Gabrielli, F. Napolitano, 2021. 3-D attenuation image of fluid storage and tectonic interactions across the Pollino fault network. Geophysical Journal International, 226(1), 536-547. - *Most recent application of Q imaging with MuRAT*\n\n**Qc and Peak Delay (Absorption and scattering)**:\n\n1. De Siena L., Calvet, M., Watson, K.J., Jonkers, A.R.T. and Thomas, C., 2016. Seismic scattering and absorption mapping of debris flows, feeding paths, and tectonic units at Mount St. Helens volcano. Earth and Planetary Science Letters, 442, pp.21-31. - *Implementation of the older peak delay and Qc technique, both with regionalisation*\n\n2. De Siena L., A. Amoruso, E. Del Pezzo, Z. Wakeford, M. Castellano, L. Crescentini, 2017.\nSpace-weighted seismic attenuation mapping of the aseismic source of Campi Flegrei 1983–84\nunrest. Geophysical Research Letters, 44.4 pp. 1740-1748. - *First implementation with kernels for Qc*\n\n3. Del Pezzo, E., De La Torre, A., Bianco, F., Ibanez, J., Gabrielli, S., and De Siena, L. (2018). Numerically Calculated 3D Space-Weighting Functions to Image Crustal Volcanic Structures Using Diffuse Coda Waves. - *Numerical implementation of kernel functions*\n\n4. Sketsiou P., F. Napolitano, A. Zenonos, L. De Siena, (2020). New insights into seismic absorption imaging. Physics of the Earth and Planetary Interiors, 298, 106337. - *Comprehensive review of the method and future outlooks*\n\n*Disclaimer*\n------------\n\nAlthough we have cross-checked the whole code, we cannot warranty it is exempt from bugs. The package is provided as-is; we will neither be held responsible for any use you make of it nor for the results and conclusions you may derive using MuRAT.\n\n*Licence*\n------------\n\nMuRAT is released under EUPL v1.1\n\n*Funding*\n------------\n\nSome developments of this software package were funded by the Deutsche Forshungsgemeinshaft under grant number SI\n1748/4-1.\n\n",
        "createdAt": "2016-08-04T10:08:02.000Z",
        "updatedAt": "2025-11-12T14:48:51.000Z",
        "language": "MATLAB",
        "homepage": "https://lucadesiena.github.io/MuRAT/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.13996752",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.13996752",
            "dataCite": "10.5281/zenodo.13996752",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/LucaDeSiena/MuRAT/master/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.13996752",
            "title": "LucaDeSiena/MuRAT: 3.24.10.26",
            "journal": "Zenodo",
            "dateReleased": "2024-10-26T00:00:00.000Z",
            "abstract": "First release with automatic Zenodo doi.",
            "citationsArray": []
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "miili/pysurf96",
        "url": "https://github.com/miili/pysurf96",
        "description": "Python wrapper for modelling surface wave dispersion curves from surf96 - Computer Programs in Seismology, R. Hermann",
        "stars": 69,
        "forks": 21,
        "readme": "# PySurf96\n\n[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)\n[![Python 3.10+](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://python.org/)\n\n_Modelling Surface Wave Dispersion Curves_\n\nThis is a slim wrapper around the program `surf96` from _Computer programs in seismology_ by R. Hermann (<http://www.eas.slu.edu/eqc/eqccps.html>) for forward modelling of Rayleigh and Love wave dispersion curves.\n\nIn this implementation the Fortran77 code is wrapped by `f2py`, which makes the forward computation approximately **8x faster** compared over calling a Python subprocess.\n\nMore useful software for seismology at <https://pyrocko.org>.\n\n## Installation\n\nThis package is for Python 3.\n\nPrerequisits is a Fortran77 compiler, like GNU GCC.\n\n```\npip install .\n```\n\nOr through pip:\n\n```\npip install git+https://github.com/miili/pysurf96\n```\n\n## Documentation\n\nEssentially this is a single function, `surf96`. Here is the docstring:\n\n```\nCalculate synthetic surface wave dispersion curves for a given earth model, wave type and periods.\n\nThis is a slim Fortran wrapper around surf96 from Computer Programs in Seismology from R. Hermann (2013)\n\nArgs:\n    thickness (np.ndarray): Layer thickness in kilometers.\n    vp (np.ndarray): Layer Vp velocity.\n    vs (np.ndarray): Layer Vs velocity.\n    rho (np.ndarray): Layer density in g/m^3.\n    periods (np.ndarray): The periods in seconds, where wave velocity is calculated\n    wave (WaveType, optional): The wave type, \"love\" or \"rayleigh\". Defaults to \"love\".\n    mode (int, optional): Mode of the wave, 1: fundamental, 2: second-mode, etc... Defaults to 1.\n    velocity (Velocity, optional): \"group\" or \"phase\" velocity. Defaults to \"group\".\n    flat_earth (bool, optional): Assume a flat earth. Defaults to True.\nRaises:\n    ValueError: Raised when input values are unexpected.\n    Surf96Error: If surf96 fortran code raises an error,\n        this may be due to low velocity zone.\nReturns:\n    np.ndarray: The surface wave velocities at defined periods.\n```\n\n## Example\n\n```python\nimport numpy as np\nfrom pysurf96 import surf96\n\n# Define the velocity model in km and km/s\nthickness = np.array([5.0, 23.0, 8.0, 0])\nvs = np.array([2, 3.6, 3.8, 3.3])\nvp = vs * 1.73\nrho = vp * 0.32 + 0.77\n\n# Periods we are interested in\nperiods = np.linspace(1.0, 20.0, 20)\n\nvelocities = surf96(\n    thickness,\n    vp,\n    vs,\n    rho,\n    periods,\n    wave=\"love\",\n    mode=1,\n    velocity=\"group\",\n    flat_earth=True,\n\n```\n\n## Citations and Acknowledgments\n\n> Herrmann, R. B. (2013) Computer programs in seismology: An evolving tool for instruction and research, Seism. Res. Lettr. 84, 1081-1088, doi:10.1785/0220110096\n\nThanks to Hongjian Fang for creating the Fortran subroutine (<https://github.com/caiweicaiwei/SurfTomo>)\n",
        "createdAt": "2018-11-21T15:02:04.000Z",
        "updatedAt": "2025-09-04T08:29:21.000Z",
        "language": "Fortran",
        "homepage": "https://pyrocko.org",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/miili/pysurf96/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "RongjiangWang/QSSP_2024",
        "url": "https://github.com/RongjiangWang/QSSP_2024",
        "description": "Complete synthetic seismograms based on a spherical self-gravitating Earth model with an atmosphere–ocean–mantle–core structure ",
        "stars": 3,
        "forks": 2,
        "readme": "This is the 2024 version of FORTRAN code QSSP for calculating complete synthetic seismograms of a spherical earth using the normal mode theory. In comparison with its earlier versions, this version changes the input format to provide more options of selecting source types as well as their composition.\n\nPlease note that the format of input file has changed in comparison with the older version, e.g., QSSP2020. The new version enables aritrary combinations of different types of sources.\n\nHighlights:\n\n(1) all-in-one code for body waves, surface waves, free oscillations, tsunami for uniform ocean, infrasound/acoustic-gravity/Lamb waves for a standard atmosphere, and static solid-earth deformation as well\n\n(2) generating Green’s function database or simulating complete seismograms for any given kinematic source model\n\n(3) hybrid algorithm (numerical integration for low frequency / small harmonic degrees and analytical propagator algorithm for high frequency / large harmonic degrees)\n\n(4) complex frequency technique for supressing the time-domain aliasing problem\n\n(5) differential filter technique for suppressing numerical phases (i.e., space-domain aliasing, see Wang and Wang (2007))\n\nRelated codes\n\nQSSPSTATIC - Co- and post-seismic viscoelastic deformation based on a spherical visco-elastic-gravitational earth model.\n\nQSSPCOSEIS - Co-seismic static deformation based on a spherical elastic-gravitational earth model.\n\nSPGRN - synthetic Green's function database based on a spherical elastic-gravitational earth model.\n\nFor Windows user, the executable file is provided under folder \"WindowsEXE\". Linux user may compile the source codes with \"gfortran\" via a single command like, e.g.,\n\n~>cd .../SourceCode\n\n~>gfortran -o qssp2024 *.f -O3\n\nto get the excutable code qssp2024.\n\nAfter start the executable code, the program ask for an input file in the ASCII format. An example input file is provided under folder \"InputFile\". You may change the input data included in this file for your own applications.\n\nReferences\n\nWang, R., S. Heimann, Y. Zhang, H. Wang, and T. Dahm (2017). Complete synthetic seismograms based on a spherical self-gravitating Earth model with an atmos-phere-ocean-mantle-core structure. Geophysical Journal International, doi: 10.1093/gji/ggx259.\n\nWang, R., and H. Wang (2007), A fast converging and anti-aliasing algorithm for Green’s functions in terms of spherical or cylindrical harmonics, Geophysical Journal International, doi: 10.1111/j.1365-246X.2007.03385.x.\n\n-------------------------------------\nLast update on Nov. 24, 2025 in Zhuhai by Rongjiang Wang\n",
        "createdAt": "2025-04-12T03:07:47.000Z",
        "updatedAt": "2025-11-26T08:33:02.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/RongjiangWang/QSSP_2024/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "nikosT/Gisola",
        "url": "https://github.com/nikosT/Gisola",
        "description": "Gisola: A High Performance Computing application for real-time Moment Tensor inversion",
        "stars": 53,
        "forks": 10,
        "readme": "<a href=\"https://github.com/nikosT/Gisola\">\n<img src=\"https://github.com/nikosT/Gisola/blob/main/gisola.png\" width=\"40%\"/>\n</a>\n<br/><br/>\n\n# Wiki\nVisit the <a href=\"https://github.com/nikosT/Gisola/wiki\">Wiki</a> for more information on:\n* Methodology\n* Installation\n* Configuration\n* Usage\n* Plugins/Modules and Tools\n* Contribution\n\n# Citation\nNikolaos Triantafyllis, Ioannis E. Venetis, Ioannis Fountoulakis, Erion‐Vasilis Pikoulis, Efthimios Sokos, Christos P. Evangelidis; Gisola: A High‐Performance Computing Application for Real‐Time Moment Tensor Inversion. Seismological Research Letters 2021; doi: https://doi.org/10.1785/0220210153\n\nGisola presentation received the Virtual Outstanding Student and PhD candidate Presentation (vOSPP) Award 2021 #egu2021\nhttps://www.egu.eu/awards-medals/ospp-award/2021/\n\n# Presentations\n\n## EGU General Assembly 2021\nAbstract: https://meetingorganizer.copernicus.org/EGU21/EGU21-15888.html<br>\nPresentation: https://github.com/nikosT/Gisola/blob/main/material/EGU21-15888-print.pdf\n\n## FOSSCOMM 2021\nvideo: https://www.youtube.com/watch?v=wqoSDMYNqMY (in Greek)<br>\nslides: https://github.com/nikosT/Gisola/blob/main/material/Gisola-FOSSCOMM.pdf\n\n## Online Workshop on Moment Tensor Inversion using Gisola for the Bandung Institute of Technology (Indonesia)\nvideo Part 1: https://youtu.be/J7gBTWYrXXQ?si=iNaHa2Rj4sQMmIoa (ISOLA)<br>\nvideo Part 2: https://youtu.be/sEBDyyXLatM?si=tyvuyts-u1iCUB7m (Gisola)<br>\nslides: https://github.com/nikosT/Gisola/blob/main/material/Gisola-Institut%20Teknologi%20Bandung-2024.pptx-1.pdf\n\n# Use Cases\n* <img src=\"http://eida.gein.noa.gr/logos/hl.png\" width=\"4%\"/> National Observatory of Athens, Institute of Geodynamics - https://bbnet.gein.noa.gr/gisola/realtime\n<a href=\"https://bbnet.gein.noa.gr/gisola/realtime\">\n<img src=\"https://github.com/nikosT/Gisola/blob/main/material/screenshot.png\" width=\"60%\"/>\n</a>\n<br/><br/>\n\n* <img src=\"https://www.geo.edu.al//gisola/2023/logo.png\" width=\"4%\"/> Institute of Geosciences (IGEO) - https://www.geo.edu.al/\n<a href=\"https://www.geo.edu.al/MonitoringForecast/Moment_Tensor_Solutions/\">\n<img src=\"https://github.com/nikosT/Gisola/blob/main/material/igeo.png\" width=\"60%\"/>\n</a>\n<br/><br/>\n\n\n* <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/12/Logo_BMKG_%282010%29.png/140px-Logo_BMKG_%282010%29.png\" width=\"4%\"/> BMKG - Badan Meteorologi, Klimatologi, dan Geofisika - https://www.bmkg.go.id/\n<a href=\"https://lampung.bmkg.go.id/info/gempa-momenttensor/98-km-BaratDaya-TANGGAMUS-LAMPUNG/bmg2025rxzh\">\n<img src=\"https://github.com/nikosT/Gisola/blob/main/material/bmkg.png\" width=\"60%\"/>\n</a>\n<br/><br/>\n\n* <img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR5whekkjnJt8HIgpk3bsrCclC8D4Eo5gg1NQ&s\" width=\"4%\"/> FMIPA Unesa - Universitas Negeri Surabaya - http://aptsunami.fmipa.unesa.ac.id\n<a href=\"http://aptsunami.fmipa.unesa.ac.id\">\n<img src=\"https://github.com/nikosT/Gisola/blob/main/material/fmipa.png\" width=\"60%\"/>\n</a>\n<br/><br/>\n\n* <img src=\"https://www.sgc.gov.co/static/media/Logo_SGC.8d2ba7de8cd155aa5f3b25f8478fb58a.svg\" width=\"4%\"/> Colombian Geological Survey (Servicio Geológico Colombiano) - https://www.sgc.gov.co\n<a href=\"https://www.sgc.gov.co/\">\n<img src=\"https://github.com/nikosT/Gisola/blob/main/material/sgc.png\" width=\"60%\"/>\n</a>\n<br/><br/>\n\n* <img src=\"https://www.uprm.edu/portada/wp-content/uploads/sites/24/2018/06/logo-upr-rum-white-150x150.png\" width=\"4%\"/> Recinto Universitario de Mayagüez (Puerto Rico Strong Motion Program) [Experimental] - https://smp.uprm.edu\n<a href=\"https://smp.uprm.edu/mtmon/2022/\">\n<img src=\"https://github.com/nikosT/Gisola/blob/main/material/uprm.png\" width=\"60%\"/>\n</a>\n<br/><br/>\n\n**MT Stress Inversion Calcuator FDSNWS-event driven**\nsee https://github.com/nikosT/Gisola/discussions/42\n\nIntroducing a cutting-edge web app for stress inversion calculations using STRESSINVERSE, accessing Moment Tensors via FDSNWS-event in the integrated utility of Gisola, tailored for predefined geographic regions. This tool offers real-time updates and flexibility for researchers.\n\n![image](https://github.com/Kayieni/thesis_app/assets/44552188/3989ea5d-a1dc-42ba-8802-2bba94b31b1e)  ![image](https://github.com/Kayieni/thesis_app/assets/44552188/12cb545c-23f9-4b70-bb40-0f51b2266c09)\n",
        "createdAt": "2021-04-21T15:10:01.000Z",
        "updatedAt": "2025-10-14T13:41:49.000Z",
        "language": "HTML",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/nikosT/Gisola/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "georgiastuart/cmp-stack",
        "url": "https://github.com/georgiastuart/cmp-stack",
        "description": "CMP Stack code for Exploration Seismology 2018",
        "stars": 1,
        "forks": 0,
        "readme": "",
        "createdAt": "2018-04-27T15:00:42.000Z",
        "updatedAt": "2024-04-17T07:47:13.000Z",
        "language": "HTML",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "CSG-USTC/.github.io",
        "url": "https://github.com/CSG-USTC/.github.io",
        "description": "This is the website for the Computational Seismology Group at USTC.",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2025-08-07T07:40:57.000Z",
        "updatedAt": "2025-08-07T07:40:58.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "vishvamitra4/Seismology_lab_Website",
        "url": "https://github.com/vishvamitra4/Seismology_lab_Website",
        "description": "Seismology Lab Website.",
        "stars": 0,
        "forks": 0,
        "readme": "# Getting Started with Create React App\n\nThis project was bootstrapped with [Create React App](https://github.com/facebook/create-react-app).\n\n## Available Scripts\n\nIn the project directory, you can run:\n\n### `npm start`\n\nRuns the app in the development mode.\\\nOpen [http://localhost:3000](http://localhost:3000) to view it in your browser.\n\nThe page will reload when you make changes.\\\nYou may also see any lint errors in the console.\n\n### `npm test`\n\nLaunches the test runner in the interactive watch mode.\\\nSee the section about [running tests](https://facebook.github.io/create-react-app/docs/running-tests) for more information.\n\n### `npm run build`\n\nBuilds the app for production to the `build` folder.\\\nIt correctly bundles React in production mode and optimizes the build for the best performance.\n\nThe build is minified and the filenames include the hashes.\\\nYour app is ready to be deployed!\n\nSee the section about [deployment](https://facebook.github.io/create-react-app/docs/deployment) for more information.\n\n### `npm run eject`\n\n**Note: this is a one-way operation. Once you `eject`, you can't go back!**\n\nIf you aren't satisfied with the build tool and configuration choices, you can `eject` at any time. This command will remove the single build dependency from your project.\n\nInstead, it will copy all the configuration files and the transitive dependencies (webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except `eject` will still work, but they will point to the copied scripts so you can tweak them. At this point you're on your own.\n\nYou don't have to ever use `eject`. The curated feature set is suitable for small and middle deployments, and you shouldn't feel obligated to use this feature. However we understand that this tool wouldn't be useful if you couldn't customize it when you are ready for it.\n\n## Learn More\n\nYou can learn more in the [Create React App documentation](https://facebook.github.io/create-react-app/docs/getting-started).\n\nTo learn React, check out the [React documentation](https://reactjs.org/).\n\n### Code Splitting\n\nThis section has moved here: [https://facebook.github.io/create-react-app/docs/code-splitting](https://facebook.github.io/create-react-app/docs/code-splitting)\n\n### Analyzing the Bundle Size\n\nThis section has moved here: [https://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size](https://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size)\n\n### Making a Progressive Web App\n\nThis section has moved here: [https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app](https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app)\n\n### Advanced Configuration\n\nThis section has moved here: [https://facebook.github.io/create-react-app/docs/advanced-configuration](https://facebook.github.io/create-react-app/docs/advanced-configuration)\n\n### Deployment\n\nThis section has moved here: [https://facebook.github.io/create-react-app/docs/deployment](https://facebook.github.io/create-react-app/docs/deployment)\n\n### `npm run build` fails to minify\n\nThis section has moved here: [https://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify](https://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify)\n",
        "createdAt": "2023-09-16T05:01:34.000Z",
        "updatedAt": "2023-09-16T05:06:49.000Z",
        "language": "JavaScript",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/vishvamitra4/Seismology_lab_Website/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "anowacki/Seis.jl",
        "url": "https://github.com/anowacki/Seis.jl",
        "description": "An open, fast and flexible framework for analysing seismic data in Julia",
        "stars": 27,
        "forks": 4,
        "readme": "# Seis.jl\n\n[![Build Status](https://github.com/anowacki/Seis.jl/workflows/CI/badge.svg)](https://github.com/anowacki/Seis.jl/actions)\n[![Coverage status](https://codecov.io/gh/anowacki/Seis.jl/branch/master/graph/badge.svg?token=zBaHNJLxRk)](https://codecov.io/gh/anowacki/Seis.jl)\n\nSeis.jl is an open, fast and flexible framework for analysing seismic\ndata in [Julia](https://julialang.org).\n\nFor detailed instructions on using Seis, see the documentation via the\nlinks below.\n\n[![Documentation](https://img.shields.io/badge/docs-stable-blue.svg)](https://anowacki.github.io/Seis.jl/stable)\n[![Development branch documentation](https://img.shields.io/badge/docs-dev-blue.svg)](https://anowacki.github.io/Seis.jl/dev)\n\nThis readme file gives a brief introduction to installing and using\nSeis.\n\n## Installation\n\nAt present, Seis is unregistered and it and its dependencies must be\ninstalled manually.  This will change once Seis is registered in\nthe General registry.\n\nTo install Seis, do:\n\n```julia\njulia> ] # As soon as you press ']', you enter Pkg mode...\n\n(v1.3) pkg> add https://github.com/anowacki/Geodesics.jl https://github.com/anowacki/Seis.jl\n```\n\n## Basic introduction\n\n`Seis` is based around single traces of evenly-sampled continuous data where\ntime is the independent variable.  (In future we may support unevenly-sampled\ndata and traces with gaps, and we plan to soon support frequency-domain traces.)\nEach trace holds its sampling interval (the property `.delta`) and a start time\n(`.b`).  Traces have the type `Trace`.\n\n`Trace`s also optionally hold information about the event (`.evt`) and station\n(`.sta`) associated with this recording.  `Event`s and `Station`s are the\ncorresponding types, which also contain useful properties.\n\nFor more information, see the docstrings for `Trace`, `Event` and `Station`.\nThis is easily done in the Julia REPL like so:\n\n```julia\njulia> using Seis\n\njulia> ? # As soon as you press '?', the prompt changes to 'help?>'\n\nhelp?> Trace\nsearch: Trace trace backtrace AbstractSet AbstractVector AbstractVecOrMat\n\n  Trace\n\n  Evenly-sampled time series recorded at a single seismic station. The start\n  time of the trace, in s, is in the b field, whilst the sampling interval, in\n  s, is delta. The trace itself is accessed using the trace method, like\n  trace(t).\n\n  [...]\n```\n\nAll three types above also hold a `.meta` property, which contains any extra\nmetadata you want to associate with the trace, event or station.\n\n## Common conventions\n\n### `f` vs `f!`\n`Trace`s are `mutable struct`s, and therefore for all functions which\npotentially modify a trace, there are two versions.  Firstly, an in-place\nfunction (e.g., `bandpass!`) which as per Julia convention ends with an\nexclamation mark, and modifies the trace.  Secondly, for convenience, there\nis always a copying version without the exclamation mark (e.g., `bandpass`)\nwhich returns a modified copy of the input trace.\n\n### Arrays of traces\nThere is no special type for holding multiple traces.  Instead, we operate\non arrays of traces.  For instance, reading multiple traces from the same\nevent, we can access all the station names like so:\n\n```julia\njulia> t = sample_data(:array);\n\njulia> t.sta.sta\n60-element Array{String,1}:\n \"ABA\"\n \"APA\"\n \"AWI\"\n \"BBH\"\n \"BBO\"\n \"BDL\"\n \"BTA\"\n \"BWH\"\n \"CRA\"\n \"CSF\"\n \"EAB\"\n \"EAU\"\n \"EBH\"\n ⋮    \n \"PMS\"\n \"TSA\"\n \"WAL\"\n \"WCB\"\n \"WME\"\n \"WPM\"\n \"XAL\"\n \"XDE\"\n \"YEL\"\n \"YLL\"\n \"YRC\"\n \"YRE\"\n```\n\nOr to get the full channel code from available header information:\n\n```julia\njulia> channel_code.(t)\n60-element Array{String,1}:\n \".ABA..SHZ\"\n \".APA..SHZ\"\n \".AWI..SHZ\"\n \".BBH..SHZ\"\n \".BBO..SHZ\"\n \".BDL..SHZ\"\n \".BTA..SHZ\"\n \".BWH..SHZ\"\n \".CRA..SHZ\"\n \".CSF..SHZ\"\n \".EAB..SHZ\"\n \".EAU..SHZ\"\n \".EBH..SHZ\"\n ⋮          \n \".PMS..SHZ\"\n \".TSA..SHZ\"\n \".WAL..SHZ\"\n \".WCB..SHZ\"\n \".WME..SHZ\"\n \".WPM..SHZ\"\n \".XAL..SHZ\"\n \".XDE..SHZ\"\n \".YEL..SHZ\"\n \".YLL..SHZ\"\n \".YRC..SHZ\"\n \".YRE..SHZ\"\n```\n\nNote the use of the broadcasted `.` operation (`channel_code.(t)`) which applied\nthe 'scalar' function (`channel_code`) to each trace in the array `t`.\n\n## IO\n\nCurrently, SAC and miniSEED data are read and written.\nSAC files may be either bigendian\n(SAC/BRIS convention) or little-endian (usual IRIS SAC convention)).\n\nUse the `read_sac` and `write_sac` functions for SAC-formatted IO,\nand `read_mseed` and `write_mseed` for miniSEED files.\n\nFuture work will add support for reading many more formats and format\nauto-detection.\n\n## Plotting\n\n### Makie.jl\nSeis.jl enables the use of the [Makie](https://docs.makie.org/stable/)\nplotting package for plotting seismic data.  To enable the functionaliy,\nload a Makie backend package, such as CairoMakie or GLMakie.  Then\nplotting functions such as `plot`, `plot_section` and `plot_hodogram`\ncan be used.\n\nAs an example:\n\n```julia\njulia> using Seis, CairoMakie\n\njulia> t = sample_data(:array);\n\njulia> plot_section(t)\n```\n\nproduces:\n\n![Record section](docs/images/record_section.jpg)\n\nSee the [online documentation](https://anowacki.github.io/Seis.jl/dev/plotting-makie/) for more information.\n\n### Plots.jl\nPlotting using [Plots.jl](https://github.com/JuliaPlots/Plots.jl) is\n[supported as a legacy option](https://anowacki.github.io/Seis.jl/dev/plotting-plots/).\nYou can enable this by doing `import Pkg; Pkg.add(\"Plots\")`.  You then need\nto do `using Plots` when you want to start using Seis.jl's plotting routines.\n\n## Basic processing\n\nBasic time series processing of traces is possible using functions such as\n`integrate`, `bandpass`, `remove_trend`.  See the\n[online documentation](https://anowacki.github.io/Seis.jl/dev/manual/)\nfor a full description of the functions available and how to use them.\n\n## Maintainers\n\nAt present, [Andy Nowacki](mailto:a.nowacki@leeds.ac.uk) (@anowacki) is\nthe primary maintainer of Seis.jl.\n\n## Contributing\n\nContributions to Seis are most welcome.  Please open issues in this\nrepo with bug reports or feature requests.  Pull requests for the same\nare also very welcome.\n",
        "createdAt": "2018-06-08T16:40:48.000Z",
        "updatedAt": "2025-11-25T12:10:15.000Z",
        "language": "Julia",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/anowacki/Seis.jl/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "lfengmle/CPSPy",
        "url": "https://github.com/lfengmle/CPSPy",
        "description": "Python interface for Computer Programs in Seismology",
        "stars": 14,
        "forks": 8,
        "readme": "# CPSPy\n",
        "createdAt": "2016-04-12T23:13:53.000Z",
        "updatedAt": "2025-09-19T05:04:24.000Z",
        "language": "AMPL",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/lfengmle/CPSPy/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "maihao14/QuakeLabeler",
        "url": "https://github.com/maihao14/QuakeLabeler",
        "description": "QuakeLabeler is a Python package to create and manage your seismic training data, processes, and visualization in a single place — so you can focus on building the next big thing.",
        "stars": 23,
        "forks": 4,
        "readme": "![](./docs/Images/QuakeLabeler.png)\n\n# QuakeLabeler\nQuake Labeler was born from the need for seismologists and developers who are not AI specialists to easily, quickly, and independently build and visualize their training data set.\n\n## Introduction\nQuakeLabeler is a Python package to customize, build and manage your seismic training data, processes, and visualization in a single place — so you can focus on building the next big thing. Current functionalities include retrieving waveforms from data centers, customizing seismic samples, auto-building datasets, preprocessing and augmenting for labels, and visualizing data distribution. The code helps all levels of AI developers and seismology researchers for querying and building their own earthquake datasets and can be used through an interactive command-line interface with little knowledge of Python.\n\nInstallation, Usage, documentation and scripts are described at\n https://maihao14.github.io/QuakeLabeler/\n\nIf your data is on local end, please switch to [`localmode`](https://github.com/maihao14/QuakeLabeler/tree/localmode).   \n\nAuthor: [`Hao Mai`](https://www.uogeophysics.com/authors/mai/)(Developer and Maintainer)\n & [`Pascal Audet`](https://www.uogeophysics.com/authors/admin/) (Developer and Maintainer)\n\n## Installation\n\n### Conda environment\n\nWe recommend creating a custom\n[conda environment](https://conda.io/docs/user-guide/tasks/manage-environments.html)\nwhere `QuakeLabeler` can be installed along with its dependencies.\n\n- Create a environment called `ql` and install `pygmt`:\n\n```bash\nconda create -n ql gmt python=3.8\n```\n\n- Activate the newly created environment:\n\n```bash\nconda activate ql\n```\n\n### Installing from source\n\nDownload or clone the repository:\n```bash\ngit clone https://github.com/maihao14/QuakeLabeler.git\ncd QuakeLabeler\n```\n\n```bash\npip install -e .\n```\n\n## Running the scripts\n\nCreate a work folder where you will run the scripts that accompany `QuakeLabeler`. For example:\n\n```bash\nmkdir ./WorkFolder\ncd WorkFolder\n```\n\nRun `QuakeLabeler`. Input ``QuakeLabeler`` to `macOS terminal` or `Windows consoles`:\n\n```bash\nQuakeLabeler\n```\n\nOr input ``quakelabeler`` also works:\n\n```bash\nquakelabeler\n```\n\nA QuakeLabeler welcome interface will be loading:\n\n```bash\n(ql) hao@HaodeMacBook-Pro QuakeLabeler % QuakeLabeler\nWelcome to QuakeLabeler----Fast AI Earthquake Dataset Deployment Tool!\nQuakeLabeler provides multiple modes for different levels of Seismic AI researchers\n\n[Beginner] mode -- well prepared case studies;\n[Advanced] mode -- produce earthquake samples based on Customized parameters.\n```\n## Example to build a dataset in STEAD format\nHere's a brief introduction of how to convert USGS dataset into STEAD format.\n[https://github.com/maihao14/QuakeLabeler/blob/main/quakelabeler/examples/GenerateSTEADformat.ipynb](https://github.com/maihao14/QuakeLabeler/blob/main/quakelabeler/examples/GenerateSTEADformat.ipynb)\n## Reference\n\nMai, H., & Audet, P. (2022). QuakeLabeler: A Fast Seismic Data Set Creation and Annotation Toolbox for AI Applications. Seismological Society of America, 93(2A), 997-1010. https://doi.org/10.1785/0220210290\n\nBibTeX:\n\n    @article{mai2022quakelabeler,\n      title={QuakeLabeler: A Fast Seismic Data Set Creation and Annotation Toolbox for AI Applications},\n      author={Mai, Hao and Audet, Pascal},\n      journal={Seismological Society of America},\n      volume={93},\n      number={2A},\n      pages={997--1010},\n      year={2022}\n    }\n\n## Contributing\nIn current version, raw waveforms data from part of Chinese Earthquake stations are unavailable to access automatically (But you can still use QuakeLabeler in China). Any collaborators are welcome to help extend the data sources, develop the codes, etc.\n\nAll constructive contributions are welcome, e.g. bug reports, discussions or suggestions for new features. You can either [open an issue on GitHub](https://github.com/maihao14/QuakeLabeler/issues) or make a pull request with your proposed changes. Before making a pull request, check if there is a corresponding issue opened and reference it in the pull request. If there isn't one, it is recommended to open one with your rationale for the change. New functionality or significant changes to the code that alter its behavior should come with corresponding tests and documentation. If you are new to contributing, you can open a work-in-progress pull request and have it iteratively reviewed. Suggestions for improvements (speed, accuracy, etc.) are also welcome.\n",
        "createdAt": "2021-05-28T01:11:24.000Z",
        "updatedAt": "2025-06-12T03:51:49.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/maihao14/QuakeLabeler/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "rpsinghcodes/earthquake-store",
        "url": "https://github.com/rpsinghcodes/earthquake-store",
        "description": "Seismological data of India from 2015-2018",
        "stars": 0,
        "forks": 0,
        "readme": "# earthquake-store\nSeismological data of India from 2015-2018\n",
        "createdAt": "2021-02-03T11:04:48.000Z",
        "updatedAt": "2021-02-04T08:38:44.000Z",
        "language": "CSS",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/rpsinghcodes/earthquake-store/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "KaairaGupta/Seismology_Assignment4_Ans5",
        "url": "https://github.com/KaairaGupta/Seismology_Assignment4_Ans5",
        "description": "Code and result of 5th question Assignment 4 of Seismology",
        "stars": 0,
        "forks": 0,
        "readme": "# Seismology Assignment\n## Question 5\n\nConsider MARMOD, a velocity-versus-depth model, which is typical of\nmuch of the oceanic crust (Table 4.1). Linear velocity gradients are assumed to exist at\nintermediate depths in the model; for example, the P velocity at 3.75 km is 6.9 km/s. Write a\ncomputer program to trace rays through this model:\n\n![Question data table](https://raw.githubusercontent.com/KaairaGupta/Seismology_Assignment4_Ans5/master/question/table_question.png)\n\n\nand produce a P-wave T(X) curve, using 100 values of the ray parameter p equally spaced\nbetween 0.1236 and 0.2217 s/km. You will find it helpful to use subroutine LAYERXT\n(provided in Fortran in Appendix D and in the supplemental web material as a Matlab script),\nwhich gives dx and dt as a function of p for layers with linear velocity gradients. Your\nprogram will involve an outer loop over ray parameter and an inner loop over depth in the\nmodel. For each ray, set x and t to zero and then, starting with the surface layer and\nproceeding downward, sum the contributions, dx and dt, from LAYERXT for each layer until\nthe ray turns. This will give x and t for the ray from the surface to the turning point. Multiply\nby two to obtain the total surface-to-surface values of X(p) and T(p). Now produce plots of:\n(a) T(X) plotted with a reduction velocity of 8 km/s, (b) X(p), and (c) τ(p). On each plot, label\nthe prograde and retrograde branches.\n",
        "createdAt": "2020-09-22T14:52:52.000Z",
        "updatedAt": "2020-09-22T15:08:16.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/KaairaGupta/Seismology_Assignment4_Ans5/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "dsiervo/sc3_ai_picker",
        "url": "https://github.com/dsiervo/sc3_ai_picker",
        "description": "Python scripts for PhaseNet and EQTransformer execution in SGC data",
        "stars": 12,
        "forks": 0,
        "readme": "# sc3_ai_picker\n\n![GitHub last commit](https://img.shields.io/github/last-commit/dsiervo/sc3_ai_picker)\n[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)\n\n--------------\n## Descripción \nCreacion de eventos en SeisComP3 usando [PhaseNet](https://github.com/wayneweiqiang/PhaseNet) y [EQTransformer](https://github.com/smousavi05/EQTransformer).\nPermite la creación de eventos en seiscomp a partir de los picks obtenidos con EQTransformer o PhaseNet.\n\n\nEl script principal **ai_picker.py** lee los parámetros desde el archivo ai_picker.inp en el directorio de trabajo y según sus preferencias permite descargar y seleccionar seismogramas, y crear eventos en seiscomp a partir de esos picks.\n\nNo se recomienda usar este script directamente, en su lugar se recomienda usar los scripts **discontinuous_picker.py** para picar periodos largos de tiempo de formas de onda ya descargadas y **ai_scheduler.py** para picar y enviar a Seiscomp los picks y eventos generados en semi tiempo real. Estos 2 scripts hace uso interno del ai_picker.py, esto significa que ambos scripts tomarán los parámetros que se especifiquen en un archivo similar al **ai_picker.inp**, `ai_picker_scdl.inp` en el caso de **ai_scheduler.py** y `temp_ai_picker.inp` en el caso de **discontinuous_picker.py**.\n\n--------------\n## Instalación\n\n### Requisitos\n\n- [Anaconda3](https://www.anaconda.com/products/individual)\n- [SeisComP3](https://www.seiscomp.de/seiscomp3/)\n\n### Descargue el ai_picker\n```bash\n(eqt) $ git clone https://github.com/dsiervo/sc3_ai_picker.git\n(eqt) $ cd sc3_ai_picker\n```\n\n### Instale EQTransformer\n\n#### Cree el ambiente virtual para EQTransformer con los siguientes comandos:\n```bash\n$ conda create -n eqt python=3.7\n$ conda activate eqt\n(eqt) $\n```\n\n#### Instale EQTransformer y las dependencias de los scripts para la generación de dashboards interactivos.\n```bash\n(eqt) $ python -m pip install -r requirements.txt\n(eqt) $ conda deactivate\n$\n```\n\n### Instale Phasenet\n#### Descargue el repositorio\n```bash\n$ git clone https://github.com/wayneweiqiang/PhaseNet.git\n```\n\n#### Cree ambiente virtual para Phasenet e instalelo con el siguiente comando:\n```bash\n$ cd PhaseNet\n$ git checkout a9383be2138c01ca4f1d514f6a5b9b95fb9a7cba\n$ conda env create -f env.yml -n pnet\n$ conda activate pnet\n(pnet) $ python -m pip install mysql-connector-python obsplus\n```\n\n\n\n## optimización de descargas de formas de onda\nEl siguiente paso es necesario para mejorar el desempeño al descargar formas de onda usando EQTransformer.\nEdite el archivo `<su ruta a anaconda3>/envs/eqcc/lib/python3.7/site-packages/EQTransformer/utils/downloader.py` y edite la siguiente línea dentro de la función **get_w** dentro del bloque try en `mdl.download` agregando `threads_per_client=10`\n\n```python\n            mdl.download(domain, \n                         restrictions,\n                         mseed_storage = save_dir,\n                         stationxml_storage = save_dir2,\n                         threads_per_client=10)\n```\n\n## Configure la optimización de descaga de formas de onda\nEl siguiente paso es necesario para mejorar el desempeño al descargar señales usando EQTransformer.\nEdite el archivo `<su ruta a anaconda3>/envs/eqt/lib/python3.7/site-packages/EQTransformer/utils/downloader.py` y agregue la siguiente línea dentro de la función **downloadMseeds** justo antes de la línea `with ThreadPool(n_processor) as p:`\n\n```python\nn_processor = len(station_dic)\n```\n\n## Configure los entornos virtuales en el ai_picker.py\n1. Abra el archivo `<su ruta a sc3_ai_picker>/anaconda_path.txt`\n2. Modifique el valor de la variable `anaconda_path` por la ruta hacia su anaconda3.\n```\nanaconda_path = '<su ruta a anaconda3>'\n```\n\n\n## Configure los entornos virtuales en el ai_picker.py\n1. Abra el archivo `<su ruta a sc3_ai_picker>/anaconda_path.txt`\n2. Modifique el valor de la variable `anaconda_path` por la ruta hacia su anaconda3.\n```\nanaconda_path = '<su ruta a anaconda3>'\n```\n3. Modifique la ruta hacia el directorio anaconda3 en los archivos prune_and_count_evnets.py. run_dashboard.sh y discontinuos_picker.py\n\n\n## Agregue sc3_ai_picker y sc3_ai_picker/utils a su PATH\n1. Edite el archivo `~/.bashrc` y agregue la siguientes 2 líneas:\n```bash\nexport PATH=\"<su ruta a sc3_ai_picker>:$PATH\"\nexport PATH=\"<su ruta a sc3_ai_picker>/utils:$PATH\"\n```\n\n2. En tu terminal ejecute `source ~/.bashrc` para recargar el entorno.\n\n--------------\n## Uso ai_picker.py (no recomendado)\nPara usar el **ai_picker.py** copie en su directorio de trabajo el archivo de configuración `ai_picker.inp` y ejecute el comando:\n```bash\n$ ai_picker.py\n```\n## Salida ai_picker.py\nUna vez ejecutado éste generará en el directorio especificado en el parámetro **general_output_dir** del archivo `ai_picker.inp` unos archivos xml con los picks (picks_final.xml), los orígenes con amplitudes (amp.xml), orígenes con amplitudes y magnitudes (mag.xml), los eventos generados con todo lo anterior (events_final.xml) y un xml que contiene solo los orígenes preferidos de los eventos generados (origenes_preferidos.xml). En el caso de PhaseNet se creará una carpeta por cada intervalo de tiempo **dt** dentro de las cuales encontrará el archivo picks.csv, los archivos xml de seiscomp de picks, amplitudes y orígenes para ese intervalo de tiempo, junto con la carpeta xml_events/ con los archivos .xml de eventos en formato seiscomp de cada subintervalo dt.\n\nTanto para PhaseNet como para EQTransformer se generará en **general_output_dir** el archivo **events_final.xml** con todos los eventos localizados en todo el rango de tiempo entre **starttime** y **endtime**.\n\n\n### Inspeccionar eventos generados\nPuede inspeccionar los eventos generados utilizando el scolv en modo offline y cargando en este el archivo events_final.xml. Para abrir el scolv en modo offline:\n\n    scolv --offline\n\nUna vez abierto el scolv cargue el archivo **events_final.xml** dando click en: File -> Open.\n\n### Archivos de control (ai_picker.inp, ai_picker_scdl.inp y temp_ai_picker.inp)\nLos scripts `ai_picker.py`, `ai_scheduler.py` y `discontinuous_picker.py` toman los parámetros desde sus respectivos archivos de configuración (`ai_picker.inp`, `ai_picker_scdl.inp` y `temp_ai_picker.inp` respectivamente) que encuentresn en el directorio de trabajo (el directorio desde donde se ejecuta el programa). Tales como las estaciones a picar, el rango de tiempo y el servidor desde donde traerá la configuración de las estaciones.\n\nPuede introducir comentarios al ai_picker.inp usando el caracter `#` al inicio de la línea.\n\nA continuación se explicarán los parámetros más relevantes. (No se recomienda modificar los parámetros no mencionados acá a menos de que sepa lo que está haciendo):\n\n<!---\nExplicación de parámetros generales: picker, starttime, endtime, dt, download_data, general_data_dir, general_output_dir, db_sc (opcional).\nLos parámetros de solo PhaseNet: filter_data, pnet_plot_figure.\nLos parámetros de solo EQTransformer: eqt_detection_threshold, eqt_P_threshold, eqt_S_threshold, eqt_number_of_plots.\n-->\n#### Parámetros generales\n**-** `picker`: Picker a utilizar. Puede escoger entre **pnet** para PhaseNet o **eqt** para EQTransformer, se recomienda EQTransformer.\n\n**-** `starttime`: Fecha y hora de inicio del rango de tiempo a picar en formato yyyy-mm-dd hh:mm:ss.\n\n**-** `endtime`: Fecha y hora de fin del rango de tiempo a picar en formato yyyy-mm-dd hh:mm:ss.\n\n**-** `dt`: Su definición depende del `picker` escogido.\n* **pnet**: Tiempo máximo en segundos para hacer una descarga de formas de onda, corrida de PhaseNet y corrida de playback de SeisComP a la vez. Es decir, si los segundos entre starttime y endtime son mayores a dt, el programa dividirá su ejecución (descarga de formas de onda, corrida de PhaseNet y playback de SeisComP) en intervalos de tamaño dt. Para cada intervalo de tamaño dt creará una carpeta con los correspondientes archivos de salida. Se recomienda dejarlo en 6 horas (21600 segundos).\n* **eqt**: Tiempo en el que dividirá los mseed descargados en segundos. Entre más grande es, más memoria RAM se consumirá. Se recomienda dejarlo en 6 horas (21600 segundos).\n\n**-** `download_data`: Controla la descarga de las formas de onda y las estaciones que se usarán. Puede escoger entre “no” (sin comillas) para no descargar formas de onda (si las descargó previamente) o entre un listado separado por coma de las estaciones a descargar siguiendo la nomenclatura “net.station.loc.ch”. En el siguiente ejemplo se descargarán todas las componentes de la banda ancha de la estación BAR2, la componente Z de la estación sismológica de PAM y todas las componentes del acelerómetro de RUS:\n\n    download_data = CM.BAR2.00.HH*, CM.PAM.20.EHZ, CM.RUS.10.*\n\n**-** `general_data_dir`: Directorio donde se guardarán los archivos de formas de onda descargadas.\n\n**-** `general_output_dir`: Directorio donde se guardarán los archivos de salida del ai_picker.py.\n\n**-** `db_sc` (opcional): Base de datos desde donde SeisComP tomará la configuración de las estaciones para el picado de las amplitudes y para la localización de los eventos. Por defecto apunta al servidor 13, si se quisiera tomar esos datos desde el 232 deberá agregar la siguiente línea en cualquier lugar del archivo ai_picker.inp:\n\n    db_sc = mysql://sysop:sysop@10.100.100.232/seiscomp3\n\n\n#### Parámetros de PhaseNet\n**-** `filter_data`: Controla los datos que se filtraran. Para estaciones lejanas a la fuente PhaseNet funciona mejor si los datos están filtrados. Puede escoger entre “no” (sin comillas) para no filtrar ninguna estación o escribir el listado de las estaciones que se filtraran. En el siguiente ejemplo se filtrarán todas los datos de PAM y RUS:\n\n    filter_data = CM.PAM, CM.RUS\n\n**-** `pnet_plot_figure`: Controla la generación de las figuras de los gráficos de los resultados de PhaseNet.  Puede ser True o False. El programa se ejecuta más lento cuando esta opción está activa.\n\n\n#### Parámetros de EQTransformer\n**-** `eqt_detection_threshold`: Umbral de probabilidad para la detección de sismos. Número flotante entre 0 y 1. Entre más pequeño este valor más sismos detectará con el riesgo de que aumenten los falsos positivos. Se recomienda usar 0.003.\n\n**-** `eqt_P_threshold`: Umbral de probabilidad para el picado de la P. Número flotante entre 0 y 1. Entre más pequeño este valor más arribos P picará con el riesgo de que aumenten los falsos positivos. Se recomienda usar 0.01.\n\n**-** `eqt_S_threshold`: Umbral de probabilidad para el picado de la S. Número flotante entre 0 y 1. Entre más pequeño este valor más arribos S picará con el riesgo de que aumenten los falsos positivos. Se recomienda usar 0.01.\n\n**-** `eqt_number_of_plots`: Número de figuras a guardar de las formas de onda con los picks generados por EQTransformer.\n\n## Ejecución offline (discontinuous_picker.py)\n Dependiendo de la cantidad de datos que se desee picar, puede usar el ai_picker.py directamente o usar el script de ayuda **discontinuous_picker.py**.\n En caso de que desee picar más de 2 días de datos con más de 10 estaciones se recomienda usar el script **discontinuous_picker.py**.\n Ambos pueden usarse para picar sismicidad en archivos de formas de onda de estaciones portátiles o para reprocesar formas de onda asociadas a ejambres sísmicos o réplicas.\n\n### Uso discontinuous_picker.py\nPara ejecutar el programa debe tener configurado previamente en el directorio de ejecución el archivo de configuración `temp_ai_picker.inp` (toma todos los parámetros excepto las fechas inicial y final, por lo que puede dejar las que estén por defecto). Para esto copie en su directorio de trabajo el archivo `ai_picker.inp` que se encuentra en la ruta `<su ruta a sc3_ai_picker>/ai_picker.inp`, luego cámbiele el nombre por `temp_ai_picker.inp` y finalmente edite los parámetros dentro de éste según sus preferencias  (estaciones a picar, ruta de los directorios donde se guardarán las formas de onda y donde se generarán los archivos de salida, etc). Una vez ubicado en su directorio de trabajo ejecute el programa de a acuerdo al rego de fechas que desee. Si por ejemplo desea picar entre el 1/1/2020 al 31/7/2020 puede ejecutar en consola el siguiente comando: \n\n    $ discontinuous_picker.py -s \"2020-01-01 00:00:00\" -e \"2020-07-31 23:59:59\"\n\nEl programa empezará a picar y guardará los resultados en carpetas de 7 días. Si desea en cambio que guarde en un rango de tiempo distinto, puede modificarlo con la opción -n seguido del número de días en el que desea que se vayan guardando los datos.\n\nPuede acceder a las opciones del programa ejecutando:\n    \n        $ discontinuous_picker.py --help\n\n\n### Salida discontinuous_picker.py\nUna vez ejecutado este generará en el directorio especificado en el parámetro general_output_dir del archivo temp_ai_picker.inp una carpeta por cada 7 días que contendrá los archivos xml de salida correspondientes a los picks, orígenes y eventos (para más detalles de los xml generados por favor remítase a la sección [Salida ai_picker.py](#salida-ai_picker.py) de este documento), por lo tanto habrán un xml de eventos por cada carpeta de 7 días.\n\n#### Inspección de eventos generados\n##### Uniendo xmls y revisión en dashboard\nDebido a que los eventos localizados se generan en diferentes xml resulta conveniente unir todos archivos xml de eventos en un solo archivo xml. Para esto puede ejecutar en el directorio que contiene las carpetas con los xml el siguiente comando:\n\n    $ prune_and_count_events.py\n\nEl programa prune_and_count_events.py unirá en el archivo **main_events_pruned.xml** todos los xml de eventos que se encuentren en las carpetas subyacentes, creará un archivo separado por comas con la información resumida (all_events.csv) y creará un dashboard que puede ser accedido desde el navegador web del computador local cambiando en la dirección en el navegado local, *localhost* por la *ip del computador desde el cual se está ejecutando el script*, siempre y cuando el computador local se encuentre conectado a la misma red LAN ya sea físicamente o a través de una VPN.\n\nEn el panel izquierdo del dashboard podrá filtrar los eventos por magnitud, coordenadas, profundidad, RMS e intervalo temporal. En el lado derecho de la página se mostrará una tabla con información resumida de los eventos localizados que puede ser ordenada por la columna de preferencia. Adicionalmente se generarán histogramas sobre el número de eventos por valor de magnitud, rms y profundidad. Cómo también la evolución temporal de la sismicidad, perfiles de profundidad y un mapa con la sismicidad.\n\n## Ejecución en tiempo semi-real (ai_scheduler.py y ai_scheduler_sc4.py)\nLos scripts **ai_scheduler.py** y **ai_scheduler_sc4.py** permiten picar en tiempo semi-real las formas de onda de la red con el ai_picker y enviar los eventos generados a un servidor de seiscomp de forma automática (SeisComP3 en el caso del ai_scheduler.py y SeisComP4 en el caso del ai_scheduler_sc4.py).\n\n### Configuración ai_scheduler.py (seiscomp3)\n\n#### Copie los archivos necesarios\nPrimero debe hacer una copia de los archivos **ai_scheduler.py** y **ai_picker_scdl.inp** a su directorio de trabajo, para ello úbiquese en su directorio de trabajo desde la terminal y ejecute:\n    \n        $ cp <ruta hacia sc3_ai_picker>/utils/ai_scheduler.py .\n        $ cp <ruta hacia sc3_ai_picker>/utils/ai_picker_scdl.inp .\n\n#### Modifique ai_picker_scdl.inp y ai_scheduler.py\nEdite el los parámetros de ai_picker_scdl.inp según sus preferencias (se recomienda usar como picker a EQTransformer, para mas información puede mirar [guía parámetros de configuración](#parámetros-generales)) y luego abra el archivo **ai_scheduler.py** y en el bloque `if __name__ == \"__main__\"` edite el valor de las variables `every_minutes`, `minutes` y `db` según sus preferencias. A continuación se explicará en que consiste cada una de éstas (no se explica la variable `buffer` pues se recomienda siempre dejarla en 0):\n\n* `every_minutes`: Especifica el intervalo de tiempo en minutos entre cada ejecución del programa.\n* `minutes`: Especifica el tamaño de las formas de onda a picar en minutos.\n* `db`: Especifica la dirección de host de la base de datos de seiscomp a la cual se enviarán los eventos generados por el programa usando scdispatch (usando el parámetro -H).\n\n**Se recomienda configurar el programa para que se ejecute cada 15 minutos picando trazas de la última media hora (implicitamente un overlaping de 5 minutos) configurando el valor de `minutes` como 30, el de `every_minutes` como 15 y el de `buffer` como 0.**\n\n### Configuración ai_scheduler_sc4.py (seiscomp4)\nDebido a que no fue posible arreglar el problema de conexión en SeisComP4 para enviar eventos desde un cliente a un servidor usando *scdispatch* de forma remota, se decidió copiar tanto el xml de picks como el de eventos al servidor de destino y desde allí ejecutar *scdispatch*. Esto se maneja desde el script ai_scheduler_sc4.py de forma automática.\n\n#### Copie los archivos necesarios\nPrimero debe hacer una copia de los archivos **ai_scheduler_sc4.py** y **ai_picker_scdl.inp** a su directorio de trabajo, para ello úbiquese en su directorio de trabajo desde la terminal y ejecute:\n    \n        $ cp <ruta hacia sc3_ai_picker>/utils/ai_scheduler_sc4.py .\n        $ cp <ruta hacia sc3_ai_picker>/utils/ai_picker_scdl.inp .\n\n#### Modifique ai_picker_scdl.inp y ai_scheduler_sc4.py\nEdite el los parámetros de ai_picker_scdl.inp según sus preferencias (se recomienda usar como picker a EQTransformer, para mas información puede mirar [guía parámetros de configuración](#parámetros-generales)) y luego abra el archivo **ai_scheduler_sc4.py** y en el bloque `if __name__ == \"__main__\"` edite el valor de las variables `every_minutes`, `minutes`, `ip_db`, `usr_db` y `psw_db` según sus preferencias. A continuación se explicará en que consisten las últimas 3 (para las primeras 2 puede mirar la sección [Modifique ai_picker_scdl.inp y ai_scheduler.py](#modifique_ai_picker_scdl.inp_y_ai_scheduler.py)):\n\n* `ip_db`: Dirección IP del servidor SeisComP4.\n* `usr_db`: Nombre de usuario del servidor SeisComP4.\n* `psw_db`: Contraseña del servidor SeisComP4.\n\n**Se recomienda configurar el programa para que se ejecute cada 15 minutos picando trazas de la última media hora (implicitamente un overlaping de 5 minutos) configurando el valor de `minutes` como 30, el de `every_minutes` como 15 y el de `buffer` como 0.**\n\n### Uso ai_scheduler.py y ai_scheduler_sc4.py\nPara ambos scripts la ejecución es igual, una ves configurado el .py y .inp, se debe primero activar el entorno de anaconda correspodiente al picker seleccionado en el archivo ai_picker_scdl.inp y luego ejecutar con python el script.\n\nSi se seleccionó `eqt` (EQTransformer), el cual es el picker recomendado, se debe ejecutar:\n    \n        $ conda activate eqt\n\nEn caso de haber seleccionado `pnet` (PhaseNet), se debe ejecutar:\n    \n        $ conda activate pnet\n\nPor ultimo ejecutar el script con python (ejemplo con ai_scheduler.py) usando `nohup` al inicio del comando y `&` al final para evitar que el programa se detenga en caso de que la terminal se cierre:\n    \n        (eqt) $ nohup python ai_scheduler.py&\n\nSe puede monitorear la salida del programa revisando las 500 últimas líneas del archivo nohup.out con el comando\n\n    (eqt) $ tail -500 nohup.out\n\n\n\n",
        "createdAt": "2020-03-05T22:01:28.000Z",
        "updatedAt": "2025-07-29T08:01:28.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/dsiervo/sc3_ai_picker/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "TaeShinKim/Seismology_for_undergraduate",
        "url": "https://github.com/TaeShinKim/Seismology_for_undergraduate",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "https://forms.gle/CbwSw2jwRoLwRq8W7\n",
        "createdAt": "2025-03-13T02:05:01.000Z",
        "updatedAt": "2025-04-10T06:35:35.000Z",
        "language": "Dockerfile",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/TaeShinKim/Seismology_for_undergraduate/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "liamtoney/sonify",
        "url": "https://github.com/liamtoney/sonify",
        "description": "Sonification of seismic and infrasound signals",
        "stars": 36,
        "forks": 3,
        "readme": "*sonify*\n========\n\n|docs_badge| |build_badge| |cov_badge| |black_badge| |isort_badge|\n\n*sonify* “squeezes” seismic or infrasound signals into audible frequencies and\ncreates animated spectrograms to accompany the audio. Data are pulled from any\nof the `FDSN data centers\n<https://service.iris.edu/irisws/fedcatalog/1/datacenters?format=html>`__\navailable through the Incorporated Research Institutions for Seismology (IRIS)\nData Management Center (DMC) `fedcatalog\n<https://service.iris.edu/irisws/fedcatalog/docs/1/help/>`__ web service.\n\n|screenshot|\n\n*sonify* `won an Honorable Mention\n<https://jhepc.github.io/2020/entry_11/index.html>`__ in the 2020 SciPy `John\nHunter Excellence in Plotting Contest (JHEPC) <https://jhepc.github.io/>`__.\n\nQuickstart\n----------\n\n1. Obtain\n\n   .. code-block:: xml\n\n     git clone https://github.com/liamtoney/sonify.git\n     cd sonify\n\n2. Create environment, install, and activate (`install Miniforge\n   <https://github.com/conda-forge/miniforge?tab=readme-ov-file#install>`__\n   first, if necessary)\n\n   .. code-block:: xml\n\n     mamba env create --file environment.yml\n     conda activate sonify\n\n3. Run using the Python interpreter\n\n   .. code-block:: python\n\n     python\n     >>> from sonify import sonify\n\n   or via the command-line interface\n\n   .. code-block:: xml\n\n     sonify --help\n\nExample\n-------\n\nTo make a movie of the seismic signal generated by a massive avalanche\noccurring in Alaska on 21 June 2019, sped up by a factor of 200:\n\n.. code-block:: python\n\n   from sonify import sonify\n   from obspy import UTCDateTime\n\n   sonify(\n       network='AV',\n       station='ILSW',\n       channel='BHZ',\n       starttime=UTCDateTime(2019, 6, 20, 23, 10),\n       endtime=UTCDateTime(2019, 6, 21, 0, 30),\n       freqmin=1,\n       freqmax=23,\n       speed_up_factor=200,\n       fps=1,  # Use fps=60 to ~recreate the JHEPC entry (slow to save!)\n       spec_win_dur=8,\n       db_lim=(-180, -130),\n   )\n\nOr (equivalently), via the command-line interface:\n\n.. ~BEGIN~\n.. code-block:: xml\n\n  sonify AV ILSW BHZ 2019-06-20T23:10 2019-06-21T00:30 --freqmin 1 --freqmax 23 --speed_up_factor 200 --fps 1 --spec_win_dur 8 --db_lim -180 -130\n.. ~END~\n\nThe result is a 4K 1fps video file named ``AV_ILSW_BHZ_200x.mp4``. A screenshot\nof the movie is shown at the top of this README.\n\nDocumentation\n-------------\n\nApplication programming interface (API) documentation for the module is available\n`here <https://sonify.readthedocs.io/en/latest/sonify.html>`__. For command-line\nusage instructions, type ``sonify --help`` (the ``sonify`` environment must be active).\n\n.. |docs_badge| image:: https://readthedocs.org/projects/sonify/badge/?version=latest\n   :alt: Documentation status\n   :target: https://sonify.rtfd.io/\n\n.. |build_badge| image:: https://github.com/liamtoney/sonify/actions/workflows/build.yml/badge.svg?\n   :alt: Build status\n   :target: https://github.com/liamtoney/sonify/actions/workflows/build.yml\n\n.. |cov_badge| image:: https://codecov.io/gh/liamtoney/sonify/branch/main/graph/badge.svg?token=3OIGM34OFL\n   :alt: Test coverage\n   :target: https://codecov.io/gh/liamtoney/sonify\n\n.. |black_badge| image:: https://img.shields.io/badge/code%20style-black-000000\n   :alt: Link to Black\n   :target: https://black.readthedocs.io/en/stable/\n\n.. |isort_badge| image:: https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336\n   :alt: Link to isort\n   :target: https://pycqa.github.io/isort/\n\n.. |screenshot| image:: screenshot.png\n   :alt: Screenshot of example\n   :target: #example\n\nContributing\n------------\n\nTo install *sonify*'s development packages, with your environment activated run\n\n.. code-block:: xml\n\n   pip install --requirement requirements.txt\n\nIf you notice a bug with *sonify* (or if you'd like to request/propose a new\nfeature), please `create an issue on GitHub\n<https://github.com/liamtoney/sonify/issues/new>`__ (preferred) or email me at\n|liam@liam.earth|_. You are also welcome to create a `pull request\n<https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests>`__.\nPlease don't allow `imposter syndrome\n<https://en.wikipedia.org/wiki/Impostor_syndrome>`__ to obstruct you from\ncontributing your valuable ideas and skills to this project — **I'm happy to help\nyou contribute in any way I can.**\n\n.. |liam@liam.earth| replace:: ``liam@liam.earth``\n.. _liam@liam.earth: mailto:liam@liam.earth\n",
        "createdAt": "2020-02-17T02:16:38.000Z",
        "updatedAt": "2025-10-10T21:00:39.000Z",
        "language": "Python",
        "homepage": "https://sonify.rtfd.io",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/liamtoney/sonify/main/README.rst",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "s-schneider/Muenster_Array_Seismology",
        "url": "https://github.com/s-schneider/Muenster_Array_Seismology",
        "description": "A collection of useful array seismology functions for ObsPy",
        "stars": 4,
        "forks": 0,
        "readme": "# Muenster_Array_Seismology\n\nA collection of useful array seismology functions and methods for ObsPy, rewritten from https://github.com/obspy\n\n",
        "createdAt": "2015-08-20T11:08:50.000Z",
        "updatedAt": "2022-03-24T06:41:20.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/s-schneider/Muenster_Array_Seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "yasuit21/EarthScienceExperiment",
        "url": "https://github.com/yasuit21/EarthScienceExperiment",
        "description": "Experiment on Earth Science (Seismology) at KU",
        "stars": 0,
        "forks": 0,
        "readme": "# EarthScienceExperiment\n\n## 概要\n　これは，地球科学実験の「地震」の受講者が，観測波形をフォーマット変換したり，波形を描画したりするためのサンプルコードです。Python及びGoogle Colaboratoryを使用しています。\n\n## 使用手順\n\n### 事前準備\n1. `Clone.ipynb`をcolabで開き，セルを指示通り実行する\n2. `/content/drive/MyDrive/EarthScienceExperiment`が作成されていることを確認し，ドライブ上でそこに移動\n3. `PlotCSV.ipynb`で右クリックし，「アプリで開く」＞「アプリを追加」＞「Colaboratory」をドライブにインストール\n\n### 各notebookでの操作\n1. ドライブのマウント\n  ```python\n  from google.colab import drive\n  drive.mount('/content/drive') \n  ```\n  指示されたURLを開いてアクセスを許可，パスをセルの標準入力に貼り付ける\n\n2. フォルダのパス設定\n    1. 左側のタブ「ファイル」から`EarthScienceExperiment`を探す（上で作成）\n    2. フォルダ上で右クリックして「パスをコピー」を選択\n    3. 下の`projectBaseDir`にパスを貼り付ける\n\n## 注意事項\n- この共有ドライブ内のコード等を使用したことによって生じるいかなる損害も，著者は責任を負いません\n- 2023年02月現在変更がある可能性があります\n",
        "createdAt": "2021-05-06T09:16:25.000Z",
        "updatedAt": "2025-07-16T01:57:01.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/yasuit21/EarthScienceExperiment/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "lsperezm/Sismologia_Sismica",
        "url": "https://github.com/lsperezm/Sismologia_Sismica",
        "description": "Repository with the code used for Seismology and Seismic courses",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2025-08-20T14:07:08.000Z",
        "updatedAt": "2025-10-28T22:58:29.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "seismology-RUB/GEMINI",
        "url": "https://github.com/seismology-RUB/GEMINI",
        "description": "GEMINI is a program package to calculate Green's functions and surface wave modes of the elastic wave equation for one-dimensional depth dependent media.",
        "stars": 7,
        "forks": 1,
        "readme": "# Welcome to GEMINI\n\nGEMINI is a program package to calculate Green's functions and surface wave modes \nof the elastic wave equation for one-dimensional depth dependent media.\nApplications of the code range from high-frequency, small-scale wave propagation\nproblems like ultrasonic waves, seam waves and shallow seismics to continental-scale seismic waves\nfrom earthquakes.\n\nA description of the underlying mathematical approach can also be found in the paper by\nFriederich and Dalkolmo, Geophysical Journal International, 122, 537-550, 1995.\nPlease cite this paper if you publish results obtained using GEMINI.\n\n\n## Requirements\n* gfortran (at least 4.2) and gcc for compilation\n* GNU make\n* m4 macro processor\n\n## Compilation\n\nSet the variable F95 in your shell environment to the path to your Fortran compiler.\nUse Makefile and Make.f90 for compiling the Green function codes:\n\n* touch *.m4\n* make gfdsvrkf\n* make gfdsvrkf_mpi\n* make -f Make.f90 gfdsvrkfseis\n* make -f Make.f90 plotGreenFKSpectra\n\nThe m4 macro processor is used to automatically generate code needed for\ngfdsvrkf from a common template. If anything goes wrong do a \"make clean\"\nand repeat the compilation procedure.\n\n\n## Usage\n\nThe code is self-explaining. Just enter the name of the executable and you get a\ndescription of arguments and options. For theoretical background look into the\ndocumentation or into the paper by Friederich and Dalkolmo (GJI, 1995).\n\nInfo file:</br>\n&nbsp;&nbsp;&nbsp;All information about source and receiver is provided by an info file (see examples folder).</br>\n&nbsp;&nbsp;&nbsp;For more information consult the module include/sourceReceiver.f90.</br>\n\ngfdsvrkf:</br>\n&nbsp;&nbsp;&nbsp;calculate frequency-wavenumber spectra for the displacement stress vector (DSV)</br>\n&nbsp;&nbsp;&nbsp;with components U, R, V, S, W, T and optionally spatial derivatives either for</br>\n&nbsp;&nbsp;&nbsp;for one source and many receiver depths or for one receiver and many source depths.\n\ngfdsvrkf_mpi:</br>\n&nbsp;&nbsp;&nbsp;an embarassingly parallel version of gfdsvrkf which distributes calculations for</br>\n&nbsp;&nbsp;&nbsp;different frequencies to available processors.\n\ngfdsvrkfseis:</br>\n&nbsp;&nbsp;&nbsp;calculate synthetic seismograms (takes stations and components from an info file),</br>\n&nbsp;&nbsp;&nbsp;current ourput formats: SFF and SSA (a special file with allows direct access).\n\nLook into the examples folder where you find Makefiles for different applications of gfdsvrkf.\n\n\n## Seismogram format SFF\n\nSFF (Stuttgart File Format) is a slight modification of GSE2.0\nwith CMPR6 compression. In the folder libstuff you find some Fortran modules which provide\nroutines for reading and wirting SFF files. As far as I know SFF files can be read in using Seismic Handler.\nLook into stuff.f for a detailed description of the format.\n\n\n## Format SSA\n\nSSA is very helpful when a big amount of seismograms are written because it permits direct access to any seismogram\nupon reading. There is a Fortran module \"include/ssaDataset.f90\" which provides routines for reading SSA-files.\n",
        "createdAt": "2020-06-06T15:43:43.000Z",
        "updatedAt": "2025-05-09T06:53:15.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/seismology-RUB/GEMINI/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "fabian-kutschera/P10_NDiES",
        "url": "https://github.com/fabian-kutschera/P10_NDiES",
        "description": "New Directions in Earthquake Seismology",
        "stars": 1,
        "forks": 0,
        "readme": "# P10 Geophysical Research\n\n![LMU Geophysics](https://www.geophysik.uni-muenchen.de/kopfbild-en.jpg)\n\n## New Directions in Earthquake Seismology\n\nThis course is part of the International Masters Programme in Geophysics at LMU and TUM.\n\nTo start Binder, click on the botton below:\n\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/fabian-kutschera/P10_NDiES/main)\n\n### Contents:\n✴ Back-projection (BP)\n- Theory of array seismology\n- Array response function\n- Data request and BP example\n- BP your own earthquake\n\n\n✴ Earthquake sequence simulation (SEAS)\n- Theory of crustal deformation\n- Rate and State friction\n- Seismic and Aseismic slip transients\n- Boundary element method & DIY\n\n\n✴ Dynamic rupture simulation (DR)\n- Theory of seismic wave propagation\n- and dynamic rupture\n- Discontinuous Galerkin finite-element method\n- Basics and applications of SeisSol.\n",
        "createdAt": "2021-11-18T14:57:42.000Z",
        "updatedAt": "2023-02-09T06:47:34.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/fabian-kutschera/P10_NDiES/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "dansand/seismic",
        "url": "https://github.com/dansand/seismic",
        "description": "Introduction to seismology lecture using Obspy",
        "stars": 17,
        "forks": 6,
        "readme": "",
        "createdAt": "2015-09-21T23:46:23.000Z",
        "updatedAt": "2025-03-27T20:08:58.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "nfsi-canada/OrientPy",
        "url": "https://github.com/nfsi-canada/OrientPy",
        "description": "Seismic station orientation tools ",
        "stars": 51,
        "forks": 19,
        "readme": "\n![](./orientpy/examples/picture/OrientPy_logo.png)\n\n## Seismic station orientation tools \n\nOrientPy is a toolbox to help determine seismometer orientation using automated (and manual) \nprocessing of earthquake data. These methods are particularly useful for broadband \nocean-bottom seismic stations, but are also applicable to broadband land stations\nor shorter period instruments (depending on the method selected). The code uses the \n``StDb`` package for querying and building a station database and can be used through \ncommand-line scripts. Currently the toolbox includes the following methods: \n\n- **DL** (Doran and Laske, 2017): Based on Rayleigh-wave polarization at a range of\n  frequencies and for the two fundamental mode Rayleigh wave orbits. \n\n- **BNG** (Braunmiller, Nabelek and Ghods, 2020): Based on P-wave polarization from \n  regional and teleseismic earthquakes.  \n\n<!-- - **LKSS** (Lim et al., 2018): Based on the harmonic decomposition of radial and \n  transverse receiver functions near zero lag times. -->\n\nEach method can be used independently to produce an estimate of station orientation, in\nterms of the azimuth of seismic component `1` (or `N`).\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3905404.svg)](https://doi.org/10.5281/zenodo.3905404)\n[![build](https://github.com/nfsi-canada/OrientPy/workflows/Build/badge.svg)](https://github.com/nfsi-canada/OrientPy/actions)\n<!-- [![codecov](https://codecov.io/gh/nfsi-canada/OrientPy/branch/master/graph/badge.svg)](https://codecov.io/gh/nfsi-canada/OrientPy) -->\n\nInstallation, Usage, API documentation and scripts are described at \nhttps://nfsi-canada.github.io/OrientPy/.\n\nAuthor: [`Pascal Audet`](https://www.uogeophysics.com/authors/admin/) (Developer and Maintainer)\n\n##### Note\n\nThe toolbox **DL** is heavily based on the software [`DLOPy`](https://igppweb.ucsd.edu/~adoran/DLOPy.html) written by [`A. Doran.`](https://igppweb.ucsd.edu/~adoran/)\n\n#### Contributing\n\nAll constructive contributions are welcome, e.g. bug reports, discussions or suggestions for new features. You can either [open an issue on GitHub](https://github.com/nfsi-canada/OrientPy/issues) or make a pull request with your proposed changes. Before making a pull request, check if there is a corresponding issue opened and reference it in the pull request. If there isn't one, it is recommended to open one with your rationale for the change. New functionality or significant changes to the code that alter its behavior should come with corresponding tests and documentation. If you are new to contributing, you can open a work-in-progress pull request and have it iteratively reviewed.\n\nExamples of straightforward contributions include editing the documentation or adding notebooks/scripts that describe example usage of the code in publications. Suggestions for improvements (speed, accuracy, flexibility, etc.) are also welcome.\n\n#### References\n\n- Braunmiller, J., Nabelek, J., and Ghods, A., 2020, Sensor orientation of Iranian broadband\n  seismic stations from P-wave particle motion, *Seism. Res. Lett.*, doi:10.1785/0220200019.\n\n- Doran, A. K., and Laske, G., 2017, Ocean-bottom seismometer instrument orientation \n  via automated Rayleigh-wave arrival-angle measurements, *Bull. Seism. Soc. Am.*,\n  107, 691-708.\n\n<!-- - Lim, H., Kim, Y., Song, T.-R. A., and Shen, Z., 2018, Measurement of \n  seismometer orientation using the tangential P-wave\n  receiver function based on harmonic decomposition, *Geophys. J. Int.*, 212,\n  1747-1765.\n\n -->",
        "createdAt": "2020-04-14T14:41:38.000Z",
        "updatedAt": "2025-10-25T07:41:55.000Z",
        "language": "Python",
        "homepage": "https://nfsi-canada.github.io/OrientPy/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.3905404",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.3905404",
            "dataCite": "10.5281/zenodo.3905404",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/nfsi-canada/OrientPy/master/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.3905404",
            "title": "OrientPy: Seismic station orientation tools",
            "journal": "Zenodo",
            "dateReleased": "2020-06-24T00:00:00.000Z",
            "abstract": "OrientPy is a toolbox to help determine seismometer orientation using automated (and manual) processing of earthquake data. These methods are particularly useful for broadband ocean-bottom seismic stations, but are also applicable to broadband land stations or shorter period instruments (depending on the method selected). The code uses the <code>StDb</code> package for querying and building a station database and can be used through command-line scripts. Currently the toolbox includes the following methods: DL (Doran and Laske, 2017): Based on Rayleigh-wave polarization at a range of frequencies and for the two fundamental mode Rayleigh wave orbits. BNG (Braunmiller, Nabelek and Ghods, 2020): Based on P-wave polarization from regional and teleseismic earthquakes. LKSS (Lim et al., 2018): Based on the harmonic decomposition of radial and transverse receiver functions near zero lag times. Each method can be used independently to produce an estimate of station orientation, in terms of the azimuth of seismic component <code>1</code> (or <code>N</code>).",
            "citationsArray": []
        },
        "publications": [
            {
                "doi": "10.5194/gi-10-183-2021",
                "name": "Passive seismic experiment “AniMaLS” in the Polish Sudetes (NE Variscides)",
                "source": "Zenodo",
                "authorNames": [
                    "Bociarska, Monika",
                    "Rewers, Julia",
                    "Wójcik, Dariusz",
                    "Materkowska, Weronika",
                    "Środa, Piotr"
                ],
                "url": [
                    "http://ui.adsabs.harvard.edu/#abs/2021GI.....10..183B",
                    "http://doi.org/10.5194/gi-10-183-2021"
                ]
            }
        ]
    },
    {
        "source": "GitHub",
        "name": "inega/EFM_EFMD",
        "url": "https://github.com/inega/EFM_EFMD",
        "description": "Implementation of the modified Energy Flux Model (Korn, 1990) and depth-dependent Energy Flux Model (Korn, 1997), together with the Bayesian inference algorithm from González-Álvarez et al. (2021).",
        "stars": 1,
        "forks": 0,
        "readme": "[![DOI](https://zenodo.org/badge/330757440.svg)](https://zenodo.org/badge/latestdoi/330757440)\n[![DOI](https://img.shields.io/badge/Preprint-10.31223/X5S89Q-blue.svg)](https://doi.org/10.31223/X5S89Q)\n\n\n\n# **Energy Flux Models for Seismic Scattering and Attenuation Characterization**\n\nThis repository contains all the files needed to characterize the small-scale heterogeneity\nstructure and attenuation of the lithosphere using the EFM/EFMD Bayesian approach from\nGonzález Álvarez et al. (2021). \n\nThe network, array name, velocity source name and paths of the directories where we want\nto store our data and/or results need to be added to the scripts. All editable fields are\nfound at the top of the scripts. All executable files are stored in the /bin directory and \nmodules than need to be loaded in the /lib directory.\n    \n### **Steps:**\n\n**0 -** Create a python environment with all the necessary dependencies for the analysis. The\n    seismodelling_env.yml file can be used for this step.\n\n**1 -** Download data. The inv_cat_and_wvf_downloader.py script can be used to download event\n    and stations metadata from the IRIS DMS, as well as waveforms in mseed format. Instrument \n    response can be removed from the data during either in this step or the next.\n    \n**2 -** Convert waveforms to SAC format and add event information to SAC headers and stats.\n    The populate_SAC_stats.py script can be used for this task.\n\n**3 -** Rotate horizontal N and E component traces to radial and transverse components. The\n    script horizontal_components_rotation.py contains the necessary code to do this.\n\n**4 -** Remove events with low signal-to-noise ratio. Only traces with SNR > 5 will be used\n    in the analysis. The data_quality_control.py script contains functions that carry\n    out the data quality control for the whole dataset, but it is necessary to manually\n    ensure that there are no secondary arrivals within the time window of the EFM/EFMD\n    analysis (~100s after the P wave arrival). Step 3 in the script contains instructions to\n    ensure these events are not included in the final dataset.\n    \n**5 -** EFM/EFMD analysis. The EFM_EFMD_inversion.py script carries out all the necessary \n    steps in the analysis. The script also allows us to choose which steps of the \n    analysis we want to carry out, since some of them only need to be done once, while \n    others will likely be repeated a number of times. \n    First, we need to calculate the normalized coda envelopes for all frequency bands. \n    Then, we run the EFM analysis and obtain the intrinsic, scattering and diffusion \n    quality factors and their frequency dependency. Next, the EFMD analysis needs to be \n    run multiple times, in order to guarantee the convergence of the algorithm. Fine \n    tuning of the step sizes defined in the EFMD_Bayesian_modelling is critical to \n    ensure correct acceptance rates. Finally, the different chains obtained from the \n    EFMD are combined into a single set of results. Careful! The files we wish to \n    combine need to be moved into a separate subdirectory before combining them to \n    avoid mistakes. The EFM_EFMD inversion will plot the combined results and save \n    the figures into this new subdirectory.\n    \nPlease do not hesitate to contact me for any questions or issues running these codes.\n\n\n## **Citation**\n\nPlease cite our project:\n\n[González Álvarez et al., 2020. Small-scale lithospheric heterogeneity characterization using Bayesian inference (Preprint)](https://doi.org/10.31223/X5S89Q)\n\n[![DOI](https://zenodo.org/badge/330757440.svg)](https://zenodo.org/badge/latestdoi/330757440)\n\n\n\n## **References**\n\n[Korn, M., 1990. A modified energy flux model for lithospheric scattering of teleseismic body waves. Geophysical Journal International, 102(1), pp.165-175.](10.1111/j.1365-246X.1990.tb00538.x)\n\n[Korn, M., 1997. Modelling the teleseismic P coda envelope: depth dependent scattering and deterministic structure. Physics of the earth and planetary interiors, 104(1-3), pp.23-36.](10.1016/S0031-9201(97)00044-7)\n\n\n\n#######################################################################################\n\n### **Notes**\n\n\n#### **Synthetic tests:**\n\n      For synthetic tests, we need to create the synthetic envelopes for a given\n      combination of scattering parameters first, and save them in the same \n      directory and format than our normalised coda envelopes from the EFM. The \n      EFM_EFMD_Bayesian_modelling function can be used for this calculation.\n      The algorithm will assume that, in the case of synthetic tests, the input\n      parameter values will also be stored in the same dictionary as the synthetic\n      envelopes, so they can be compared with the ones obtained from the inversion.\n\n#### **Velocity data:**\n\n      The velocity model for each station/array needs to be saved to a csv file.\n      The format of this file is as follow:\n      Line 1 = name of the velocity data source (CRUST1,etc)\n      Line 2 = array/station code\n      Line 3 = crustal thickness in km (int)\n      Line 4 = lithospheric thickness in km (int)\n      Line 5 = depth values\n      Line 6 = velocity values\n      \n      Example:\n\t  CRUST1\n\t  PSA\n\t  32\n\t  200\n\t  9.69,19.68,29.98,200\n\t  6.20,6.40,6.80,8.24\n\n#### **Extra/auxiliary functions**\n\n    These are stored in the lib directory.\n   \n    - EFM - core functions of the EFM\n\n    - EFM_EFMD_tools.py - functions to create streams of traces and do basic processing/plotting\n\t\t\tand other auxiliary functions\n\t\t\t\n    - F_EFM - auxiliary functions for the EFM\n\n    - EFMD_Bayesian_modelling - core functions of the EFMD\n\n    - F_EFMD - auxiliary functions for the EFMD\n\n    - fk_analysis.py - functions to carry out an FK analysis, get array geometry and/or get \n\t\t     theoretical P wave arrivals\n\t\t     \n    - trace_alignment.py - functions to align traces within a stream and stack them\n\n    - vel_models - functions to load velocity data and create the EFM/EFMD velocity model\n   \n\n\n\n\n      \n",
        "createdAt": "2021-01-18T18:46:27.000Z",
        "updatedAt": "2023-12-13T13:27:44.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/inega/EFM_EFMD/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "schipp/azimuthal_anisotropy",
        "url": "https://github.com/schipp/azimuthal_anisotropy",
        "description": "Retrieve azimuthal anisotropy of surface waves from inter-station measurements, following Schippkus et al. 2020",
        "stars": 9,
        "forks": 3,
        "readme": "# Azimuthal anisotropy from inter-station velocity measurements\n\n[![DOI](https://zenodo.org/badge/264389474.svg)](https://zenodo.org/badge/latestdoi/264389474)\n\nDetermine the 2-Theta and 4-Theta terms of azimuthal anisotropy from inter-station group-velocity residuals in overlapping cells. This methodology is established in Schippkus et al. (2020), but the original version used in the paper requires the specific output of a specific isotropic inversion code, relying on the parametrization of the isotropic inversion. This new version is based purely on a .csv file of station-pair locations and inter-station group-velocity residuals.\n\nFormat of the header-less `.csv` input: `lat1, lon1, lat2, lon2, vel`\n\n[1] Schippkus, S., Zigone, D., Bokelmann, G., AlpArray Working Group. (2020). Azimuthal anisotropy in the wider Vienna basin region: a proxy for the present-day stress field and deformation. Geophysical Journal International, 220(3), 2056–2067. http://doi.org/10.1093/gji/ggz565\n\n## Requirements\n\n- Python 3.5+\n- Numpy\n- Matplotlib\n- Obspy (https://github.com/obspy/obspy/wiki)\n- Shapely (https://pypi.org/project/Shapely/)\n\n## TODO\n\n- [x] Implement weighted average using path-lengths within cells.\n- [ ] Actually test with real data.\n- [ ] Synthetic example that can handle varying anisotropy.\n",
        "createdAt": "2020-05-16T08:09:21.000Z",
        "updatedAt": "2025-10-08T11:15:09.000Z",
        "language": "Python",
        "homepage": "https://doi.org/10.1093/gji/ggz565",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/schipp/azimuthal_anisotropy/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "The-Geology-Guy/GEOSLAM",
        "url": "https://github.com/The-Geology-Guy/GEOSLAM",
        "description": "The seismo-lineament analysis method is a tool to spatially correlate a shallow-focus earthquake to the surface trace of the fault that generated it. SLAM is the intellectual property and work product of Vince Cronin. The GEOSLAM Python code was a translation from Vince Cronin's Mathematica files, where this translation was performed by Luke Pajer.",
        "stars": 6,
        "forks": 2,
        "readme": "# GEOLOGICAL SEISMO-LINEAMENT ANALYSIS METHOD (GEOSLAM)\n\nMain Project Resources: [PAJER, Luke](mailto:luke.pajer@gmail.com); [CRONIN, Vincent](mailto:vince_cronin@baylor.edu)\n\n_Last Updated: October 2020_\n\n[![lifecycle](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://www.tidyverse.org/lifecycle/#experimental)\n[![croninprojects.org home](https://img.shields.io/badge/croninprojects.org-home-F78C26.svg)](http://croninprojects.org/)\n[![License](https://img.shields.io/badge/LICENSE-mit-43B02A.svg)](/LICENSE)\n[![jupyterlab](https://img.shields.io/badge/jupyterlab-0.35.4-F37821.svg)](https://jupyterlab.readthedocs.io/en/stable/)\n[![python](https://img.shields.io/badge/python-3.6.5-yellow.svg)](https://jupyterlab.readthedocs.io/en/stable/)\n[![wiki](https://img.shields.io/badge/wiki-complete-orange)](https://github.com/The-Geology-Guy/GEOSLAM/wiki)\n\n-----\n\n# PROJECT OVERVIEW\n\nThe Geological Seismo-Lineament Analysis Method (GEOSLAM) Python package is a translation of the Seismo-Lineament Analysis Method (SLAM) mathematica code developed by Vince Cronin. The purpose of this package is to make SLAM available to those who are interested in SLAM, but are more comfortable using Python. _For clarification, GEOSLAM is the python package for the SLAM process._\n\n_From [Vince Cronin's website](https://croninprojects.org/Vince/SLAM/index.htm):_\n\n> The seismo-lineament analysis method is a tool to spatially correlate a shallow-focus earthquake to the surface trace of the fault that generated it. SLAM is the intellectual property and work product of Vince Cronin, and has been developed with assistance from Brandon Rasaka, Victoria Worrell, Jeremy Ashburn, Brian Bayliss, Chris Breed, Bruce Byars, Ryan Campbell, David Cleveland, Jon Cook, Kelly Cronin, Jordan Dickinson, Daniel Lancaster, Ryan Lindsay, Mark Millard, Shane Prochnow, Tyler Reed, Stephen Secrest, Lauren Seidman Robinson, Keith Sverdrup and Lisa Zygo, with funds from AAPG, Baylor University, Colorado Scientific Society, Ellis Exploration, Ft. Worth Geological Society, Geological Society of America, GCAGS, Samson Resources, Roy Shlemon Scholarship Fund, Sigma Xi, and SIPES.\n\nIf there are any issues or concerns with the GEOSLAM python package, please reach out to [Luke Pajer](mailto:luke.pajer@gmail.com). For any questions regarding SLAM, please reach out to [Vince Cronin](mailto:vince_cronin@baylor.edu).\n\n## CONTRIBUTORS\n\nThis project is an open project, and contributions are welcome from any individual. All contributors to this project are bound by a [code of conduct](/CODE_OF_CONDUCT.md). Please review and follow this code of conduct as part of your contribution.\n\n#### Contributions to the SLAM Python Package\n- [Luke Pajer](mailto:luke.pajer@gmail.com) [![orcid](https://img.shields.io/badge/orcid-0000--0002--5218--7650-brightgreen.svg)](https://orcid.org/0000-0002-5218-7650)\n\n#### SLAM Author/Developer\n- [Vince Cronin](mailto:vince_cronin@baylor.edu) [![orcid](https://img.shields.io/badge/orcid-0000--0002--3069--6470-brightgreen.svg)](https://orcid.org/0000-0002-3069-6470)\n\nIn addition, there are a few thesis projects used when developing the SLAM package. These were instrumental in developing and troubleshooting the package. Below are the Thesis projects referenced during development:\n\n- Victoria E. Worrell: [\"The Seismo-Lineament Analysis Method (SLAM) Applied to the South Napa Earthquake and Antecedent Events\"](https://baylor-ir.tdl.org/bitstream/handle/2104/9796/WORRELL-THESIS-2016.pdf?sequence=1&isAllowed=y) \n- Jeremy A. Ashburn: [\"Investigation of a Lineament that Might Mark the Ground-Surface Trace of the Dog Valley Fault, Truckee Area, Northern California\"](https://croninprojects.org/Vince/AshburnBSThesis2015.pdf) \n- Brandon M. Rasaka: [\"Correlation of Selected Earthquakes with Seismogenic Faults, Central Oklahoma\"](https://croninprojects.org/Rasaka/Rasaka-MS-Thesis-2016.pdf)\n\n### Tips for Contributing\n\nIssues and bug reports are always welcome.  Code clean-up, and feature additions can be done either through pull requests to [project forks]() or branches.\n\nAll products of the SLAM project are licensed under an [MIT License](LICENSE) unless otherwise noted.\n\n-----\n\n## HOW TO USE THIS REPOSITORY\n\nThis repository is available to be \n\nBase overview for SLAM map generation (_see the [GEOSLAM Wiki](https://github.com/The-Geology-Guy/GEOSLAM/wiki) for more information_):\n1. Get a DEM file for a specific area (_max lat/lon and min lat/lon must be set via variables_)  \n2. Instantiate qFaults shapefile for quaternary faults and folds to be plotted on map  \n3. Gather Event data (_you can either use the SLAM function to query or enter data yourself_)  \n4. Calculate Grid North Adjustment  \n5. Produce computations relative to the 1st Nodal Plane  \n6. Determine errors relative to the 1st Nodal Plane  \n7. Calculate light direction for hill-shade map  \n8. Calculate the 7 swaths using the `swath_calc` function  \n9. Determine Seismo-Lineament Boundaries using the `get_shade` function (_there also may be a need to fill in some gaps in the boundary area, to do this, specify where the gaps are on the map via the 'corners_fill' attribute -- see the [GEOSLAM Wiki](https://github.com/The-Geology-Guy/GEOSLAM/wiki) for more information_)  \n10. Plot the focal mechanism and Seismo-Lineament boundaries on a map using any of the map types available -- (_see the [GEOSLAM Wiki](https://github.com/The-Geology-Guy/GEOSLAM/wiki) for more information_)\n\nOnce again, this is a simple overview of a typical SLAM task. This is in no way the limit of what can be done. See the [GEOSLAM Wiki](https://github.com/The-Geology-Guy/GEOSLAM/wiki) for more information.\n\n### System Requirements\n\nThis project is developed using Python. There should be no issues with these projects running on Mac, Windows, or Linux. If there are any issues, please submit an issue and it will be investigated.\n\n### Data Resources used in GEOSLAM\n\n#### A. Data Sources\n\n- The [USGS Earthquake Catalog API](https://earthquake.usgs.gov/fdsnws/event/1/) is used to query for earthquake events within the user defined latitude and longitude bounds. When citing the earthquake, use the ID number provided in the query table generated to find the information needed to create an appropriate citation.\n\n- The [USGS Quaternary Faults and Folds database](https://www.usgs.gov/natural-hazards/earthquake-hazards/faults?qt-science_support_page_related_con=4#qt-science_support_page_related_con) provides a shapefile to overlay faults and folds of the last 1.65 Million years on the output maps. To see how to properly cite the faults and folds used when using this data, please navigate to the folowing page: https://www.usgs.gov/natural-hazards/earthquake-hazards/faults.\n\n#### B. Physical Maps and Digital Elevation Model (DEM) Sources\n\n- [OpenTopography](https://opentopography.org/) This work is based on [data, processing] services provided by the OpenTopography Facility with support from the National Science Foundation under NSF Award Numbers 1948997, 1948994 & 1948857.\n\nOpenTopography also hosts and makes available some global data, _all of which are accessible via the OpenTopography services and made available with functions found in the GEOSLAM code_. Below are the citations for the Global Data: \n\n_GMRT:_ ([Terms of Use](https://www.marine-geo.org/about/terms_of_use.php))\n> Ryan, W.B.F., S.M. Carbotte, J.O. Coplan, S. O'Hara, A. Melkonian, R. Arko, R.A. Weissel, V. Ferrini, A. Goodwillie, F. Nitsche, J. Bonczkowski, and R. Zemsky (2009), Global Multi-Resolution Topography synthesis, Geochem. Geophys. Geosyst., 10, Q03014. https://doi.org/10.1029/2008GC002332  \n\n_ALOS World 3D:_\n> J. Takaku, T. Tadono, K. Tsutsui : Generation of High Resolution Global DSM from ALOS PRISM, The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, pp.243-248, Vol. XL-4, ISPRS TC IV Symposium, Suzhou, China, 2014. \n> Here is a link to a [PDF file](https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XL-4/243/2014/isprsarchives-XL-4-243-2014.pdf) of this publication.\n\n_SRTM:_\n> Farr, T.G., Rosen, P.A., Caro, E., Crippen, R., Duren, R., Hensley, S., Kobrick, M., Paller, M., Rodriguez, E., Roth, L., Seal, D., Shaffer, S., Shimada, J., Umland, J., Werner, M., Oskin, M., Burbank, D., Alsdorf, D. (2007), The Shuttle Radar Topography Mission, Rev. Geophys., 45, RG2004. https://doi.org/10.1029/2005RG000183\n\n- [Stamen Map Tile Sets](http://maps.stamen.com/#watercolor/12/37.7706/-122.3782) are used to generate the physical maps in this package. The Stamen map tile sets are copyright Stamen Design, under a Creative Commons Attribution (CC BY 3.0) license.\n\n### Key Outputs\n\nGEOSLAM provides the user a map with seismo-lineament bounds defined. Below are two examples:\n\n#### Example map when the Strike, Dip, and Rake errors are unknown\n![image](images/truckee_elev_1966.png)\n\n#### Example map when the Strike, Dip, and Rake errors are known\n![image](images/truckee_elev_1983.png)\n\n-----\n\n",
        "createdAt": "2020-10-27T17:08:18.000Z",
        "updatedAt": "2024-12-19T15:48:15.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/The-Geology-Guy/GEOSLAM/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "heurezjusz/pyRF",
        "url": "https://github.com/heurezjusz/pyRF",
        "description": "Program to calculate receiver function from seismology data ",
        "stars": 3,
        "forks": 0,
        "readme": "# pyRF\nProgram to calculate receiver function from seismology data \n\nUsage:\n`./calcrf`\nor\n`./calcrf DATAFOLDER`\n\nIn first case DATAFOLDER should be defined in config.py\n\nDATAFOLDER should contain DATAFILES in following format:\n* every DATAFILE should contain one stream of one event registered by one station\n* DATAFILE should be named [station_name]_[event_id], where event_id is event date in format YYMMDD_hhmmss,\n    for example KSP_070716_011322.mseed\n",
        "createdAt": "2017-02-19T16:43:28.000Z",
        "updatedAt": "2020-10-07T01:04:27.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/heurezjusz/pyRF/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "changguo1998/SeisToolsPlot.jl",
        "url": "https://github.com/changguo1998/SeisToolsPlot.jl",
        "description": "Plot datas used in seismology",
        "stars": 0,
        "forks": 0,
        "readme": "# SeisToolsPlot.jl",
        "createdAt": "2025-04-24T01:50:37.000Z",
        "updatedAt": "2025-08-05T07:31:12.000Z",
        "language": "Julia",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/changguo1998/SeisToolsPlot.jl/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "heinerigel/2016-Advanced-Computational-Seismology",
        "url": "https://github.com/heinerigel/2016-Advanced-Computational-Seismology",
        "description": "Course material for the Seminar on Advanced Methods Winter Term 2016",
        "stars": 2,
        "forks": 9,
        "readme": "# 2016-Advanced-Computational-Seismology\nCourse material for the Seminar on Advanced Methods Winter Term 2016\n\n<p style=\"width:10%;float:right;padding-left:50px\">\n<img src=book_small.jpg>\n<span style=\"font-size:smaller\">\n</span>\n</p>\n\n\n## Scope\nThis course aims at discussing advanced computational methods in seismology, extending the basic methods encountered in the lecture on computational seismology in the summer term. In addition we learn (in more depth) some basic tools for code development (github, Jupyter notebooks). Some of the methods we will discuss are 1) advanced finite-difference concepts (optimal operators, summation by parts operators), 2) finite-volume methods, 3) discontinuous Galerkin methods, and others. \n\n## Format\nParticipating students will prepare basic theoretical concepts, simple python-based notebooks, and possibly report on some applications of methodologies in geosciences.\n\n## Date and Location\nWednesdays 11:00-12:30, begin November 2, 2016\nRoom C111\n\n## Programme\n\n| Date  |   Topic |  Presenters |   Misc |  \n|---|---|---|---|\n|  Nov 2  |  GitHub |  T. Megies | Introduction, Applications |\n|  Nov 9  |  Optimal FD operators (Geller) |  T. Taufiqurrahman  | Notebook ready  |\n|  Nov 16 |  SE on planets | R. Joshi   | Notebooks in seismolive or other   |\n|  Nov 23 |  FD summation by parts | K. Duru   | New notebooks?  |\n|  Dec 2 | Special Seminar Supercomputing in Earth Science | H. Igel, T. Thiel | 18:00 Luisenstr. 37 C106 |\n|  Dec 7 |  FE linear  | B. Chow, A. Verna | Notebooks  seismo-live  |\n|  Dec 14  | FV method  | S. Anger, H. Khoshkhoo   | Notebooks  seismo-live  |\n|  Jan 25 | DG method  |  A. Rijal, S. Singh |  Notebooks  seismo-live |\n|  Feb 1 | Parallel programming  |  D. Vargas | New notebook with PythonMPI  |\n|  Feb 8 | Parallel programming Part 2  |  D. Vargas | New notebook with PythonMPI  |\n\n\n## Potential topics\n\n* Optimal Operators (Geller), update notebook with analytical solution, quantify accuracy\n* Introduction to homogenization, develop simple notebook with example for acoustic wave equation\n* Simple flux schemes in the finite-volume method, test on extremely heterogeneous case, compare with finite-difference method\n* Improved finite-difference techniques (summation by parts), development of simple 1D and/or 2D notebooks\n* Octree meshes for boundaries, develop notebook from Matlab code (Varduhn)\n* The discontinuous Galerkin method, basic theory, fluxes, heterogeneous case, applications\n* Parallel programming with (python-) mpi, simple examples\n* Simulation of planetary objects (e.g. Instaseis)\n\n \n",
        "createdAt": "2016-10-17T13:48:31.000Z",
        "updatedAt": "2023-05-23T21:17:27.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/heinerigel/2016-Advanced-Computational-Seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "jltichy/Example-Seismology-Code",
        "url": "https://github.com/jltichy/Example-Seismology-Code",
        "description": "This code was shared with me as an example of how to write in Python.",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2016-02-23T15:51:57.000Z",
        "updatedAt": "2016-03-01T21:03:52.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "StefaniaGentili/NESTORE",
        "url": "https://github.com/StefaniaGentili/NESTORE",
        "description": "NESTORE is a MATLAB package capable to estimate, during ongoing of an aftershock sequence following a damaging earthquake, the likelihood of the occurrence of another strong earthquake.",
        "stars": 5,
        "forks": 1,
        "readme": "",
        "createdAt": "2022-09-29T07:42:15.000Z",
        "updatedAt": "2024-12-14T10:37:28.000Z",
        "language": "MATLAB",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.7472134",
            "openAlex": "10.5281/zenodo.7472134",
            "openCitations": "10.5281/zenodo.7472134",
            "dataCite": "10.5281/zenodo.7472134",
            "rsd": ""
        },
        "mainPaper": {
            "doi": "10.5281/zenodo.7472134",
            "title": "StefaniaGentili/NESTORE: Reproducibility package for NESTOREv1.0: A MATLAB package for strong forthcoming earthquake forecasting",
            "journal": "Zenodo",
            "dateReleased": "2022-12-22T00:00:00.000Z",
            "abstract": "",
            "citationsArray": []
        },
        "repoDoi": "10.5281/zenodo.7472134",
        "publications": [
            {
                "doi": "10.5281/zenodo.7472134",
                "name": "NESTORE",
                "source": "",
                "authorNames": [],
                "abstract": "",
                "publicationDate": "2022-12-22T00:00:00.000Z"
            }
        ]
    },
    {
        "source": "GitHub",
        "name": "Mallcock1/asymmetric_waveguide_inversion",
        "url": "https://github.com/Mallcock1/asymmetric_waveguide_inversion",
        "description": "De-trending, fitting curves, and magneto-seismological inversion of observational data",
        "stars": 0,
        "forks": 0,
        "readme": "# Applying asymmetric waveguide seismology to data\n\nUses the amplitude ratio method from [here](https://iopscience.iop.org/article/10.3847/1538-4357/aaad0c/meta).\n\nUsed for the data analysis of Section 4 of [this](https://www.frontiersin.org/articles/10.3389/fspas.2019.00048/full) paper.\n\nanalysis.py is the main file from which you run your inversion.\n\nThe process is made up of two parts:\n- time_distance.py creates a class for your data, and has methods to draw out the boundary data.\n- fibril_inversion.py creates a class for your boundary data, which can then be used for the inversion procedure.\n\n### Prerequisites\n\nastropy",
        "createdAt": "2018-10-25T15:45:57.000Z",
        "updatedAt": "2020-01-31T15:05:45.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Mallcock1/asymmetric_waveguide_inversion/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "owlpy/owlpy",
        "url": "https://github.com/owlpy/owlpy",
        "description": "Tools for many-component seismology",
        "stars": 8,
        "forks": 2,
        "readme": "# OwlPy\nTools for many-component seismology\n\n## Documentation and installation\n\nFind documentation and installation instructions in [OwlPy's documentation\npreview](https://owlpy.org/).\n\n## Contributing\n\nPlease see the file [`CONTRIBUTING.md`](CONTRIBUTING.md).\n\n## Licence\n\nOwlPy is licenced under the [GNU Affero General Public License\n(AGPLv3)](LICENCE).\n\n## Contact\n\n* Package maintainer: Sebastian Heimann, sebastian.heimann@uni-potsdam.de\n",
        "createdAt": "2021-07-06T13:53:32.000Z",
        "updatedAt": "2022-11-04T12:21:03.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/owlpy/owlpy/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "romaguir/geol_593",
        "url": "https://github.com/romaguir/geol_593",
        "description": "Course materials for GEOL593 (Seismology and Earth Structure)",
        "stars": 9,
        "forks": 1,
        "readme": "# geol_593\nCourse materials for GEOL593 (Seismology and Earth Structure)\n",
        "createdAt": "2023-01-18T17:48:39.000Z",
        "updatedAt": "2025-01-05T09:00:57.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/romaguir/geol_593/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Ramana91/Epidemic-Type-Aftershock-Sequence-ETAS-Models-in-Statistical-Seismology",
        "url": "https://github.com/Ramana91/Epidemic-Type-Aftershock-Sequence-ETAS-Models-in-Statistical-Seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# Epidemic-Type-Aftershock-Sequence-ETAS-Models-in-Statistical-Seismology\n\nThis project is an attempt to compare two spatial temporary statistical models in its application against earthquakes, looking at two variations of the Epidemic Type Aftershock Sequence (ETAS) model, applying it to a region in Iran. \n\nThe aim of this exercise is to compare these models and its effectiveness in analysing seismic data in relation to the events of the Earthquakes within South Eastern Iran, encompassing the city of Bam. This particular area was selected for this project due to the devastating impacts of a series of earthquakes in 2003 that resulted in the deaths of at least 30,000 people. \n\n## Dataset Used & Model Introduction\n\nFor this approach, the dataset used was obtained from USGS. The data retrieved consists of all Earthquakes above a magnitude of 2, recorded in the square region bounded by 26° - 34°N, and 55°- 60°E. The time period for this study is from the year 2004 until the year 2019, which yielded 1002 observations or events. These dates were selected after reaching out to the author of the ETAS package, as there was a trend change in the occurrence time of the events from 2004 onwards (A. Jalilian, personal communication, October 16, 2019). \n\nThis is a necessary assumption that needs to be fulfilled, as both versions of the ETAS model require the data to be stationary in time, as the results of the output, particularly in the case of the first model, is not reliable if this assumption is violated\n\nThe original dataset that was used is included, and is titled [\"originaldata.csv\"](https://github.com/Ramana91/Epidemic-Type-Aftershock-Sequence-ETAS-Models-in-Statistical-Seismology/blob/main/originaldata.csv). The data was then split out into two files, one file [\"bametas.csv\"](https://github.com/Ramana91/Epidemic-Type-Aftershock-Sequence-ETAS-Models-in-Statistical-Seismology/blob/main/bametas.csv) for the ETAS model with semi-Parametic Estimation which was first introduced by [Ogata in 1988](https://www.jstor.org/stable/2288914), and another file [bametasFLP.csv](https://github.com/Ramana91/Epidemic-Type-Aftershock-Sequence-ETAS-Models-in-Statistical-Seismology/blob/main/bametasFLP.csv)for the version of ETAS model which utilised the forward likelihood predictive approach to estimate the non-parametric components of background seismicity, with the triggered seismicity being estimated through maximum likelihood [(Chiodi & Adelfio, 2011)](https://onlinelibrary.wiley.com/doi/10.1002/env.1121). \n\nThe models have nuances in their mathematic formulation, yet significant differences in the methods that are employed to estimate the background seismicity. Both models employ Maximum Likelihood Estimation (MLE) to estimate the main parameters. However, the difference between the two comes with the estimation of the background seismicity. The first model estimates it through a semiparametric approach, whereas the second variation of the ETAS model, estimates the background seismicity through nonparametric estimation, utilising Forward Likelihood Prediction (FLP). \n\nThe project concludes with the second variation of the ETAS model being more applicable to the data, as the model considers parameters that are deemed to be significant in the model in particular its applications to the aftershock activity.  \n\n## Reproduce the analysis\n\nThe following steps through how one can reproduce the data gathering, analysis and presentation. One important note to make is that depending on your version of R, or further updates to the packages that are being used after this analysis was conducted, results may vary slightly due to the nature of the iterative processes used in the analysis.\n\n### Software Utilised and Prerequisites\n\nThis project was run utilising [R](https://cran.r-project.org/bin/windows/base/), with the corresponding IDE [RStudio ](https://posit.co/download/rstudio-desktop/) being used. After downloading and installing these programs, a few library's were installed, one for each of the models ([etas](https://cran.r-project.org/web/packages/ETAS/index.html) and [etasFLP](https://cran.r-project.org/web/packages/etasFLP/index.html)) and another for basic data transformation [dplyr](https://cran.r-project.org/web/packages/dplyr/index.html)\n\n* Steps to install required Libraries for this exercise\n  ```sh\n  install.packages(\"ETAS\")\n  install.packages(\"etasFLP\")\n  install.packages(\"dplyr\")\n\n### Run the analysis for the ETAS Model (Model 1)\n\nGiven the two separate models that were run, slight variations in datasets are required. This is for the first ETAS model, that does not utilise the forward likelihood predictive approach for non-parametric estimation. The data obtained from USGS is first transformed prior to loading into R. First, only the headers relating to columns time, latitude, longitude, depth and magnitude are retained, with the rest being removed from the dataset. The next step was to separate out the time column into two separate columns, one representing the date the earthquake occurred, and the next column representing the time of the earthquake. It is important for the analysis that the date is represented in format “yyyy/mm/dd”. The data was then loaded into R 3.6.1, utilising the ETAS package to develop this version of the model. \n\nNine models were devleoped, each with a different threshold for the magnitude of earthquakes to be included within the model as required for this model. In addition to this, starting parameters were required to be used which are summarised in the model below.\n\n| Parameter       | μ1    | A1    | c1    | α1  | p1    | D1     | q1    | γ1  |\n|-----------------|-------|-------|-------|-----|-------|--------|-------|-----|\n| Initial Estimate| 1.025 | 1.18  | 0.02  | 0.1 | 1.08  | 0.0018 | 1.22  | 0.5 |\n\n\nThe necessary R code to run this is summarised below. Note that not the below code does not include the code for each of the nine models built, as that would include irrelevant repetition. \n\n* Use the following command line to set the working directory to the relevant repository, load in the necessary files and run load the libraries highlighted above. \n  ```sh\n  # Set the working directory to this repository\n  setwd(\"Epidemic-Type-Aftershock-Sequence-ETAS-Models-in-Statistical-Seismology\")\n  # Load the dplyr package\n  library(dplyr)\n  # Load the ETAS package\n  library(ETAS)\n\n* Load the necessary data files and data transformation\n  ```sh\n  # Load in the file csv file for this model referenced above \n  dat <- read.csv(file=\"bametas.csv\")\n  # Renaming of columns so they can be loaded into the model.\n  dat <- dat[, c(1, 2, 4, 3, 6)]\n  names(dat) <- c('date', 'time', 'long', 'lat', 'mag')\n\n* Create the catalog of data, and run the model and assess results.\n  ```sh\n  # Create the catalog of data, where the threshold of 4.0 is set as highlighted above. This is the value that is\n  # set differently for each of the nine models, ranging from 3.0 to 4.5 as shown in the table below.\n  cat <- catalog(dat, time.begin = \"1970/01/01\", study.start = \"2004/01/01\", lat.range = c(26, 34), long.range=c(55, 60),mag.threshold = 4.0)\n  # Plot of this catalog allows one to assess the necessary charts, and determine the ideal cutoff using a plot of the logarithm of the magnitude of frequencies\n  plot(cat)\n  # Starting parameters for estimation as per the above table\n  param1 <- c(1.025, 1.18, 0.02, 0.1, 1.08, 0.0018, 1.22, 0.5)\n  # Fit the ETAS model and display results\n  fit<- etas(cat, param0=param1)\n  fit\n  # Display seismicity rates, clusttering coefficients and heatmaps\n  rates(fit, lat.range = NULL, long.range = NULL,dimyx=NULL, plot.it=TRUE)\n  # Residuals from model output to review model assumptions and accuracy\n  resid.etas(fit)\n\n\n### Run the analysis for the etasFLP Model (Model 2)\n\nThe data obtained from USGS is transformed prior to loading into R, in that only the headers relating to columns time, latitude, longitude, depth and magnitude are retained, with the rest being removed from the dataset. The next step was slightly different however, in that the date and time are included in the same column, with the format being “yyyy/mm/dd  hh:mm:ss”. The data was then loaded into R 3.6.1, utilising the etasFLP package to develop this version of the model. \n\nNine models were devleoped, each with a different threshold for the magnitude of earthquakes to be included within the model as required for this model. In addition to this, starting parameters were also required to be used.\n\n* Use the following command line to set the working directory to the relevant repository, load in the necessary files and run load the libraries highlighted above. \n  ```sh\n  # Set the working directory to this repository\n  setwd(\"Epidemic-Type-Aftershock-Sequence-ETAS-Models-in-Statistical-Seismology\")\n\n* Load the necessary data files, create dataset for model and data transformation\n```sh\n  # Load the etasFLP package\n  library(etasFLP)\n  # Load the data into the IDE\n  dat1 <- read.csv(file=\"bametasflp.csv\")\n  # Basic data transformation to the it into the right format, and convert time to seconds.\n  dat1$lat <- dat1$latitude\n  dat1$long <- dat1$longitude\n  tsec =timecharunique2seq(dat1$date)[[\"sec\"]]\n  dat1$datetime2 <- tsec\n  dat1 <- dat1[c(8, 7, 6, 4, 5)]\n  # Load the dplyr package for data transformation and renaming of columns. Finalisation of dataset for model.\n  library(dplyr)\n  dat1 <- rename(dat1, time = datetime2)\n  dat1 <- rename(dat1, z = depth)\n  dat1 <- rename(dat1, magn1 = mag)\n  # Plot magnitude of frequencies\n  magn.plot(bamdetas4)\n\n* Run the model and assess results.\n  ```sh\n  # Set starting parameters for the model\n  etas.starting(dat1, m0=3, p.start=1, a.start=1.5, gamma.start=0.5, q.start=2,longlat.to.km=TRUE, sectoday=TRUE)\n  # Set threshold to 3 for the first model tested.\n  magn.threshold = 3\n  # Create model using data provided\n  M1 <- etasclass(dat1, magn.threshold=magn.threshold, magn.threshold.back=magn.threshold+0.5, mu=0.08720895, k0= 0.06239737,c=0.1441001,p=1,a=1.5,gamma=.5,d=37.44874,q=2, params.ind=replicate(8,TRUE), hdef=c(1,1), declustering=TRUE,thinning=FALSE, flp=TRUE, m1=NULL, ndeclust=5, onlytime=FALSE,is.backconstant=FALSE, w=replicate(nrow(dat1[dat1$magn1 >=magn.threshold, ]),1), description=\"\", cat.back=NULL, back.smooth=1.0, sectoday=TRUE,longlat.to.km=TRUE, usenlm=TRUE, method =\"BFGS\", compsqm=TRUE, epsmax=0.0001, iterlim=100, ntheta=100)\n  # Return results from model run\n  M1\n  summary(M1)\n  # Return plots associated with model \n  plot(M1,pdf=TRUE,file =\"Bam, Iran ETASFLP Plots - Threshold 4\", ngrid=201,nclass=10,tfixed=0,flag.3D=FALSE,flag.log=FALSE)\n\n\n",
        "createdAt": "2023-11-26T09:13:49.000Z",
        "updatedAt": "2023-11-26T09:14:34.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Ramana91/Epidemic-Type-Aftershock-Sequence-ETAS-Models-in-Statistical-Seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ghfbsd/qseis",
        "url": "https://github.com/ghfbsd/qseis",
        "description": "Reflectivity synthetics and SAC output",
        "stars": 4,
        "forks": 1,
        "readme": "# QSEIS\n\nThis is a slightly modified version of R. Wang's QSEIS (2006 version).  See\n[this url](https://www.gfz-potsdam.de/en/section/physics-of-earthquakes-and-volcanoes/infrastructure/tool-development-lab/) \nfor information and the most up-to-date version.\nThe programs are all written in Fortran 77.\n\nThe modifications include:\n\n- Added Unix Makefile to build programs.\n\n- Changed input to read input file name from the command line.\n\n- Changed input so that synthetics for other planets may be made (e.g. Mars).\n\n- Merged some bug fixes with QSEIS source code from Nov. 2006 version.\n\n- Output receiver positions in their units of input (km or deg).\n\n- Added program that will write SAC output files.\n\nFirst make the programs:\n```\nmake qsmain qseissac\n```\n\nSee these annotated test input files included with the source code for details\non preparing input.\n\n- **Simple test** (regional crustal propagation synthetics).\n  There will be 60 seismograms written at 5 km offsets out to 300 km distance.\n  ```\n\n   qsmain qs6testinput.dat          ## runs synthetics\n   qseissac qs6testinput.dat        ## writes SAC files\n\n  # After this, examine one of the files with SAC:\n   sac\n   r 0001.seis.z    ;* reads data file\n   lh               ;* lists header information\n   p1               ;* plots seismogram\n   quit\n  ```\n\n- **Complex test** (teleseismic propagation, synthetics take longer).\n  There will be 31 seismograms written between 60 and 90 degrees distance.\n  ```\n   qsmain qs6inp.dat                ## runs synthetics\n   qseissac qs6inp.dat              ## writes SAC files\n   sac\n   r 0001.seis-2.z  ;* reads data file\n   m ttsac          ;* plots seismogram, marks phase arrivals\n   r 0011.seis-2.z  ;* reads data file\n   m ttsac          ;* plots seismogram, marks phase arrivals\n   r 0021.seis-2.z  ;* reads data file\n   m ttsac          ;* plots seismogram, marks phase arrivals\n   r 0031.seis-2.z  ;* reads data file\n   m ttsac          ;* plots seismogram, marks phase arrivals\n   quit\n  ```\n  See [this url](https://members.elsi.jp/~george/sac-bugs.html#ttimes) and\n  [this url](https://github.com/ghfbsd/sacbook/tree/master/methods/split) and\n  for making the ttsac macro available to your version of SAC.\n\n- **Mars test** (vertical incidence, ambient noise autocorrelation synthetic).\n  There will be 8 seismograms written at offsets from 0-5 km, 10 and 50 km.\n  Explosive source at 60 km depth.\n  ```\n   qsmain qs6testmars.dat                ## runs synthetics\n   qseissac qs6testmars.dat              ## writes SAC files\n   sac\n   r D*K.mex.z      ;* reads data files\n   xlim 250 400     ;* window arrival of 410 km disc. analog and PcP\n   p1               ;* plots seismograms\n   quit\n  ```\n",
        "createdAt": "2022-01-04T15:35:40.000Z",
        "updatedAt": "2025-09-30T01:05:21.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ghfbsd/qseis/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "TcheL/SeisTP",
        "url": "https://github.com/TcheL/SeisTP",
        "description": "Some seismology minitools and miniprograms",
        "stars": 1,
        "forks": 0,
        "readme": "# SeisTP\n\nSome seismology minitools and miniprograms.\n\n## License\n\n[The MIT License](http://tchel.mit-license.org)\n\n## PREM\n\n### Function\n\nUniformly discrete the Preliminary Reference Earth Model by the fixed depth interval (the thickness of layer).\n\n### Usage\n\nAs an example, you can just run the follow command in  **matlab**:\n\n```matlab\n>> hintv = 10;\n>> [h, vp, vs, rho] = PREM(hintv);\n```\n\nwhere `hintv` is the depth sampling interval, `h` is the depth from the surface of earth to the depth point, `vp` is the P-wave velocity at every depth point, `vs` is the S-wave velocity at every depth point, and `rho` is the density at every depth point.\n\n![PREM(10)](./figures/PREM-10.png)\n\n**NOTICE**: After the uniformly discretization of PREM, the velocity variation at the discontinuity interfaces may be NOT enough accurate. If you have a strict requirement, you can make a few adjustments at the interfaces.\n\n## ReadSAC\n\n### Function\n\nRead a SAC-formatted file.\n\n### Usage\n\nAs an example, you can just run the follow command in  **matlab**:\n\n```matlab\n>> [h, d] = ReadSAC('examples/IC.XAN.00.BH1.M.2016.036.195527.SAC')\n```\n\nwhere `h` is file header information of the example file _examples/IC.XAN.00.BH1.M.2016.036.195527.SAC_, and `d` is waveform data in the file.\n\n![ReadSAC('examples/IC.XAN.00.BH1.M.2016.036.195527.SAC')](./figures/IC.XAN_TW20160205.png)\n\n## US2016VM\n\n### Function\n\nRead USA velocity model from US.2016.nc distributed by [Weisen SHEN](http://weisen.wustl.edu/).\n\n### Usage\n\nFirstly, download the model netCDF file `US.2016.nc` from [the data link](http://ds.iris.edu/files/products/emc/emc-files/US.2016.nc). And for more details about the model, please visit [the mainpage](http://ds.iris.edu/ds/products/emc-us2016/).\n\nThen under the download directory start **matlab**, and keyboard input in the **matlab** command line window:\n\n```matlab\n>> pdep = [  0.0,  0.5, 150.0];\n>> plat = [ 20.0, 0.25,  50.0];\n>> plon = [235.0, 0.25, 295.0];\n>> [vdep, vlat, vlon, vsv, vp, rho] = US2016VM('US.2016.nc', pdep, plat, plon);\n```\n\nwhere `pdep`, `plat` and `plon` are setting as [start, stride, end], respectively corresponding to depth, latitude and longitude; `vdep`, `vlat` and `vlon` are respectively returned value of adapted depth, latitude and longitude;  `vsv`, `vp` and `rho` are respectively retured value of S wave velocity, P wave velocity and density.\n\nFinally, you can plot and further process with these data `vdep`, `vlat`, `vlon`, `vsv`, `vp` and `rho`.\n\n![US2016VM-Vp140](./figures/US2016VM-Vp140.png)\n\n",
        "createdAt": "2018-10-21T01:44:03.000Z",
        "updatedAt": "2022-10-31T23:49:10.000Z",
        "language": "MATLAB",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/TcheL/SeisTP/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "trgy1/Seismology-Signal-processing",
        "url": "https://github.com/trgy1/Seismology-Signal-processing",
        "description": "I have used basic signal processing concepts to analyse 2 different earthquake which one of them is shallow and other is deep",
        "stars": 1,
        "forks": 0,
        "readme": "This is the assignment for the course \"Signal Processing for Geosciences\" thought by Jerome Vergne\n\n",
        "createdAt": "2021-02-13T16:54:13.000Z",
        "updatedAt": "2023-01-07T11:07:50.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/trgy1/Seismology-Signal-processing/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "waidyanatha/openeew-seismology",
        "url": "https://github.com/waidyanatha/openeew-seismology",
        "description": null,
        "stars": 0,
        "forks": 10,
        "readme": "# OpenEEW seismological code\n\n## Introduction\n\nTo provide useful earthquake alerts, an Earthquake Early Warning (EEW) system has to recognize strong earthquakes quickly and reliably. The speed of the alert, that is, the time elapsed between the origin of an earthquake and the delivery of the alert to a user, determines the time available for the user to take protective action. The alert reliability is necessary for ensuring that the system issues alerts for all strong earthquakes and does not issue alerts for small earthquakes or no earthquakes at all.  \n\nThis code receives raw data from the seismic network (or uses historical data), detects and recognizes earthquake signals and evaluates the earthquake's magnitude and location. This is a short overview of the first version of the code.\n\n## Main processing components\nIn general, the seismological apparatus for recognition and characterization of earthquakes need to comprise four major components:\n*\tEarthquake detection: An algorithm for detection of significant (earthquake) shaking at individual seismic stations.\n*\tEvent association: An algorithm evaluating whether individual detections belong to a common source (earthquake).\n*\tMagnitude determination: An algorithm for magnitude estimation.\n*\tLocation determination: An algorithm for hypocentral/epicentral location estimation.\n\n## Overview of the code structure\nThere are four folders in the repo:\n*\t**data:** contains historical earthquake data and csv file with location of devices\n*\t**obj:** this is where events and travel time tables are saved\n*\t**src:** main codes that will run on the server\n*\t**utils:** helper codes for preparation of historical data, plotting events and detections etc.\n\n## Using historical data\nFor the testing purposes, the algorithm is set up to use historical data. The data from 16 intermediate-to-major earthquakes is stores in data/ folder.\n\n## Major code components\n\nThe diagram shows the basic flow of the program.\n\n![seismology_roadmap_v2_server](https://user-images.githubusercontent.com/37088604/111028616-08efe680-83f8-11eb-8cce-367c873da914.png)\n\n*\t### main.py\nThe program is run by running main.py. In its current form, it takes the historical data and supplies them to the algorithms. The loop runs once a second.\n\n*\t### params.py\nAll the parameters used by the program are defined in here.\n\n*\t### detection.py\n\nThe detection.py handles the detection of earthquake primary (P) earthquake waves as well as the station magnitude calculation.\n\n**Input:** Sensor data - three component records of ground motion acceleration, data arrive in 1 second increments  \n**Output:** Detections with station magnitude in the detections table in the db  \n**Detection:** The detection is using one of the following methods: STA/LTA - taking a ratio of short-term vs. long-term average of the signal and ML - a convolutional neural network that recognizes P waves. Currently, I recommend using STA/LTA as the ML model still needs some more work.  \n**Station magnitude:** Is based on the peak ground displacement (e.g. Lancieri and Zollo, 2008; Li et al., 2017; Wu and Zhao, 2006) (doing a double integration of the first 2-4 seconds of the P wave).\n\n* ### event.py\nThe event.py handles association, location and magnitude calculation of earthquakes.\n\n**Input:** Uses detections from the detections table in db  \n**Output:** Earthquake events saved in obj/events\n\nThe event.py creates an event instance of a class that stores all the important information about the event. You can see the structure in the diagram below.\n\n![event_scheme](https://user-images.githubusercontent.com/37088604/111028535-8404cd00-83f7-11eb-86ab-3e96e73ad175.png)\n\n**Event association:** Event association is a process that gathers individual phase picks into events and throws away picks that are not associated to earthquakes (and are probably just noise). \n\n**Event location:** We adopted real-time Bayesian evolutionary algorithm (e.g. Satriano et al., 2008). It is a grid search that tries to minimize misfits of P wave travel-time differences to different stations, i.e. it is independent of the origin time. It uses the concept of 'not yet arrived' data, which means that when you receive data at the first station, you already can have a very rough idea about the earthquake location.\n\n**Magnitude determination:** We used Bayesian approach to calculate the magnitude from the peak ground displacement in the window from 1  to 9 s. The parameters for the calculation were determined from ~1000 earthquake records recorded at Mexico network. This saturates at about M7.5.\n",
        "createdAt": "2021-03-16T04:10:12.000Z",
        "updatedAt": "2021-03-16T04:10:13.000Z",
        "language": null,
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/waidyanatha/openeew-seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ibrewster/volcano_seismology",
        "url": "https://github.com/ibrewster/volcano_seismology",
        "description": "Volcano Seismology website",
        "stars": 1,
        "forks": 0,
        "readme": "",
        "createdAt": "2021-05-10T21:22:57.000Z",
        "updatedAt": "2024-12-12T20:34:44.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "massimo1962/irods_landing_page",
        "url": "https://github.com/massimo1962/irods_landing_page",
        "description": "a tornado landing page with sub-sliced feature ( for seismological waveform)",
        "stars": 0,
        "forks": 0,
        "readme": "# irods_landing_page\n",
        "createdAt": "2018-08-31T09:01:30.000Z",
        "updatedAt": "2018-08-31T12:22:14.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/massimo1962/irods_landing_page/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "phekdey-pheng/Recent_developments_in_earthquake_seismology_present_and_fut",
        "url": "https://github.com/phekdey-pheng/Recent_developments_in_earthquake_seismology_present_and_fut",
        "description": "Auto-generated repository for Recent_developments_in_earthquake_seismology_present_and_fut",
        "stars": 0,
        "forks": 0,
        "readme": "# Recent_developments_in_earthquake_seismology_present_and_fut\n\nThis repository was auto-generated. \n\n## Description\n\nThis is a placeholder README for the Recent_developments_in_earthquake_seismology_present_and_fut repository.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details. \n\n## Author\n\nPhekdey PHORN \n\nCreated on 2025-08-30.\n\n## Contact\n\nFor any inquiries, please contact [Phekdey PHORN](+855 89755770). Thank you for visiting my GitHub profile!\n",
        "createdAt": "2025-08-29T18:10:28.000Z",
        "updatedAt": "2025-08-29T18:10:43.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/phekdey-pheng/Recent_developments_in_earthquake_seismology_present_and_fut/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "xtyangpsp/HypoInvPy",
        "url": "https://github.com/xtyangpsp/HypoInvPy",
        "description": "A Python interface for hypoinverse for relocating earthquakes",
        "stars": 15,
        "forks": 3,
        "readme": "# HypoInvPy\n*A Python interface for HypoInverse earthquake relocation software*\n\n![plot1](/figs/hypoinvpylogo.png)\n\nThis package is not intended to show documentation of how to use `hypoinverse`. Instead, we focus on buiding an user-friendly Python interface to run `hypoinverse`. For detailed documentation of `hypoinverse`, please read the documentation for hypoinverse in `hyp1.40/doc/hyp1.40.pdf`. The documentation for this interface package is in `doc/HypoInvPy_manual.pdf`.\n\n## Credit note\nThe codes and workflow in this package are modified and simplified from Hypo-Interface-Py (https://github.com/YijianZhou/Hypo-Interface-Py). The hypoinverse version is 1.4 and can be downloaded from USGS (https://www.usgs.gov/software/hypoinverse-earthquake-location). \n\n## Installation\n### Install as a pip package\nHypoInvPy is available on pypl as a standalone package. It could be installed as regular pip package: `pip install hypoinvpy`. The latest version is always available on GitHub.\n\n### Install with local copy from GitHub\nThis installation method will get the latest version from GitHub.\n\n1. Create and activate a Python virtual environment `hypoinv`. This is optional but recommended. This will help isolate the computational needs from other packages. This will also help avoid version incompatibility in case new packages for some codes are updated in the `base` conda environment. However, since `HypoInvPy` doesn't currently need complicated packages, running under the `base` environment may just work fine. \n```\n$ conda create -n hypoinv -c conda-forge jupyter numpy scipy pandas python cartopy obspy mpi4py\n```\nThen, activate the environment with: `$ conda activate hypoinv`\n\n2. Clone the github repository, in terminal under the desired directory:\n```\n$ git clone https://github.com/xtyangpsp/HypoInvPy.git\n```\n\n3. `cd` to the repository folder in terminal and install the package with the parameters in `setup.py`.\n```\n$ pip install .\n```\n\n4. Create jupyter notebook kernel with the environment (after activating the environment).\n```\n$ pip install --user ipykernel\n$ python -m ipykernel install --user --name=hypoinv\n```\n\n## Run the hypoinverse example:\n1. Make sure hypo1.40 has been installed on your computer. Download the version from the USGS website (https://www.usgs.gov/software/hypoinverse-earthquake-location). For your convinience, a copy of the hyp1.40 codes is available under folder `hyp1.40`.\n2. In terminal under the `example` directory, run the jupyter notebook `HypoInvPy_run_example.ipynb`\n3. The output directory includes the resultant earthquake catalogs.\n\n## Upcoming changes:\n* Add visualization functionalities, e.g., earthquake locations and depth distribution and temporal sequence, etc.\n\n\n\n\n",
        "createdAt": "2023-12-19T15:27:53.000Z",
        "updatedAt": "2025-07-14T07:03:30.000Z",
        "language": "Jupyter Notebook",
        "homepage": "https://github.com/xtyangpsp/HypoInvPy",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/xtyangpsp/HypoInvPy/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "AndrewReynen/Lazylyst",
        "url": "https://github.com/AndrewReynen/Lazylyst",
        "description": "Lazylyst is a GUI created for time series review, using a flexible framework for new workflows",
        "stars": 16,
        "forks": 3,
        "readme": "Check out Lazylyst's [show case](https://github.com/AndrewReynen/Lazylyst/wiki/Show-Case) and [wiki](https://github.com/AndrewReynen/Lazylyst/wiki)\n",
        "createdAt": "2017-01-10T23:21:50.000Z",
        "updatedAt": "2025-04-30T09:47:12.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/AndrewReynen/Lazylyst/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "fabian-kutschera/Seismo_Live",
        "url": "https://github.com/fabian-kutschera/Seismo_Live",
        "description": "Live Jupyter Notebooks for Seismology ",
        "stars": 0,
        "forks": 0,
        "readme": "# Seismo-Live\n\nThis repository contains scripts downloaded from Seismo-Live (http://seismo-live.org), with some changes compared to the original notebooks.\n",
        "createdAt": "2021-04-16T17:07:03.000Z",
        "updatedAt": "2021-11-11T07:40:01.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/fabian-kutschera/Seismo_Live/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "monikagonciarz/Seismology2",
        "url": "https://github.com/monikagonciarz/Seismology2",
        "description": "Seismology 2 project - MATLAB scripts",
        "stars": 0,
        "forks": 0,
        "readme": "# Seismology2\nSeismology 2 project - MATLAB scripts\n\n# This repository contains 4 MATLAB scripts:\n1. cmt_inversion.m\n\n   \nThis script contains the code used to perform a CMT source inversion on three-component waveform data, plus some additional calculations.\n\n\n2. cmt_inversion_v.m\n\n\nThis script contains the code used to perform a CMT source inversion on single-component (vertical) waveform data, plus some additional calculations.\n\n\n3. cmt_inversion_zero_trace.m\n\n\nThis script contains the code used to perform a CMT source inversion on three-component waveform data with imposed trace=0, plus some additional calculations.\n\n\n4. cmt_inversion_subset.m\n\n\nThis script contains the code used to perform a CMT source inversion on three-component waveform data on only 6 selected seismic stations closest to the epicenter(SJG, SACV, SFJD, ESK, BFO, PAB), plus some additional calculations.\n",
        "createdAt": "2025-04-01T11:11:05.000Z",
        "updatedAt": "2025-04-01T17:12:27.000Z",
        "language": "MATLAB",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/monikagonciarz/Seismology2/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "L-Kh-Hovhannisyan/iges.am.github.io",
        "url": "https://github.com/L-Kh-Hovhannisyan/iges.am.github.io",
        "description": "We present a systematic overview of trends, challenges, and opportunities in applications of deep-learning methods in geophysics and seismology.  Our team develop high quality educational programs in machine learning and data analysis in geophysics and seismology.",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2023-01-11T10:36:25.000Z",
        "updatedAt": "2023-01-10T15:40:13.000Z",
        "language": null,
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "AIsaacGeophysics/Volcano-Seismology",
        "url": "https://github.com/AIsaacGeophysics/Volcano-Seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2023-07-12T16:36:28.000Z",
        "updatedAt": "2023-07-12T16:36:28.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "uafgeotools/array_processing",
        "url": "https://github.com/uafgeotools/array_processing",
        "description": "Various array processing tools for infrasound and seismic data",
        "stars": 23,
        "forks": 12,
        "readme": "array_processing\n================\n\n[![](https://readthedocs.org/projects/uaf-array-processing/badge/?version=master)](https://uaf-array-processing.readthedocs.io/)\n\nVarious array processing tools for infrasound and seismic data. By default uses\nleast-squares to determine the trace velocity and back-azimuth of a plane wave\ncrossing an array in sliding time windows. More advanced processing (such as\nleast-trimmed squares) is easily integrated. Also provides tools to characterize\nthe array response, uncertainty, source-location of a spherical wave crossing\nthe array, etc. See\n[documentation](https://uaf-array-processing.readthedocs.io/) and\n[`example.py`](https://github.com/uafgeotools/array_processing/blob/master/example.py)\nfor more info.\n\n**General References and Suggested Citations**\n\n_Least squares and array uncertainty:_\n\nSzuberla, C. A. L., & Olson, J. V. (2004). Uncertainties associated with\nparameter estimation in atmospheric infrasound arrays, J. Acoust. Soc. Am.,\n115(1), 253–258.\n[https://doi.org/doi:10.1121/1.1635407](https://doi.org/doi:10.1121/1.1635407)\n\n_Least-trimmed squares:_\n\nBishop, J. W., Fee, D., & Szuberla, C. A. L. (2020). Improved infrasound array\nprocessing with robust estimators, Geophys. J. Int., 221 p. 2058-2074.\n[https://doi.org/10.1093/gji/ggaa110](https://doi.org/10.1093/gji/ggaa110)\n\nInstallation\n------------\n\nWe recommend you install this package into a new\n[conda](https://docs.conda.io/projects/conda/en/latest/index.html) environment.\n(Please install [Anaconda](https://www.anaconda.com/products/individual) or\n[Miniconda](https://docs.conda.io/en/latest/miniconda.html) before proceeding.)\nThe environment must contain all of the packages listed in the\n[Dependencies](#dependencies) section. For ease of installation, we've provided\nan\n[`environment.yml`](https://github.com/uafgeotools/array_processing/blob/master/environment.yml)\nfile which specifies all of these dependencies as well as instructions for\ninstalling _array_processing_ itself. To install _array_processing_ in this\nmanner, execute the following commands:\n```\ngit clone https://github.com/uafgeotools/array_processing.git\ncd array_processing\nconda env create -f environment.yml\n```\nThis creates a new conda environment named `uafinfra` and installs\n_array_processing_ and all of its dependencies there.\n\nThe final line in the `environment.yml` file installs _array_processing_ in \"editable\" mode, which\nmeans that you can update it with a simple `git pull` in your local repository.\nWe recommend you do this often, since this code is still under rapid\ndevelopment.\n\n<details>\n<summary>\nFor installation into a pre-existing conda environment, click here.\n</summary>\n<br>\nFirst ensure you have ObsPy and FastKML installed (<code>conda install -c conda-forge\nobspy fastkml</code>) and then download and install the <em>uafgeotools</em>\ndependencies and this package with:\n<br>\n<br>\n\n```\npip install git+https://github.com/uafgeotools/waveform_collection.git\npip install git+https://github.com/uafgeotools/lts_array.git\npip install git+https://github.com/uafgeotools/array_processing.git\n```\n(Note that this option does not produce a local clone of the repository.)\n</details>\n\nDependencies\n------------\n\n_uafgeotools_ packages:\n\n* [_waveform_collection_](https://github.com/uafgeotools/waveform_collection)\n* [_lts_array_](https://github.com/uafgeotools/lts_array)\n\nPython packages:\n\n* [ObsPy](https://docs.obspy.org/)\n* [FastKML](https://fastkml.readthedocs.io/)\n\nUsage\n-----\n\nImport the package like any other Python package, ensuring the correct\nenvironment is active. For example,\n```\n$ conda activate uafinfra\n$ python\n>>> import array_processing\n```\nDocumentation is available online\n[here](https://uaf-array-processing.readthedocs.io/). For a usage example, see\n[`example.py`](https://github.com/uafgeotools/array_processing/blob/master/example.py).\n\nNote: The `array_plot()` function does not allow both `sigma_tau` and dropped elements from least trimmed squares to be plotted.\n\nThe `sigma_tau` variable is an indicator of nonplanar propagation across an array (using all elements), and least trimmed squares drops element pairs that appear \"too far\" from planar. In this way, having a large `sigma_tau` value and having consistently dropped element pairs (while not the same) suggest a departure from the plane wave model.\n\n`sigma_tau` is only calculated when ordinary least squares (`ALPHA=1.0`) is used. The ability to plot one or the other was intended as a safeguard against potentially conflicting processing assumptions. To maintain a consistent output data structure, the `sigma_tau` key returns a `np.nan` in the case that subset pairs are trimmed (0.5 <= `ALPHA` < 1.0).\n\nIf `ALPHA=1.0`, the dropped stations are not plotted since least trimmed squares is not used, and `sigma_tau` may be plotted if specified. If `ALPHA` < 1.0, then `sigma_tau` is not plotted or calculated.\n\n\nAuthors\n-------\n\n(_Alphabetical order by last name._)\n\nJordan Bishop<br>\nDavid Fee<br>\nCurt Szuberla<br>\nLiam Toney<br>\nAndrew Winkelman\n",
        "createdAt": "2019-09-27T17:55:07.000Z",
        "updatedAt": "2025-07-17T19:59:49.000Z",
        "language": "Python",
        "homepage": "https://uaf-array-processing.readthedocs.io/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/uafgeotools/array_processing/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "pstafford/GroundMotionModels.jl",
        "url": "https://github.com/pstafford/GroundMotionModels.jl",
        "description": "Julia implementation of common ground-motion models employed within Engineering Seismology",
        "stars": 0,
        "forks": 0,
        "readme": "# GroundMotionModels\n\n[![Stable](https://img.shields.io/badge/docs-stable-blue.svg)](https://pstafford.github.io/GroundMotionModels.jl/stable)\n[![Dev](https://img.shields.io/badge/docs-dev-blue.svg)](https://pstafford.github.io/GroundMotionModels.jl/dev)\n[![Build Status](https://github.com/pstafford/GroundMotionModels.jl/workflows/CI/badge.svg)](https://github.com/pstafford/GroundMotionModels.jl/actions)\n[![Coverage](https://codecov.io/gh/pstafford/GroundMotionModels.jl/branch/main/graph/badge.svg)](https://codecov.io/gh/pstafford/GroundMotionModels.jl)\n\n## Citing\n\nSee [`CITATION.bib`](CITATION.bib) for the relevant reference(s).\n",
        "createdAt": "2021-10-13T10:57:38.000Z",
        "updatedAt": "2022-06-09T12:31:21.000Z",
        "language": "Julia",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/pstafford/GroundMotionModels.jl/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "namedummy568-stack/seismology-classroom-activity",
        "url": "https://github.com/namedummy568-stack/seismology-classroom-activity",
        "description": "Resources for a classroom activity on seismology, including instructions and data.",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2025-09-18T11:32:06.000Z",
        "updatedAt": "2025-09-18T11:32:26.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "anger1087/hiwi",
        "url": "https://github.com/anger1087/hiwi",
        "description": "HiWi position in computational seismology",
        "stars": 0,
        "forks": 0,
        "readme": "# hiwi\nHiWi position in computational seismology\n",
        "createdAt": "2016-11-19T10:22:05.000Z",
        "updatedAt": "2016-11-19T15:21:47.000Z",
        "language": "TeX",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/anger1087/hiwi/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "srivastavaresearchgroup/SAIPy",
        "url": "https://github.com/srivastavaresearchgroup/SAIPy",
        "description": null,
        "stars": 17,
        "forks": 7,
        "readme": "# SAIPy\nSeismology has witnessed significant advancements in recent years with the application of deep learning methods to address a broad range of problems. These techniques have demonstrated their remarkable ability to effectively extract statistical properties from extensive datasets, surpassing the capabilities of traditional approaches to an extent. In this repository we present SAIPy, an open-source Python package developed for fast seismic waveform data processing by implementing deep learning. SAIPy offers solutions for multiple seismological tasks such as earthquake detection, magnitude estimation, seismic phase picking, and polarity identification. This brings together the capabilities of previously published models such as [CREIME](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2022JB024595), [DynaPicker](https://arxiv.org/abs/2211.09539v1?trk=public_post_main-feed-card_feed-article-content) and [PolarCAP](https://www.sciencedirect.com/science/article/pii/S2666544122000247) and introduces upgraded versions of previously published models such as CREIME_RT capable of identifying earthquakes with an accuracy above 99.8% and a root mean squared error of 0.38 unit in magnitude estimation. These upgraded models outperform state-of-the-art approaches like the Vision Transformer network. SAIPy provides an API that simplifies the integration of these advanced models with benchmark datasets like STEAD and INSTANCE.\n\n# Version 2.0.0 Release Notes\nIn this release, the package uses CREIME_RT, DynaPicker_v2, and PolarCAP for the analysis of seismograms in a single-station setting. Optionally, this version introduces a new approach for analyzing the results of these models across multiple stations, making use of a seismic network database.\nFor further details and usage instructions, please refer to the documentation (see below).\n\n## Installation\nTo install this package clone this repository using \n\n    git clone https://github.com/srivastavaresearchgroup/SAIPy.git\n\n  It is recommended that you create a virtual environment to install SAIPy. To do this, create a folder, create a virtual environment in that folder, and activate the environment:\n     \n    mkdir SAIPy_venv\n    python3 -m venv SAIPy_venv\n    source SAIPy_venv/bin/activate\n  \n  Then change working directory to SAIPy and run the following command:\n\n    python3 -m pip install .\n\n  Make sure to use the correct version of python installed in your system for the above command (example for python3).\n\n## Documentation\nThe documentation for using the last updates of the SAIPy package is in [SAIPy_Documentation_v2.0.0.pdf](https://github.com/srivastavaresearchgroup/SAIPy/blob/main/SAIPy_Documentation_v2.pdf). Additionally, you can test the examples 7, 8 and 9, that we provide in (https://github.com/srivastavaresearchgroup/SAIPy/blob/main/examples_v2.0.0/).\n\n\n## Reach out to us\nWe strive to constantly improve and update SAIPy.\n\nShould you have any queries or suggestions do not hesitate to contact the authors:\n* Megha Chakraborty (chakraborty@fias.uni-frankfurt.de)\n* Wei Li (wli@fias.uni-frankfurt.de)\n* Claudia Quinteros Cartaya (quinteros@fias.uni-frankfurt.de / quinterosclaudia@gmail.com)\n* Jonas Köhler (jkoehler@fias.uni-frankfurt.de)\n* Johannes Faber (faber@fias.uni-frankfurt.de)\n* Nishtha Srivastava-Team leader (srivastava@fias.uni-frankfurt.de)\n  \n## Citation\nCite as:\n\nWei Li, Megha Chakraborty, Claudia Quinteros-Cartaya, Jonas Köhler, Johannes Faber, Men-Andrin Meier, Georg Rümpker, Nishtha Srivastava. SAIPy: A Python package for single-station earthquake monitoring using deep learning. Computers & Geosciences, Volume 192, 2024, 105686, ISSN 0098-3004, https://doi.org/10.1016/j.cageo.2024.105686.\n",
        "createdAt": "2022-11-23T10:28:47.000Z",
        "updatedAt": "2025-11-07T02:33:52.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/srivastavaresearchgroup/SAIPy/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "JuliaGeophysics/SeismologyGeophysics.jl",
        "url": "https://github.com/JuliaGeophysics/SeismologyGeophysics.jl",
        "description": "Seismology research, development and applications in Julia ",
        "stars": 0,
        "forks": 0,
        "readme": "# SeismologyGeophysics.jl\nSeismology research, development and applications in Julia \n",
        "createdAt": "2025-03-08T13:18:48.000Z",
        "updatedAt": "2025-03-08T13:18:52.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/JuliaGeophysics/SeismologyGeophysics.jl/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "earthinversion/Compare-Synthetics-for-Earth-Models-Ocean-Effects",
        "url": "https://github.com/earthinversion/Compare-Synthetics-for-Earth-Models-Ocean-Effects",
        "description": "Benchmarks for the effects of ocean. Compared between global solver SPECFEM3D Globe, regional solver RegSEM, and normal-mode calculations",
        "stars": 1,
        "forks": 0,
        "readme": "## How to run?\n1. Edit the [input_params.yml](input_params.yml)\n1. Run the script [`check_traces_single_station.py`](check_traces_single_station.py)\n    ```\n    python check_traces_single_station.py\n    ```\n\n## Dependencies\n1. Numpy\n1. Matplotlib\n1. Obspy\n1. Pyyaml\n1. Pandas\n\n## Tested Cases\n- Compared the Specfem Layered with Polynomial: matches fairly well\n- Compared the Specfem Layered with ocean vs Specfem layered without ocean\n- Compared RegSEM with Normal-mode synthetics \n- Compared RegSEM with SPECFEM for no ocean, no topography: matches very well\n- Compared the SPECFEM runs for R_EARTH = 6371 and 6368 km\n- Compared SPECFEM with/without elevated receivers\n- Compared SPECFEM with SPECFEM version with etopo values 0\n\n## Input parameters\nModel: **Preliminary Earth Reference Model**, with the ocean layer of thickness 3 km\n* Spherical Earth: ON\n* Anisotropy: ON\n* **Oceans: ON**\n* Topography: ON\n* Gravity: OFF\n* Attenuation: OFF\n* Ellipticity: OFF\n\n## Input Event and Stations Locations\n<hr>\n<p align=\"center\">\n<img src=\"event_station_map.png\" alt=\"Event-Stations Map\" />\n</p>\n<p align=\"center\"><b>Event-Stations Map</b></p>\n<hr>\n\n## Results\n- Comparison between SPECFEM, RegSEM and NMS (Normal-mode synthetics) with OCEAN, anisotropy and Topography. No mirror for the cases of SPECFEM and RegSEM.\n\n<hr>\n<p align=\"center\">\n<img src=\"example_comp_nms_regsem_specfem/BBB_time.png\" alt=\"BBB Time\" />\n</p>\n<p align=\"center\"><b>BBB Time</b></p>\n<hr>\n\n<p align=\"center\">\n<img src=\"example_comp_nms_regsem_specfem/BBB_frequency.png\" alt=\"BBB Frequency\" />\n</p>\n<p align=\"center\"><b>BBB Frequency</b></p>\n<hr>",
        "createdAt": "2022-02-27T22:37:00.000Z",
        "updatedAt": "2022-03-03T00:34:31.000Z",
        "language": "Gnuplot",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/earthinversion/Compare-Synthetics-for-Earth-Models-Ocean-Effects/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "colej/bsg_seismology",
        "url": "https://github.com/colej/bsg_seismology",
        "description": "bsg_seismology  This is a collection of codes / routines that will be used to detect and characterise asteroseismic signatures in pulsating blue super giant stars",
        "stars": 0,
        "forks": 0,
        "readme": "# bsg_seismology\nThis is a collection of codes / routines that will be used to detect and characterise asteroseismic signatures in pulsating blue super giant stars observed with TESS.  \n\n### Code author:\n  - Cole Johnston\n\n### Project 1\n\n  - Lead:\n    - Linhao Ma\n\n  - Supervisors:\n    - Cole Johnston\n    - Earl Bellinger\n    - Selma de Mink\n    - Jim Fuller\n  - Published as: \n    - https://ui.adsabs.harvard.edu/abs/2023arXiv231019546M/abstract\n\n### Project 2: \n  - Lead: \n    - Cole Johnston\n  - Collaborators:\n    - Selma de Mink\n    - Earl Bellinger\n\n## Notebooks\n\nI'll keep a few example notebooks for how to use the various functions in the repo.\n\n  - example_01: At the moment, the first example uses functions stripped from LATTE (by N. Eisner) that are modified to work for our specific use case. \n  - example_02: Notebook that fits the power excess for a given BSG using a simple Harvey profile + white noise + gaussian excess component\n\n\n## Installation\n\nI've created an environment held in bsg.yml. This has all of the packages required for this project and they all have been checked for \nthe appropriate dependencies. To install this environment, you must first EDIT THE PREFIX at the bottom of the yml file to point to your\ninstallation of (ana/mini)conda. After you have modified this, you can you can issue:\n\nconda env create -f bsg.yml\n\nto create and populate the virtual environment. After this, you can launch the environment using either:\n\nsource activate bsg\n\n ### OR\n\nconda activate bsg\n\ndepending on how you've set up (ana/mini)conda.\n\n### Non-conda installs\nFurthermore, we need to use the latest development version of astroplan. This can be accomplished by downloading it directly from \ngithub by issuing:\n\ngit clone https://github.com/astropy/astroplan\n\npython setup.py build\npython setup.py install\n\nThis will install the package in the relevant location so long as you do this while the bsg environment is activated.\n\nFinally, we will need the pythia package:\n  git clone https://github.com/colej/pythia\n\nThe package has its own install instructions that can be found on the page.\n",
        "createdAt": "2023-07-10T09:07:37.000Z",
        "updatedAt": "2024-06-07T10:11:02.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/colej/bsg_seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ValentinCassayre/pointe-sismo",
        "url": "https://github.com/ValentinCassayre/pointe-sismo",
        "description": "Projet informatique de 1A à EOST sur le pointé automatique du temps d'arrivé des ondes P et S.",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2023-01-02T16:37:58.000Z",
        "updatedAt": "2023-05-05T18:03:40.000Z",
        "language": "TeX",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "yellowagents/rattler",
        "url": "https://github.com/yellowagents/rattler",
        "description": "Receiver for seismological network datagrams",
        "stars": 7,
        "forks": 2,
        "readme": "",
        "createdAt": "2010-05-25T10:41:34.000Z",
        "updatedAt": "2014-09-16T13:46:16.000Z",
        "language": "Python",
        "homepage": "http://yellowagents.com/seismometer/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "wangliang1989/oh-my-cap",
        "url": "https://github.com/wangliang1989/oh-my-cap",
        "description": null,
        "stars": 41,
        "forks": 16,
        "readme": "# Oh My CAP\n\nCAP 是计算震源机制的一种常用方法，\n很多地震同行都需要学习、使用和研究它。\n我基于 gCAP1.0 建立了 Oh My CAP 这个开源项目，\n总结整理了在使用 CAP 中的经验，\n希望能够为大家提供一个学习、探讨 CAP 的平台。\n本项目始自 2016 年，我在桂林理工大学读硕士学位期间。\n\n**项目主页：http://seiswave.cn/oh-my-cap/**\n\n## 作者\n\n王亮 贺州学院公共基础教学部\n\n## 特别贡献者\n\n田冬冬 中国地质大学（武汉）\n\n## 版权协议\n\n本项目中所使用的 fk 和 gcap 的源码修改自朱露培的原始代码。\n这部分代码遵循 GPL 协议，任何人均可免费获取、使用、修改和再发布代码，\n但是修改后的版本必须也按 GPL 协议公开和授权。\n\n本项目中的其余源码和文档采用更加宽松的 Apache 协议，\n即在尊重本项目署名权的前提下，可以选择不公开自己的修改。\n\n## 引引我的论文吧\n\n如果本项目帮助了你，请引用我的论文。\n有一些同学不愿意引用中文论文，或者不愿意引用非大牛的论文。\n同学，讲点良心吧。\n以下是我的论文的中英文信息：\n\n> 王亮, 薛霆虓, 季海磊. 2016. 集集强余震震源机制解分析[J]. 地球物理学进展. 31(5): 1998-2004\n\n> Wang L, Xue T X, Ji H L. 2016. Focal mechanisms of Taiwan ChiChi earthquake aftershocks[J]. Progress in Geophysics. 31(5): 1998-2004\n\n这篇论文可以在地球物理学进展的官网下载：\nhttp://www.progeophys.cn/CN/10.6038/pg20160515\n\n另外，我也提供我的所有第一作者论文和学位论文。\n在我的\n[坚果云网盘](https://www.jianguoyun.com/p/DSgkfDkQ5s_iCRjznPMEIAA)\n中可以找到这篇论文，也可以找到我的以 CAP 为主题的硕士论文。\n",
        "createdAt": "2016-08-13T11:44:59.000Z",
        "updatedAt": "2025-09-26T14:16:53.000Z",
        "language": "C",
        "homepage": "http://seiswave.cn/oh-my-cap/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/wangliang1989/oh-my-cap/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "JesusGarSan/MSPC-Seismic",
        "url": "https://github.com/JesusGarSan/MSPC-Seismic",
        "description": "MSPC application to seismic data developed as part of the DigiVolCan project.",
        "stars": 2,
        "forks": 1,
        "readme": "# MSPC-Seismic: Multivariate Statistical Process Monitoring System for Seismic Processes\n\nMSPC-Seismic is a software system developed for the Multivariate Statistical Process Monitoring (MSPC) of seismic data. It was created as part of the DigiVolCan: A digital infrastructure for eruption forecast in the Canary Islands project (Ref. PLEC2022-009271).\n\nThe primary goal of this platform is to aggregate and process data from the various seismic sensors in the Canary Islands belonging to the Involcan network. The system provides a unified platform for observing real-time updates and performing interpretable diagnostics of seismic measurements using multivariate statistical techniques.\n\nThe system is managed through an interactive streamlit based web application that allows for the joint visualization and analysis of signals from multiple seismic stations. For more information on the installation and usage of the tool, refer to the pdf manual in the repository.\n\n![DigiVolCan Logo](https://github.com/user-attachments/assets/a0c83808-8126-4d4d-8e56-8f6ced681b95)\n\n\n### Authors and Contact\n\n    Developed by: Jesús García Sánchez and Daniel Vallejo España \n\n    Supervised by: José Camacho Páez\n\n    Contact Email: gsus@ugr.es\n\n",
        "createdAt": "2024-06-12T10:20:59.000Z",
        "updatedAt": "2025-12-01T14:47:48.000Z",
        "language": "Python",
        "homepage": "https://codas.ugr.es/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/JesusGarSan/MSPC-Seismic/package/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "phekdey-pheng/Recent_developments_in_earthquake_seismology",
        "url": "https://github.com/phekdey-pheng/Recent_developments_in_earthquake_seismology",
        "description": "Auto-generated repository for Recent_developments_in_earthquake_seismology",
        "stars": 0,
        "forks": 0,
        "readme": "# Recent_developments_in_earthquake_seismology\n\nThis repository was auto-generated. \n\n## Description\n\nThis is a placeholder README for the Recent_developments_in_earthquake_seismology repository.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details. \n\n## Author\n\nPhekdey PHORN \n\nCreated on 2025-08-30.\n\n## Contact\n\nFor any inquiries, please contact [Phekdey PHORN](+855 89755770). Thank you for visiting my GitHub profile!\n",
        "createdAt": "2025-08-29T18:09:44.000Z",
        "updatedAt": "2025-08-29T18:09:57.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/phekdey-pheng/Recent_developments_in_earthquake_seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "spin-itn/SeismoMate",
        "url": "https://github.com/spin-itn/SeismoMate",
        "description": "\"SeismoMate\" is a repository of useful links to coding and data management resources specifically curated for geophysicists in seismology.",
        "stars": 2,
        "forks": 0,
        "readme": "# SeismoMate\n\nThe \"SeismoMate\" repository is a simple collection of useful links to coding and data management resources for geophysicists working in the field of seismology. It aims to provide easy access to valuable resources that can help seismologists improve their coding skills and manage their data more effectively.\n\nIn the future, SeismoMate may add some tutorials and sample projects to help seismologists develop good coding practices and improve their documentation skills. However, the main focus of the repository is to provide a curated list of links that can help seismologists find useful resources quickly and easily.\n\nWhether you are an experienced seismologist or a beginner in the field, SeismoMate can be a helpful resource to enhance your coding and data management skills.\n\n\n# Structure\n\nFor now the project is just simply a GitHub repository with folders and Readme's to keep it as simple as possible. For the future the aim is to host all the information in an easily accesible website.\n\n# How to commit\n\nThe easiest way to commit is to use Github.dev, which allows editing a repository in your browser without cloning or downloading the repository.\n\n\nTo access Github.dev change the link of the repository from `github.com/spin-itn/SeismoMate` to `github.dev/spin-itn/SeismoMate` or just click on the following link: [github.dev/spin-itn/SeismoMate](https://github.dev/spin-itn/SeismoMate)\n\nThis opens a web VScode envirnoment in which you can change the markdown files making up the repositories as you like and commit any changes directly.",
        "createdAt": "2023-03-28T16:32:48.000Z",
        "updatedAt": "2024-04-19T19:54:51.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/spin-itn/SeismoMate/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "mgsuarezp/Seismology-",
        "url": "https://github.com/mgsuarezp/Seismology-",
        "description": "Some scripts about seismology and data science for seismic signals. ",
        "stars": 0,
        "forks": 0,
        "readme": "# Seismology-\nSome scripts about seismology and data science for seismic signals. \n",
        "createdAt": "2020-08-20T19:55:16.000Z",
        "updatedAt": "2020-08-20T20:01:02.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/mgsuarezp/Seismology-/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "hemmelig/AnisotropIO",
        "url": "https://github.com/hemmelig/AnisotropIO",
        "description": "Toolkit for interfacing with shear-wave splitting codes.",
        "stars": 1,
        "forks": 1,
        "readme": "<p align=\"center\">\n  <!-- DOI -->\n  <a href=\"https://doi.org/XX.XXXX/zenodo.XXXXXXX\">\n    <img src=\"https://zenodo.org/badge/DOI/XX.XXXX/zenodo.XXXXXXX.svg\" />\n  </a>\n  <!-- Build Action -->\n  <a href=\"https://github.com/hemmelig/AnisotropIO\">\n    <img src=\"https://img.shields.io/github/actions/workflow/status/hemmelig/anisotropio/build_wheels.yml?branch=main\" />\n  </a>\n  <!-- ReadTheDocs -->\n  <a href=\"https://anisotropio.readthedocs.io/en/latest\">\n    <img src=\"https://readthedocs.org/projects/anisotropio/badge/?version=latest\" />\n  </a>\n  <!-- PyPI -->\n  <a href=\"https://pypi.org/project/anisotropio/\">\n    <img src=\"https://img.shields.io/pypi/v/anisotropio\" />\n  </a>\n  <!-- Python version-->\n  <a href=\"https://www.python.org/downloads/release/python-380/\">\n    <img src=\"https://img.shields.io/badge/python-3.8+-blue.svg\" />\n  </a>\n  <!-- License -->\n  <a href=\"https://www.gnu.org/licenses/gpl-3.0\">\n    <img src=\"https://img.shields.io/badge/License-GPLv3-blue.svg\" />\n  </a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://seismicanisotropy.readthedocs.io/en/latest/index.html\">AnisotropIO</a> is a cross-platform Python toolkit for parsing, composing, and translating between\ninputs/outputs for various shear-wave splitting analysis codes.\n</p>\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/hemmelig/AnisotropIO/main/docs/img/anisotropio_logo.png\" width=\"80%\" />\n</p>\n\nKey features\n------------\nAnisotropIO is a sibling Python package to [AnisotroPy](https://github.com/hemmelig/AnisotroPy), providing a standard, unified file format and internal representation used for shear-wave splitting analyses.\n\nAnisotropIO currently provides parsers for:\n\n- MFAST\n- SplitRacer\n\nMore to come.\n\nDocumentation\n-------------\nDocumentation for AnisotropIO is hosted [here](https://anisotropio.readthedocs.io/en/latest/index.html).\n\nInstallation\n------------\nAnisotropIO requires Python version 3.8 and above. Installation of AnisotropIO, including all dependencies, can be done using pip:\n\n```console\npip install --extra_index_url https://test.pypi.org/simple/ anisotropio\n```\n\nFor further information regarding installation—including virtual environment management and installation from source—please consult [our documentation](https://anisotropio.readthedocs.io/en/latest/installation.html).\n\nCitation\n--------\nIf you use AnisotropIO in your work, please cite the following:\n\n```console\nBacon, C. A. (2023). AnisotropIO (Version 0.0.1) [Computer software]. https://doi.org/XX.XXXX/zenodo.XXXXXXX\n```\n\nContributing to AnisotropIO\n---------------------------\nContributions to AnisotropIO are welcomed. The first stop should be to reach out, either directly or—preferably—via the GitHub Issues panel, to discuss the proposed changes. Next, simply fork the AnisotropIO repository, make your changes/add your new contribution, then make a [pull request](https://help.github.com/articles/about-pull-requests/). All contributors to AnisotropIO will be listed as authors on the releases.\n\nBug reports, suggestions for new features and enhancements, and even links to projects that have made use of AnisotropIO are most welcome.\n\nSee our [contributions page](https://github.com/hemmelig/AnisotropIO/blob/main/.github/CONTRIBUTING.md) for more information.\n\nContact\n-------\nAny comments/questions can be directed to:\n* **Conor Bacon** - cbacon [ at ] ldeo.columbia.edu\n\nLicense\n-------\nAnisotropIO is **free** and **open source**, distributed under the GPLv3 License. Please see the [LICENSE](LICENSE) file for a complete description of the rights and freedoms that this provides the user.\n",
        "createdAt": "2022-06-05T20:56:01.000Z",
        "updatedAt": "2023-04-03T16:13:01.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/hemmelig/AnisotropIO/main/README.md",
        "mainPaper": {
            "doi": "",
            "title": "AnisotropIO",
            "dateReleased": "2023-04-02T00:00:00.000Z"
        },
        "repoDoi": "",
        "publications": [
            {
                "doi": "",
                "name": "AnisotropIO",
                "source": "",
                "authorNames": [],
                "publicationDate": "2023-04-02T00:00:00.000Z"
            }
        ]
    },
    {
        "source": "GitHub",
        "name": "Thomas-Ulrich/StressInversionNotebook",
        "url": "https://github.com/Thomas-Ulrich/StressInversionNotebook",
        "description": "stress inversion notebook for seismology class",
        "stars": 9,
        "forks": 2,
        "readme": "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/Thomas-Ulrich/StressInversionNotebook/HEAD?labpath=StressInversionNotebook.ipynb)\n# StressInversionNotebook\nThis notebook was compiled by Thomas Ulrich and provides exercises on stress inversion. It is part of the Master's level \"P7.2 Seismology\" lecture at LMU Munich taught by Alice-Agnes Gabriel.\n",
        "createdAt": "2020-05-08T16:55:42.000Z",
        "updatedAt": "2025-08-30T12:04:10.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Thomas-Ulrich/StressInversionNotebook/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "gianca1994/seismology-web",
        "url": "https://github.com/gianca1994/seismology-web",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# seismology-web",
        "createdAt": "2022-05-13T14:55:54.000Z",
        "updatedAt": "2022-08-01T19:50:51.000Z",
        "language": "HTML",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/gianca1994/seismology-web/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "workshop-earth/3d-event-engine",
        "url": "https://github.com/workshop-earth/3d-event-engine",
        "description": "Tools to visualize seismology concepts and data.",
        "stars": 1,
        "forks": 0,
        "readme": "# 3D Seismic Event Engine\nThis D3 powered visualization plots earthquake data in [3D space.](https://github.com/Niekes/d3-3d)\n\n* Each individual plotted point represents a unique/measured earthquake event\n* X/Y/Z axes represent physical relationships in KM\n* Point radius represents event magnitude (minimum magnitude visualized is adjustable)\n* Color is scaled across time while history range is adjustable\n* X/Z axes labels are toggleable for visibility\n* Time-based playback can be paused/scrubbed/replayed\n\n## Preview\n[![Netlify Status](https://api.netlify.com/api/v1/badges/8c38e76a-9345-4978-ab2f-dcd53c6d8f6a/deploy-status)](https://app.netlify.com/sites/3d-event-engine/deploys)  \nhttps://3d-event-engine.netlify.app/\n\n## Development\n`npm install` for dependencies\n\n`gulp` defaults to a full build (compiles JS & CSS)\n\n`http-server ./` for local development. Navigate to `public/` folder.\n\n## 2019 Ridgecrest Earthquake\nOn July 5th 2019 a magnitude 7.1 earthquake struck Ridgecrest, CA. Effects were felt for miles and aftershocks were numerous. It's scale is visualized here.\n\nCurrently the engine is hard-coded with this specific dataset. Future roadmap for this engine is to support dataset import.\nhttps://3d-event-engine.netlify.com/",
        "createdAt": "2019-07-09T17:18:22.000Z",
        "updatedAt": "2022-06-22T22:48:43.000Z",
        "language": "JavaScript",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/workshop-earth/3d-event-engine/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "earthscopeoceans/mmsoftware",
        "url": "https://github.com/earthscopeoceans/mmsoftware",
        "description": "Routines pertaining to MERMAID seismological data analysis. Written by Guust Nolet.",
        "stars": 2,
        "forks": 0,
        "readme": "# mmsoftware\nRoutines pertaining to MERMAID seismological data analysis.\n\nWritten by Guust Nolet.\n",
        "createdAt": "2023-02-08T21:31:51.000Z",
        "updatedAt": "2024-05-01T03:35:09.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/earthscopeoceans/mmsoftware/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "sharmamd/Environmental-seismology-presentations",
        "url": "https://github.com/sharmamd/Environmental-seismology-presentations",
        "description": "Literature review on environmental seismology done at IISc bengaluru during internship.",
        "stars": 0,
        "forks": 0,
        "readme": "I conducted an extensive literature review, concentrating on a range of environmental seismology topics, such as fluvial seismology, volcano seismology, and glacial seismology. I presented case studies for each of these subjects.\nIn the realm of fluvial seismology, my focus was on analyzing seismic noise along the Trisuli River in Nepal. Shifting to volcano seismology, I delved into a case study centered on the Kilauea Volcano in 2018. Finally, I scrutinized a \ncase study involving the Chamoli disaster in India in 2021, where a massive ice and rock avalanche occurred in the Himalayas.\n",
        "createdAt": "2023-10-23T20:44:53.000Z",
        "updatedAt": "2024-01-24T06:34:28.000Z",
        "language": null,
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/sharmamd/Environmental-seismology-presentations/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "krischer/cig_llnl_computational_seismology_workshop_obspy_instaseis",
        "url": "https://github.com/krischer/cig_llnl_computational_seismology_workshop_obspy_instaseis",
        "description": null,
        "stars": 13,
        "forks": 7,
        "readme": "# ObsPy and Instaseis Tutorials for the CIG/LLNL Workshop in Computational Seismology\n\n## Installation\n\nYou basically need a version of Python (best 3.5 or 3.6), ObsPy (http://obspy.org), the Jupyter notebooks (http://jupyter.org/), and Instaseis (http://instaseis.net).\n\nWe strongly recommend you install via `conda`. Follow these instructions here to install `conda` as well as `ObsPy`: https://github.com/obspy/obspy/wiki/Installation-via-Anaconda\n\nOnce this is installed, you can install the Jupyter notebooks and Instaseis with:\n\n```bash\n$ conda install -c conda-forge instaseis jupyter basemap-data-hires\n```\n\nPlease note that we currently have no Windows packages for `instaseis` so please just install it as if you are running windows.\n\nAlternatively you can also use a bundled `conda` installer that comes with ObsPy and the Jupyter notebook: https://github.com/obspy/obspy/wiki/Installation-via-ObsPy-bundled-Anaconda-installer\n\nIn that case Instaseis must also be installed:\n\n```bash\n$ conda install -c conda-forge instaseis\n```\n\n## Running the Notebooks\n\nMake sure the correct `conda` environment is active:\n\n```bash\n$ source activate NAME_OF_ENV\n```\n\nThen launch the notebooks from within the repository folder:\n\n```bash\n$ git clone https://github.com/krischer/cig_llnl_computational_seismology_workshop_obspy_instaseis.git\n$ cd cig_llnl_computational_seismology_workshop_obspy_instaseis\n$ juypter notebook\n```\n\nThis should open your web browser and you can now navigate to open the notebook\nof your choice.\n\n## Updating the Repository\n\n\n**WARNING:** This will delete any of your changes by first resetting the\nrepository and then updating it. Do not do this if you still need any of it;\notherwise make a backup first.\n\n```bash\n$ git clean -fd  # Removes all files not tracked by git.\n$ git reset --hard HEAD  # Reset the repository.\n$ git pull  # Updates it.\n```\n",
        "createdAt": "2017-08-10T21:40:16.000Z",
        "updatedAt": "2024-08-01T14:19:39.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/krischer/cig_llnl_computational_seismology_workshop_obspy_instaseis/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "michpaulatto/ElasticC",
        "url": "https://github.com/michpaulatto/ElasticC",
        "description": "Elastic properties calculator using effective medium theory for composite materials",
        "stars": 9,
        "forks": 3,
        "readme": "",
        "createdAt": "2020-06-05T14:31:05.000Z",
        "updatedAt": "2024-05-22T09:33:57.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "jpjones76/SeisIO.jl",
        "url": "https://github.com/jpjones76/SeisIO.jl",
        "description": "Julia language support for geophysical time series data",
        "stars": 46,
        "forks": 20,
        "readme": "# SeisIO.jl\n[![Build Status](https://travis-ci.org/jpjones76/SeisIO.jl.svg?branch=main)](https://travis-ci.org/jpjones76/SeisIO.jl) [![Build status](https://ci.appveyor.com/api/projects/status/ocilv0u1sy41m934/branch/master?svg=true)](https://ci.appveyor.com/project/jpjones76/seisio-jl/branch/master) [![codecov](https://codecov.io/gh/jpjones76/SeisIO.jl/branch/master/graph/badge.svg)](https://codecov.io/gh/jpjones76/SeisIO.jl)[![Coverage Status](https://coveralls.io/repos/github/jpjones76/SeisIO.jl/badge.svg?branch=master)](https://coveralls.io/github/jpjones76/SeisIO.jl?branch=master) [![Documentation Status](https://readthedocs.org/projects/seisio/badge/?version=latest)](https://seisio.readthedocs.io/en/latest/?badge=latest)\n[![Project Status: Active – The project has reached a stable, usable state and is being actively developed.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)\n\nA minimalist, platform-agnostic package for univariate geophysical data.\n\n## Installation | [Documentation](http://seisio.readthedocs.org)\nFrom the Julia prompt, type: `] add SeisIO`; (Backspace); `using SeisIO`\n\n## Summary | [Collaboration](docs/CONTRIBUTE.md)\nDesigned for speed, efficiency, and ease of use. Includes web clients, readers for common seismic data formats, and fast file writers. Utility functions allow time synchronization, data merging, padding time gaps, and other basic data processing.\n\n* Web clients: SeedLink, FDSN (dataselect, event, station), IRIS (TauP, timeseries)\n* File formats: ASDF (r/w), Bottles, GeoCSV (slist, tspair), QuakeML (r/w), SAC (r/w), SEED (dataless, mini-SEED, resp), SEG Y (rev 0, rev 1, PASSCAL), SLIST, SUDS, StationXML (r/w), Win32, UW\n\n## Getting Started | [Formats](docs/FORMATS.md) | [Web Clients](docs/WEB.md)\nStart the tutorials in your browser from the Julia prompt with\n\n```julia\nusing SeisIO\ncd(dirname(pathof(SeisIO)))\ninclude(\"../tutorial/install.jl\")\n```\n\nTo run SeisIO package tests and download sample data, execute\n\n```julia\nusing Pkg, SeisIO; Pkg.test(\"SeisIO\")\n```\n\nSample data downloaded for the tests can be found thereafter at\n\n```julia\ncd(dirname(pathof(SeisIO))) \nsfdir = realpath(\"../test/SampleFiles/\")\n```\n\n## Publications | [Changelog](docs/CHANGELOG.md) | [Issues](docs/ISSUES.md)\nJones, J.P.,  Okubo, K., Clements. T., \\& Denolle, M. (2020). SeisIO: a fast, efficient geophysical data architecture for the Julia language. *Seismological Research Letters* doi: https://doi.org/10.1785/0220190295\n\nThis work has been partially supported by a grant from the Packard Foundation.\n",
        "createdAt": "2016-05-17T20:46:22.000Z",
        "updatedAt": "2025-09-05T15:17:19.000Z",
        "language": "Julia",
        "homepage": "http://seisio.readthedocs.org",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/jpjones76/SeisIO.jl/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "VenkateshwaranB/Coda-Waves-Attenuation",
        "url": "https://github.com/VenkateshwaranB/Coda-Waves-Attenuation",
        "description": "The attenuation coda waves has been estimated using single backscattering method. The frequency dependency relation of attenuation has been determined by quality factor with their lapse times. These datasets incorporate with the central parts of japan seismological activity. its characterized by high and low seismic attenuation.",
        "stars": 4,
        "forks": 2,
        "readme": "# Coda-Waves-Attenuation\nThe attenuation coda waves has been estimated using single backscattering method. The frequency dependency relation of attenuation has been determined by quality factor with their lapse times. These datasets incorporate with the central parts of japan seismological activity. its characterized by high and low seismic attenuation.\n",
        "createdAt": "2021-10-20T19:03:05.000Z",
        "updatedAt": "2024-09-18T07:28:38.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/VenkateshwaranB/Coda-Waves-Attenuation/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "yurical/seismology-docker",
        "url": "https://github.com/yurical/seismology-docker",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# Seismology-Docker\n\nSeismology-Docker is a docker image with seismology-related packages installed.\n\n## Requirements\n\n* Docker with BuildKit\n* X Server (for GUI support on Windows and macOS)\n    * For Windows, `VcXsrv` or `MobaXterm`.\n    * For Linux, it is installed by default on most of Linux distributions.\n    * For macOS, `XQuartz`.\n* xhost\n    * For Windows, it is part of `VcXsrv`.\n    * For Arch-based distros, `xorg-xhost`.\n    * For Debian-based distros, `x11-server-utils`.\n    * For macOS, it is part of `XQuartz`.\n\n## Requirements installation guide\n\n### Windows\n\n#### Install VcXsrv (X Server)\n\n> [!NOTE]\n> If you have MobaXterm installed, you don't need to install VcXsrv.\n\n1. Download and install latest version of [VcXsrv](https://sourceforge.net/projects/vcxsrv/files/latest/download).\n2. Make sure that `Full` dropdown selected and click `Next`.\n3. Leave the Destination Folder untouched and click `Install`.\n4. After the installation process successfully completes you may click `Close`.\n5. Configure VcXsrv with the application named `XLaunch`.\n6. Make sure that `Multiple windows` option selected.\n7. Set `Display number` to `0` and click `Next`.\n8. Make sure that `Start no client` option selected and click `Next`.\n9. Select `Disable access control` option and click `Next`.\n10. Click `Finish`.\n11. Now you should see an `X icon` appears on your taskbar.  \nThis indicates that the VcXsrv (X Server) is running.\n\n> [!NOTE]\n> If you killed the X Server or the X icon disappeared from the taskbar, just relaunch `XLaunch` and follow the configuration steps.\n\n#### Install Docker Desktop\n1. Download and install latest version of [Docker Desktop for Windows](https://docs.docker.com/desktop/install/windows-install/).\n2. Make sure that `Use WSL 2 instead of Hyper-V` option selected and click `Ok`.\n3. After the installation process successfully completes you may click `Close`.\n4. Reboot the PC.\n5. Launch `Docker Desktop`.\n6. Read the terms and conditions completely and click `Accept`.\n7. Click `Continue without signing in`.\n8. Click `Skip survey`.\n9. Now you should see an `Docker icon` appears on your taskbar.  \nThis indicates that the Docker Desktop (with Docker Engine) is running.\n10. You may close Docker Desktop window when you confirm that Docker Desktop is running on the taskbar icon.\n\n> [!NOTE]\n> If you killed the Docker Desktop or the Docker icon disappeared from the taskbar, just relaunch `Docker Desktop`.\n\n### macOS\n\n#### Install XQuartz (X Server)\n1. Download and install latest version of [XQuartz](https://www.xquartz.org/index.html).\n2. Click `Continue`.\n3. Read the information carefully and click `Continue`.\n4. Read the terms and conditions carefully and click `Continue`.\n5. Click `Agree`.\n6. Click `Install`.\n7. After the installation process successfully completes you may click `Close`.\n8. Reboot the PC.\n9. Open Terminal.\n```shell\n# Enable Indirect GLX\n$ defaults write org.xquartz.X11 enable_iglx -boolean true\n\n# Allow connections from network\n$ defaults write org.xquartz.X11 nolisten_tcp -boolean false\n```\n10. Reboot the PC.\n\n## Installation\n\n### Windows\n\n> [!IMPORTANT]\n> Make sure that the Docker Engine is running before issuing `docker` command.\n\n1. Run PowerShell.\n\n```shell\n# Download this repository\nPS > iwr 'https://github.com/yurical/seismology-docker/archive/refs/heads/main.zip' -OutFile '.\\seismology-docker.zip'\nPS > Expand-Archive '.\\seismology-docker.zip' .\nPS > cd seismology-docker-main\n\n# Go to the directory where you cloned repository or extracted `seismology-docker.zip`.\nPS > cd seismology-docker\n\n# Make sure that `Dockerfile` exists in the current directory\nPS > Get-ChildItem\n\n    Directory: ...\n\nMode       LastWriteTime       Length Name\n----       -------------       ------ ----\n-a---                ...          ... Dockerfile\n-a---                ...          ... LICENSE\n-a---                ...          ... README.md\n-a---                ...          ... requirements.txt\n\n# Build a docker image\nPS > docker buildx build . --network host --tag seismology\n\n[+] Building 137.1s (13/13) FINISHED                                                                     docker:default\n => [internal] load build definition from Dockerfile                                                               0.0s\n => [internal] load .dockerignore                                                                                  0.0s\n => [1/7] FROM docker.io/library/python:3.10-slim-bookworm@sha256:e53bad75661571d23d9fd632d10f192b09228f31b14af1f  7.7s\n => [2/7] RUN apt-get update                                                                                       2.8s\n => [3/7] RUN DEBIAN_FRONTEND=noninteractive apt-get install -y -q     dbus     pkg-config     libxcb*-dev     l  71.6s\n    ...\n => => naming to docker.io/library/seismology                                                                      0.0s\n\nView build details: ...\n\n# Once build finishes, make sure that `seismology` docker image exists\nPS > docker image ls\n\nREPOSITORY   TAG      IMAGE ID   CREATED   SIZE\nseismology   latest   ...        ...       <about a few GB>\n```\n\n### Linux\n\n> [!IMPORTANT]\n> Make sure that the docker.service is running before issuing `docker` command.\n\n```shell\n# Clone this repository\n# or you can just download code archive from https://github.com/yurical/seismology-docker/archive/refs/heads/main.zip and extract it.\n$ git clone https://github.com/yurical/seismology-docker.git\n\nCloning into 'seismology-docker'...\nremote: Enumerating objects: 8, done.\nremote: Counting objects: 100% (8/8), done.\nremote: Compressing objects: 100% (7/7), done.\nremote: Total 8 (delta 0), reused 8 (delta 0), pack-reused 0\nReceiving objects: 100% (8/8), 4.42 KiB | 4.42 MiB/s, done.\n\n# Go to the directory where you extracted `seismology-docker.zip`\n$ cd seismology-docker\n\n# Make sure that `Dockerfile` exists in the current directory\n$ ls\nDockerfile  LICENSE  README.md  requirements.txt\n\n# Build a docker image\n$ docker buildx build . --network host --tag seismology\n\n[+] Building 137.1s (13/13) FINISHED                                                                     docker:default\n => [internal] load build definition from Dockerfile                                                               0.0s\n => [internal] load .dockerignore                                                                                  0.0s\n => [1/7] FROM docker.io/library/python:3.10-slim-bookworm@sha256:e53bad75661571d23d9fd632d10f192b09228f31b14af1f  7.7s\n => [2/7] RUN apt-get update                                                                                       2.8s\n => [3/7] RUN DEBIAN_FRONTEND=noninteractive apt-get install -y -q     dbus     pkg-config     libxcb*-dev     l  71.6s\n    ...\n => => naming to docker.io/library/seismology                                                                      0.0s\n\nView build details: ...\n\n# Once build finishes, make sure that `seismology` docker image exists\n$ docker image ls\n\nREPOSITORY   TAG      IMAGE ID   CREATED   SIZE\nseismology   latest   ...        ...       <about a few GB>\n```\n\n## Usage\n\n> [!IMPORTANT]\n> Make sure that the X Server and Docker Engine is running before issuing `docker` command.\n\n### Windows\n\n```shell\n# Start the docker container with bash entrypoint\nPS > docker run -e DISPLAY=\"host.docker.internal:0.0\" --shm-size 4G -it --rm --entrypoint bash seismology\n\n# Now it continues on bash as root\nroot@67ce68476fdc:/work＃\n\n# Run some applications to confirm GUI works\nroot@67ce68476fdc:/work＃ geany\n\n# Type exit to quit from container\nroot@67ce68476fdc:/work＃ exit\n\nPS >\n```\n\n### Linux\n```shell\n# Adjust the permissions the X server host\n$ xhost +local:docker\n\n# Start the docker container with bash entrypoint\n$ docker run -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY --shm-size 4G -it --rm --entrypoint bash seismology\n\n# Now it continues on bash as root\nroot@67ce68476fdc:/work＃\n\n# Run some applications to confirm GUI works\nroot@67ce68476fdc:/work＃ geany\n\n# Type exit to quit from container\nroot@67ce68476fdc:/work＃ exit\n\n$\n\n# You can revert the permissions after you are finished using the container (If you concerned)\n$ xhost -local:docker\n```\n\n## Troubleshooting\n\n### dial unix /var/run/docker.sock: connect: permission denied\n\n```shell\n# Create a docker group\n$ sudo groupadd -f docker\n\n# Add docker group to the current user\n$ sudo usermod -aG docker \"${USER}\"\n\n# Change /var/run/docker.sock owner group\n$ sudo chown root:docker /var/run/docker.sock\n\n# Apply the changes to groups\n$ newgrp docker\n\n# Restart the docker service\n$ sudo systemctl restart docker\n```\n\n### E: Failed to fetch http://deb.debian.org/debian/pool/main/*.deb  Cannot initiate the connection to deb.debian.org:80. - connect (101: Network is unreachable)\n\nTry the build command again.\n\n## TODO\n\n* Add further instructions for macOS\n",
        "createdAt": "2024-02-10T14:38:52.000Z",
        "updatedAt": "2025-12-04T07:31:19.000Z",
        "language": "Dockerfile",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/yurical/seismology-docker/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "chen-gui/CSDL",
        "url": "https://github.com/chen-gui/CSDL",
        "description": "A Coordinate Driven Self Supervised Deep Learning Method for OTG Interpolation and Denoising of Multiple Seismological Datasets",
        "stars": 0,
        "forks": 0,
        "readme": "**CSDL**\n======\n\n## Description\n\n**CSDL** The Coordinate-driven Self-supervised Deep Learning (CSDL) method represents a new paradigm in data-driven geoscience, offering a flexible, physics-aware, and generalizable approach for improving the quality, resolution, and completeness of seismological datasets. Designed originally for off-the-grid (OTG) interpolation and denoising, CSDL departs from conventional supervised or patch-based learning by embedding geometric coordinates directly into its neural representation, allowing it to learn continuous spatial and temporal mappings of seismic fields without requiring labeled or paired training data. This coordinate-driven formulation and its self-supervised training strategy make it highly applicable to a wide variety of geophysical tasks, including earthquake monitoring, seismic exploration, and subsurface imaging.\n\n## Reference\n    Chen et al. (2025). CSDL: Coordinate-driven self-supervised deep learning for grid-independent denoising and interpolation in seismology, TBD.\n    \nBibTeX:\n\n\t@article{csdl,\n\t  title={CSDL: Coordinate-driven self-supervised deep learning for grid-independent denoising and interpolation in seismology},\n\t  author={Chen et al.},\n\t  journal={TBD},\n\t  volume={TBD},\n\t  number={TBD},\n\t  issue={TBD},\n\t  pages={TBD},\n\t  year={2026}\n\t}\n\n## Scientific Application\n\nCSDL can be applied to the USArray data, which is a pioneer project to study the crustal and upper mantle structure of the North American continent. See the figure below. Large data gaps occur in the southwest and northeast due to sparse station coverage. The red star indicates the epicenter of January 18, 2009, Kermadec Islands, New Zealand, Mw 6.4 earthquake. The blue triangles and red circles represent the original and OTG-interpolated arrays, respectively.\n\n<img src='https://github.com/chenyk1990/gallery/blob/main/csdl/geometry.png' alt='Slicing' width=960/>\n\n\n-----------\n## Copyright\n    CSDL developing team, 2024-present\n-----------\n\n## License\n    MIT License \n\n-----------\n\n## Install\nUsing the latest version\n\n    git clone https://github.com/chen-gui/CSDL\n    cd CSDL\n    pip install -v -e .\n\n-----------\n## Examples\n    The \"Demos\" directory contains all runable scripts to demonstrate different applications of CSDL. \n    \n-----------\n## Gallery\nThe gallery figures of the CSDL package can be found at\n    https://github.com/chenyk1990/gallery/tree/main/csdl\nEach figure in the gallery directory corresponds to a DEMO script in the \"Demos\" directory with a similar file name.\n\n-----------\n## Dependence Packages\n* scipy \n* numpy\n* torch \n* matplotlib\n\n-----------\n## Development\n    The development team welcomes voluntary contributions from any open-source enthusiast. \n    If you want to make contribution to this project, feel free to contact the development team. \n\n-----------\n## Contact\n    Regarding any questions, bugs, developments, or collaborations, please contact      \n    Gui Chen and Yangkang Chen\n\tchenguicup@163.com and chenyk2016@gmail.com\n\n-----------\n# SeismoNet\n<!-- Generated by [notebooks/DEMO_test1.ipynb](https://github.com/chenyk1990/CSDL/tree/main/notebooks/fake_real_eqcct_evaluator_clean_test1.ipynb) -->\nThe following is the architecture of SeismoNet, which consists of 12 KAN layers, each using a varying number of learnable functions.\n<img src='https://github.com/chenyk1990/gallery/blob/main/csdl/network.png' alt='Slicing' width=960/>\n\n\n# Receiver Function Example\nThe following is a 2D denoising example on receiver function data. (a) Original data corrupted by both random and erratic noise. (b)--(d) Denoised data from CSDL after 250, 500, and 750 epochs, respectively. (e) Removed noise after 750 epochs. The black ellipse highlights the gradual recovery of data details through CSDL iterations.\n\nGenerated by [Demos/RF3D](https://github.com/chen-gui/CSDL/blob/CSDL/Demos/RF2d.py)\n<img src='https://github.com/chenyk1990/gallery/blob/main/csdl/receiver2D.png' alt='Slicing' width=960/>\n\n# 3D Real Seismic data\nThe following is a 3D denoising example on post-stack data. Each 3D cube has been reshaped into a 2D matrix. (a) Original data contaminated by both random and erratic noise. (b) Denoised data corresponding to the PATCHUNET method. (c) Denoised data corresponding to CSDL. (d) Removed noise from PATCHUNET. (e) Removed noise from CSDL. (f) Local similarity data (calculated between the removed noise and denoised data) from PATCHUNET. (g) Local similarity data from CSDL. The similarity is calculated using the pyortho package (https://github.com/aaspip/pyortho).\n\n<img src='https://github.com/chenyk1990/gallery/blob/main/csdl/real3D.png' alt='Slicing' width=960/>\n\n# 3D Synthetic Seismic data\nThe following is a 3D OG interpolation and denoising example on synthetic data. (a) Clean data. (b) Noisy data (SNR = 0.91 dB) contaminated by both random and erratic noise. (c) Decimated data (SNR = 0.57 dB) with 50\\% of the traces randomly missing. (d) Recovered data (SNR = 10.81 dB) corresponding to POCS. (e) Recovered data (SNR = 22.54 dB) corresponding to CSDL. (f) Recovery error using POCS. (g) Recovery error using CSDL. The red arrows highlight the improved performance of CSDL over POCS.\n\nGenerated by [Demos/Syn3D](https://github.com/chen-gui/CSDL/blob/CSDL/Demos/syn3d.py)\n<img src='https://github.com/chenyk1990/gallery/blob/main/csdl/syn3d.png' alt='Slicing' width=960/>\n\n3D de-aliased OG interpolation and denoising example on synthetic data. (a) Decimated data (SNR = 0.10 dB) with 75\\% of the traces regularly missing. (i) Densified data (SNR = 14.17 dB) after a factor of 4 densification using CSDL. (j) Densification error using CSDL. The error is determined between the clean and densified data.\n\nGenerated by [Demos/Syn3D](https://github.com/chen-gui/CSDL/blob/CSDL/Demos/syn3d.py)\n<img src='https://github.com/chenyk1990/gallery/blob/main/csdl/syn3d-densifi.png' alt='Slicing' width=960/>\n\n# 5D Synthetic Seismic data\nThe following is a 5D OG interpolation and denoising example on synthetic data. Each displayed 3D cube has been reshaped into a 2D matrix.(a) Clean data. (b) Noisy data (SNR = 2 dB). (c) Decimated data (SNR = 0.16 dB) with 90\\% of the traces randomly missing. (d--(f) Recovered data corresponding to POCS (SNR = 4.11 dB), CGTN (SNR = 11.05 dB), and CSDL (SNR = 20.56 dB), respectively. Note that the SNR values are determined over the entire 5D volume.\n\n<img src='https://github.com/chenyk1990/gallery/blob/main/csdl/parabolic-noisy-hx8hy8.png' alt='Slicing' width=960/>\n\nThe following are the 5D OG interpolation and denoising errors corresponding to (a) POCS, (b) CGTN, and (C) CSDL. \n\n<img src='https://github.com/chenyk1990/gallery/blob/main/csdl/parabolic-noisy-hx8hy8-error.png' alt='Slicing' width=640/>\n\nThe following is the F–K spectrum comparison of the 5D OG interpolation and denoising example on synthetic data. (a) Clean spectrum. (b) Noisy spectrum. (c) Decimated spectrum. (d)--(f) Recovered spectra corresponding to POCS, CGTN, and CSDL, respectively.\n\n<img src='https://github.com/chenyk1990/gallery/blob/main/csdl/parabolic-noisy-hx8hy8-fk.png' alt='Slicing' width=960/>\n\n\n# 5D Real Seismic data\nThe following is a 5D OG interpolation and denoising example on field data. Each displayed 3D cube has been reshaped into a 2D matrix. (a) Original data with about 80\\% of the traces missing. (b)--(d) Recovered data corresponding to POCS, CGTN, and CSDL, respectively. \n\n<img src='https://github.com/chenyk1990/gallery/blob/main/csdl/field_regular-hx6hy6.png' alt='Slicing' width=960/>\n\nThe following is the F–K spectrum comparison of 5D OG interpolation and denoising example on field data. (a) Original spectrum. (b)--(d) Recovered spectra corresponding to POCS, CGTN, and CSDL,respectively.\n\n<img src='https://github.com/chenyk1990/gallery/blob/main/csdl/field_regular-hx6hy6-fk.png' alt='Slicing' width=960/>\n\n\n# SS Precursor data\nThe following is a 4D OG interpolation and denoising example on SS-precursor data. (a) Schematic of the SS-precursor acquisition geometry. (b) Original data with about 70\\% of the traces missing and contaminated by both random and erratic noise. (c) Recovered data using CSDL without densification. (d) Densified data using CSDL by a factor of 2. (e) Densified data using CSDL by a factor of 4.\n\nGenerated by [Demos/SS4D](https://github.com/chen-gui/CSDL/blob/CSDL/Demos/SS4d.py)\n<img src='https://github.com/chenyk1990/gallery/blob/main/csdl/ss4d-inter.png' alt='Slicing' width=960/>\n\n\nThe following are (a)--(d) Stacked results corresponding to the 4D SS-precursor data shown in the above figure (b)--(e), respectively. The red arrows indicate that CSDL can provide clear SS-precursor arrivals.\n\nGenerated by [Demos/SS3D](https://github.com/chen-gui/CSDL/blob/CSDL/Demos/SS3d.py)\n<img src='https://github.com/chenyk1990/gallery/blob/main/csdl/ss4d-stack.png' alt='Slicing' width=960/>\n\n\n# USArray data example\nBelow is an example of 3D OTG interpolation and denoising on USArray data. (a) Original OTG data. (b) OTG interpolated and denoised data using OTG DRR. (c) OTG interpolated and denoised data using CSDL.\n\nGenerated by [Demos/USArray3d](https://github.com/chen-gui/CSDL/blob/CSDL/Demos/USArray3d.py)\n<img src='https://github.com/chenyk1990/gallery/blob/main/csdl/usarray3Dfield.png' alt='Slicing' width=960/>\n\n\nBelow is a Time slice (1600s) comparison of 3D OTG interpolation and denoising example on USArray data. (a) The time slice corresponding to the original OTG data. (b) The time slice corresponding to recovered data using OTG DRR. (c) The time slice corresponding to recovered data using CSDL.\n\nGenerated by [Demos/USArray3d](https://github.com/chen-gui/CSDL/blob/CSDL/Demos/USArray3d.py)\n<img src='https://github.com/chenyk1990/gallery/blob/main/csdl/usarray3d-timeslice.png' alt='Slicing' width=960/>\n\n\n# Latent space\nBelow is a demonstration of the learned features from CSDL. (a) Learned features for the 3D SS-precursor denoising example. (b) Learned features for the 4D SS-precursor interpolation example\n\n<img src='https://github.com/chenyk1990/gallery/blob/main/csdl/learnedFM.png' alt='Slicing' width=960/>\n\n\n",
        "createdAt": "2025-02-16T07:25:59.000Z",
        "updatedAt": "2025-11-03T01:06:50.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/chen-gui/CSDL/CSDL/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "sean0921/seislabcourse",
        "url": "https://github.com/sean0921/seislabcourse",
        "description": "Linux Container (Docker) Environment for Seismology Lab",
        "stars": 0,
        "forks": 0,
        "readme": "# Linux Container (Docker) Environment for Seismology Lab\n\n* usage: `docker-compose up -d`\n",
        "createdAt": "2020-07-24T12:45:45.000Z",
        "updatedAt": "2020-07-24T12:55:00.000Z",
        "language": "Dockerfile",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/sean0921/seislabcourse/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "volkanozsarac/OSW-6ICEES",
        "url": "https://github.com/volkanozsarac/OSW-6ICEES",
        "description": "OpenSees Workshop - 6th International Conference on Earthquake Engineering and Seismology (6ICEES)",
        "stars": 21,
        "forks": 14,
        "readme": "![](img/logo.png)\n![](img/OSW_Logo.png)\n\n# OSW-6ICEES\nThis is the official repository for the course module presented on October 10th **OpenSees Workshop - 6th International Conference on Earthquake Engineering and Seismology (6ICEES)**. The outline of the course module is provided below. The participants are recommended to execute python code lines through jupyter notebook together with the instructors to make their learning more efficient. In order to reduce the amount of time to set up python enviroments, the participants are recommended to open notebooks with binder, in a readily available executable environment, by clicking the buttons provided below. \n\n[![DOI](https://zenodo.org/badge/408118856.svg)](https://zenodo.org/badge/latestdoi/408118856)\n\n## 1. Introduction\n\nThe chapter describes the motivation for the course module. The participants are informed about the advantages of using *Python* over *Tcl* as an OpenSees interpreter. Installation and use of *Python* in different platforms (cloud or PC) is explained.\n\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/volkanozsarac/OSW-6ICEES/HEAD?labpath=1.%20Introduction.ipynb)\n\n## 2. Python for Beginners\n\nThe chapter introduces main characteristics of the Python language (e.g. syntax, data types, operators, control flow and loops, functions, libraries). The most commonly used libraries for scientific purposes are introduced. Finally, for a given site in Turkey, the ground motion record selection is carried out in accordance with Turkish Building Earthquake Code 2018 (TBEC 2018) using a readily available python package. \n\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/volkanozsarac/OSW-6ICEES/HEAD?labpath=2.%20Python.ipynb)\n\n## 3. SDOF Systems\n\nThe numerical modelling and structural analysis of different single degree of freedom (SDOF) systems using OpenSeesPy is demonstrated. Three of the available uniaxial materials in OpenSeesPy are tested using cyclic and monotonic loading. Single and sequential dynamic analyses are carried out using the selected ground motion records.\n\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/volkanozsarac/OSW-6ICEES/HEAD?labpath=3.%20SDOF.ipynb)\n\n## 4. 2D Frame Systems\n\nThe numerical modelling and structural analysis of 2D reinforced concrete frame system using OpenSeesPy is demonstrated. In particular, two different models of the same frame are considered: elastic and inelastic. Moreover, different analyses are carried out: linear and nonlinear static analysis, response spectrum analysis, and nonlinear response history analysis.\n\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/volkanozsarac/OSW-6ICEES/HEAD?labpath=4.%20Frame.ipynb)\n\n## 5. Hackathon\n\nIn this chapter the participants are being asked to demonstrate what they learned by solving a simple exercise using OpenSeesPy. The instructors will help the participants throughout the session if they encounter any issue or problem.\n\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/volkanozsarac/OSW-6ICEES/HEAD?labpath=5.%20Hackathon.ipynb)\n\n<img src=\"./img/aad_OSW-6ICEES.drawio.png\">\n",
        "createdAt": "2021-09-19T12:16:38.000Z",
        "updatedAt": "2025-08-29T11:54:18.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/volkanozsarac/OSW-6ICEES/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "earthinversion/Fnet_IRIS_data_automated_download",
        "url": "https://github.com/earthinversion/Fnet_IRIS_data_automated_download",
        "description": "Automated download of the most recent Fnet data and the IRIS data",
        "stars": 4,
        "forks": 3,
        "readme": "# Fnet_IRIS_data_automated_download\n\nAutomatically download IRIS and Fnet data\n",
        "createdAt": "2019-11-24T13:15:41.000Z",
        "updatedAt": "2023-02-07T02:06:54.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/earthinversion/Fnet_IRIS_data_automated_download/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "eilonzach/seis_tools",
        "url": "https://github.com/eilonzach/seis_tools",
        "description": "Selection of maltab tools useful for seismology",
        "stars": 13,
        "forks": 2,
        "readme": "",
        "createdAt": "2014-02-17T22:26:28.000Z",
        "updatedAt": "2024-10-17T16:19:58.000Z",
        "language": "MATLAB",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Mau-dvr75/Seismic",
        "url": "https://github.com/Mau-dvr75/Seismic",
        "description": "Computational Seismology ",
        "stars": 0,
        "forks": 0,
        "readme": "# Seismic\nComputational Seismology \n",
        "createdAt": "2019-11-27T23:08:29.000Z",
        "updatedAt": "2019-11-27T23:08:32.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Mau-dvr75/Seismic/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "jishnub/SeismoRegistry",
        "url": "https://github.com/jishnub/SeismoRegistry",
        "description": "Useful packages for seismological applications",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2020-08-05T04:49:14.000Z",
        "updatedAt": "2020-08-05T05:28:35.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "monmon0/planetory-seismology-deploy",
        "url": "https://github.com/monmon0/planetory-seismology-deploy",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2024-10-06T20:11:24.000Z",
        "updatedAt": "2024-10-06T22:34:06.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "wangliang1989/GFMF_tiny",
        "url": "https://github.com/wangliang1989/GFMF_tiny",
        "description": "验证GFMF安装的示例",
        "stars": 0,
        "forks": 0,
        "readme": "# GFMF_tiny\n\n此处是使用 GFMF 的一个最小示例，目的是验证 GFMF 本身的安装和给初学者最基本的使用体验。\n使用本示例的最低前提是正确安装了[GFMF](https://github.com/wangliang1989/gfmf) 和 sac。\n\n本示例按照 GPL v3 协议发布，即你可以使用、修改和再发布。但修改后也需要开源（包括增量部分）。\n详情请看[GPL v3 协议英文版](LICENSE)\n\n## 文件含义\n\n**本仓库因文件大小的原因，没有包含下面的地震波形文件和格林函数库。**\n使用方法中说明了如何获取这部分文件。\n\n                     20180815         地震波形文件\n                      LICENSE         GPL v3 协议英文版\n                    README.md         本文件\n                    cali.conf         搜索配置文件\n               cali_grids.txt         网格文件\n                       cat.pl         生成地震目录\n                         glib         格林函数库\n                       run.pl         搜索运行文件\n                 stations.txt         台站列表\n     wangliang_cali_11_10.txt         我做出来的地震目录\n    wangliang_result_cali.txt         我做出来的初步地震目录\n\n## 使用方法\n\n1. 下载[地震波形文件](https://www.jianguoyun.com/p/DfcbAv0Q5s_iCRiVyIkE)和\n[格林函数库](https://www.jianguoyun.com/p/DYluM3cQ5s_iCRiLyIkE)并解压\n\n2. 生成格林函数\n\n我在上一步中提供的格林函数库压缩包已经包含了格林函数文件。你可以直接使用，\n也可以用下面的步骤自己计算（以 FK 已经正确安装为前提）。我建议初学者先直接用我给的格林函数文件。\n**如果你选择自己生成格林函数，千万不要忘记执行 cutglib.pl**。\n\n````bash\ncd glib\ncp $GFMF/script/run_parallel.pl . # run_parallel.pl 是并行计算格林函数的脚本\ncp $GFMF/script/config.pm .       # config.pm 是 run_parallel.pl 依赖的一些子函数\ncp $GFMF/script/cutglib.pl .      # 格林函数的处理程序\nperl run_parallel.pl cvms5-1.fk   # 计算格林函数\nperl cutglib.pl                   # 对格林函数滤波，计算归一化互相关的部分归一化参数\n````\n\n目前在 ARM Mac 上使用 cutglib.pl，\n**可能**会产生大量 SAC 的警告信息，原因和影响程度未知。\n\n3. 建立工作目录\n\n**在包含了脚本 run.pl 的路径下：**\n\n````bash\nmkdir junk # junk 路径下所有文件在搜索前会被删除\n````\n\n4. 执行搜索\n\n这一步会非常耗时，而且你的计算机会变得很卡。在运行前，建议关闭其它应用。\n我的电脑（价格 6000 多）需要接近 1 小时来完成搜索。\n不过，我弟弟说学校配的 3 万块的工作站算了一晚上也没算完。\n\n````bash\nperl run.pl junk\n````\n\n计算完毕后，`20180815/result_cali.txt` 应该和 `wangliang_result_cali.txt` 的差异微小，\n但不必追求完全一致。\n\n5. 生成地震目录\n\n````bash\nperl cat.pl\n````\n\n`cali_11_10.txt` 应该和 `wangliang_cali_11_10.txt` 的差异微小，但不必追求完全一致。\n\n6. 画图\n\n如果你正确安装了 GMT6，可以画图：\n\n````bash\nperl draw.pl\n````\n\n如果一切顺利，你应该看到你的结果（蓝色，Your result）\n和我的结果（红色，Wangliang result）差别很小。\n\n## 文章下载与引用信息\n\n下载论文及其 BibTex 和 Endnote 文件，请直接前往《地球物理学报》官网：\nhttp://www.geophy.cn/CN/abstract/abstract15922.shtml\n\n> 王亮, 梁春涛. 2021. 以虚拟地震的理论格林函数为模板搜寻小地震. 地球物理学报,64(7): 2374-2393, doi: 10.6038/cjg2021O0361\n\n> WANG Liang, LIANG ChunTao. 2021. Detecting small earthquakes using the theoretical Green's function of virtual earthquakes as templates Chinese Journal of Geophysics(in Chinese), 64(7): 2374-2393, doi: 10.6038/cjg2021O0361\n\n## 已引用本方法的论文\n\n如果你在论文中引用了我的上述论文。无论你的文章的主题为何，你都可以把你的已正式刊出的论文发给我。\n我会在此处列出。这样可以让别人知道你的研究工作，潜在地增加你的论文的引用量。\n",
        "createdAt": "2021-08-20T16:08:56.000Z",
        "updatedAt": "2021-09-08T03:32:27.000Z",
        "language": "Perl",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/wangliang1989/GFMF_tiny/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "huazhz/seismology",
        "url": "https://github.com/huazhz/seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2019-09-08T13:54:07.000Z",
        "updatedAt": "2019-09-08T14:14:32.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Jingyi-Luo/Surface-Wave_Waveform_Classification_Using_ML",
        "url": "https://github.com/Jingyi-Luo/Surface-Wave_Waveform_Classification_Using_ML",
        "description": "The logistic regression, K-nearest neighbors, support vector machine, and artificial neural network (ANN) algorithms were developed to automatically classify seismic observations.",
        "stars": 4,
        "forks": 0,
        "readme": "# Surface-Wave Waveform Classification Using Machine Learning Algorithms\n\nCollaborated with researchers from Oak Ridge National Laboratory and other institutions to automatically classify seismic observations using a labeled dataset of around 400,000 records. In order to achieve it, the logistic regression, K-nearest neighbors, support vector machine, and artificial neural network algorithms were developed. Among all the models, the artificial neural network outperform other algorithms and achieved an accuracy of over 92%.\n\nThe results were presented at 2019 Seismological Society of America Annual meeting. Refer to poster to obtain the detailed information.\n",
        "createdAt": "2019-05-15T00:08:00.000Z",
        "updatedAt": "2022-09-27T08:49:13.000Z",
        "language": null,
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Jingyi-Luo/Surface-Wave_Waveform_Classification_Using_ML/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "echavess/teaching",
        "url": "https://github.com/echavess/teaching",
        "description": "Jupiter notebooks and python codes for teaching seismology and Earth Sciences",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2019-09-25T14:17:14.000Z",
        "updatedAt": "2019-09-25T14:20:25.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "anowacki/SeisModels.jl",
        "url": "https://github.com/anowacki/SeisModels.jl",
        "description": "Seismic models of the interior of the Earth and other planets",
        "stars": 8,
        "forks": 2,
        "readme": "# SeisModels.jl\n\n## Build status\n[![Build Status](https://github.com/anowacki/SeisModels.jl/workflows/CI/badge.svg)](https://github.com/anowacki/SeisModels.jl/actions)\n[![Code coverage](https://codecov.io/gh/anowacki/SeisModels.jl/branch/master/graph/badge.svg?token=OX6WPKMG7G)](https://codecov.io/gh/anowacki/SeisModels.jl)\n\n## Documentation\n[![](https://img.shields.io/badge/docs-stable-blue.svg)](https://anowacki.github.io/SeisModels.jl/stable)\n[![](https://img.shields.io/badge/docs-dev-blue.svg)](https://anowacki.github.io/SeisModels.jl/dev)\n\n## Publication status\n[![DOI](https://joss.theoj.org/papers/10.21105/joss.02043/status.svg)](https://doi.org/10.21105/joss.02043)\n\n\n## What is SeisModels.jl?\nA [Julia](http://julialang.org) package for dealing with models of the Earth's\n(and other quasi-1D planets') seismic properties.\n\nCurrently, only three kinds of one-dimensional models are supported, but all model\nparameterisations and models are acceptable for inclusion.  Contributions\nare welcome.\n\nBuilt in models are [listed in the documentation](https://anowacki.github.io/SeisModels.jl/dev/inbuilt_models/).\n\n\n## How to install\nSeisModels.jl can be added to your Julia install like so:\n\n```julia\njulia> import Pkg; Pkg.add(\"SeisModels\")\n```\n\n\n## How to use\n### Model types\nThe module defines the SeisModel type and subtypes of this specify the kind of\nmodel (i.e., symmetry, nature of basis function, etc.).\n\n```julia\njulia> using SeisModels\n\njulia> subtypes(SeisModel)\n1-element Array{Any,1}:\n SeisModel1D\n\njulia> subtypes(SeisModel1D)\n3-element Array{Any,1}:\n LinearLayeredModel\n PREMPolyModel\n SteppedLayeredModel\n```\n\nSo, there are currently three types of models implemented, all 1D models, with\npolynomial, linear or constant basis within each layer.\n\n### Calculating properties\n\nYou can either create your own models by creating a new instance of one of the\nimmutable types, or use the inbuilt models.  For instance, for PREM, one can\nevaluate at an arbitrary radius:\n\n* *V*<sub>P</sub>\n* *V*<sub>S</sub>\n* density *&rho;*\n* anisotropic parameters *V*<sub>PH</sub>, *V*<sub>PV</sub>, *V*<sub>SH</sub>,\n  *V*<sub>SV</sub> and *&eta;*\n  \nCalculate these by calling the function with the model as the first argument:\n\n```julia\njulia> vp(PREM, 3500)\n13.71171655163979\n\njulia> Qκ(PREM, 1000)\n1327.7\n\njulia> density(AK135, radius(AK135, 20))\n2.449\n```\n\nIn the last example, we used the `radius` function to convert depth in the AK135 model\nto radius and calculate the density at 20 km depth.  Some functions also accept the\n`depth` keyword argument to instead evaluate properties at a point below the surface:\n\n```julia\njulia> density(AK135, radius(AK135, 20)) == density(AK135, 20, depth=true)\ntrue\n```\n\nYou can also evaluate values programmatically (i.e., where the parameter of\ninterest is a variable) by using the exported `evaluate` function, and broadcast\nthe call to get multiple values:\n\n```julia\njulia> evaluate(AK135, :vp, 3580)\n13.653094354838709\n\njulia> parameters = (:vp, :vs, :density);\n\njulia> evaluate.(AK135, parameters, 3680)\n(13.591187999999999, 7.226264, 5.4003499999999995)\n```\n\n### Model input and output\nSupport for reading and writing model files is currently limited.  However, SeisModels\ndoes support reading and writing of\n[Mineos](https://geodynamics.org/cig/software/mineos/)-format &lsquo;tabular&rsquo; models\n(i.e., `SteppedLayeredModel`s) via the `read_mineos` and `write_mineos` functions.\n\n\n## Reference\n### Exported types\n- `SeisModel`: Abstract supertype of all models\n  - `SeisModel1D`: Abstract supertype of 1D models\n    - `LinearLayeredModel`: 1D model with linearly-varying properties between node points\n    - `PREMPolyModel`: 1D model defined by PREM-style polynomials (of arbitrary degree)\n    - `SteppedLayeredModel`: 1D model with constant properties between node points\n\n### Exported model instances\n- Earth\n  - `AK135`\n  - `IASP91`\n  - `PREM`\n  - `PREM_NOOCEAN`\n  - `STW105`\n- Moon\n  - `MOON_WEBER_2011`\n\n### Exported functions\n#### Model properties\n- `depth`: Return depth in km given a radius and model\n- `hasattenuation`: Whether a model includes attenuation\n- `hasdensity`: Whether a model includes density\n- `isanisotropic`: Whether a model is anisotropic\n- `radius`: Return radius in km given a depth and model\n- `surface_radius`: Radius in km of planet\n\n#### Evaluation functions\n- `evaluate`: Evaluate a given field for a model at any radius\n- `vp`: P-wave velocity in km/s\n- `vs`: S-wave velocity in km/s\n- `density`: Density in g/cm^3\n- `vph`: Horizontal P-wave velocity in km/s\n- `vpv`: Vertical (radial) P-wave velocity in km/s\n- `vsh`: Horizontally-polarised S-wave velocity in km/s\n- `vsv`: Vertically-polarised S-wave velocity in km/s\n- `eta`: Anisotropic parameter\n- `Qμ`, `Qmu`: Shear quality factor\n- `Qκ`, `Qkappa`: Bulk quality factor\n\n#### Derived properties\n- `bulk_modulus`: Bulk modulus (_K_) in Pa\n- `gravity`: Acceleration due to gravity in m/s^2 at a given radius\n- `mass`: Mass in kg from centre of model to a given radius\n- `moment_of_inertia`: MOI in kg m^2\n- `poissons_ratio`: Poisson's ratio\n- `pressure`: Pressure in Pa\n- `shear_modulus`: Shear modulus (_G_) in Pa\n- `surface_mass`: Mass between two radii\n- `youngs_modulus`: Young's modulus in Pa\n\n#### IO\n- `read_mineos`: Read Mineos tabular-format file\n- `write_mineos`: Write Mineos tabular-format file\n- `read_tvel`: Write tvel-format file\n- `write_tvel`: Write tvel-format file\n\n\n## Getting help\nTypes and methods are documented, so at the REPL type `?` to get a `help?>`\nprompt, and type the name of the function:\n\n```julia\nhelp?> PREMPolyModel\nsearch: PREMPolyModel\n\n  PREMPolyModel <: SeisModel1D\n\n  Type describing the Earth as a set of layers within which properties vary according to a set of\n  polynomials.\n\n  Physical parameters are represented by arrays of size (order+1,n), where n is the number of\n  layers, and order is the order of polynomial which is used to represent the parameter. Hence a\n  constant layer has size (1,n), and to compute the value of x in layer i, for an Earth radius of\n  a km, at a radius of r km, the expression is:\n\n  val_x = x[i,1] + (r/a)*x[i,2] + (r/a)^2*x[i,3] ... (r/a)^order*x[i,order+1]\n\n```\n\n## Citing\nIf you use SeisModels.jl for your work, please cite the following paper:\n- Nowacki, A., 2020. SeisModels.jl: A Julia package for models of the\n  Earth’s interior. Journal of Open Source Software 5, 2043.\n  doi:[10.21105/joss.02043](https://doi.org/10.21105/joss.02043)\n",
        "createdAt": "2019-10-30T14:53:46.000Z",
        "updatedAt": "2025-11-25T13:04:45.000Z",
        "language": "Julia",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/anowacki/SeisModels.jl/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "seisman/HinetPy",
        "url": "https://github.com/seisman/HinetPy",
        "description": "A Python package for accessing and processing NIED Hi-net seismic data.",
        "stars": 91,
        "forks": 39,
        "readme": ".. image:: https://github.com/seisman/HinetPy/actions/workflows/tests.yml/badge.svg\n    :target: https://github.com/seisman/HinetPy/actions/workflows/tests.yml\n.. image:: https://codecov.io/gh/seisman/HinetPy/branch/main/graph/badge.svg\n   :target: https://codecov.io/gh/seisman/HinetPy\n.. image:: https://img.shields.io/github/release/seisman/HinetPy.svg\n    :target: https://github.com/seisman/HinetPy/releases\n.. image:: https://img.shields.io/pypi/v/HinetPy.svg\n    :target: https://pypi.org/project/HinetPy/\n.. image:: https://img.shields.io/pypi/pyversions/HinetPy.svg\n    :target: https://pypi.org/project/HinetPy/\n.. image:: https://img.shields.io/github/license/seisman/HinetPy.svg\n    :target: https://github.com/seisman/HinetPy/blob/main/LICENSE\n.. image:: https://joss.theoj.org/papers/10.21105/joss.06840/status.svg\n   :target: https://doi.org/10.21105/joss.06840\n.. image:: https://zenodo.org/badge/23509035.svg\n    :target: https://zenodo.org/badge/latestdoi/23509035\n\n\n`NIED Hi-net <https://www.hinet.bosai.go.jp/>`__ |\n`Source Code <https://github.com/seisman/HinetPy>`__ |\n`Documentation <https://seisman.github.io/HinetPy>`__ |\n`中文文档 <https://seisman.github.io/HinetPy/zh_CN/>`__\n\n----\n\n.. placeholder-for-doc-index\n\n`HinetPy <https://github.com/seisman/HinetPy>`_ is a Python package for accessing and\nprocessing seismic data from `NIED Hi-net <https://www.hinet.bosai.go.jp/>`__.\n\nKey Features\n============\n\n- Facilitates easy access to NIED Hi-net seismic data, including continuous/event\n  waveform data and event catalogs.\n- Supports multiple seismic networks (e.g., F-net, S-net, MeSO-net and more in addition\n  to Hi-net) in Japan.\n- Selects a subset of stations based on geographical location or station name (Supports\n  Hi-net, F-net, S-net and MeSO-net only).\n- Converts waveform data to SAC format and instrumental responses to SAC polezero files.\n- Speeds up the downloading and processing workflow via the use of multithreading.\n\nA simple example\n================\n\nHere is an example showing how to use HinetPy to request continuous waveform data from\nHi-net, convert the data into SAC format, and extract instrumental responses as SAC\npolezero files.\n\n.. code-block:: python\n\n    from HinetPy import Client, win32\n\n    # You need a Hi-net account to access the data\n    client = Client(\"username\", \"password\")\n\n    # Let's try to request 20-minute data of the Hi-net network (with an internal\n    # network code of '0101') starting at 2010-01-01T00:00 (JST, GMT+0900)\n    data, ctable = client.get_continuous_waveform(\"0101\", \"201001010000\", 20)\n\n    # The request and download process usually takes a few minutes\n    # waiting for data request ...\n    # waiting for data download ...\n\n    # Now you can see the data and corresponding channel table in your working directory\n    # waveform data (in win32 format) : 0101_201001010000_20.cnt\n    # channel table (plaintext file)  : 0101_20100101.ch\n\n    # Let's convert data from win32 format to SAC format\n    win32.extract_sac(data, ctable)\n\n    # Let's extract instrument response as PZ files from the channel table file\n    win32.extract_sacpz(ctable)\n\n    # Now you can see several SAC and SAC_PZ files in your working directory\n\n    # N.NGUH.E.SAC  N.NGUH.U.SAC  N.NNMH.N.SAC\n    # N.NGUH.N.SAC  N.NNMH.E.SAC  N.NNMH.U.SAC\n    # ...\n    # N.NGUH.E.SAC_PZ  N.NGUH.U.SAC_PZ  N.NNMH.N.SAC_PZ\n    # N.NGUH.N.SAC_PZ  N.NNMH.E.SAC_PZ  N.NNMH.U.SAC_PZ\n    # ...\n\nCitation\n========\n\nIf you find this package useful, please consider citing the package in either of the\nfollowing ways:\n\n**Cite the HinetPy paper (preferred)**\n\nA formal paper is published on `The Journal of Open Source Software <https://joss.theoj.org/>`__\nsince HinetPy v0.9.0. This is the **preferred** way for citation.\n\n    Tian, D. (2024). HinetPy: A Python package for accessing and processing NIED Hi-net seismic data.\n    Journal of Open Source Software, 9(98), 6840. https://doi.org/10.21105/joss.06840\n\n**Cite a specific HinetPy version**\n\nIf you'd like to cite a specific HinetPy version, you can visit\n`Zenodo <https://zenodo.org/records/12523911>`__, choose the version you want to cite,\nand cite like this:\n\n    Tian, D. (20XX). HinetPy: A Python package for accessing and processing NIED Hi-net seismic data (X.X.X).\n    Zenodo. https://doi.org/10.5281/zenodo.xxxxxxxx\n\nContributing\n============\n\nFeedback and contributions are welcome! Please feel free to open an issue or pull\nrequest if you have any suggestions or would like to contribute a feature.\nFor additional information or specific questions, please open an issue directly.\n\nLicense\n=======\n\nThis project is licensed under the terms of the MIT license.\n",
        "createdAt": "2014-08-31T07:56:38.000Z",
        "updatedAt": "2025-11-27T02:19:28.000Z",
        "language": "Python",
        "homepage": "https://seisman.github.io/HinetPy",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.14193527",
            "openAlex": "10.5281/zenodo.14193527",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/seisman/HinetPy/main/README.rst",
        "mainPaper": {
            "doi": "10.5281/zenodo.14193527",
            "title": "HinetPy: A Python package for accessing and processing NIED Hi-net seismic data",
            "dateReleased": "2024-11-21T00:00:00.000Z",
            "abstract": "",
            "citationsArray": []
        },
        "repoDoi": "10.5281/zenodo.14193527",
        "publications": [
            {
                "doi": "10.5281/zenodo.14193527",
                "name": "HinetPy: A Python package for accessing and processing NIED Hi-net seismic data",
                "source": "",
                "authorNames": [],
                "abstract": "",
                "publicationDate": "2024-11-21T00:00:00.000Z"
            }
        ]
    },
    {
        "source": "GitHub",
        "name": "keurfonluu/EvoSeis",
        "url": "https://github.com/keurfonluu/EvoSeis",
        "description": "EvoSeis is a package of softwares for earthquake seismology using Evolutionary Algorithms as optimization methods.",
        "stars": 3,
        "forks": 0,
        "readme": "# EvoSeis &middot; <a href=\"https://zenodo.org/badge/latestdoi/74918018\"><img src=\"https://zenodo.org/badge/74918018.svg\" alt=\"DOI\"></a>\nEvoSeis is a package of softwares for earthquake seismology using Evolutionary Algorithms as optimization methods. The first release only contains a program for earthquake location in 3D heterogeneous medium called EvoLoc.\n\n## Install\nTo install, simply type\n```bash\nmake all\n```\n\n## How to use EvoLoc\nTo see an example on a synthetic data set, type\n```bash\nbin/evoloc.exe\n```\nIn File, select Import parameters and navigate to example directory to import input.par, then Locate!\n",
        "createdAt": "2016-11-27T22:24:22.000Z",
        "updatedAt": "2022-01-05T15:06:28.000Z",
        "language": "FORTRAN",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/keurfonluu/EvoSeis/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "dttrugman/StatSei.jl",
        "url": "https://github.com/dttrugman/StatSei.jl",
        "description": "Statistical Seismology Tools in Julia",
        "stars": 2,
        "forks": 1,
        "readme": "# StatSei.jl\nStatistical Seismology Tools in Julia\n\n\nThis repository hosts various Julia scripts for performing statistical seismology tasks. It is currently a work in progress and under development, so please have patience.\n\nThis compilation of codes are dedicated to Ilya Zaliapin, a dear friend, a mentor, and an outstanding scientist whose ideas will resonate through the statistical seismology community for years to come.\n\n---\n\nThe (unregistered) package can be installed using the Julia Pkg manager:\n\n` pkg> add https://github.com/dttrugman/StatSei.jl`\n\n\nNote that at present writing, the plotting functionality involves `PyPlot` which while functional is not ideal for a Julia package. Pull requests to help fix this are most welcome.\n\n---\n\nThe `notebooks/` directory contains several example notebooks doing various calculations. These notebooks use additional functionality from the external packages, so if you would like to run them please also add the following:\n\n` pkg> add DataFrames, GLM, CSV, StatsBase, PyPlot`\n\nThe examples are as follows:\n\n- `fmd_bval_example.ipynb`: example calculations involving frequency-magnitude distributions (FMDs) and b-value estimation.\n\n- `nearest_neighbors_example.ipynb`: example implementation of the nearest neighbor seismicity clustering analysis proposed by Zaliapin et al. (2008) and Zaliapin and Ben-Zion (2013).\n\n- `projection_fractal_example.ipynb`: examples of transformation of seismicity catalogs to cartesian coordinates in order to calculate the spatial fractal dimension.\n",
        "createdAt": "2024-02-13T18:58:46.000Z",
        "updatedAt": "2025-07-07T08:02:25.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/dttrugman/StatSei.jl/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "jmackereth/neural-ages",
        "url": "https://github.com/jmackereth/neural-ages",
        "description": "an (over?) simplified neural network model to get ages for APOGEE stars trained on APOKASC-2 seismology",
        "stars": 0,
        "forks": 0,
        "readme": "# Estimating stellar ages from element abundances with models trained on Asteroseismology\n\nThis repo contains a notebook that takes you through the training of a very simple neural network model that predicts ages for stars in the APOGEE survey.\n",
        "createdAt": "2021-11-01T15:32:37.000Z",
        "updatedAt": "2021-11-04T14:56:43.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/jmackereth/neural-ages/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "abcdef1991/CCLSN",
        "url": "https://github.com/abcdef1991/CCLSN",
        "description": "本地地震检测机器学习模型CCLSN（CWT-CNN local seismological network） 作者：彭钊",
        "stars": 4,
        "forks": 0,
        "readme": "# CCLSN\n本地地震检测机器学习模型CCLSN（CWT-CNN local seismological network） 作者：彭钊\n",
        "createdAt": "2020-12-05T02:06:21.000Z",
        "updatedAt": "2025-02-08T07:06:34.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/abcdef1991/CCLSN/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "barsch/seishub.plugins.seismology",
        "url": "https://github.com/barsch/seishub.plugins.seismology",
        "description": "Seismology package for SeisHub.",
        "stars": 4,
        "forks": 2,
        "readme": "",
        "createdAt": "2012-07-18T00:25:57.000Z",
        "updatedAt": "2023-01-28T07:50:21.000Z",
        "language": "Python",
        "homepage": "http://www.seishub.org",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "seismopy/mopy",
        "url": "https://github.com/seismopy/mopy",
        "description": "A python package to calculate seismic source parameters",
        "stars": 12,
        "forks": 1,
        "readme": "# MoPy\nMoPy is a python package for calculating seismic source parameters.\nIt is still very much a work in progress and not yet intended for non-development use.\n",
        "createdAt": "2019-05-14T02:18:15.000Z",
        "updatedAt": "2024-12-06T22:57:02.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/seismopy/mopy/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "NEWTON-g/AQG2MSEED",
        "url": "https://github.com/NEWTON-g/AQG2MSEED",
        "description": "Script to generate seismological data standard (mSEED) from AQG data files",
        "stars": 1,
        "forks": 0,
        "readme": "# Script to generate mSEED from AQG .csv file\n\nThis script loads raw AQG data files and converts them to the seismological mSEED standard.\n\n## Usage\n\nModify the input file in the `run.py` script and feed the `AQG2MSEED` convertor with an AQG file.\n\n## Specifics\n\nThe sampling rate of the AQG is exactly 540ms but the computer clock may provide slightly different timestams. We assume that the sampling rate is precise, introducing some misfit between the converted and raw data. Data is split into continuous segments that are often bounded by recalibration pauses of the instrument every two hours. Each gap introduces a new data segment with a new start time. The traces are bundled in a stream and written to a mSEED file using ObsPy.\n\n## Naming\n\nThe mSEED standard has four identifiers (ASCII, N bytes): network (2), station (5), location (2), and channel (3). The network is defined here as `2Q`, the station as `AQG`, the location is an empty string and will change when the AQG is moved. The channel is defined following the mSEED standard (M) because of the 2Hz sampling rate, (G) for gravimeter, and (Z) for the z-component. In total the identifier is `2Q.AQG..MGZ`.\n\n## Channels\n\nThe convertor handles nine (if available) channels:\n\n  * Raw vertical gravity (MGZ.D) - this is the absolute gravity value corrected for the AQG columns: quartz, tilt, pressure, syst, height, laser polarization. The tide has not been corrected and is left to the user. However, a tidal model is supplied for user convenience in the MXZ channel.\n  * Earth tide (MXZ.D) including: solid earth tide, ocean loading, polar motion. This is the effect (not correction) and therefore needs to be subtracted from the gravity data.\n  * Atmospheric pressure (MDO.D)\n  * Sensor Head, Vacuum Chamber, Tiltmeter, External temperature (MK1, MK2, MK3, MK4)\n  * X, and Y tilt (MA1.D, MA2.D)\n\nThe gain (sensitivity) to convert COUNTS to physical units is given in the metadata.\n\n## Example\n\n    >>> from obspy import read\n    >>> st = read(\"2Q.AQG..MGZ.D.2020.192\")\n    >>> print(st)\n\n    2 Trace(s) in Stream:\n    2Q.AQG..MGZ | 2020-07-10T20:44:03.695000Z - 2020-07-10T22:42:34.415000Z | 1.9 Hz, 13169 samples\n    2Q.AQG..MGZ | 2020-07-10T22:44:45.636000Z - 2020-07-10T23:59:59.496000Z | 1.9 Hz, 8360 samples\n",
        "createdAt": "2020-09-09T12:34:14.000Z",
        "updatedAt": "2022-05-03T12:50:03.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/NEWTON-g/AQG2MSEED/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "bch0w/spectral",
        "url": "https://github.com/bch0w/spectral",
        "description": "Personal catch-all script and metadata repository for seismology-related work",
        "stars": 3,
        "forks": 0,
        "readme": "",
        "createdAt": "2018-01-30T03:30:04.000Z",
        "updatedAt": "2025-11-25T01:12:32.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "bqpseismology/seismology",
        "url": "https://github.com/bqpseismology/seismology",
        "description": "A package for seismic source inversion. For testing, not completed!",
        "stars": 8,
        "forks": 3,
        "readme": "# seismology\nA package for seismic source inversion. For testing, not completed!\n\n## preprocess\nA toolbox for process the seismic data. As SEED files, SAC files et al.\n",
        "createdAt": "2019-08-03T07:31:45.000Z",
        "updatedAt": "2024-05-30T02:28:28.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/bqpseismology/seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "KlausStammler/eida-tools",
        "url": "https://github.com/KlausStammler/eida-tools",
        "description": "Seismological data and metadata analysis",
        "stars": 1,
        "forks": 1,
        "readme": "# eida-tools\n\n## pzcheck\n\nTool to display and check poles and zeros of all response stages at one or\nmore stations. It parses the XML response of an EIDA/FDSN webserver\n(station webservice) and shows it in a compressed, human readable form\nor performs a formal check of the given metadata.\n\n\n## eida_availability\n\nSoftware for EIDA metadata performance and data availability check, results\nshown on page \"https://www.szgrf.bgr.de/eida_availability_report.html\".\n",
        "createdAt": "2020-11-18T08:56:03.000Z",
        "updatedAt": "2021-02-25T08:07:08.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/KlausStammler/eida-tools/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ja536/seismology_macros_working",
        "url": "https://github.com/ja536/seismology_macros_working",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2017-07-31T01:19:35.000Z",
        "updatedAt": "2017-07-31T01:32:23.000Z",
        "language": "Shell",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "schipp/repeating_direct_waves",
        "url": "https://github.com/schipp/repeating_direct_waves",
        "description": "Accompanying repository for \"Continuous isolated noise sources induce repeating waves in the coda of ambient noise correlations\" by Schippkus et al. 2023",
        "stars": 2,
        "forks": 2,
        "readme": "# Continuous isolated noise sources induce repeating waves in the coda of ambient noise correlations\n\n[![DOI](https://zenodo.org/badge/601300942.svg)](https://zenodo.org/badge/latestdoi/601300942) ![python 3.11 only](https://img.shields.io/badge/python-3.11-blue)\n\n![Beamforming results for master station IV.BRMO](./figures/Fig1_IV.BRMO.png)\n\nThis repository contains all data products, metadata, and code necessary to reproduce all figures of the manuscript \"Continuous isolated noise sources induce repeating waves in the coda of ambient noise correlations\" by Schippkus et al. (2023), Seismica.\n\n`\\manuscript` contains the revised manuscript pre-print pdf. Published in [Seismica](https://doi.org/10.26443/seismica.v2i2.499).\n\n`\\notebooks` contains three notebooks: `fig3_repeating_impulsive_source.ipynb` reproduces Figure 3, `fig4_sketch.ipynb` reproduces Figure 4, `fig6_secondary_mic roseism_stf.ipynb` reproduces Figure 6, `fig10_processing.ipynb` reproduces Figure 10, and `figs.ipynb` reproduces all other figures. Please read the instructions in the first cell of `figs.ipynb` carefully. `settings.toml` describes the parameters used for each figure. `schippkus_2023_lib.py` contains much of the logic for computing waveforms, cross-correlating them, and beamforming the cross correlations.\n\n`\\correlations` contains all cross-correlation functions our measurements are based on. These are computed from 2 years of continuous data in 2019 & 2020 between master stations `IV.BRMO` & `PL.OJC` and the Gräfenberg array `GR.GR*`.\n\n`\\figures` contains all figures as produced by the notebooks provided and used in the manuscript.\n\n`colorblind.mplstyle` is the matplotlib style-file used for some notebooks.\n\n## Requirements\n\nTo run these notebooks, the following is required\n\n* Python >= 3.11\n* Scientific Python stack (scipy, matplotlib, numpy)\n* obspy\n* cartopy\n* tqdm\n* pygc (for easy great-circle computations)\n* notebook\n\nA functioning installation can be achieved, e.g., via conda by\n\n```bash\n>> conda create -n schippkus_et_al_2023 python=3.11\n>> conda activate schippkus_et_al_2023\n>> conda install -c conda-forge obspy cartopy tqdm pygc notebook\n```\n",
        "createdAt": "2023-02-13T19:30:03.000Z",
        "updatedAt": "2025-11-07T13:55:54.000Z",
        "language": "Jupyter Notebook",
        "homepage": "https://doi.org/10.26443/seismica.v2i2.499",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/schipp/repeating_direct_waves/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "RongjiangWang/QSSPFO_2020",
        "url": "https://github.com/RongjiangWang/QSSPFO_2020",
        "description": "A variant of FORTRAN code QSSP for calculating normal modes (dispersion curves) of a spherically symmetric earth with the atmosphere, mantle, liquid and solid core structure",
        "stars": 0,
        "forks": 0,
        "readme": "A variant of FORTRAN code QSSP for calculating normal modes (dispersion curves) of a spherically symmetric earth with the atmosphere, mantle, liquid and solid core structure.\n\nFor Windows user, the executable file is provided under folder \"WindowsEXE\". Linux user may compile the source codes with \"gfortran\" via a single command like, e.g.,\n\n~>cd .../SourceCode\n\n~>gfortran -o qsspfo2020 *.f -O3\n\nto get the excutable code qsspfo2020.\n\nAfter start the executable code, the program ask for an input file in the ASCII format. An example input file is provided under folder \"InputFile\". You may change the input data included in this file for your own applications.\n\nReferences\n\nWang, R., S. Heimann, Y. Zhang, H. Wang, and T. Dahm (2017). Complete synthetic seismograms based on a spherical self-gravitating Earth model with an atmos-phere-ocean-mantle-core structure. Geophysical Journal International, doi: 10.1093/gji/ggx259.\n",
        "createdAt": "2025-04-11T07:51:41.000Z",
        "updatedAt": "2025-07-06T12:06:57.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/RongjiangWang/QSSPFO_2020/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "mmunson2/BME450-Project3-Seismology",
        "url": "https://github.com/mmunson2/BME450-Project3-Seismology",
        "description": null,
        "stars": 1,
        "forks": 2,
        "readme": "# BME450-Project3-Seismology\n#### Matthew Munson\n#### 2/26/2020\n#### Professor Shima Abadi\n\n# Introduction:\n\nMany of the Pacific Northwest's stunning features are a result of its vast and complex geology. Situated along the Ring of Fire, shifting tectonic plate boundaries off the Pacific Coast are responsible for the region's distinct terrain. Immediately off the Washington Coast sits the Juan De Fuca Plate, one of the smallest tectonic plates on earth. It's wedged between the Pacific Plate and the North American plate and contains nearly all plate boundary types, including divergent, convergent, and transform boundaries. This makes the Jaun De Fuca Plate a prime location for geologic study.\n\nThis assignment tasked students with mapping earthquake data on the Jaun De Fuca plate. Data was retrieved from the United States Geological Survey in a ten year period from 2010 to 2020. The plotted earthquake data illustrates the outline of many of the plate boundaries off the Pacific Coast.\n\n\n# How I Did It:\n\nI began the assignment by retrieving the relevant data from the USGS earthquake catalog. I limited my earthquake data to between the latitudes of 38.891 and 52.107 and the longitudes of -122.168 and -131.836. This encompassed the entirety of the Juan De Fuca plate while keeping the processing requirements minimal. I downloaded this data in a .csv format which can be easily processed in Python.\n\nOnce the data was read into my Python file, I performed some minor processing to prepare it for plotting. Time values are converted into datetime and converted from UTC to Pacific time. This process is actually dependent on the time zone of the machine running the code, so time values will vary slightly depending on where this program is executed.\n\nTo fulfill the first assignment requirement, I plotted earthquake magnitude versus time over a ten year period. To make the graph more readable and visually appealing I made the scatter plot entries larger for greater magnitudes and applied a color gradient to further illustrate the data trend.\n\nTo create the main earthquake map I retrieved a map image from OpenStreetMap and created a bounding box the same size as my earthquake data's outermost lat/long positions. I plotted each earthquake based on its latitude and longitude and once again made the scatter points larger based on their magnitude. Creating the map for April 2015 was simple as I only needed to reduce the number of data points to a one month period. \n\n\n# Results:\n\n\n## Earthquake Magnitude vs. Time:\n\n![alt text](https://github.com/mmunson2/BME450-Project3-Seismology/blob/master/Results/EarthquakeMagnitudeVsTime.png \"Magnitude_vs_time\") \n\n\n\n\n## Earthquake Locations and Magnitude:\n\nEarthquake Locations           |  Tectonic Plates\n:-------------------------:|:-------------------------:\n![](https://github.com/mmunson2/BME450-Project3-Seismology/blob/master/Results/EarthquakeMap.png \"Earthquake Locations\") | <img src=\"https://github.com/mmunson2/BME450-Project3-Seismology/blob/master/Results/TectonicPlates.png\" width=\"432\" height=\"582\">\n\n\n## Earthquake Locations and Magnitude in April 2015:\n\n\nEarthquake Locations           |  Tectonic Plates\n:-------------------------:|:-------------------------:\n![](https://github.com/mmunson2/BME450-Project3-Seismology/blob/master/Results/EarthquakeMap2015.png \"Earthquake Locations\") | <img src=\"https://github.com/mmunson2/BME450-Project3-Seismology/blob/master/Results/TectonicPlates.png\" width=\"432\" height=\"582\">\n\n\n# Analysis:\n\n#### Across what geographic area are you able to observe earthquake data in this map? Why do you see most of the earthquakes in that area?   \n\nThe largest clusters of earthquakes are along the tectonic plate boundaries to the north and south of the Juan De Fuca plate. These locations actually contain even smaller tectonic plates, the Explorer Plate and South Gorda Plate, which broke off of the Juan De Fuca Plate millions of years ago. In the south, the Blanco Fracture Zone, Gorda Ridge, and Mendocino Fracture Zone have hundreds of earthquakes of various magnitudes. To the north, Explorer Ridge and the Sovanco Fracture Zone have a cluster of earthquakes with a greater magnitude than other regions. \n\nThe large amount of earthquakes along the Blanco Fracture Zone are likely due to the transform boundary. As the Juan De Fuca Plate moves east towards the Cascadia Subduction Zone, it slides along a portion of the Pacific Plate which protrudes inwards towards the coast. The earthquakes along the Mendocino Fracture Zone are likely due to a similar transform boundary. In this case it's the Gorda Plate making contact with the Pacific plate.\n\nThe larger earthquakes on the northern end of the Juan De Fuca Plate are likely due to the transform boundary between the Pacific Plate and the North American Plate. This trend, while outside of the scope of my data, is visible north and south of the Juan De Fuca Plate.\n\nThe scatter of earthquakes inland of the Juan De Fuca plate are likely a result of the covergence boundary at the Cascadia Subduction zone. This is where the Juan De Fuca Plate slides under the North American Plate. These boundaries are known to create up to 80% of all earthquakes, and are also responsible for the numerous volcanos in the Pacific Northwest.\n\n\n#### What is the range of earthquake size (magnitude) in these data? What is the average earthquake size in this area?  \n\nThe magnitude of earthquakes between 2010 and 2020 ranged between 2.5 and 6.8. The average magnitude was 3.24, indicating that the vast majority of earthquakes were small.\n\n#### In the month of April 2015, where are the earthquakes mostly located? What event can you link these earthquakes to?  \n\nDuring this time period there was a cluster of earthquakes along the divergence boundary between the Pacific Plate and the Juan De Fuca Plate. This boundary is divided into the Cobb, Coaxial, and Cleft Segement, and the earthquakes appear to be primarily located on the Coaxial Segment.\n\nThese earthquakes can be attributed to the eruption of the Axial Seamount Volcano on April 23rd 2015. The volcano is active and last erupted in 2011 and 1998.\n\n## Part II:\n\nIdentify a divergent boundary and a transform boundary on the map you selected in part I\n\n### Divergent Boundary - Gorda Ridge\n\n#### Magnitude Vs. Time\n\n![alt text](https://github.com/mmunson2/BME450-Project3-Seismology/blob/master/Results/MagnitudeVsTime_Gorda_Ridge.png \"Gorda_Ridge_Magnitude_vs_time\") \n\n\n#### Earthquake Locations\n\n![alt text](https://github.com/mmunson2/BME450-Project3-Seismology/blob/master/Results/DiverganceBoundary.png \"Gorda_Ridge_Locations\") \n\n\n#### Discussion:\n\n##### What kind of patterns in earthquake magnitude and location you observe over time along each boundary?\n\n1074 earthquakes occured in the Gorda Ridge divergence zone between 2010 and 2020. The magnitudes ranged from 2.5 to 6.8 and averaged 3.26. This is exactly the same range as the full data set and a slightly higher average. \n\nThe divergence zone has a much higher density of small magnitude earthquakes compared to medium and high intensity earthquakes over time. A vertical line is visible in early 2014 indicating a large earthquake that was followed by smaller aftershocks.\n\nOn a map this region is represented by a line of earthquakes tracing the plate boundary.\n\n\n### Transform Boundary - Blanco Fracture Zone\n\n#### Magnitude Vs. Time\n\n![alt text](https://github.com/mmunson2/BME450-Project3-Seismology/blob/master/Results/MagnitudeVsTime_Blanco_Fracture_Zone.png \"Blanco_Magnitude_vs_time\") \n\n#### Earthquake Locations\n\n![alt text](https://github.com/mmunson2/BME450-Project3-Seismology/blob/master/Results/TransformBoundary.png \"Blanco_Locations\") \n\n#### Discussion:\n\n##### What kind of patterns in earthquake magnitude and location you observe over time along each boundary?\n\n553 earthquakes occured in the Blanco Fracture Zone between 2010 and 2020. The magnitudes ranged from 2.5 to 6.3 and averaged 3.62. This is the same lower bound for range but a lessened maximum value compared to the main data set. The average is notably higher however indicating stronger earthquakes in this region. \n\nUnlike the main data set and Gorda Region, the density of low-magnitude earthquakes in the Blanco Fracture Zone is roughly equivilant to the medium-magnitude earthquakes. This is also indicated by the higher average magnitude. \n\nSimilar to the Gorda Region, the Blanco Fracture Zone appears as a line of earthquakes tracing the plate boundary.\n\n\n# Conclusion\n\nMy Part I data demonstrates the sheer number of earthquakes that occur in a ten year period. In a reasonably small region of the earth the count was over 3,200 earthquakes, which is much more than one would expect. The magnitude vs. time graph demonstrates that the vast majority of earthquakes have a magnitude of 4 or less which is explains why they are seldom noticed. When plotted on a map, the earthquakes trace the fault lines, and are especially noticeable along transform boundaries. The divergence zone on the western end of the Juan De Fuca Plate has a surprisingly small amount of earthquakes, with the exception of the April 2015 Axial Seamount eruption. The convergence boundary at the Cascadia Subduction Zone also lacks a defining line of earthquakes, but instead is visible in a scatter of inshore earthquakes. This is likely due to the Juan De Fuca plate traveling under the North American Plate and pushing magma upwards in the process. This cycle is responsible for the Northwest's famous volcanoes.\n\nIn Part II a divergent and transform boundary are compared by analyzing the Gorda Ridge and Blanco Fracture Zone. While it's difficult to discern a clear pattern in the magnitude vs. time graph, the transform boundary had fewer, stronger earthquakes on average. In comparison, the divergence boundary had almost twice as many earthquakes, most of which were very small.\n\n\n# References\n\nGlobal Volcanism Program, 2013. Volcanoes of the World, v. 4.8.6. Venzke, E (ed.). Smithsonian Institution. Downloaded 02 Mar 2020. https://doi.org/10.5479/si.GVP.VOTW4-2013\n\n“Why Study Cascade Volcanoes?,” Volcano Hazards Program, Mar. 2019.\n\nA. Qassim, “Easy Steps To Plot Geographic Data on a Map — Python,” Towards Data Science, Aug. 2019.\n\nGulick, S., Meltzer, A., and S. Clarke (1998) Seismic structure of the southern Cascadia subduction zone and accretionary prism north of the Mendocino triple junction. Journal of Geophysical Research-Solid Earth, 103(B11).\n\nUSGS Data:  \nhttps://earthquake.usgs.gov/earthquakes/map/\n\nOpen Street Map:  \nhttps://www.openstreetmap.org/#map=5/43.386/-124.194\n\n\n\n\n\n",
        "createdAt": "2020-02-25T01:12:28.000Z",
        "updatedAt": "2022-01-05T15:27:18.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/mmunson2/BME450-Project3-Seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "lennijusten/Kaikoura",
        "url": "https://github.com/lennijusten/Kaikoura",
        "description": "A suite of scripts to compare human-based seismic wave (P&S) arrival time detection with the deep learning-based PhaseNet picking method.",
        "stars": 3,
        "forks": 0,
        "readme": "# Kaikoura Earthquake\nContains scripts to read, filter, and process seismic waveform data.  \n  \nThe waveform_dsp.py script creates displays the probability output from the PhaseNet neural network seismic wave detection algorithm and compares it with human picks. Sample plots are shown in the repository. \n\n![Waveform 1](https://raw.githubusercontent.com/lennijusten/Kaikoura/master/plots/NZ_MSWZ_EH_2020p253701_13001.png)\n![Waveform 2](https://raw.githubusercontent.com/lennijusten/Kaikoura/master/plots/NZ_MRNZ_EH_2020p253701_13001.png)\n\nPhaseNet Neural Network:   \nZhu, W., & Beroza, G. C. (2018). PhaseNet: A Deep-Neural-Network-Based Seismic Arrival Time Picking Method. arXiv preprint arXiv:1803.03211.\n",
        "createdAt": "2020-05-28T18:33:26.000Z",
        "updatedAt": "2025-02-04T10:40:46.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/lennijusten/Kaikoura/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "djpugh/MTfit",
        "url": "https://github.com/djpugh/MTfit",
        "description": "MTfit code for Bayesian Moment Tensor Fitting",
        "stars": 86,
        "forks": 17,
        "readme": ".. image:: https://travis-ci.org/djpugh/MTfit.svg?branch=develop\n    :target: https://travis-ci.org/djpugh/MTfit/\n\n.. image:: https://ci.appveyor.com/api/projects/status/rvi74lcro7q3od85?svg=true\n    :target: https://ci.appveyor.com/project/djpugh/mtfit \n\nThe documentation is available at `https://djpugh.github.io/MTfit/ <https://djpugh.github.io/MTfit/>`_ and can be built using `sphinx` from the source in MTfit/docs/, or using the `build_docs.py`.\n\nThe documentation includes tutorials and explanations of MTfit and the approaches used.\n\nPlease note that this code is provided as-is, and no guarantee is given that this code will perform in the desired way. Additional development and support is carried out in the developer's free time.\n\n**Restricted:  For Non-Commercial Use Only**\nThis code is protected intellectual property and is available solely for teaching\nand non-commercially funded academic research purposes.\nApplications for commercial use should be made to Schlumberger or the University of Cambridge.\n\n\nInstalling MTfit\n*********************************\n\nMTfit is available on `PyPI` and can be installed using:\n\n    >>pip install MTfit\n\nAlternative this repository can be cloned and the package then installed simply by calling::\n    \n    >>python setup.py install\n\nMTfit is dependent on numpy and scipy, and for MATLAB -v7.3 support also requires h5py.\nCluster support will be automatically installed via pyqsub from github\nMPI support requires mpi4py built against a valid MPI distribution.\n\nTo build the C extensions when compiling from source you will need cython and associated C compilers\n\n\n\n\n! Known Bug - running with MPI and very large non-zero MT results can lead to an error: mpi4py SystemError: Negative size passed to PyString_FromStringAndSize - to fix, re-run with smaller sample sizes\n",
        "createdAt": "2017-04-07T14:40:19.000Z",
        "updatedAt": "2025-11-10T02:48:06.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/djpugh/MTfit/develop/README.rst",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "cangyeone/QuantitativeSeismology",
        "url": "https://github.com/cangyeone/QuantitativeSeismology",
        "description": "a book about Quantitative Seismology in Chinese",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2017-02-14T12:48:26.000Z",
        "updatedAt": "2017-02-14T12:48:26.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "YuanYusung/SeisCodes",
        "url": "https://github.com/YuanYusung/SeisCodes",
        "description": "A collection of useful codes in seismology for my personal research.",
        "stars": 0,
        "forks": 0,
        "readme": "# SeisCodes\nA collection of useful codes in seismology for my personal research.\n<img width=\"576\" height=\"384\" alt=\"image\" src=\"https://github.com/user-attachments/assets/30cf4440-ae21-4de1-be43-8013ad5850bb\" />\n\n",
        "createdAt": "2025-12-01T11:00:32.000Z",
        "updatedAt": "2025-12-03T10:49:06.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/YuanYusung/SeisCodes/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "bsmithyman/pygeo",
        "url": "https://github.com/bsmithyman/pygeo",
        "description": "Python tools for exploration seismology",
        "stars": 14,
        "forks": 5,
        "readme": "pygeo\n=====\n\nPython tools for exploration seismology\n",
        "createdAt": "2013-05-22T23:27:39.000Z",
        "updatedAt": "2024-04-23T19:31:12.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/bsmithyman/pygeo/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "zhangzj1209/SEGYMAT",
        "url": "https://github.com/zhangzj1209/SEGYMAT",
        "description": "Matlab module of exploration seismology",
        "stars": 1,
        "forks": 0,
        "readme": "# Matlab module of exploration seismology\n\n## SeismicLab\n\n- SeismicLab is a MATLAB seismic data processing package. It was developed by the [***Signal Analysis and Imaging Group***](https://saig.physics.ualberta.ca/) of the University of Alberta, Canada.\n- The package can be used to process small seismic data sets and, it is mainly intended for research and teaching purposes. Scripts to read and write SU data (the SEGY flavor used by [Seismic Un*x](www.cwp.mines.edu/cwpcodes/index.html)) are provided. A particular feature of this package is that the SEGY headers are loaded into a structure that can be easily assessed by MATLAB.\n  - [GNU General Public License](http://seismic-lab.physics.ualberta.ca/gpl.html)\n  - Download SeismicLab [ [SeismicLab.tar.gz](http://seismic-lab.physics.ualberta.ca/SeismicLab.tar.gz) | [SeismicLab.tar.Z](http://seismic-lab.physics.ualberta.ca/SeismicLab.tar.Z) | [SeiemicLab.tar](http://seismic-lab.physics.ualberta.ca/SeismicLab.tar)]\n  - Add the script `setpath.m` in your working directory\n    - download [`setpath.m`](http://seismic-lab.physics.ualberta.ca/setpath.m)\n    - see [`setpath.m`](http://seismic-lab.physics.ualberta.ca/setpath.html)\n    - try this one for windows [`setpath_Windows.m`](http://seismic-lab.physics.ualberta.ca/setpath_Windows.m)\n  - Edit `setpath.m` to reflect the path to SeismicLab in your system\n  - Start Matlab, run setpath\n  - Try the [demos](http://seismic-lab.physics.ualberta.ca/help.html#A11)\n- Users can download the relevant information through the website http://seismic-lab.physics.ualberta.ca/.\n\n## CREWES\n\n### Introduction\n\n- **CREWES MATLAB Software Library** (CMSL) is a software package developed by Gary F. Margrave for teaching and research of exploration seismology.\n- An older, less complete, free version is provided [here](https://www.crewes.org/ResearchLinks/FreeSoftware/).\n- The software package accompanies the textbook [***Numerical Methods of Exploration Seismology: With Algorithms in MATLAB***](https://www.cambridge.org/core/books/numerical-methods-of-exploration-seismology/53A21CAD45D4047D117191E6BF4408E2) (NMES) by Gary F. Margrave and Michael P. Lamoureux (Cambridge University Press, 2019).\n\n### CREWES Toolbox Version: 2104\n\n- [CREWES Matlab Toolbox (ZIP)](https://www.crewes.org/ResearchLinks/FreeSoftware/crewes_educational.zip) 156.6 MB\n- [Sample data to accompany Methods of Seismic Data Processing (ZIP)](https://www.crewes.org/ResearchLinks/FreeSoftware/NMESdata.zip) 13.65 MB\n- [Guide to the CREWES Matlab toolbox (PDF)](https://www.crewes.org/ResearchLinks/FreeSoftware/NumMeth.pdf)  6.98 MB\n- [Introductory seismic data processing course (PDF)](https://www.crewes.org/ResearchLinks/FreeSoftware/Methods_of_Seismic_Data_Processing.pdf)  88.09 MB\n\n### Installation instructions\n\n- Extract the contents of crewes_educational.zip to:\n  - **Microsoft Windows:** ```%USERPROFILE%\\Documents\\MATLAB\\crewes```\n  - **Linux:** ```$HOME/Documents/MATLAB/crewes```\n- Then start Matlab and look for ```Set Path``` on the Home Ribbon.\n- In the window that appears, click on the button ```Add with subfolders...``` .  Locate the folder called ```crewes``` -- Click on it once to highlight it, then click ```Select Folder```.  Save the new path by clicking ```Save```. \n\n## SegyMAT\n- SegyMAT is a set of m-files for reading and writing SEG-Y files from [MATLAB](http://mathworks.com) and [Octave](https://www.gnu.org/software/octave/), that aims to\n  - completely support SEG-Y revision 0 and 1;\n  - be easy to use in other projects;\n  - be a Swiss Army knife dealing with the SEGY-Y format in MATLAB/Octave.\n\n- SegyMAT is not lightning fast. SegyMAT makes heavy use of ‘structures’. Unfortunately structures are not very effective in terms of speed in MATLAB. (Or they have not been implemented very effectively in SegyMAT). However structures make the implementation and maintenance easier, and the code (hopefully) easy to read. That said, some effort has been made to optimize SegyMAT for speed.\n- The latest **stable** version of SegyMAT is available from [Sourceforge](https://sourceforge.net/projects/segymat/).\n- The current **development** version of SegyMAT is available from [Github](https://github.com/cultpenguin/segymat).\n- The SegyMAT's documentation is available from https://segymat.readthedocs.io/en/latest/.\n",
        "createdAt": "2023-04-27T08:57:44.000Z",
        "updatedAt": "2025-04-23T08:11:16.000Z",
        "language": "MATLAB",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/zhangzj1209/SEGYMAT/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "pupusqira/hello-world",
        "url": "https://github.com/pupusqira/hello-world",
        "description": "I'am new at seismology and machine learning",
        "stars": 0,
        "forks": 0,
        "readme": "# hello-world\n\nHi everyone,\nMy name is Pupus Qira, from Master student from Institut Technology of Bandung, Indonesia.\n\nI'am new at seismology and machine learning\n\nThere is a reason why I interested with that is my enviromental background. Indonesia lies on 2 tectonic plate. Eurasia and Australia plate. According to Metrology and Geophysics Indonesia agency, form 1967-2006 there are 3.486 earthquake with up tp 6 SR magnitude, 27 devastating earthquake, and 13 induced Tsunami. Because of that, i want to start learn about seismology to make hazard mitigation analysis and build some code with so all of you can discuss..\n\nit will be great if you contact me and make some project about earthquake and machine learning together\n\nBest Regrad\n\n",
        "createdAt": "2019-09-04T03:17:41.000Z",
        "updatedAt": "2019-09-04T03:55:54.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/pupusqira/hello-world/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "jojomale/eidaQC",
        "url": "https://github.com/jojomale/eidaQC",
        "description": "Tools to test data availability on Eida network",
        "stars": 1,
        "forks": 1,
        "readme": "*This project is new and maybe not everything works* *smoothly yet.*\n\n\nRun tests on the \n[European Integrated Data Archive](http://www.orfeus-eu.org/data/eida/) \nfor seismological data\nto check the availability of waveform data and the processing of metadata requests.\n\nThis package provides two different tests to check the availability of waveform data and the processing of \nmeta data requests on EIDA. \nThe tests are intended to be run on a regular \nbasis, e.g. via a cron job. The package also creates an\nautomatic summary report of the results.\n\n# Documentation\n\nhttps://eidaqc.readthedocs.io/\n\n(in progress)\n\n# Installation\n\nFor more details, take a look at https://eidaqc.readthedocs.io/en/latest/eidaqc.html#installation\n\n## Install dependencies\nYou need `pip` or `pipenv` to install eidaqc. Eidaqc was developed on Python 3.9.7 but\nshould run on lower versions of Python 3 (tested Python 3.6, requires additional package importlib_resources). Python 2 is not supported.\n\nThe following packages are required to run eidaqc:\n- Obspy: https://github.com/obspy/obspy/wiki#installation\n- Numpy\n- Matplotlib\n\nTheir **installation will be forced** along the installation of eidaqc if not already present.\n\n\nOptionally, if you want your results plotted on a nice map, you should install one of these mapping libraries:\n- Cartopy: https://scitools.org.uk/cartopy/docs/latest/installing.html\n- Basemap Matplotlib Toolkit: https://matplotlib.org/basemap/\n\nThey are **not installed automatically** with eidaqc if not already available to your Python.\n\nCartopy is recommended and preferred, but on an older system (Ubuntu 18.04)\ninstallation of its predecessor basemap might be easier. However, development of basemap has stopped and shifted to cartopy.\n\n\n## Install eidaqc\n\nAfter installing the dependencies, eidaqc can be installed \nvia pip from Github (also recommended if you use conda):\n\n\n    ```\n    $ pip install git+https://github.com/jojomale/eidaQC.git\n    ```\n\n\nAlternatively can also use pipenv:\n\n    ```\n    $ pipenv install git+https://github.com/jojomale/eidaQC.git#egg=eidaqc\n    ```\n\n# Usage\n\n1. as API\n2. as command line tool:\n    ```\n    eida <subcommand> <args> <configfile>\n    ```\n\n## Command line interface\n\nThe main functionalities of the eidaqc-package are \navailable as command line tools. The commands work similar to svn or git commands.\nThe options for ``<args>`` and ``<configfile>`` depend \non the [subcommand](https://eidaqc.readthedocs.io/en/latest/eidaqc.html#usage).\n\n\n\n### eida\n\nIf you call ``eida`` without arguments, you get a man page.\n\n\n### eida template\n\nUsage:\n```\neida template [-h] [-o OUTPUTFILE]\n```\n\nCreates a default configuration file and css-file for \nhtml-report in the current directory.\n\nOptional arguments:\n- ``-h``, ``--help``  \n    show help message\n- ``-o OUTPUTFILE``, ``--outputfile OUTPUTFILE``: \n    file name for \n    default file. If not given, file name will be \n    ``OUTPUTFILE = ./default_config.ini``\n    in current directory.\n\nAlias:\n- ``templ``\n\n\n### eida availability\n\nRun availability test.\n\nUsage:\n```\n    eida availability [-h] [-i] configfile\n```\n\nRequired arguments:\n- ``configfile``            \n    Configuration file with parameter settings. Use \n    ``eida templ`` to create default template.\n\nOptional arguments:\n- ``-h``, ``--help``  \n    show help message\n- ``-i``, ``--ignore_missing``  \n    If set missing networks will be ignored, when inventory \n    is requested from server. Use when run for the first time \n    and no cached inventory ('outdir/chanlist_cache.pickle')\n    is available.\n\nAlias:\n- ``avail``\n\n\n### eida inventory\n\nRun inventory test.\n\nUsage:\n```\n    eida inventory [-h] {network,station,channel} configfile\n```\n\nRequired arguments:\n- ``{network,station,channel}``\n    Level of detail for the requested inventories. \n    ``network`` provides the least information (and puts the \n    least amount of load on the servers).\n- ``configfile``            \n    Configuration file with parameter settings. \n    Use ``eida templ`` to create default template.\n\nOptional arguments:\n  \n- `-h`, `--help`\n    show this help message and exit\n\n\nAlias:\n- ``inv``\n\n\n### eida report\n\nCreate html and pdf report.\n\nUsage:\n```\neida report [-h] configfile\n```\n\nRequired arguments:\n- ``configfile``            \n    Configuration file with parameter settings. \n    Use ``eida templ`` to create default template.\n\nOptional arguments:\n- ``-h``, ``--help``  \n    show help message\n\nAlias:\n- ``rep``\n\n\n# Build documentation\nDocumentation is build using sphinx and the readthedocs-template. You need to install both:\n\n```bash\n$ conda install sphinx\n$ pip install sphinx-rtd-theme\n```\n\nThen you can run:\n```bash\n$ mkdir doc\n$ cd doc\n$ sphinx-quickstart \n```\nwhich asks for some user input. It creates a sceleton structure for\nthe documentation and an initial configuration file `conf.py`.\n\nEdit `conf.py` as follows:\n\n```python\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath('../src'))\n\n[...]\n\nextensions = ['sphinx.ext.autodoc', \n              'sphinx.ext.napoleon']\n\n[...]\n\nhtml_theme = 'sphinx_rtd_theme'\n```\n\nThen run:\n```bash\n$ sphinx-apidoc -e -M -f -o . ../src/eidaqc\n$ sphinx-build -b html docs/source/ docs/html\n```\n\nThe first lays out the package structure in `modules.rst`\nand `eidaqc.rst`.\nThe second command builds a HTML-page in a subdirectory `html`.\n\n# Resources\n\n## Build and packaging\n```bash\n$ python -m build\n```\n- https://packaging.python.org/tutorials/packaging-projects/\n- https://packaging.python.org/guides/distributing-packages-using-setuptools/\n- https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n- https://packaging.python.org/discussions/install-requires-vs-requirements/#install-requires-vs-requirements-files\n- https://github.com/pypa/sampleproject\n\n\n## Access data \n- https://stackoverflow.com/questions/779495/access-data-in-package-subdirectoryhttps://stackoverflow.com/questions/779495/access-data-in-package-subdirectory\n- https://stackoverflow.com/questions/6028000/https://stackoverflow.com/questions/6028000/how-to-read-a-static-file-from-inside-a-python-package\n- https://docs.python.org/3/library/importlib.html#module-importlib.resources\n- https://importlib-resources.readthedocs.io/en/latest/using.html\n\n## Entry points\n- https://setuptools.pypa.io/en/latest/userguide/entry_point.html#dynamic-discovery-of-services-and-plugins\n\n## Project Layout\n- https://realpython.com/python-application-layouts/\n- https://github.com/pypa/sampleproject\n\n## Import horror\n- https://docs.python.org/3/reference/import.html\n- https://realpython.com/python-import/\n\n## Docs\n- https://sphinx-rtd-tutorial.readthedocs.io/en/latest/index.html\n",
        "createdAt": "2021-10-27T17:01:45.000Z",
        "updatedAt": "2022-10-21T12:59:10.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/jojomale/eidaQC/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "zijinping/CUHK_Seismology_Python",
        "url": "https://github.com/zijinping/CUHK_Seismology_Python",
        "description": null,
        "stars": 4,
        "forks": 1,
        "readme": "# Welcome to CUHK Seismology Group Shared Repository\n\n## Guidelines to contribute the repository\n**_NOTE:_** Please upload minimum amount of (waveform) data to keep the repository light. \n\nDemos:\n  Demo use of builded packages/scripts\n \n ---\n### Basic Usage of Github\nGit(hub) is a code version-control platform to maintain the development and collaborations of code. The following shows the basic instruction to use and contribute the project.\n\n#### First-time user\n**_NOTE:_** This repository is for sharing of codes. Please do not work directly on the repository. It is suggested to make a copy of the useful code. Or else, your work will be shared in here.\nSetup a \"cloned\" version of the repository to your computer\n\n* git clone [url]\n\nSynchronize the latest code from github to your local computer repository\n\n* git pull\n\nContribute after amending codes in your local machine\n\n* git add .\n* git commint -m \"msg of the commit\"\n* git push\n\n### Common python package requirement\nobspy\nnumpy\npandas\nmatploblib\n\n#run setup.py to add the path to your python environment\n\n---\n### List of useful packages for seismology analysis\n\n- Earthquake Detection\n  - [PhaseNet](https://github.com/wayneweiqiang/PhaseNet.git)\n  - [GeneralizedPhaseDetection](https://github.com/interseismic/generalized-phase-detection)\n  - EQscancorr\n  \n- Earthquake Phase Association and Location (& Relocation)\n  - [REAL: Rapid Earthquake Assocation](https://github.com/Dal-mzhang/REAL.git)\n  - Velest\n  - Hypoinverse\n  - HypoDD\n  - Glowclast\n  \n- Earthquake Source Study\n  - [HASH - First-motion DC inversion](Earthquake_Source)\n  \n- Analysis\n  - topodd\n  - Ambient Noise\n    - [MSnoise](http://www.msnoise.org/)\n    \n- Plotting\n  - GMT\n  - PYGMT\n  - Matplotlib\n\n- Data Management\n  - ASDF\n  - Antelope\n\n- Data Source\n  - [IRIS](Data_Management/IRIS_fetch/)\n \n\n---\n## Contributors\nLast but not least, thanks all for contribution the community\n*   @zijinping\n*   @jwjeremy\n*   @JunhaoSong\n",
        "createdAt": "2020-12-29T14:29:30.000Z",
        "updatedAt": "2025-06-16T22:11:45.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/zijinping/CUHK_Seismology_Python/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "maswiet/Volcano_Seismology",
        "url": "https://github.com/maswiet/Volcano_Seismology",
        "description": "Lecture for Volcano Seismology ",
        "stars": 0,
        "forks": 1,
        "readme": "# Volcano_Seismology\nLecture for Volcano Seismology \n",
        "createdAt": "2019-02-14T00:37:38.000Z",
        "updatedAt": "2019-02-21T00:43:25.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/maswiet/Volcano_Seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "rintr-suzuki/WINLocator",
        "url": "https://github.com/rintr-suzuki/WINLocator",
        "description": "Tool to do hypocenter location from associated phases by WIN system (Urabe and Tsukada, 1992).",
        "stars": 0,
        "forks": 0,
        "readme": "# WINLocator\n## Related Publication\n\n[![DOI](https://img.shields.io/badge/DOI-10.1126%2Fscience.adt6389-blue)](https://doi.org/10.1126/science.adt6389)\n\nIf you use this code or data, please cite the following paper: <br>\nRintaroh Suzuki _et al._, The forearc seismic belt: A fluid pathway constraining down-dip megathrust earthquake rupture. _Science_ **389**, 190-194 (2025). https://doi.org/10.1126/science.adt6389\n\n## Summary\n\n![](docs/assets/WINLocator_overview.png)\n \n* Tool to do hypocenter location from associated phases by WIN system (Urabe and Tsukada, 1992).\n* Easy to run on various OS by using **docker**.\n\n## Requirements\n* OS <br>\n  Support Windows, macOS and Linux\n\n* (Only required for Windows) Git Bash <br>\n  https://gitforwindows.org/ <br>\n  For Windows, run \"Git Bash\" and use it to execute commands for following steps.\n\n* docker <br>\n  * Installation <br>\n  For Windows and macOS, install \"Docker Desktop\" and run it to activate docker. <br>\n  https://docs.docker.com/get-docker/ <br>\n  For Linux, install \"Docker Engine\". <br>\n  https://docs.docker.com/engine/install/ <br>\n\n  * (Only required for Linux) Create the docker group and add your user <br>\n  https://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user <br>\n\n  * Verify installation <br>\n    ```\n    $ docker run hello-world\n    ...\n    Hello from Docker!\n    This message shows that your installation appears to be working correctly.\n    ...\n    ```\n    \n## Usage\n* Installation\n  ```\n  $ git clone https://github.com/rintr-suzuki/WINLocator.git\n  $ cd WINLocator\n  ```\n\n* Execution\n  ```\n  $ ./WINLocator.bash --infile data/associated_picks.json\n  # See 'data' directory for the result.\n  ```\n\n* See [here](docs/README-usage.md) for the detailed information.\n\n* See [here](docs/Tips.md) for the tips of this tool.\n\n## References\n* Hasegawa, A., Umino, N., & Takagi, A. (1978), Double-planed structure of the deep seismic zone in the northeastern Japan arc. Tectonophysics, 47(1–2), 43–58. https://doi.org/10.1016/0040-1951(78)90150-6\n* Tamaribuchi, K., Hirose, F., Noda, A., Iwasaki, Y., Iwakiri, K., & Ueno, H. (2021), Noise classification for the unified earthquake catalog using ensemble learning: the enhanced image of seismic activity along the Japan Trench by the S-net seafloor network. Earth, Planets and Space, 73, 91. https://doi.org/10.1186/s40623-021-01411-6\n* Urabe, T., & Tsukada, S. (1992), win - A Workstation Program for Processing Waveform Data from Microearthquake Networks. Programme and Abstracts, the Seismological Society of Japan, 2, 331-331. (In Japanese)\n",
        "createdAt": "2024-03-20T14:39:15.000Z",
        "updatedAt": "2025-07-22T13:24:01.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/rintr-suzuki/WINLocator/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "MIGG-NTU/SeisTomo_Tutorials",
        "url": "https://github.com/MIGG-NTU/SeisTomo_Tutorials",
        "description": "Seismic Body-wave Traveltime Tomography Tutorials",
        "stars": 10,
        "forks": 7,
        "readme": "# Seismic Body-wave Traveltime Tomography Tutorials\n\n**From 2021/07/03, the tutorials are maintained on [Seismology101](https://seismo-learn.org/seismology101/),\nwhich are tutorials for absolute beginners in Seismology in Chinese.**\n\n**From 2021/07/03, the tutorials on this website are not maintained any more.**\n\n----\n\nThis repository contain tutorials for seismic body-wave traveltime tomography.\n\n## Contributors\n\n- [core-man](https://github.com/core-man)\n- [HouseJaay](https://github.com/HouseJaay)\n- [Tianjueli](https://github.com/tianjueli)\n- [Shucheng-Wu](https://github.com/Shucheng-Wu)\n\nHuge thanks to [seisman](https://github.com/seisman) for some suggestions and instructions.\n",
        "createdAt": "2020-10-23T06:10:20.000Z",
        "updatedAt": "2025-07-26T02:12:18.000Z",
        "language": "Python",
        "homepage": "https://migg-ntu.github.io/SeisTomo_Tutorials/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/MIGG-NTU/SeisTomo_Tutorials/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "christopher-lindsay/KeplerSeismologyProject",
        "url": "https://github.com/christopher-lindsay/KeplerSeismologyProject",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2021-01-20T06:49:54.000Z",
        "updatedAt": "2021-01-20T06:51:02.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "YasmineAche/seismology",
        "url": "https://github.com/YasmineAche/seismology",
        "description": "This repository contains a collection of Python scripts and programs related to seismology.",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2023-09-22T21:40:38.000Z",
        "updatedAt": "2023-10-26T12:15:18.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "xdas-dev/xdas",
        "url": "https://github.com/xdas-dev/xdas",
        "description": "Python framework for Distributed Acoustic Sensing (DAS).",
        "stars": 55,
        "forks": 7,
        "readme": "<div align=\"center\">\n<picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"./docs/_static/logo-dark.png\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"./docs/_static/logo-light.png\">\n    <img alt=\"Xdas Logo\" height=\"250px\">\n</picture>\n</div>\n\n-----------------\n\n[![Documentation Status](https://readthedocs.org/projects/xdas/badge/?version=latest)](https://xdas.readthedocs.io/en/latest/?badge=latest)\n[![Tests Status](https://github.com/xdas-dev/xdas/actions/workflows/tests.yaml/badge.svg)](https://github.com/xdas-dev/xdas/actions/workflows/tests.yaml)\n[![codecov](https://codecov.io/gh/xdas-dev/xdas/graph/badge.svg?token=00MD52JRA3)](https://codecov.io/gh/xdas-dev/xdas)\n[![PyPI](https://img.shields.io/pypi/v/xdas)](https://pypi.org/project/xdas/)\n[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)\n[![DOI](https://zenodo.org/badge/560867006.svg)](https://zenodo.org/badge/latestdoi/560867006)\n\n# Xdas: a Python Framework for Distributed Acoustic Sensing\n\n*Xdas* is an python library for managing, processing and visualizing **Distributed Acoustic Sensing (DAS)** data. It reads any DAS format into self-described python abstractions that encapsulates both the data and the metadata (coordinates and attributes). Xdas reuses concepts of labeled N-dimensional arrays developped by the [Xarray](https://xarray.dev) library. It takes inspiration from [Dask](https://www.dask.org/) in term of lazy computing.\n\n## Key Features\n\n- Seamless manipulation of large multi-file datasets in their native DAS-specific format.\n- Signal Processing: Multi-threaded implementations of common routines.\n- Extensibility: build your pipeline with a mix of Xdas, NumPy/SciPy, and/or your own custom routines. \n- Larger-than-memory processing: apply your piplines with optimized I/O latencies.\n\nXdas can also be used in other context than DAS, for example in the context of other dense and heavy N-dimenional arrays such as large-N seismic arrays.\n\n## Installation\n\nXdas can be fetched from [PyPI](https://pypi.org/project/xdas):\n\n    pip install xdas\n\n## Documentation\n\nThe documentation is available at: [https://xdas.readthedocs.io](https://xdas.readthedocs.io).\n\n## Tutorials\n\nA comprehensive series of tutorials can be found at: [https://github.com/xdas-dev/tutorials](https://github.com/xdas-dev/tutorials).\n\n## Contributing\n\nYou can find information about contributing to Xdas in our [Contributing Guide](https://xdas.readthedocs.io/en/latest/contribute.html).\n\n## Get in touch\n\n- Ask usage questions and discuss any ideas on [GitHub Discussions](https://github.com/xdas-dev/xdas/discussions).\n- Report bugs, suggest features or view the source code on [GitHub](https://github.com/xdas-dev/xdas).\n- To follow the main announcements such as online trainning sessions please register to our [Newsletter](https://groups.google.com/g/xdas).\n\n## Citing\n\nIf you use Xdas for your DAS data processing, please consider [citing the project](https://xdas.readthedocs.io/en/latest/cite.html).\n",
        "createdAt": "2022-11-02T12:53:53.000Z",
        "updatedAt": "2025-12-02T07:48:05.000Z",
        "language": "Python",
        "homepage": "https://xdas.readthedocs.io",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/xdas-dev/xdas/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "vegaonline/seismo",
        "url": "https://github.com/vegaonline/seismo",
        "description": "Seismology with SPH c++ based",
        "stars": 0,
        "forks": 0,
        "readme": "# seismo\nSeismology with SPH c++ based\n",
        "createdAt": "2016-05-10T08:20:25.000Z",
        "updatedAt": "2016-05-10T08:22:39.000Z",
        "language": "C++",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/vegaonline/seismo/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "anangsahroni/Quantitative-Seismology",
        "url": "https://github.com/anangsahroni/Quantitative-Seismology",
        "description": "This repo will store my works and assignments related to quantitative seismology subject",
        "stars": 0,
        "forks": 0,
        "readme": "# Quantitative-Seismology\nThis repo will store my works and assignments related to quantitative seismology subject\n",
        "createdAt": "2020-12-03T02:18:47.000Z",
        "updatedAt": "2020-12-14T04:28:49.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/anangsahroni/Quantitative-Seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "andreww/prem4derg",
        "url": "https://github.com/andreww/prem4derg",
        "description": "Implementation of PREM in Python for the Deep Earth Research Group in Leeds",
        "stars": 6,
        "forks": 2,
        "readme": "# prem4derg\n\nThis is a python module and some example notebooks to allow us (the Deep Earth Research Group at Leeds) to play around with\nPREM, a famous 1D model of the Earth's interior published by Dziewonski and Anderson in 1981.\n\n## Installation\n\nThis module relies on python (version 3), Numpy and Scipy. Examples are distributed as Jupyter notebooks, which need Jupyter\nand Matplotlib to run. Calculation of travel time curves (in the third example notebook) needs Obspy. Installation and managment\nof all these dependencies is most easily done in a conda environment. This can be done using the command:\n\n    conda create -n p4d -c conda-forge python=3.7 obspy scipy numpy matplotlib jupyter\n   \nThis creates a new environment called `p4d`. Once an environment is created, this module can be downloaded by running:\n\n    git clone https://github.com/andreww/prem4derg.git\n    \nThis creates a directory called `prem4derg`. Experianced users of git who want to contribute to the code may want to fork this\nrepository in github and clone from their fork.\n\nTwo final steps will need to be done each time you want to use the code. These are to change directory into the `prem4derg` directory,\nand activate the `p4d` environemnt by running `source activate p4d`.\n\n### Mineos\n\nIn order to compute normal mode frequencies, we use [mineos](https://geodynamics.org/cig/software/mineos/). I \ninstalled this as follows (working outside the prem4derg directory):\n\n    wget http://geoweb.cse.ucdavis.edu/cig/software/mineos/mineos-1.0.2.tgz\n    mkdir cig\n    tar -xzvf mineos-1.0.2.tgz\n    cd mineos-1.0.2\n    ./configure --prefix=/Users/andreww/Code/cig\n\nEdit the Makefile to add: `-Wno-error -Wno-return-type` to the `CFLAGS` because the C contains errors.\nAdd `-fallow-argument-mismatch` to FFLAGS because the Fortran contains errors.\n\n    make \n    make install\n\n## Examples of use\n\nExample Jupyter notebooks can be accessed and explored by running `jupyter notebook`. Four examples are\ncurrently provided:\n\n* A [density example](./PREM_density_example.ipynb), showing how a model can be defined and used to calculate mass, moment of inertia, gravity and pressure.\n* A [velocity example](./PREM_velocity_example.ipynb), showing how a model can be defined and used to calculate seismic velocities as a function of depth and period.\n* A [travel time example](./PREM_travel_times_example.ipynb), showing how an obspy taupy model can be created and used to compute travel time curves.\n* A [normal modes example](./PREM_normal_modes_example.ipynb), showing how Mineos can be used to compute normal mode frequencies.\n\nThese examples are only starting point. For example, the code could be used to fit new models. \n\n## Development and support\n\nPrem4derg is new software, bugs and rough edges may abound. Users who are interested in \nchanging the code are encouraged to [create a fork on GitHub and submit \nchanges via pull requests](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests).\nProblems can be reported via [issues](https://github.com/andreww/prem4derg/issues), which also list areas\nwhere further development may be useful.\n  \n\n## References\n\nDziewonski, A. M. and Anderson, D. L. (1981) Preliminary reference Earth model *Physics of the Earth and \nPlanetary Interiors* vol.25 pp.297—356. \n",
        "createdAt": "2019-10-25T17:24:19.000Z",
        "updatedAt": "2025-12-01T14:57:06.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/andreww/prem4derg/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "raspishake/Berkeley-mSEED-downloader",
        "url": "https://github.com/raspishake/Berkeley-mSEED-downloader",
        "description": "Developed for Angela Chung at Berkeley Seismology Lab for testing the use of RS data in an Earthquake Early Warning (EEW) application in California.",
        "stars": 3,
        "forks": 0,
        "readme": "# Berkeley mSEED Downloader\n### Developed for Angela Chung at Berkeley Seismology Lab for testing the use of RS data in an Earthquake Early Warning (EEW) application in California.\n\n*Coded by Giuseppe Petricca (@gmrpetricca)*\n\n[![GitHub](https://img.shields.io/github/license/raspishake/rsudp)](https://github.com/raspishake/rsudp/blob/master/LICENSE)\n\nThis script will read data both from the Raspberry Shake AM network servers and from the attached `.xlsx` file. It will automatically download a list of .mSEED files in relation to user-input earthquake details and range in km.\n\nRequired software and packages:\n- Python 3\n- Jupyter\n- Obspy\n- Matplotlib\n- Numpy\n- Pandas\n- urllib\n\nInstallation via Anaconda:\n```bash\n# install the environment with the correct software:\nconda create -n berkeleymseed python=3 jupyter matplotlib obspy numpy pandas urllib\n# activate the environment\nconda activate berkeleymseed\n# start Jupyter Notebook\njupyter-notebook\n```\n\nOnce this is done, it is possible to open the `.ipynb` file in the repository. Microsoft Excel is required to open the `.xlsx` file, please keep the two in the same folder.\n\nThe process and file are commented throughout the various steps, however, this is a brief guide to use them: \n\n1. Open the `.xlsx` file and add the UTC Date/Time (with the format `YYYY-MM-DDThh-mm-ss.ddd`) of an earthquake, together with Latitude and Longitude of its epicenter, in decimals (i.e. `34.256`, `-116.289`)\n2. The earthquake list can be as long as it is desired. Please do not insert empty rows between an event and another. Three test events are listed for demonstration purposes.\n3. Save the `.xlsx` file and open the `.ipynb` file. Nothing should be modified in this file unless a particular condition is required.\n4. Execute the script. \n5. For each event, the script will ask the user for a range in km in which it will try to find any Raspberry Shake unit that was connected at the time. In the case of not finding any, the script will prompt the user.\n6. At the end of the process, every event `.mseed` downloaded files, for each channel of each Shake unit included in the km range, will be put in a separate folder, with the following naming convention: `YYYYMMDDhhmmssddd.network.stationcode.channel.mseed`\n7. Another `.xlsx` file will be created in each event folder, now listing all the Shake units in the input range, alongside their connectivity status and their channels. \n8. If needed, is possible to add new Raspberry Shake units in the second sheet of the `.xlsx` file, in the same format as the previous entries.\n\nDone!\n\n",
        "createdAt": "2020-11-11T16:37:58.000Z",
        "updatedAt": "2024-09-15T18:32:11.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/raspishake/Berkeley-mSEED-downloader/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Shihao-Yuan/SWD-HV-DFA",
        "url": "https://github.com/Shihao-Yuan/SWD-HV-DFA",
        "description": "H/V Spectral Ratio and Surface Wave Dispersion Forward Solver",
        "stars": 4,
        "forks": 1,
        "readme": "## HV-SWD-DFA: H/V Spectral Ratio and Surface Wave Dispersion Modeling (Diffuse Wavefield Assumption)\n\nFortran implementation of H/V spectral ratio and surface-wave dispersion, with a thin Python wrapper. The Python API mirrors the original CLI and offers simple functions.\n\n- **Original authors**: HV‑INV project team (see headers in `HV.f90`)\n- **Modifications and API**: Shihao Yuan (`syuan@mines.edu`)\n\n### DISCLAIMER:\nThis is a development build. The code may contain errors or unstable functionality. Contributions and feedback are welcome.\n\n### Repository layout\n- `src/hvswdpy/`: Python package (wrapper)\n- `src/hvswdpy/_fortran/`: Fortran sources and f2py interface (`hvdfa.pyf`)\n- `src/cli/`: Fortran CLI sources (`HV.f90`, `cli_args*.f90`, optional drivers)\n- `src/Makefile`: build targets (`hv_orig`, `python`, `python-dev`)\n- `bin/`: built CLI executable (`bin/hv_orig`)\n- `examples/`: scripts and notebooks\n- `examples/results/`: output plots\n\n### Requirements\n- gfortran \n- Python 3.9+ with NumPy\n- macOS/Linux (tested on macOS ARM with Conda)\n\n### Build / Install\n- From `src/` (recommended):\n  - Original CLI:\n    ```bash\n    cd src\n    make hv_orig        \n    ```\n  - Python extension and wrapper:\n    ```bash\n    cd src\n    make python          \n    ```\n- From `src/` as an alternative:\n  ```bash\n  cd src\n  python -m pip install -e .  \n  ```\n\n### Model format (API)\n- API arrays:\n  - `vp` (m/s), `vs` (m/s), `rho` (kg/m³), `thickness` (m) for all layers except halfspace\n  - At least 2 layers (including halfspace) so `thickness` has length `nlayers-1`\n- CLI `model.txt` (used by examples):\n  - First line: `N_LAYERS`\n  - Next lines: `THICKNESS VP VS RHO`, with `THICKNESS=0` for the halfspace\n\n### Original CLI usage\n```bash\n# Build from src/\ncd src && make hv_orig  \n\n# Run from repo root\nbin/hv_orig -f examples/model.txt -fmin 0.1 -fmax 100 -nf 100 -logsam -nmr 3 -nml 3 -prec 1.0 -nks 0 -ph -hv > examples/HV.dat\n# Outputs in examples/: Rph.dat (Rayleigh slowness), Lph.dat (Love slowness), HV.dat (freq, hv)\n```\n\n### Python API \n  ```python\n  import numpy as np\n  import hvswdpy as hv\n  vp = np.array([300., 1500.])\n  vs = np.array([150., 800.])\n  rho = np.array([1800., 2200.])\n  thickness = np.array([20.])  # nlayers-1 values (halfspace excluded)\n  f = np.logspace(-1, 2, 100)\n\n  # H/V spectral ratio\n  hv_curve, status = hv.hv(\n      frequencies_hz=f,\n      vp=vp, vs=vs, rho=rho, thickness=thickness,\n      n_rayleigh_modes=1, n_love_modes=0, precision_percent=1.0,\n  )\n  ```\n- Components\n  ```python\n  comps = hv.hv_components(f, vp, vs, rho, thickness)\n  ```\n- Dispersion (convert slowness to velocity via 1/slowness)\n  ```python\n  disp = hv.dispersion(\n      frequencies_hz=f,\n      vp=vp, vs=vs, rho=rho, thickness=thickness,\n      n_rayleigh_modes=1, n_love_modes=0, precision_percent=1.0,\n  )\n  mask = (disp.rayleigh_valid[:, 0] != 0)\n  rayleigh_vel_mode1 = 1.0 / disp.rayleigh_slowness[mask, 0]\n  ```\n\nTroubleshooting imports in notebooks:\n- If running from a subfolder (e.g., `examples/`), make sure the project root is on `sys.path` or `PYTHONPATH` so `import hvswdpy` works:\n  ```python\n  import os, sys\n  ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n  if ROOT not in sys.path:\n      sys.path.insert(0, ROOT)\n  import hvswdpy\n  ```\n\n### Examples\n- Compare HV and dispersion (CLI vs API) — generates plots into `examples/results/`:\n  ```bash\n  jupyter notebook examples/compare_API_CLI.ipynb\n  ```\n  - `examples/results/compare_hv.png`\n  - `examples/results/compare_rayleigh_dispersion.png` (phase velocity vs frequency)\n  - `examples/results/compare_love_dispersion.png` (if Love modes requested)\n\n\n### License\nThis project is licensed under the MIT License. See `LICENSE` for details.\n",
        "createdAt": "2025-08-09T01:39:01.000Z",
        "updatedAt": "2025-10-28T22:25:22.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Shihao-Yuan/SWD-HV-DFA/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "RobertoGuardo/VolGIS",
        "url": "https://github.com/RobertoGuardo/VolGIS",
        "description": "a Volcano-oriented Geographic Information System",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2024-09-02T16:03:44.000Z",
        "updatedAt": "2024-09-02T16:18:23.000Z",
        "language": "Processing",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "NerdyNinja11/krafla-ambient-noise",
        "url": "https://github.com/NerdyNinja11/krafla-ambient-noise",
        "description": "Repository for Earth Sciences Part III seismology project ",
        "stars": 1,
        "forks": 0,
        "readme": "",
        "createdAt": "2025-10-09T12:01:04.000Z",
        "updatedAt": "2025-10-17T10:20:03.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ydluo/qdyn",
        "url": "https://github.com/ydluo/qdyn",
        "description": "A Quasi-DYNamic earthquake simulator",
        "stars": 61,
        "forks": 32,
        "readme": "# QDYN <img src=\"docs/img/qdyn_logo_small.jpeg\" alt=\"QDYN logo\" align=\"right\" />\n\n## A Quasi-DYNamic earthquake simulator\n\n[![Build Status](https://travis-ci.com/ydluo/qdyn.svg?branch=master)](https://travis-ci.com/ydluo/qdyn) [![Documentation Status](https://readthedocs.org/projects/ansicolortags/badge/?version=latest)](https://ydluo.github.io/qdyn/) [![GPLv3 license](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://perso.crans.org/besson/LICENSE.html)\n\n*QDYN* is a boundary element software to simulate earthquake cycles (seismic and aseismic slip on tectonic faults) under the quasi-dynamic approximation (quasi-static elasticity combined with radiation damping) on faults governed by rate-and-state friction and embedded in elastic media.\n\n*QDYN* includes various forms of rate-and-state friction and state evolution laws, and handles non-planar fault geometry in 3D and 2D media, as well as spring-block simulations. Loading is controlled by remote displacement, steady creep or oscillatory load. In 3D it handles free surface effects in a half-space, including normal stress coupling. The medium surrounding the fault is linear, isotropic and elastic, and may be uniform or (in 2D) contain a damaged layer.\n\n*QDYN* implements adaptive time stepping, shared-memory parallelization, and can deal with multi-scale earthquake cycle simulations with fine details in both time and space. It is equipped with user-friendly MATLAB and Python interfaces and graphical output utilities.\n\nTo get started with QDYN, see [the documentation](https://ydluo.github.io/qdyn/).\n\n\n\n------\n\n## Features\n\n- rate-and-state friction, with velocity cut-offs, aging and slip laws\n\n- microphysically based frictional model (*Chen-Niemeijer-Spiers* model)\n\n- heterogeneous frictional properties\n\n- slow and fast, aseismic and seismic slip transients\n\n- dynamic weakening (thermal pressurization)\n\n- non-planar faults (currently limited to variable dip, rectangular elements)\n\n- 3D, 2D and 1D (spring-block)\n\n- tectonic and transient loads\n\n- normal stress coupling\n\n- faults surrounded by damaged zones\n\n- MATLAB and Python wrappers, and graphic output display utilities\n\n- parallelized for shared memory systems (OpenMP)\n\n- parallelized for distributed memory systems (MPI)\n\n\n\n-----------------------\n\n\n## News \n\n*22 July 2020* | QDYN version 2.3 has arrived! In this release we improved the output modules, making them more consistent across simulation features and paving the way for HDF5 support. See the [release notes](https://github.com/ydluo/qdyn/releases/tag/qdyn_2.3.0) for details.\n\n*24 June 2020* | Article published on modeling earthquake cycles on heterogeneous faults using the CNS microphysical model:\n  M. van den Ende, J. Chen, J.P. Ampuero and A. Niemeijer, *Rheological transitions facilitate fault‐spanning ruptures on seismically active and creeping faults*, J. of Geophys. Res.: Solid Earth (doi:[10.1029/2019JB019328](https://doi.org/10.1029/2019JB019328))\n\n*January 2020* | The QDYN team participated in the *Community Code Verification Exercise for Simulating Sequences of Earthquakes and Aseismic Slip (SEAS)*, funded by SCEC. The results of this community benchmark exercise are now published in [Seismological Research Letters](https://doi.org/10.1785/0220190248) (see also this [preprint on EarthArXiv](https://eartharxiv.org/2dmp5/)). A script to run these simulations in QDYN is included in the `/examples/scec_benchmarks/BP2` directory.\n\n*29 August 2019* | QDYN version 2.2 has been released! This version is now more user-friendly than QDYN has ever been, with a new documentation website, Jupyter Notebooks, and automated testing for developers. See the [release notes](https://github.com/ydluo/qdyn/releases/tag/qdyn_2.2) for details.\n\n*21 August 2019* | QDYN version 2.1 has been released! See the [release notes](https://github.com/ydluo/qdyn/releases/tag/qdyn_2.1) for new features and (minor) bug fixes.\n\n*13 June 2019* | Article published on modeling earthquake cycles on geometrically-complex fault systems with the QDYN-SPECFEM Bridge:  \n  P. Galvez, P. Somerville, A. Petukhin, J. P. Ampuero and D. Peter, *Earthquake cycle modelling of multi-segmented faults: dynamic rupture and ground motion simulation of the 1992 Mw 7.3 Landers earthquake*, Pure Appl. Geophys., doi:[10.1007/s00024-019-02228-x](https://doi.org/10.1007/s00024-019-02228-x)\n\n*24 April 2019* | Recent work using QDYN earthquake cycle simulations to constrain seismic hazard models:  \n  L. Ceferino, P. Galvez, J. P. Ampuero, A. Kiremidjian, G. Deierlin and J. C. Villegas-Lanza, *Bayesian parameter estimation for space and time interacting earthquake rupture model using historical and physics-based simulated earthquake catalogs*, pre-print, [doi:10.31223/osf.io/3wfr4](https://doi.org/10.31223/osf.io/3wfr4)\n\n*11 December 2018* | QDYN version 2.0 has been released!\n\n*6 December 2018* | Results of QDYN earthquake cycle simulations on faults surrounded by damaged zones were presented at:  \n\n* the AGU Fall Meeting 2017 (San Francisco, 11-15 December 2017): B. Idini and J. P. Ampuero, *Rupture complexity promoted by damaged fault zones in earthquake cycle models* (doi:10.1002/essoar.10500080.1, [poster](https://www.essoar.org/doi/abs/10.1002/essoar.10500080.1))  \n* the [10th ACES International Workshop](http://quaketm.bosai.go.jp/~shiqing/ACES2018/index_aces.html) (Japan, September 25-28, 2018), J. P. Ampuero et al., *Rupture complexity promoted by damaged fault zones in earthquake cycle models* ([slides](http://quaketm.bosai.go.jp/~shiqing/ACES2018/abstracts/aces_abstract_ampuero.pdf)) \n* the [Workshop on Modeling Earthquake Source Processes](http://www.seismolab.caltech.edu/workshop.html) (Caltech, October 8-10, 2018). \n\n*29 November 2018* | The QDYN team participated in  in the two first [benchmarks](https://scecdata.usc.edu/cvws/seas/index.html) and [workshops](https://www.scec.org/workshops/2018/seas) of the SCEC SEAS project (Southern California Earthquake Center, Sequences of Earthquakes and Aseismic Slip). Yingdi Luo presented work using QDYN on *A heterogenous fault model of episodic tremor and slow-slip events with spatial-temporal variability* ([slides](https://files.scec.org/s3fs-public/2018_SEAS_Workshop_1115_Luo_A_heterogenous_fault_model_of_episodic_tremor_and_slow-slip_event_with_spatial-temporal_variability.pdf)).\n\n*11 September 2018* | Results based on QDYN were highlighted at the [2018 WEGENER conference](https://wegener2018.sciencesconf.org/) in the keynote presentation *The spectrum of slip behaviors emerging from the interactions between seismic and aseismic slip* by J. P. Ampuero ([slides](https://wegener2018.sciencesconf.org/data/pages/Jean_Paul_Ampuero_Wegener_2018.pdf)).\n\n*28 May 2018* | In order to address a problem in the QDYN history tree, the version history of the QDYN repository was rewritten. All developers that have cloned the QDYN repository before this date should follow the steps described in [this manual](docs/git_fix_2018-05-28.pdf). Users of QDYN that did not make any code changes can simply delete the old repository and clone the latest version to get a clean QDYN repository.\n\n*May 2018* | Two papers with simulation results based on *QDYN*, published in Tectonophysics - Special Issue on \"[Physics of Earthquake Rupture Propagation](https://www.sciencedirect.com/journal/tectonophysics/vol/733/suppl/C)\":  \n  M. van den Ende, J. Chen, J. P. Ampuero and A. Niemeijer\n  *A comparison between rate-and-state friction and microphysical models, based on numerical simulations of fault slip* (doi:[10.1016/j.tecto.2017.11.040](https://doi.org/10.1016/j.tecto.2017.11.040))  \n  Y. Luo and J. P. Ampuero  \n  *Stability and effective friction of faults with heterogeneous friction properties and fluid pressure* (doi:[10.1016/j.tecto.2017.11.006](https://doi.org/10.1016/j.tecto.2017.11.006))\n\n*Apr 2017* | Earthquake cycle simulations with a micro-physics model of granular flow and ductile creep of fault gouges based on *QDYN* presented at the EGU General Assembly 2017, session on *Earthquakes: from slow to fast, from the field to the laboratory*:    \n  M. van den Ende, J. Chen, J. P. Ampuero and A. Niemeijer (2017)  \n  *Earthquake and slow-slip nucleation investigated with a micro-physics based seismic cycle simulator*  \n  Geophysical Research Abstracts, Vol. 19, EGU2017-7249  \n  Abstract [PDF](http://meetingorganizer.copernicus.org/EGU2017/EGU2017-7249.pdf) -- Presentation [PPT](http://presentations.copernicus.org/EGU2017-7249_presentation.pptx)  \n\n*Feb 2017* | [**QDYN v1.1 has been released**](https://github.com/ydluo/qdyn/releases/tag/qdyn_1.1) and published online via [zenodo](https://zenodo.org/record/322459#.WLNq3BiZNE4). This new version introduces MPI parallelization for 3D simulations in HPC clusters and faults surrounded by damaged zones in 2D. You can cite it as:  \n  Y. Luo, J. P. Ampuero, P. Galvez, M. van den Ende and B. Idini (2017)  \n  *QDYN: a Quasi-DYNamic earthquake simulator (v1.1)*  \n  Zenodo. doi:10.5281/zenodo.322459  \n  [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.322459.svg)](https://doi.org/10.5281/zenodo.322459)\n\n*Jan 2017* | In the following paper we used *QDYN* to simulate earthquake cycles and obtained events spanning a broad range of magnitudes to study earthquake scaling relations:  \n  Y. Luo, J. P. Ampuero, K. Miyakoshi and K. Irikura (2017)  \n  *Surface effects on earthquake moment-area scaling relations*     \n  PAGEOPH, Topical Volume on *\"Best Practices in Physics-based Fault Rupture Models for Seismic Hazard Assessment of Nuclear Installations\"*  [doi:10.1007/s00024-017-1467-4](https://link.springer.com/article/10.1007/s00024-017-1467-4)  \n  [PDF](https://rdcu.be/oOL9)  \n\n*Nov 2014* | A *QDYN* tutorial session was offered at the [School on \"Earthquakes: nucleation, triggering and relationship with aseismic processes\"](http://earthquakes.sciencesconf.org/) at Cargese, Corsica, 3 - 10 November 2014 \n\n*QDYN* was previously hosted on [GoogleCode](https://code.google.com/p/qdyn/) and is now on [Github](http://ydluo.github.io/qdyn). Both [SVN](https://subversion.apache.org) and [GIT](https://git-scm.com) version control systems are supported. \n\n\n\n--------------------------------\n\n## Downloads, documentation and support\n\nPrevious (stable) versions of QDYN can be downloaded from the [release page](https://github.com/ydluo/qdyn/releases). Development versions are available as separate [branches](https://github.com/ydluo/qdyn/branches) following the naming convention `release/x.x.x`.\n\nTo install QDYN please follow the *Getting started* section in [the documentation](http://ydluo.github.io/qdyn/).\n\nQuestions, feedback or suggestions can be submitted via our [issue tracking system](https://github.com/ydluo/qdyn/issues).\n\n\n\n-------------------------\n\n## Introduction Poster\n\n![](https://lh4.googleusercontent.com/-OjKBE5_Ipf8/T9wk2GtVRXI/AAAAAAAAABg/a1diUWu7tFU/s763/Poster_QDYN.jpg)\n\n[Download Poster](http://code.google.com/p/qdyn/downloads/detail?name=Poster_QDYN.pdf) \n\n-------------------------\n\n\n## Featured Simulations\n\n### [Simulation_Tohoku](https://github.com/ydluo/qdyn/wiki/Simulation_Tohoku)\n![](https://lh5.googleusercontent.com/-JPaTpBXo5eA/USdSArzQ0QI/AAAAAAAAKew/9wnVu30Lhf4/s900/Tohoku_cycle_logo.gif)\n\n### [Simulation_Cascadia_Tremor](https://github.com/ydluo/qdyn/wiki/Simulation_Cascadia_Tremor)\n![](https://lh5.googleusercontent.com/-a_2MRxcUgf8/T-v2JCjmxBI/AAAAAAAAAB8/NlQTwfra4fY/s900/Tremor_3D_Cascadia.gif)\n\n------------------------\n## Developers\n\n*(listed alphabetically)*\n\n[Jean-Paul Ampuero](http://www.seismolab.caltech.edu/ampuero_jp.html) (IRD/UCA, Géoazur, France; Caltech Seismolab, USA)\n\n[Martijn van den Ende](https://www.linkedin.com/in/martijnvandenende) (Université Côte d'Azur, Géoazur, France)\n\n[Percy Galvez](https://smi.kaust.edu.sa/Pages/People-Galvez.aspx) (KAUST, Saudi Arabia; AECOM, Switzerland)\n\n[Benjamin Idini](http://www.seismolab.caltech.edu/idini_b.html) (Caltech Seismolab, USA)\n\n[Yingdi Luo](https://science.jpl.nasa.gov/people/YLuo/) (NASA JPL, USA)\n\n-------------------------\n\n## Suggested References\n\n#### For all uses of the QDYN software\n\nLuo, Y., Ampuero, J. P., Galvez,  P., van den Ende, M., & Idini, B. (2017). \nQDYN: a Quasi-DYNamic earthquake simulator (v1.1) [Data set]. Zenodo. doi:10.5281/zenodo.322459  \n [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.322459.svg)](https://doi.org/10.5281/zenodo.322459)\n\n#### For simulations on heterogeneous faults\n\nLuo, Y., & Ampuero, J. P. (2018). \nStability of faults with heterogeneous friction properties and effective normal stress. \nTectonophysics, 733, 257-272, doi:[10.1016/j.tecto.2017.11.006](https://doi.org/10.1016/j.tecto.2017.11.006)\n\nLuo, Y., Ampuero, J. P., Miyakoshi, K., & Irikura, K. (2017). Surface rupture effects on earthquake moment-area scaling relations. Pure and Applied Geophysics, 174(9), 3331-3342, doi:[10.1007/s00024-017-1467-4](https://link.springer.com/article/10.1007/s00024-017-1467-4).\nIn Topical Volume on *\"Best Practices in Physics-based Fault Rupture Models for Seismic Hazard Assessment of Nuclear Installations\"*.  \n[PDF](https://rdcu.be/oOL9)\n\nLuo, Y., & Ampuero, J. P. (2012), Simulation of Complex Tremor Migration Patterns, AGU Fall Meeting 2012 Abstract S44B-02\n\nLuo, Y., & Ampuero, J. P. (2011), Numerical Simulation of Tremor Migration Triggered by Slow Slip and Rapid Tremor Reversals, AGU Fall Meeting 2011, abstract S33C-02\n\n#### For the microphysically based (CNS) simulations\n\nvan den Ende, M. P. A., Chen, J., Ampuero, J. P., & Niemeijer, A. R. (2018).\nA comparison between rate-and-state friction and microphysical models, based on numerical simulations of fault slip.\nTectonophysics, 733, 273-295, doi:[10.1016/j.tecto.2017.11.040](https://doi.org/10.1016/j.tecto.2017.11.040)\n\n#### For simulations on faults surrounded by damaged zones\n\nIdini, B., & Ampuero, J. P. (2017).\nRupture complexity promoted by damaged fault zones in earthquake cycle models.\nAGU Fall Meeting 2017, abstract T41C-0632.  \nPoster [PDF](https://www.essoar.org/doi/abs/10.1002/essoar.10500080.1), doi:[10.1002/essoar.10500080.1](https://dx.doi.org/10.1002/essoar.10500080.1)\n",
        "createdAt": "2015-07-30T00:37:55.000Z",
        "updatedAt": "2025-06-10T13:23:34.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.322459",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.322459",
            "dataCite": "10.5281/zenodo.322459",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ydluo/qdyn/master/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.322459",
            "title": "QDYN: a Quasi-DYNamic earthquake simulator (v1.1)",
            "journal": "Zenodo",
            "dateReleased": "2017-02-24T00:00:00.000Z",
            "abstract": "<strong>Summary</strong> <strong><em>QDYN</em></strong> is a boundary element software package to simulate earthquake cycles (tectonic fault slip) under the quasi-dynamic approximation (quasi-static elasticity with radiation damping). The code implements adaptive time stepping and shared-memory (OpenMP) and MPI parallelization to simulate earthquake cycles including seismic and aseismic slip. QDYN includes various forms of rate-and-state friction and state evolution laws. It handles non-planar fault geometries in 3D and 2D elastic media, as well as spring-block simulations. It has a user-friendly matlab interface and graphical output. <strong>Features in v1.1</strong> rate-and-state friction, with velocity cut-offs, aging and slip laws heterogeneous frictional properties slow and fast, aseismic and seismic slip transients non-planar faults (currently limited to variable dip, rectangular elements) 3D, 2D and 1D (spring-block) tectonic and transient loads matlab wrapper and graphic output display utilities parallelized for shared memory systems (OpenMP) MPI parallelization normal stress coupling fully coupled with SPECFEM3D via QSB (QDYN-SPECFEM Bridge) <strong>Host website</strong> https://ydluo.github.io/qdyn/",
            "citationsArray": [
                "10.1029/2021jb023519"
            ]
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "kura-okubo/SeisMonitoring.jl",
        "url": "https://github.com/kura-okubo/SeisMonitoring.jl",
        "description": "Julia monitoring tools for ambient noise seismology. ",
        "stars": 7,
        "forks": 1,
        "readme": "# SeisMonitoring.jl\n\nA Julia package of the tools for ambient noise seismology.\n\nDocumentation is available from the badge below:\n\n[![](https://img.shields.io/badge/docs-dev-blue.svg)](https://kura-okubo.github.io/SeisMonitoring.jl/dev)\n[![](https://img.shields.io/badge/docs-stable-blue.svg)](https://kura-okubo.github.io/SeisMonitoring.jl/stable)\n\nOther badges:\n\n[![Documentation](https://github.com/kura-okubo/SeisMonitoring.jl/actions/workflows/documentation.yml/badge.svg)](https://github.com/kura-okubo/SeisMonitoring.jl/actions/workflows/documentation.yml)\n[![Run tests](https://github.com/kura-okubo/SeisMonitoring.jl/actions/workflows/test.yml/badge.svg)](https://github.com/kura-okubo/SeisMonitoring.jl/actions/workflows/test.yml)\n[![codecov](https://codecov.io/gh/kura-okubo/SeisMonitoring.jl/graph/badge.svg?token=iNq1WJH5bK)](https://codecov.io/gh/kura-okubo/SeisMonitoring.jl)\n[![DOI](https://zenodo.org/badge/259752194.svg)](https://zenodo.org/badge/latestdoi/259752194)\n[![Github All Releases](https://img.shields.io/github/downloads/kura-okubo/SeisMonitoring.jl/total.svg)]()\n\n## Install SeisIO.jl to Mac M1\nCurrently we have a problem when adding the `SeisIO.jl` using Mac M1 chip.\nTo avoid the error, try one of the options below:\n\n- Option 1. Use the forked version of SeisIO. Type the follwing command in the Julia REPL:\n```\nusing Pkg; Pkg.add(url=\"https://github.com/kura-okubo/SeisIO.jl\");\n```\n\n- Option 2. Use the Docker container following \n[**SeisMonitoring_Example**](https://github.com/kura-okubo/SeisMonitoring_Example). \n\n- Option 3.  Follow (https://github.com/jpjones76/SeisIO.jl/pull/94/commits/9a8b4510636e442e89f3d0a76f63abc56f1ab054).\nWe need to install the `SeisIO.jl` in develop directory:\n```\n]dev SeisIO \n```\nThen, edit the `~/.julia/dev/SeisIO/Project.toml` such that\n```\nHDF5 = \"0.12.3, 0.13, 0.14.2\"\nLightXML = \"0.8.1, 0.9\"\n```\nYou can avoid the precompile error for those packages. We corrected these dependencies in the forked version used in the option 1 above.\n\n## SeisBase.jl\nWe have developed this software package with `SeisIO.jl` v1.2.1, which is currently maintained as [SeisBase.jl](https://github.com/JuliaSeismo/SeisBase.jl). \n\n## Installation\n\nType the commands below in the Julia REPL:\n\n```julia\nusing Pkg; Pkg.update();\n# Pkg.add(PackageSpec(name=\"SeisIO\", version=\"1.2.1\")); \nPkg.add(url=\"https://github.com/kura-okubo/SeisIO.jl\") # forked version used for the environment including Mac M1\nPkg.add(PackageSpec(name=\"SeisNoise\", version=\"0.5.3\"));\nPkg.develop(url=\"https://github.com/kura-okubo/SeisDvv.jl\");\nPkg.develop(url=\"https://github.com/kura-okubo/SeisMonitoring.jl\");\n```\n\n## Tutorial\nWe created the notebook of the tutorial in the different github repository, [**SeisMonitoring_Example**](https://github.com/kura-okubo/SeisMonitoring_Example). You can find how to download the data, remove the transient signals, compute cross-correlations, stack the correlation functions and measure the dv/v.\n\n\nYou can access to the from the badge below:\n\n<a href=\"https://nbviewer.org/github/kura-okubo/SeisMonitoring_Example/blob/main/code/run_seismonitoring.ipynb\" target=\"_blank\">\n   <img align=\"left\"\n      src=\"https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.png\"\n      width=\"109\" height=\"20\">\n</a>\n<br><br>\n\n\nor read the QR code:\n\n<img src=\"/docs/src/assets/QRcode_seismonitoring_example.png\" alt=\"QR\" width=\"150\"/>\n\nSee also the repoitory of [**SeisMonitoring_Paper**](https://github.com/kura-okubo/SeisMonitoring_Paper) for the post-processing using the dv/v over 20 years at Parkfield.\n\n## Reference\nOkubo, K., Delbridge, B. G., & Denolle, M. A. (2024). Monitoring velocity change over 20 years at Parkfield. Journal of Geophysical Research: Solid Earth, 129, e2023JB028084. https://doi.org/10.1029/2023JB028084\n",
        "createdAt": "2020-04-28T21:11:12.000Z",
        "updatedAt": "2024-12-16T20:37:58.000Z",
        "language": "Julia",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/kura-okubo/SeisMonitoring.jl/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "pkikp/seismic_code",
        "url": "https://github.com/pkikp/seismic_code",
        "description": "misc code useful for seismology",
        "stars": 0,
        "forks": 0,
        "readme": "# seismic_code\nmisc code useful for seismology\n\nmostly matlab & shell scripts\n\niasp91 is some matlab code that computes iasp91 travel times \n  tables are stored as mat files\n  original txt files are provided with code that converted txt to mat\n",
        "createdAt": "2018-08-26T22:29:08.000Z",
        "updatedAt": "2018-08-26T23:24:34.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/pkikp/seismic_code/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Hanxiao-Wu/GeoInverse",
        "url": "https://github.com/Hanxiao-Wu/GeoInverse",
        "description": "A program designed for seismic inversion.",
        "stars": 3,
        "forks": 0,
        "readme": "# Monte Carlo Joint Inversion Program\n\n## Overview\nThis program, written in C++, is designed for geophysical inversion applications based on a classical Monte Carlo inversion framework. It provides a tool for estimating subsurface structure with multiple types of geophysical data and is particularly useful for studying crustal and uppermost mantle structures. This project builds upon the methodologies described in these paper:\n> - [Wu, H., Sui, S., & Shen, W. (2024). Incorporating H-k stacking with Monte Carlo joint inversion of multiple seismic observables: A case study for the northwestern US. Journal of Geophysical Research: Solid Earth, 129, e2023JB027952. ](https://doi.org/10.1029/2023JB027952)\n> - Wu, H., Sui, S., Lin, F., Shen, W. (2024) Evidence of partial melting in the western U.S.: Insights from new seismic observable.\\[Manuscript in preparation\\]\n\n## Features\n### Support for Multiple Data Types:\n- **Receiver Functions (RF)**: Can take stacked RF to fit its waveform or multiple RFs from different events to fit theirs arrivals (H-k stacking)\n- **Rayleigh wave Dispersion Data**: Include both phases and group velocity. Potentially can also take Love wave dispersion, but not tested yet.\n- **Rayleigh wave H/V ratio (Ellipticity)**\n- **Rayleigh wave Local Amplification**\n### Inverted Results:\n- **Vs** (fine 1-D structure)\n- **Vp/Vs** (can be layered structure instead of just a bulk value)\n- **Density** (currently not tested)\n- **Moho Depth**\n- **Temperature** (future feature)\n- **Pressure** (future feature)\n### Inversion method:\n- Based on **Monte Carlo algorithm**.\n\t- Can handle non-linear and complex inversion;\n   \t- Explores the entire parameter space via random sampling, avoiding local minima and increasing the likelihood of finding the **global optimum**.\n   \t- Allows for the estimation of uncertainties in model parameters by generating a **distribution of possible solutions**, making it especially useful for quantifying confidence in the inversion results.\n- Integrates traditional surface wave & receiver function **joint inversion method** with the **receiver function H-kappa stacking method**, effectively mitigating the **trade-off** between Vs, Moho depth, and crustal bulk Vp/Vs. It ensures accurate and robust subsurface models, allowing for simultaneous inversion of multiple seismic observables.\n\n> [!IMPORTANT]\n> - While the GeoInverse program is designed to be highly flexible, allowing a wide range of parameters to participate in the inversion, the reliability of the results fundamentally depends on the **input data**, not the inversion methodology itself.\n> - Users must carefully choose inversion parameters based on the sensitivity of their data. For instance, although the program permits inversion for detailed Vp/Vs structures, such results are only trustworthy if the input data have a depth-related sensitivity to Vp/Vs variations. Otherwise, the inversion might produce a \"result,\" but that result is just 'garbage in, garbage out'. Always assess the sensitivity and quality of your input data before proceeding.\n\n## Installation\nClone the repository and compile the program:\n```\ngit clone https://github.com/Hanxiao-Wu/GeoInverse.git\ncd GeoInverse/src\nmake MC_main\nmake MC_Posterior\ncd ..\n```\n\n## Inversion Setup\nTo run the program, three main files are required:\n1. **Inversion Control File**(`*.control`)\n   - This file defines the overall configuration of the inversion process, such as the number of Monte Carlo searches, the iteration count per search, and the data source weights.\n   - It acts as the main setup file that connects all components of the inversion.\n2. **Model File**\n   - This file describes detailed 1D model using a smaller number of parameter.\n   - The model described in this file also serves as the center of the model space.\n3. `in.para`**File**\n   This file, together with the model file, defines the model space. It specifies which parameter to perturb, its bounds (absolute or percentage), and the step size for Monte Carlo sampling.\n\nThe following is a line-by-line explanation of these files. After reading this README, one should be able to construct the files based on one's own needs. However, it is still strongly recommended to modify the example files provided here, rather than starting from scratch.\n\n### 1. Inversion Control File (`*.control`)\nThe `*.control` file is essential for configuring inversion parameters and Monte Carlo settings. It tells the code where to read the data from, the weights for each dataset, how many searches to perform, how many iterations to run for each search, and so on. Below is an [example control file](test.control) with explanations for each parameter:\n```\nmodel 2 tar.mod        # Number of basic layers; model file\npara in.para           # Parameter file for setting up perturbed parameters\n\ndisp R 4 p template_p.dat g template_g.dat e template_e.dat a template_a.dat  \n# Type of surface wave data:\n#  - \"R\" for Rayleigh waves\n#  - 4 indicates the type of surface wave data, with the following files for phase (p), group (g), ellipticity (e), and amplification (a)\n\nrf 2.5 0.06 template_rf.dat  \n# Gaussian parameter for receiver function data\n# Ray parameter for receiver function data\n# File name for receiver function data\n\nrfweight 0.4           # Weight for receiver function data\n\nhk tar_hk.lst 1 0      \n# File listing paths to all individual receiver function files\n# Number of discontinuities\n# Index of the discontinuity, starting from 0\n\nhkweight 0.3 0.4 0.3   # Weights for each phase (Ps, PpPs, PsPs+PpSs)\n\nmonol 0                # index of the group which if forced to be monotonicly increasing\n\nEweight 1.0 0.035      \n# Weight for energy generated from each discontinuity\n# Reference energy used for normalization\n\n#model -1               # Number of models in each search; -1 for forward calculation\n#search 30              # Number of Monte Carlo searches; -1 for prior sampling\n                         # If #model = -1, this parameter is ignored\n\noutdir tar tar          # Output directory and file name\n\nend                     # End of control file\n```\n\n### 2.Model Setup File\nThe program is designed to derive a detailed 1D subsurface model (e.g., Vs, Vp, density). Since a model with hundreds of layers would require an impractical number of parameters for Monte Carlo inversion, we instead adopt a layered approach:\n1. Basic Layers: The model is divided into several main sections ( refered as group, usually two or three) to represent primary divisions:\n\t- Two-Layer Example: Crust and uppermost mantle.\n\t- Three-Layer Example: Sediment, crystalline crust and uppermost mantle.\n2. Finer Layers within Basic Layers: Each basic layer is further divided into a finer 1D model. This detailed layering is controlled by user-defined parameters in the model file. Each fine layer's properties are determined by interpolation (e.g., B-spline) using a few coefficients, typically no more than five. This approach ensures a smoother transition in model parameters and reduces the inversion complexity.\nFor further information, refer to [Shen et al., 2013](https://academic.oup.com/gji/article/192/2/807/580799), and [Wu et al., 2024](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2023JB027952).\n#### Model File\nThe model file defines the configuration of the basic layers and their fine structure, supporting the generation of a detailed 1D model using a variety of interpolation approaches. Also the model described in the model file serves as the **reference model** whicn will be the center of the model space. Below is a breakdown of each column's purpose.\n##### Column Description\n1. Group Index (Column 1):\n\t- Represents the index of the \"basic layer\" or **group**, starting from 0.\n2. Parameter Type (Column 2):\n\t- indicates the property type for this row:\n\t\t- `1`- Vs(km/s)\n\t\t- `2`- Vp/Vs\n\t\t- `3`- Density(g/cm^3)\n\t\t- `4`- Qs (Quality factor for Vs)\n\t\t- `5`- Qp (Quality factor for Vp)\n\t\t- `6`- Temperature\n\t\t- `7`- Pressure\n\t\t\n3. Interpolation Approach (Column 3):\n\t- Specifies how to generate the detailed 1D model within the group:\n\t\t- `1`- Gradient interpolation (linear gradient from top to bottom of the group)\n\t\t- `2`- Layered model (using this way ,this group is essentially already a layered model instead of interpolated from some coefficients)\n\t\t- `3`- B-spline interpolation (smooth curve fitted to the parameters)\n\t\t- `4`- Bulk value for entire group (using this way, if no additional anomaly added, then the whole group will be just one layer)\n\t\t- `-1`- Water layer (using this way, then the Vs will be set to be 0)\n\t\t- `-2`- Ice layer (to be done)\n\t\t- `-3`- Brocher's empirical relationship, only applied to Vp or Density (using this way the Vp or density will be scaled from Vs using corresponding empirical relationship)\n\t\t- `-4`- Hacker's empirical relationship, only applied to mantle Density\n\n4. Group Thickness (Column 4): \n\t- Total thickness of the group in km\n\t- For Vs, Vp, and density, the group thickness should generally be the same since discontinuities in these properties are assumed to align within a group. If temperature and pressure are included in the future, groups may have differing thicknesses, but currently, group thickness should **remain consistent for all properties within the same group**.\n\n5. Number of Parameters (Column 5):\n\t- Specifies the number of parameters provided to generate the detailed 1D model within the group.\n\t- For example, if this number is `n`, then the next `n` columns contain the corresponding parameter values.\n\n6. Parameters for Interpolation (Column 6 to 6+n):\n\t- These are the actual parameter values used for interpolation within the group.\n\t- The parameters vary by interpolation approach:\n\t\t- For `Gradient`, **two** parameters are required: the top and bottom values of the property in this group (e.g., top and bottom Vs).\n\t\t- For `Layered`, the number of parameters should match the number of fine layers.\n7. Number of Anomalies (column 7+n):\n\t- Optional. Specifies the number of additional anomalies to add to the smooth 1D model within the grou (Let's say the number is `m` )p.\n\n8. Anomaly Parameters (column 8+n to 8+n+3m):\n\t- Each anomaly requires three parameters:\n\t\t- **Upper boundary depth**: described in percentage of group thickness\n\t\t- **Lower boundary depth**: described in percentage of group thickness\n\t\t- **Anomaly value**: only allowed to be a bulk value currently\n\t- This section will contain `3m` values (three per anomaly)\n\n9. Number of Fine Layers:\n\t- The desired number of layers in the fine 1D model for this group. The thickness of each fine layer is calculated as the **group thickness divided by the number of layers**.\n\n##### Example\nOne can find an example model file [here](test.mod).\nThe following is an explanation in a group-by-group format. Each group has 7 rows to describe its Vs, Vp(or VP/Vs), density, Qs, Qp, T, P, respectively.\n\n***group 0*** Sedimentary layer\n```\n0 1 1 1.0 2 1.5 2.7 0 5 0.\n0 2 -3 1.0 0 0 5\n0 3 -3 1.0 0 0 5\n0 4 -3 1.0 0 0 5\n0 5 -3 1.0 0 0 5\n0 6 4 1.0 1 300 0 5\n0 7 4 1.0 1 400 0 5\n```\n- Line 1: `0 1 1 1.0 2 1.5 2.7 0 5 0.`\n\t- **Group Index (0)**: Identifies the sedimentary layer as the first group in this model.\n\t- **Property Type (1)**: This line describes the shear wave velocity (Vs).\n\t- **Interpolation Approach (1)**: Uses gradient interpolation from top to bottom within this group.\n\t- **Group Thickness (1.0)**: Specifies the thickness of the sedimentary layer as 1.0 kilometers.\n\t- **Number of Parameters (2)**: Two parameters are needed for gradient interpolation.\n\t- **Parameter (1.5 2.7)**: Top (1.5km/s) and bottom (2.7km/s) Vs values for linear gradient.\n\t- **Number of Anomalies (0)**: No additional anomalies are added within this layer.\n\t- **Number of Fine Layers (5)**: This group is  divided into 5 fine layers.\n\t- **Depth of the Top Boundary (0.)**: Only the first group needs this value.\n- Lines 2–5: `0 2 -3 1.0 0 0 5` to `0 5 -3 1.0 0 0 5`\n\t- **Property Types (2, 3, 4, 5)**: Define Vp/Vs, density, Qs, and Qp respectively.\n\t- **Interpolation Approach (-3)**: Uses Brocher’s empirical formula for automatic calculation based on Vs.\n\t- **Parameters**: No additional parameters are needed.\n\t- **Fine Layers (5)**: Matches the fine layer count of Line 1.\n- Lines 6–7: `0 6 4 1.0 1 300 0 5` and `0 7 4 1.0 1 400 0 5`\n\t- **Property Types (6, 7)**: Define temperature (T) and pressure (P).\n\t- **Interpolation Approach (4)**: Uses bulk assignment with a single value across the layer.\n\t- **Parameter (300 for T, 400 for P)**: Sets temperature and pressure values.\n\t- **Fine Layers (5)**: Matches other properties in this group.\nSo basically, this group describes a 1.0 km thick sedimentary layer with a Vs gradient from 1.5 to 2.7 km/s, where the other properties of this layer (Vp, density, Qs, and Qp) are derived empirically from Vs. This way, these parameters can not be directly perturbed during the inversion. In the current version, the algorith doesn't do anything with the T and P properties, so \n\n**group 1** Crystalline Crust\n- Line 8: `1 1 3 29.0 5 3.0 3.2 3.5 3.7 3.9 0 25`\n\t- **Group Index (1)**: Identifies the crystalline crust layer as the second group in this model.\n\t- **Property Type (1)**: This line defines Vs.\n\t- **Interpolation Approach (3)**: Uses B-spline interpolation for a smooth Vs profile.\n\t- **Group Thickness (29.0)**: Specifies thickness as 29.0 km.\n\t- **Number of Parameters (5)**: Five parameters for the B-spline.\n\t- **Parameters (3.0, 3.2, 3.5, 3.7, 3.9)**: B-spline control points for Vs.\n\t- **Fine Layers (25)**: Divides this group into 25 fine layers.\n\n- Line 9: `1 2 4 29.0 1 1.71 1 0.5 1.0 1.78 25`\n\t- **Property Type (2)**: Represents Vp/Vs.\n\t- **Interpolation Approach (4)**: Bulk Vp/Vs across the group.\n\t- **Parameter (1.71)**: Vp/Vs ratio.\n\t- **Number of Anomalies (1)**: One anomaly is added.\n\t- **Anomaly (0.5 1.0 1.78)**:\n\t\t- **Upper Boundary (0.5)**: 50% depth of this layer.\n\t\t- **Lower Boundary (1.0)**: 100% depth of this layer.\n\t    - **Anomaly Value (1.78)**: Vp/Vs within anomaly.\n\t- **Fine Layers (25)**: Consistent with Vs layering.\n\n- Lines 10–14: `1 3 -3 29.0 0 0 25` to `1 7 4 29.0 1 1000 0 25`\n\t- Define density, Qs, Qp (using empirical relations), temperature (700), and pressure (1000) in the crystalline crust.\n\nSo, layer 1 represents the 29 km thick crystalline crust, where Vs is smoothly interpolated with B-spline control points and the Vp/Vs structure is essentially divided into upper crust (top 50% of the crust) and lower crust (bottom 50% of the crust). Temperature and pressure are constant across the layer, while other properties are computed based on Vs.\n\n**group 2** Uppermost Mantle\n- Line 15: `2 1 3 150.0 5 4.2 4.35 4.45 4.53 4.6 0 30`\n\t- **Group Index (2)**: Identifies theuppermost mantle layer as the third group in this model.\n\t- **Property Type (1)**: Defines Vs.\n\t- **Interpolation Approach (3)**: B-spline interpolation for Vs.\n\t- **Group Thickness (150.0)**: Thickness of 150 km.\n\t- **Parameters (4.2, 4.35, 4.45, 4.53, 4.6)**: B-spline control points for Vs.\n\t- **Fine Layers (30)**: Divides this group into 30 fine layers.\n\n- Line 16: `2 2 4 150.0 1 1.789 0 30`\n\t- **Property Type (2)**: Vp/Vs with bulk application.\n\t- **Parameter (1.789)**: Sets Vp/Vs ratio.\n\n- Lines 17–21: `2 3 -3 150.0 0 0 30` to `2 7 4 150.0 1 1000 0 30`\n\t- Define density, Qs, Qp (empirical relations), temperature (800), and pressure (1000) in the mantle.\n\n**Overall Summary**\nThe model file defines a three-layer structure (sedimentary layer, crystalline crust, and uppermost mantle), using a combination of linear, bulk, and B-spline interpolations. Each layer is characterized by specific Vs, Vp/Vs, density, Qs, and Qp, enabling a detailed 1D model suitable for forward calculation. \n\nSo the model looks like:\n\n![Model figure](model.png)\n\n> [!NOTE]\n> The designations \"sedimentary layer,\" \"crystalline crust layer,\" and \"uppermost mantle layer\" are conceptual labels applied for user interpretation. The GeoInverse program does not explicitly recognize these as specific geological layers; rather, it organizes model groups from shallow to deep solely based on their `group index`. This indexing enables flexibility in setting up layers without requiring geological definitions in the input files.\n\n### 3. `in.para` File\nThe `in.para` file is used to define the parameters for the Monte Carlo inversion, specifying which parameters will be perturbed and their perturbation range. Each row in this file corresponds to a single parameter for a specific group and describes how this parameter will be handled during the inversion process. The columns in this file serve different purposes, from identifying the group and parameter type to specifying boundary conditions and anomaly values.\n\n#### Column descriptions\n1. **Group Index** (Column 1): This column  which group the parameter belongs to. The index is based on the model's group numbering.\n2. **Property Type** (Column 2): This column defines the property being described:\n\t- `0`: Thickness (km)\n\t- `1`: Vs (km/s)\n\t- `2`: Vp/Vs\n\t- ... (Basically the same as the model setting file)\n \t- `-10`,`-11`,`-12`: Additional anomaly for Vs (top boundary, bottom boundary, value, respectively)\n  \t-  `-20`,`-21`,`-22`: Additional anomaly for Vp/Vs (top boundary, bottom boundary, value, respectively)\n   \t- ... (Similarly patterns for other anomalies) \n3. **Absolute/Percentage Value Indicator** (Column 3): This column defines whether the value in the fourth column is an absolute value or a percentage of the model space.\n\t- `0`: The value in the fourth column is expressed as a **percentage** of a reference value (which is the value in the model file).\n\t- `1`: The value in the fourth column is an **absolute** value. \n4. **Model Space Radius** (Column 4): This column defines the radius (or model space variation) of the parameter in the specific dimension. This represents the extent to which the parameter can vary during the inversion process.\n   - The final model space will be the value in the model file plus/minus this radius\n5. **Monte Carlo Step Size** (Column 5): This column defines the step size for the Monte Carlo inversion process, representing the standard deviation of the Gaussian distribution from which perturbations are drawn. This determines how much the parameter can change in each iteration of the Monte Carlo search.\n6. **Parameter index** (Column 6): This column is used to identify the specific parameter when there are multiple parameters of the same type within the same group.\n   - The value indicates the **sequence number** of the parameter within the group. For example, if there are two Vs values in group 0, the first one would be assigned `0` and the second one would be assigned `1` in this column.\n\n#### Example\n[Here](in.para) is an example `in.para` file, with each row explained:\n- Line 1: `0 0 0 1.0 0.1`\n\t- This row defines the thickness (telled by the 2nd number `0` in this row) of group 0 (telled by the 1st number `0`)\n \t- The radius of this model space is 100% (the 3rd number `0` tells us the radius is defined in percentage, the 3th number `1.0` means 100%) of the reference value (which is 1.km - set in the model file)\n  \t- It has a step size of `0.1` (telled by the 5th number) for MC interations.\n  \t- Thickness is described by a single value in one group so it does not need the 6th column.\n  \t- So in summary, the thickness of the group 0 is perturbed between 0.0~2km, with a step size of 0.1 km.\n- Line 2: `0 1 1 0.5 0.05 0`\n\t- This row defines the first (telled by the 6th number `0`) Vs (telled by the 2nd number `1`) parameter for group 0 (telled by the 1st number `0`).\n   \t- The radius in model space is `0.5` (absolute value,telled by the 3rd and 4th number), with a step size of `0.05`.\n- Line 3: `0 1 1 0.5 0.05 1`\n  \t- This row defines the second (telled by the 6th number `1`) Vs parameter for group 0.\n  \t- The rest is the same as Line2\n- Line 4:`1 0 1 5.0 0.5`\n  \t- The thickness of the group 1 is perturbed between `29-5=24` to `29+5=34`km, with a step size of 0.5 km\n- Line 5:`1 1 1 0.5 0.05 0`\n  \t- The **first** Vs parameter (which is a B-spline coefficient according to the model file) is perturbed between `3.0-0.5=2.5` and `3.0+0.5=3.5` km/s, with a step size of 0.05.\n- Line 6: `1 1 1 0.5 0.05 1`\n  \t- The **second** Vs parameter (which is a B-spline coefficient according to the model file) is perturbed between `3.2-0.5=2.7` and `3.2+0.5=3.7` km/s, with a step size of 0.05.\n- Line 7 to Line 9: ...(Similar to Line5, Line6)\n- Line 10: `1 2 1 0.15 0.02 0`\n  \t- The first **Vp/Vs** parameter (telled by the 2nd number `2`) is perturbed between `1.71-0.15` and `1.71+0.15`, with a step size of 0.02\n- Line 11: `1 -22 1 0.15 0.02 0`\n  \t- The first Vp/Vs **anomaly value** (telled by the 2nd number `-22`) is perturbed between `1.78-0.15` and `1.78+0.15` with a step size of 0.02\n- Line 12 to Line 16: ... (Similar to Line5~9)\n\n> [!WARNING]\n> **Fixed Total Model Thickness**.The total thickness of the model is fixed during the inversion process. Therefore, the thickness of the last group in the model cannot be actively perturbed in the inversion. Instead, it is automatically adjusted to ensure the total thickness remains constant. If the thicknesses of other groups are perturbed, the program recalculates the thickness of the last group as the difference between the total thickness and the sum of the thicknesses of all preceding groups. \n\n## Input Data Format\nThe **surface wave dispersion, H/V ratios, and waveform-fitting receiver functions** are stored in plain `.txt` files. The first row of the file specifies the number of rows and columns in the dataset (exclude the first row). From the second row onward, the data is structured as follows:\n- Surface Wave Dispersion (km/s) and H/V Ratios:\n\t- Column 1: Period (s)\n\t- Column 2: Measured Value\n\t- Column 3: Measurement Error\n- Surface Wave Local Amplification\n\t- Column 1: Period (s)\n   \t- Column 2: Measured Value (This is a relative value)\n     \t- Column 3: Measurement Error\n\t- Column 4: Reference Value\n- Receiver Functions (For waveform fitting)\n\t- Column 1: Time (s)\n   \t- Column 2: Amplitude\n     \t- Column 3: Measurement Error\nThe **receiver functions that is used for H-kappa stacking** are stored in **SAC Format**.These SAC files include the following:\n- Standard SAC headers (beggining time, number of data points, sampling rate, and so on).\n- The user3 variable in the SAC header stores the ray parameter of the corresponding receiver function.\nSome example data can be found [here](FowardTest) (The file whose name contains 'template')\n\n## Inversion Workflow\n1. **Prepare Input Files:**\n   - Modify the provided example files based on one's own case (e.g., `test.control`, `test.mod`, `in.para`).\n   - Prepare the input data file.\n2. **Run the Inversion:**\n```\n./src/MC_main<<EOF \n${fcontrol} 1\nEOF\n```\nThe `${fcontrol}` should be the file name of the `*.control` file.\n\nThe `1` in this command represents the number of threads one wants to use. However, the program currently **cannot** do parallel computing, so this value has to be `1`. (:P)\n\n3.**Run the Posterior Processing:**\n```\nN=`awk '$4==1&&$5==1' ${dir}/MC.*.para | wc -l`\n./src/MC_Posterior<<EOF\n${fcontrol} $N\nEOF\n```\nThe `N` is the number of models that are accepted by the Monte-Carlo search.\n",
        "createdAt": "2024-11-14T20:23:46.000Z",
        "updatedAt": "2025-09-04T01:54:29.000Z",
        "language": "C++",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Hanxiao-Wu/GeoInverse/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "VayMoran/Intro_To_Seismology",
        "url": "https://github.com/VayMoran/Intro_To_Seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2022-10-22T01:51:50.000Z",
        "updatedAt": "2022-10-22T01:51:50.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "IAmS4n/Earthquake-Alert",
        "url": "https://github.com/IAmS4n/Earthquake-Alert",
        "description": "This script checks near earthquakes in Iran using Iranian Seismological Center. The audio alert is prepared for raspberry pi.",
        "stars": 2,
        "forks": 0,
        "readme": "# Earthquake Alert\nThis script checks near earthquakes **in Iran** using the Iranian Seismological Center. The audio alert is prepared for raspberry pi.\n\n[Foreshock activity has been detected for about 40% of all moderate to large earthquakes, and about 70% for events of M>7.0.](https://en.wikipedia.org/wiki/Foreshock#Occurrence) So, Awareness of small earthquakes can be effective.\n\n## Running on Raspberry Pi\n1. Connect an external speaker to Raspberry Pi.\n2. Install requirements:\n    ~~~\n    pip3 install -r requirements.txt\n    ~~~\n3. Set latitude and longitude in the `main.py` file.\n4. Check the accuracy of your Raspberry Pi Clock using the `date` command.\n5. Run the `main.py` .",
        "createdAt": "2020-05-10T10:04:40.000Z",
        "updatedAt": "2020-08-07T20:41:39.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/IAmS4n/Earthquake-Alert/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "angela1011/Principles-of-Seismology2024",
        "url": "https://github.com/angela1011/Principles-of-Seismology2024",
        "description": "Principles of Seismology",
        "stars": 1,
        "forks": 0,
        "readme": "# Principles-of-Seismology2024\n\nUsing Python code to slove and plot\n\nNormal dispersion and Abnormal dispersion\nPhase velocity (c) and group velocity (U)\nThe Least Squares Solution of the Linear Inverse Problem\n",
        "createdAt": "2024-04-16T10:49:27.000Z",
        "updatedAt": "2024-04-16T15:15:25.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/angela1011/Principles-of-Seismology2024/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "oTaira/ShotGatherExample",
        "url": "https://github.com/oTaira/ShotGatherExample",
        "description": "Graphic Interpretation of a modeled shot gather. Code is in Matlab with use of the seismology library",
        "stars": 0,
        "forks": 0,
        "readme": "# ShotGatherExample\nGraphic Interpretation of a modeled shot gather. Code is in Matlab with use of the seismology library\n",
        "createdAt": "2022-01-06T20:50:46.000Z",
        "updatedAt": "2022-01-06T20:56:13.000Z",
        "language": "MATLAB",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/oTaira/ShotGatherExample/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "pdtdenton/piseis",
        "url": "https://github.com/pdtdenton/piseis",
        "description": "educational seismology on a raspberry pi using python ",
        "stars": 1,
        "forks": 1,
        "readme": "piseis\n======\n\neducational seismology on a raspberry pi using python \n",
        "createdAt": "2014-09-30T09:54:17.000Z",
        "updatedAt": "2020-10-07T01:12:08.000Z",
        "language": "C",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/pdtdenton/piseis/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "SteinmannRene/AI4Seismology2025-UnsupervisedLearning",
        "url": "https://github.com/SteinmannRene/AI4Seismology2025-UnsupervisedLearning",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# AI4Seismology Summer School Tutorial\n\nWelcome to the tutorial repository for the **AI4Seismology Summer School**! This repository contains the material for the course \"Unsupervised Learning\".\n\n### Table of Contents\n1. [Installation](#installation)\n2. [Project Structure](#project-structure)\n3. [License](#license)\n4. [Contact](#contact)\n\n### Installation\n1. **Clone the repository or download the zip**:\n\n2. **Set up the environment**:\n   - Using Conda:\n     ```bash\n     conda env create -f environment.yml\n     conda activate your-env-name\n     ```\n\n### Project Structure\n\n```bash\ndecoding_footsteps/\n│\n├── data/             # contains the raw seismogram data and its products from the notebooks\n│\n├── img/              # contains images used for the notebooks\n│\n├── notebooks/        # notebooks for the tutorial\n│\n├── slides/           # contains the slides for the lecture\n│\n├── environment.yml   # Conda environment configuration\n├── README.md         # This readme file\n└── license.txt       # License file\n```\n\n### License\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n### Contact\nFor any questions or support, please contact:\n\n- Name: Rene Steinmann\n- Email: rene.steinmann@gfz.de\n- GitHub: [@SteinmannRene](https://github.com/SteinmannRene)\n\n---",
        "createdAt": "2025-04-22T13:54:52.000Z",
        "updatedAt": "2025-04-23T14:55:51.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/SteinmannRene/AI4Seismology2025-UnsupervisedLearning/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "yudhastyawan/lindu-software",
        "url": "https://github.com/yudhastyawan/lindu-software",
        "description": "Progress of Lindu Software Codes (for seismological data processing; determining and relocating hypocenter and traveltime tomography). All release will be updated in comp-geoph-itera repository",
        "stars": 6,
        "forks": 5,
        "readme": "# The Package Branch of Lindu Software\n[![PyPI version](https://badge.fury.io/py/lindu.svg)](https://badge.fury.io/py/lindu)\n[![GitHub issues](https://img.shields.io/github/issues/comp-geoph-itera/lindu-software)](https://github.com/comp-geoph-itera/lindu-software/issues)\n[![GitHub forks](https://img.shields.io/github/forks/comp-geoph-itera/lindu-software)](https://github.com/comp-geoph-itera/lindu-software/network)\n[![GitHub stars](https://img.shields.io/github/stars/comp-geoph-itera/lindu-software)](https://github.com/comp-geoph-itera/lindu-software/stargazers)\n[![GitHub license](https://img.shields.io/github/license/comp-geoph-itera/lindu-software)](https://github.com/comp-geoph-itera/lindu-software/blob/dev/LICENSE.md)\n\n<p align=\"center\">\n\t<img src=\"https://github.com/comp-geoph-itera/lindu-software/blob/package/src/lindu/Widgets/Images/screenshots/lindu-logo.png\" alt=\"Lindu Logo\" width=\"200\"/>\n\t<br>\n\tLindu Software Logo\n\t<br>\n</p>\nProgress of Lindu Software Codes (for seismological data processing: determining and relocating hypocenter; traveltime tomography)\n\nThis is the package branch for the future release.\n\nSee the [changelog file](https://github.com/comp-geoph-itera/lindu-software/blob/package/CHANGELOG.md)\n\n# Developer\nIf you would like to be the collaborator, feel free to pull request.\n\n# Installation\nNow, pip can be used for installing Lindu.\n```bash\npip install lindu\n```\n\n# Python version\nThe minimum version is 3.8.\n\n# How to Use\n```python\nimport lindu\nlindu.run()\n```\n\n# Build a package\nIn the parent repository, type these commands:\n```bash\npip install --upgrade build\npython -m build\n```\n\n# References\nIf you will use this software, please add these references to your research:\n```\n@article{styawan_lindu_2020,\n\ttitle = {Lindu {Software}: {A} {Free} {Seismological} {Data} {Processing} {Software} {For} {Traveltime} {Tomography} {Using} {Python} {Framework}},\n\tvolume = {537},\n\tcopyright = {All rights reserved},\n\tissn = {1755-1315},\n\tshorttitle = {Lindu {Software}},\n\turl = {https://doi.org/10.1088%2F1755-1315%2F537%2F1%2F012017},\n\tdoi = {10.1088/1755-1315/537/1/012017},\n\tabstract = {Earthquake data can be used to infer some physical properties for representing the subsurface condition. The 3-Dimensional (3D) seismic velocity structure as a kind of these important properties contains the information of variation in lithology change and fluid saturation. The most common method for inverting from the travel time of seismic event into 3D seismic velocity structure is travel time tomography which is based on the relation between velocity and travel time of P- and S-wave. Based on this concept, we develop a module of Lindu software to infer this seismic velocity structure from travel time data. This module is a part of seismological data processing sequences that have been integrated into Lindu software. The Lindu software uses Python framework, a kind of high-level programming languages. The pseudo-bending raytracing method is employed to calculate the travel time between the event sources and stations and also to build the kernel matrix. The resolution test that relates density of rays and resulted tomogram uses the synthetic Checkerboard Resolution Test (CRT) by using Damped-Least Squares (DLS) method for the inversion. For validating this module, it has been tested by using both synthetic and real data.},\n\tlanguage = {en},\n\turldate = {2020-09-07},\n\tjournal = {IOP Conference Series: Earth and Environmental Science},\n\tauthor = {Styawan, Yudha and Firdaus, Ruhul and Yudistira, Tedi and Suhendi, Cahli},\n\tmonth = aug,\n\tyear = {2020},\n\tnote = {Publisher: IOP Publishing},\n\tpages = {012017}\n}\n\n@article{styawan_preliminary_2019,\n\ttitle = {The preliminary results of {Lindu} software: a free seismological data processing using python framework},\n\tvolume = {311},\n\tcopyright = {All rights reserved},\n\tissn = {1755-1315},\n\tshorttitle = {The preliminary results of {Lindu} software},\n\turl = {https://doi.org/10.1088%2F1755-1315%2F311%2F1%2F012078},\n\tdoi = {10.1088/1755-1315/311/1/012078},\n\tabstract = {LINDU software is developed to solve integrated earthquake data processing. It is GUI based software that fulfil the needed for user friendly type of software. The Python framework is used for computation and visualization and integrates the common programs for earthquake data processing, such as GAD.exe, JHD.exe, and HypoDD.exe. It is also integrates the common procedure of routine data processing in earthquake seismology and works in local and regional scale. In this paper, we shows the preliminary results of LINDU software for several functions. To identify arrival time of P-wave we employ Akaike Information Criterion (AIC), MER (Modified Energy Ratio) and S/L Kurt�s method. The results of these method will be considered as guided � auto picking. However, the results also can be treated as reference for picking manually with Seisgram2k.jar. Geiger�s method is employed to locate the event location. The events can be relocated and 1D velocity can be updated by employing Joint Hypocenter Determination (JHD). The next method to relocate the event location is Double Difference (DD) algorithm. The precision result of Lindu software has been tested using IRIS and real data available which run seamlessly.},\n\tlanguage = {en},\n\turldate = {2020-09-07},\n\tjournal = {IOP Conference Series: Earth and Environmental Science},\n\tauthor = {Styawan, Yudha and Andika, Putu Pradnya and Suhendi, Cahli and Firdaus, Ruhul and Sudibyo, Maria R. P. and Erlangga, I. F. and Ry, Rexha Verdhora},\n\tmonth = aug,\n\tyear = {2019},\n\tnote = {Publisher: IOP Publishing},\n\tpages = {012078}\n}\n\n@article{andika_lindu_2019,\n\ttitle = {Lindu {Software}: {An} {Open} {Source} {Seismological} {Data} {Processing} {Using} {Python} {Framework} {To} {Relocate} {Hypocenter} ({Preliminary} {Software})},\n\tvolume = {318},\n\tcopyright = {All rights reserved},\n\tissn = {1755-1315},\n\tshorttitle = {Lindu {Software}},\n\turl = {https://doi.org/10.1088%2F1755-1315%2F318%2F1%2F012021},\n\tdoi = {10.1088/1755-1315/318/1/012021},\n\tabstract = {Recorded seismogram of an earthquake data contains the earth structure information. Researchers developed the method to extract the information and derives it into the program codes. However, generally, the program codes developed only for specific function and work on only specific scale. Almost the existing programs have a limitation, for example, they work on command-line based and less user-friendly. Lindu software is developed to solve these problems. In this paper, we show the preliminary results of Lindu software, a GUI � based software which is open source and developed in python platform. This software integrates the common procedure of routine data processing in earthquake seismology and works in local and regional scale. It is designed to read multi-component data on multi-station. To identify events automatically, we employ SL Kurt�s method and use the results as guided auto�picking. However, the picked time also can be changed manually. Furthermore, we employ Joint Hypocenter Determination (JHD) algorithm to locate the hypocenter of earthquake events and update the 1D velocity model simultaneously. Then the events can be relocated by employing the double-difference method. The software was tested on the available data from IRIS and BMKG and shows the acceptable and reliable results.},\n\tlanguage = {en},\n\turldate = {2020-09-07},\n\tjournal = {IOP Conference Series: Earth and Environmental Science},\n\tauthor = {Andika, Putu Pradnya and Styawan, Yudha and Suhendi, Cahli and Firdaus, Ruhul},\n\tmonth = aug,\n\tyear = {2019},\n\tnote = {Publisher: IOP Publishing},\n\tpages = {012021}\n}\n```\n\n# Galleries\n<p align=\"center\">\n\t<img src=\"https://github.com/comp-geoph-itera/lindu-software/blob/package/src/lindu/Widgets/Images/screenshots/lindu-dev-5.PNG\" alt=\"Lindu Development\" width=\"800\"/>\n\t<br>\n\tLindu Progress (2021-10-27)\n\t<br>\t\n</p>",
        "createdAt": "2018-11-03T13:23:51.000Z",
        "updatedAt": "2023-01-13T04:15:25.000Z",
        "language": "Python",
        "homepage": "https://github.com/comp-geoph-itera/lindu-software",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/yudhastyawan/lindu-software/package/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "pegasus-isi/seismology-workflow",
        "url": "https://github.com/pegasus-isi/seismology-workflow",
        "description": null,
        "stars": 1,
        "forks": 1,
        "readme": "# Seismology Workflow\n\nThis workflow performs seismogram deconvolutions to estimate earthquake source time functions (STFs) for the 2013 Craig, Alaska Earthquake. Signals in the subdirectory `input/EGF` are deconvolved from the corresponding signals in the subdirectory `input/MShock`.\n\n<img src=\"docs/images/seismology-workflow.png?raw=true\" width=\"600\">\n\n### Description\n\n__`sG1IterDecon`__: receives a pair of signals, one from `input/EGF` and another from `input/MShock`, and computes seismogram deconvolutions to estimate earthquake source time functions (STFs). The output file is in the SAC (Seismic Analysis Code) format.\n\n__`siftSTFByMisfit`__: receives all STFs generated from the `sG1IterDecon` jobs and sifts the data by misfit. Only the good fits are kept and compressed into a single `.tar.gz` file.\n\n## Generating the Workflow\nThe `generate_dax.sh` script creates a Pegasus DAX workflow using the signals found in the subdirectories `input/EGF` and `input/MShock`.\nThe number of `sG1IterDecon` jobs will depend on the number of EGF signals and their corresponding signals in MShock. The command should be executed as follows:\n\n```\n./generate_dax.sh seismology.dax\n```\nThis command will generate a `seismology.dax` file, which is the Pegasus workflow containing all jobs (with their corresponding executables) and their dependencies (data dependency in this case).\n\n## Running the Seismology Workflow\nTo run the workflow, execute the following command:\n```\n./plan_dax.sh seismology.dax\n```\nOnce the workflow execution is completed, the compressed output file with the good fits, will be available in `output/good-fit.tar.gz`.\n\n",
        "createdAt": "2016-07-19T21:42:30.000Z",
        "updatedAt": "2016-07-23T07:28:36.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/pegasus-isi/seismology-workflow/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "shanalyb/Seismology",
        "url": "https://github.com/shanalyb/Seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# Seismology",
        "createdAt": "2019-12-13T19:39:46.000Z",
        "updatedAt": "2019-12-13T20:08:07.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/shanalyb/Seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "arif-geo/IU_Seismology_HW",
        "url": "https://github.com/arif-geo/IU_Seismology_HW",
        "description": "Seismology HWs for sharing Jupyter notebooks",
        "stars": 0,
        "forks": 0,
        "readme": "# IU_Seismology_HW\nSeismology HWs for sharing Jupyter notebooks\n",
        "createdAt": "2023-10-30T17:26:42.000Z",
        "updatedAt": "2023-10-30T17:31:27.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/arif-geo/IU_Seismology_HW/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "fes478/CoPara-CGFD3D",
        "url": "https://github.com/fes478/CoPara-CGFD3D",
        "description": "multi-GPU-based seismic wave simulation program",
        "stars": 1,
        "forks": 0,
        "readme": "## CoPara-CGFD3D\n\nCollaborative Parallel 3D Curve Grid Finite Difference method for Elastic Wave Simulation.\n\n\n\n## Author\n\nZhenjiang Yu,  at SUSTech\n\nyuzj@sustech.edu.cn\n\n\n\n## Description\n\nThis is a strong ground motion simulation program suitable for multi-GPU platform with the computing capability greater than 6.0.\n\nIt can address elastic wave propagating issues in three-dimensional complex media with undulating surfaces.\n\nIt was first proposed by Zhang and Chen (2006), added PML absorbing boundary by  Zhang and Shen (2010), improved by Zhang et al. (2012), and applied in many historical earthquake studies.\n\nHere is a new multi-GPU-based collaborative parallel implementation, and the usage should refer to [manual](./user_manual.md).\n\n\n\n## Reference\n\nZhang, W., Chen, X., 2006. Traction image method for irregular free surface boundaries in finite difference seismic wave simulation. Geophysical Journal International 167, 337–353.\n\nZhang, W., Shen, Y., 2010. Unsplit complex frequency-shifted PML implementation using auxiliary differential equations for seismic wave modeling. Geophysics 75, T141–T154.\n\nZhang, W., Zhang, Z., Chen, X., 2012. Three-dimensional elastic wave numerical modelling in the presence of surface topography by a collocated-grid finite-difference method on curvilinear grids. Geophysical Journal International 190, 358–378.\n",
        "createdAt": "2017-05-24T13:42:39.000Z",
        "updatedAt": "2025-06-12T00:10:21.000Z",
        "language": "C++",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/fes478/CoPara-CGFD3D/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "NoiseCIEI/AFTAN",
        "url": "https://github.com/NoiseCIEI/AFTAN",
        "description": "Seismic Surface Wave Dispersion Measurements with Automatic Frequency Time Analysis",
        "stars": 15,
        "forks": 16,
        "readme": "",
        "createdAt": "2018-11-12T00:28:58.000Z",
        "updatedAt": "2025-06-04T01:14:12.000Z",
        "language": "PostScript",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ghfbsd/sacRUtils",
        "url": "https://github.com/ghfbsd/sacRUtils",
        "description": "Tools for SAC file and facility access through the R system",
        "stars": 1,
        "forks": 0,
        "readme": "sacRUtils\n=========\n\nTools for SAC file and facility access through the R system\n",
        "createdAt": "2013-08-09T15:22:08.000Z",
        "updatedAt": "2023-02-27T15:05:30.000Z",
        "language": "C",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ghfbsd/sacRUtils/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "elifo/elifo.github.io",
        "url": "https://github.com/elifo/elifo.github.io",
        "description": "elif oral. cv. research. earthquakes. engineering. geohazards. seismology",
        "stars": 1,
        "forks": 1,
        "readme": "# github.io-elifo\n\n\n<!-- PROJECT SHIELDS -->\n<!--\n*** I'm using markdown \"reference style\" links for readability.\n*** Reference links are enclosed in brackets [ ] instead of parentheses ( ).\n*** See the bottom of this document for the declaration of the reference variables\n*** for contributors-url, forks-url, etc. This is an optional, concise syntax you may use.\n*** https://www.markdownguide.org/basic-syntax/#reference-style-links\n-->\n\n<!-- PROJECT LOGO -->\n<br />\n<p align=\"center\">\n  <a href=\"https://elifo.github.io/index.html\">\n    <img src=\"./images/elif_oral_header.png\" alt=\"Logo\" width='502px;>\n  </a>\n\n  <h3 align=\"center\">elifo.github.io</h3>\n\n  <p align=\"center\">\n    Elif Oral's personal website\n  </p>\n\n</p>\n\n  \n<!-- ABOUT THE PROJECT -->\n## About the website\nI am a scientist working on geohazards. This website is to share my work with you.\n<!-- **To avoid retyping too much info. Do a search and replace with your text editor for the following:**\n`github_username`, `repo_name`, `twitter_handle`, `email`, `project_title`, `project_description`\n -->\n\n### Content\nYou can easily access to:\n\n* [About me](https://elifo.github.io/about.html)\n* [My CV](https://drive.google.com/file/d/1mF96xNhRptWsd5A1F3P99ItNbPr1fexF/view?usp=sharing)\n* [My research portfolio](https://elifo.github.io/highlights.html)\n* [Interesting reading materials](https://workflowy.com/s/golden-biblio-list/P8rZyO6IBaBKqqth)\n* [Codes & Tutorials related to my work](https://github.com/elifo/elifo.github.io/blob/main/tutorials/all_topics.md)\n\n\nThanks for looking, and stay tuned!\n\n\n\n\n",
        "createdAt": "2020-12-11T12:14:48.000Z",
        "updatedAt": "2025-05-30T20:41:09.000Z",
        "language": "HTML",
        "homepage": "https://elifo.github.io",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/elifo/elifo.github.io/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "huazhz/obspy_dev_ml",
        "url": "https://github.com/huazhz/obspy_dev_ml",
        "description": "Ml base obspy  for Seismology",
        "stars": 0,
        "forks": 0,
        "readme": "# obspy_dev_ml\nMl base obspy  for Seismology\n",
        "createdAt": "2018-03-20T11:32:13.000Z",
        "updatedAt": "2018-03-21T01:00:04.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/huazhz/obspy_dev_ml/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "panzhengyang/seismology_study",
        "url": "https://github.com/panzhengyang/seismology_study",
        "description": "seismology learning_学习地震学（介绍）",
        "stars": 1,
        "forks": 0,
        "readme": "# seismology_study\nhow to study seismology in my opinion so give me your advise\n\ncatalog:\n\ntwo groups  one is in chinese and the other is in english!\n\n\n\n\n",
        "createdAt": "2015-10-09T06:54:06.000Z",
        "updatedAt": "2015-12-01T19:33:16.000Z",
        "language": null,
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/panzhengyang/seismology_study/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "UW-geophysics-edu/ess-412-512-intro2seismology",
        "url": "https://github.com/UW-geophysics-edu/ess-412-512-intro2seismology",
        "description": "Introduction to seismology - computing labs to supplement a course from P Shearer's book",
        "stars": 5,
        "forks": 0,
        "readme": "# ess-412-512-intro2seismology\nIntroduction to seismology - computing labs to supplement a course from P Shearer's book\n",
        "createdAt": "2023-02-27T16:43:31.000Z",
        "updatedAt": "2025-02-22T08:51:54.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/UW-geophysics-edu/ess-412-512-intro2seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "rafiadzamankhan/Earthquake-Magnitude-Estimation-Using-Machine-Learning",
        "url": "https://github.com/rafiadzamankhan/Earthquake-Magnitude-Estimation-Using-Machine-Learning",
        "description": "Earthquake Magnitude Estimation from  Seismological and Geographical Data Using  Machine Learning.",
        "stars": 0,
        "forks": 0,
        "readme": "# 🌍 Earthquake Magnitude Estimation\n\nThis project explores the use of **machine learning models** to estimate earthquake magnitudes based on **seismological and geographical data**.\nThe study applies multiple regression techniques — including Random Forest, Support Vector Regression, K-Nearest Neighbors, XGBoost, and Neural Networks — to identify patterns in seismic datasets (features like depth, latitude, longitude, signal strength, and ground intensity indices).\n\nThe goal is to improve **prediction accuracy** and support the development of **early-warning systems** and **disaster preparedness strategies**.\n",
        "createdAt": "2025-09-21T12:36:40.000Z",
        "updatedAt": "2025-09-21T12:50:54.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/rafiadzamankhan/Earthquake-Magnitude-Estimation-Using-Machine-Learning/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "nwhoffman/seismology",
        "url": "https://github.com/nwhoffman/seismology",
        "description": "Jupyter notebooks for plotting waveform data for local and regional earthquakes. Used for mentoring undergraduate students in seismology.",
        "stars": 0,
        "forks": 0,
        "readme": "# seismology\n\nThis repository contains notebooks that I use for seismic data tutorials.\n",
        "createdAt": "2017-05-09T21:34:19.000Z",
        "updatedAt": "2018-05-15T00:17:31.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/nwhoffman/seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "TomWagg/mass-gainer-seismology",
        "url": "https://github.com/TomWagg/mass-gainer-seismology",
        "description": "Modelling the imprint of mass transfer on asteroseismic signals in SPB stars - Wagg+2024",
        "stars": 1,
        "forks": 0,
        "readme": "<div align='center'>\n  <h1>The Asteroseismic Imprints of Mass Transfer</h1>\n  <h5><ins>Tom Wagg</ins>, Cole Johnston, Earl Bellinger, Mathieu Renzo, Rich Townsend & Selma de Mink</h5>\n\n  <a href=\"https://ui.adsabs.harvard.edu/abs/2024A%26A...687A.222W/abstract\">\n    <img src=\"https://img.shields.io/badge/read-paper-blue\"/>\n  </a>\n  <a href=\"mailto:tomwagg@uw.edu\">\n    <img src=\"https://img.shields.io/badge/contact-authors-purple\"/>\n  </a>\n  <a href=\"notebooks/main.ipynb\">\n    <img src=\"https://img.shields.io/badge/reproduce-figures-yellow\"/>\n  </a>\n  <a href=\"https://zenodo.org/records/10011675\">\n    <img src=\"https://img.shields.io/badge/zenodo-record-green\"/>\n  </a>\n  <a href=\"https://www.tomwagg.com/html/interact/mass-gainer-asteroseismology.html\">\n    <img src=\"https://img.shields.io/badge/interactive-plots-orange\"/>\n  </a>\n  \n  <p>Project on modelling the potential imprint of mass transfer on asteroseismology of SPB stars (started at the Kavli Summer Program 2023).</p>\n</div>\n",
        "createdAt": "2023-07-03T12:41:29.000Z",
        "updatedAt": "2025-04-18T21:12:46.000Z",
        "language": "Jupyter Notebook",
        "homepage": "https://www.tomwagg.com/html/interact/mass-gainer-asteroseismology.html",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/TomWagg/mass-gainer-seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "jeffreyjsung/3d-ground-motion-simulations",
        "url": "https://github.com/jeffreyjsung/3d-ground-motion-simulations",
        "description": "Data science research for the Berkeley Seismological Laboratory.",
        "stars": 0,
        "forks": 0,
        "readme": "# 3D Ground-Motion Simulations of Moderate Earthquakes on the Hayward Fault\nBerkeley Seismological Laboratory Research Project: Quantifying potential bias of seismic velocity model prediction data to improve accuracy of future simulations.\n\n## Jupytext\nCode and visualizations are written in Jupyter Notebooks - I am using [Jupytext](https://jupytext.readthedocs.io/en/latest/install.html) for version control (converting from .ipynb -> .py for Git then .py -> ipynb for working).\n\n### Convert notebook.ipynb to a .py file\nCommand line: jupytext --to py notebook.ipynb\n\n### Convert notebook.py to an .ipynb file with no outputs\nCommand line: jupytext --to notebook notebook.py\n\n\\\n&nbsp;\n\\\n&nbsp;\n\n![poster](poster.jpg)\n",
        "createdAt": "2022-03-07T07:39:43.000Z",
        "updatedAt": "2023-03-24T08:28:54.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/jeffreyjsung/3d-ground-motion-simulations/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "jbrussell/EI_Live_2020",
        "url": "https://github.com/jbrussell/EI_Live_2020",
        "description": "Earth Institute Live 2020 - Exploring seismology with Python",
        "stars": 15,
        "forks": 7,
        "readme": "## Earth Institute LIVE 2020\nThis repository holds codes developed for the K-12 [EI Live](https://www.earth.columbia.edu/videos/channel/k12-education) 2020 series presented in two parts in May 2020. \n\n[Part 1 of 2: Data Storytelling in Python](https://www.earth.columbia.edu/videos/view/part-1-of-2-data-storytelling-in-python-grades-10-12)\n\n[Part 2 of 2: A Deep Dive into Earthquake Sonification with Python](https://www.earth.columbia.edu/videos/view/part-2-of-2-a-deep-dive-into-earthquake-sonification-with-python-grades-10-12)\n\nThese Jupyter Notebooks produce interactive plots (via Bokeh and HoloViews), which allow students to explore the data on their own. The presentations are aimed at grades 10–12. The full walkthrough and interactive plots can be found here: https://jbrussell.github.io/eilive2020/\n\nPart 1 is focused primarily on exploration of earthquake patterns and plate tectonics. In **ExploreEarthquakes**, we look at patterns in global earthquake distributions, at what depths earthquakes occur, and how earthquake magnitudes are distributed. In **NYC_SeismicNoise_and_Electricity**, we explore the effects of COVID-19 lockdown on human activity in New York City using ground vibration data from the Central Park seismometer. We also compare this to NYC electricity consumption and find that both ground vibrations and electricity consumption drop as people begin to quarantine.\n\nPart 2 focuses on data sonification – the transformation of data into audible sound, which augments our ability to perceive patterns in complex data. In **earthquake_sonification**, we start off with simple waveform shapes in order to build intuition about how the frequency and amplitude of a wave alters what we hear. We then move on to listening to earthquakes as well as the NYC ground vibrations from Part 1.\n\n",
        "createdAt": "2020-04-09T20:28:25.000Z",
        "updatedAt": "2025-11-13T18:37:31.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/jbrussell/EI_Live_2020/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ytian159/Venus-Seismology",
        "url": "https://github.com/ytian159/Venus-Seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# Venus-Seismology\nTo run the notebook, \"obspy\" package will be needed\n",
        "createdAt": "2022-03-11T00:11:44.000Z",
        "updatedAt": "2022-03-11T00:12:51.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ytian159/Venus-Seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "GinvLab/SeismicWaves.jl",
        "url": "https://github.com/GinvLab/SeismicWaves.jl",
        "description": "Acoustic and elastic seismic wave propagation and gradients (using the adjoint method).",
        "stars": 5,
        "forks": 0,
        "readme": "# SeismicWaves\n\n[![Docs Stable](https://img.shields.io/badge/docs-stable-blue.svg)](https://ginvlab.github.io/SeismicWaves.jl/stable)\n[![Docs Latest](https://img.shields.io/badge/docs-latest-blue.svg)](https://ginvlab.github.io/SeismicWaves.jl/dev)\n\nSeismicWaves.jl is a Julia package for acoustic and elastic wave propagation simulations designed to be used in a full-waveform inversion (FWI) framework. It solves different flavours of the wave equation using the finite difference method.\n\n![Wave propagation example](docs/src/images/acoustic2d_xpu_complex_freetop_halo20_360.gif)\n\nThe main features of SeismicWaves.jl are:\n- acoustic and elastic simulations\n- gradients using the adjoint method\n- different misfit/objective functionals provided or the possibility to specifing your own\n- device-agnostic backends for parallelization, i.e., CPUs or GPUs (CUDA, AMDGPU, Metal - Apple M-chips) thanks to [`ParallelStencil.jl`](https://github.com/omlins/ParallelStencil.jl) \n- C-PML boundary conditions\n- checkpointing of simulations for the adjoint method\n\nWork in progress:\n- noise correlations (seismic interferometry)\n- arbitrary-order stencils\n- 3D elastic simulations\n\nThis package integrates well with the inversion framework provided by [`InverseAlgos`](https://github.com/GinvLab/InverseAlgos.jl), which includes both deterministic (L-BFGS, Quasi-Newton, etc.) and probabilistic (e.g., Hamiltonian Monte Carlo) inverse algorithms, as part of the [`G⁻¹Lab`](https://ginvlab.github.io) framework. \n\nMore information and an extensive list of features can be found in the documentation, which can be found either [online](https://ginvlab.github.io/SeismicWaves.jl/stable) or built locally by running the docs/make.jl file.\n\n> [!WARNING]\n> **Documentation is currently minimal** and work in progress!\n",
        "createdAt": "2025-02-03T11:27:45.000Z",
        "updatedAt": "2025-10-27T18:35:36.000Z",
        "language": "Julia",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/GinvLab/SeismicWaves.jl/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "mprocha/scripts",
        "url": "https://github.com/mprocha/scripts",
        "description": "Scripts in shell and python to use in seismology",
        "stars": 2,
        "forks": 2,
        "readme": "# scripts\nScripts in shell and python to use in seismology\nunder contruction\n",
        "createdAt": "2014-03-22T18:19:35.000Z",
        "updatedAt": "2022-03-30T20:59:10.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/mprocha/scripts/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "msbdd/PPSD_Plotter",
        "url": "https://github.com/msbdd/PPSD_Plotter",
        "description": "A simple PPSD plotting utility based on ObsPy with GUI",
        "stars": 0,
        "forks": 1,
        "readme": "# PPSD Plotting Utility\n\n![License: GPL v3](https://img.shields.io/badge/License-GPLv3-brightgreen.svg)\n![Code style: flake8](https://img.shields.io/badge/Code%20style-flake8-brightgreen)\n\n[![PPSD_Plotter Build Windows](https://github.com/msbdd/PPSD_Plotter/actions/workflows/Distribute_Windows.yml/badge.svg)](https://github.com/msbdd/PPSD_Plotter/actions/workflows/Distribute_Windows.yml)\n\n<sup>There is a saying: GUI makes simple tasks simpler and more convenient, while CLI makes complex tasks possible.\n<br>\nThis is primary aimed for solving a rather simple task, but without writing even simple scripts at the end user side.\n</sup>\n\nThis utility automates the calculation, plotting, and export of Power Spectral Density (PPSD) data from seismic waveform files using [ObsPy](https://docs.obspy.org). It is designed to process many datasets using a simple YAML configuration and parallel execution and has a simple tkinter-based GUI.\n\n## TODO:\n\n- Custom plotting function and additional plotting parameters (day/night) (?)\n- Linux building (?)\n- Major refactor to automate all the data and station information collection (?)\n\n---\n\n## Requirements\n\n- Python 3.8.10+\n- ObsPy\n- PyYAML\n- tqdm\n\n## Installation\n\nIf you are on Windows and don't want to handle any python-related installations, please use the provided binary.\n\nIf you are on Linux/MacOS or want to install on Windows using source code you could:\n\n1) Create a new venv\n2) Install dependencies:\n\n```\npip install -r requirements.txt\n```\n3) Run \n```\npython src\\gui.py\n```\n---\n\n## Configuration File: `config.yaml`\n\nThe utility uses a YAML file to define how each dataset is processed. <br> An example configuration is provided in the ```example``` folder.<br>\nYou can pass additional plotting parameters to the ```PPSD.plot()``` function from ObsPy.\nFor the full list of supported options, please refer to the [ObsPy documentation](https://docs.obspy.org/packages/autogen/obspy.signal.spectral_estimation.PPSD.plot.html).<br>\nAll these parameters are now visible in the GUI.\n\nAdded a possibility to plot a custom RMS noise level on the plot.\n\n## Output Structure\n\nDepending on the action used, the script generates the following output:\n\n- `.npz` files saved to:\n  ```\n  <folder>/npz_<location>_<channel>/\n  ```\n\n- `.png` plots saved to:\n  ```\n  <output_folder>/<trace.id>.png\n  ```\n\n- `.csv` exported data (for action \"convert\") saved to:\n  ```\n  <folder>/npz_<location>_<channel>_text/export.csv\n  ```\n\n## Example Data Acknowledgment\n\nA simple example is provided in the example folder.\nThis example uses the seismic data from the [IU Network](https://www.fdsn.org/networks/detail/IU/)\n\nThe original data was collected by the **Albuquerque Seismological Laboratory / USGS** and distributed via the **International Federation of Digital Seismograph Networks (FDSN)**:\n\n> Albuquerque Seismological Laboratory/USGS. (1988). *Global Seismograph Network (GSN - IRIS/USGS)* [Dataset]. International Federation of Digital Seismograph Networks. [https://doi.org/10.7914/SN/IU](https://doi.org/10.7914/SN/IU)\n\nSeismic waveform and station metadata were accessed using the [EarthScope FDSN Web Services](https://service.iris.edu/), which provide public access to seismic data managed by the IRIS Data Management Center.\n\n## Notes\n\n- The waveform file extensions supported are:\n  ```\n  .mseed, .miniseed, .msd\n  ```\n\n- Each dataset must include:\n  - One waveform folder\n  - One response file\n  - One or more channel codes\n\n- Channels are processed independently, and multiple stations can be run in parallel.\n\n---\n\n## Contact\n\nPlease feel free to report issues or submit improvements!\n\n---\n",
        "createdAt": "2025-06-24T09:18:44.000Z",
        "updatedAt": "2025-10-28T10:05:26.000Z",
        "language": "Python",
        "homepage": "https://github.com/msbdd/PPSD_Plotter",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/msbdd/PPSD_Plotter/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "gzenhaeusern/polarisation-package",
        "url": "https://github.com/gzenhaeusern/polarisation-package",
        "description": null,
        "stars": 4,
        "forks": 1,
        "readme": "# Polarisation Analysis \n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7220543.svg)](https://doi.org/10.5281/zenodo.7220543)\n\n## Introduction\nThis package uses seismic data and applyies a polarisation analysis to obtain a back azimuth based on an arriving wave.\nOriginally used for Martian seismic data from the InSight mission, it can also be applied to earthquake data or synthetics.\n\nA detailed description of the method and resulting plot can be found in [BSSA](https://pubs.geoscienceworld.org/ssa/bssa/article/112/4/1787/613988/Low-Frequency-Marsquakes-and-Where-to-Find-Them), or, in the absence of institutional access, [arXiv](https://arxiv.org/abs/2204.12959).\n\n\n## Reference\nGéraldine Zenhäusern, Simon C. Stähler, John F. Clinton, Domenico Giardini, Savas Ceylan, Raphaël F. Garcia; Low‐Frequency Marsquakes and Where to Find Them: Back Azimuth Determination Using a Polarization Analysis Approach. *Bulletin of the Seismological Society of America* **2022**; 112 (4): 1787–1805. doi: https://doi.org/10.1785/0120220019\n\n\n\n## Code\nThe code can output either only a back azimuth with uncertainty (obtained for the first supplied time-frequency window), or it can produce a comprehensive polarisation plot (as in [BSSA](https://pubs.geoscienceworld.org/ssa/bssa/article/112/4/1787/613988/Low-Frequency-Marsquakes-and-Where-to-Find-Them), [arXiv](https://arxiv.org/abs/2204.12959)).\nIt is recommended to produce the plots so any time-frequency tradeoffs can be analysed more carefully.\n\n\n## How to use\nThe code is set up to run from the command line, e.g.\n```\npython polarisation_main.py 'Data/*.mseed' -p --name 'test'\n```\nThis produces a plot called 'test.png' in a subfolder called 'Plots' from mseed files stored in 'Data'. To change the save location, please adjust the code at the end of `plot_polarization_event_noise` in the [plot code](./polarisation_package/polarisation_plot.py). The data location should be given in '', otherwise there is a shell issue with wildcards.\nTo omit the plot and simply get the back azimuth estimate, leave out -p (and --name).\n\nIn the [main code](./polarisation_package/polarisation_main.py), timings for P, S, and noise (or other phases!) should be added. If desired, the time windows and frequency band can also be adjusted. Most polarisation tweaks can be made within that file when calling `plot_polarization_event_noise`.\n\n## License\nThis project is licensed under the terms of the GPLv3 license.\n\nCopyright (C) 2022  Géraldine Zenhäusern (geraldine@zenhausern.net), Simon Stähler (mail@simonstaehler.com), Martin van Driel (Martin@vanDriel.de)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see [here](https://www.gnu.org/licenses/).\n\n\n## Notes\nThere are some artifacts of InSight data processing in the code (e.g. 1 Hz tick removal option, using manual back azimuths). In case anyone wants to use this code for an analysis of this data set, they were left in the code.\nSome terminology in variable naming is also InSight-based, but is explained in comments of the respective functions.\n\n* Data should be in an ObsPy compatible format\n* Data should be instrument corrected and rotated to ZNE. There is an option to rotate the traces to ZRT/LQT using a supplied catalog back azimuth\n* Data should be in velocity, or displacement with `differentiate = True`. The code runs in any case, but the plot labels were made for velocity data\n* Ellipticity data is currently not plotted, but can be added/swaped in the code. There is an example comment in the [plot code](./polarisation_package/polarisation_plot.py)\n* If no second signal window is needed, use `phase_S = '-'`. The histogram plots will then remain empty. Supply some dummy time to `timing_S` in that case \n\n\nThe output plot follows this style (example of marsquake S0235b):\n![image](./polarisation_package/Plots/S0235b.png)\n",
        "createdAt": "2022-10-10T17:21:12.000Z",
        "updatedAt": "2025-01-26T02:56:35.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.7220543",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.7220543",
            "dataCite": "10.5281/zenodo.7220543",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/gzenhaeusern/polarisation-package/main/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.7220543",
            "title": "Polarisation analysis for seismic data",
            "journal": "Zenodo",
            "dateReleased": "2022-10-18T00:00:00.000Z",
            "abstract": "This repository uses seismic data and applyies a polarisation analysis to obtain a back azimuth based on an arriving wave. Originally used for Martian seismic data from the InSight mission, as described in Géraldine Zenhäusern, Simon C. Stähler, John F. Clinton, Domenico Giardini, Savas Ceylan, Raphaël F. Garcia; Low‐Frequency Marsquakes and Where to Find Them: Back Azimuth Determination Using a Polarization Analysis Approach. <em>Bulletin of the Seismological Society of America</em> <strong>2022</strong>; 112 (4): 1787–1805. doi: https://doi.org/10.1785/0120220019",
            "citationsArray": []
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "kelreeeeey/seismology-lab-odin",
        "url": "https://github.com/kelreeeeey/seismology-lab-odin",
        "description": "Seismology Lab of 5th semester as Geophysics Student in Gadjah Mada University. Dedicated repository to reviewing things I learned while learning code in Odin.",
        "stars": 0,
        "forks": 0,
        "readme": "# seismology-lab-odin\n\nSeismology Lab of 5th semester as Geophysics Student in Gadjah Mada University.\nDedicated repository to reviewing things I learned while\nlearning code in Odin.\n\n## Plans of contents\n\n1. Calculate distance and azimuth (and back azimuth) between 2 points on earth surface with respect to Nort Pole which essential to;\n    - Pre-processing location of earthquake source (assumed to be any known location on earth) that being recorded by seismogram in certain locations i.e. source-stations\n    - Later be used when determine locatioon of earthquake\n\n2. Introduction of seismogram and time origin calcutation\n    - to be added soon\n\n3. Epicenter calculation\n    - to be added soon\n\n4. Epicenter calculation; Ritcher Method\n    - to be added soon\n\n5. Epicenter calculation; Geiger Method\n    - to be added soon\n\n6. Earthquake Magnitude\n    - to be added soon\n\n7. Focal Mechanism Analysis\n    - to be added soon\n\n8. Seismology contribution to earth's Internal structure\n    - to be added soon\n\n9. Introduction to Volcanoseismology\n    - to be added soon\n\n10. Seismology data management with `Obspydmt`\n    - to be added soon\n",
        "createdAt": "2025-03-22T18:33:57.000Z",
        "updatedAt": "2025-03-25T19:19:54.000Z",
        "language": "Odin",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/kelreeeeey/seismology-lab-odin/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "usmanyaro/Glossary-of-words",
        "url": "https://github.com/usmanyaro/Glossary-of-words",
        "description": "This is a list of words often used in my research project (Heat flow and Regional Seismology)",
        "stars": 1,
        "forks": 0,
        "readme": "",
        "createdAt": "2017-03-07T22:31:22.000Z",
        "updatedAt": "2022-01-05T13:32:17.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "albertleonardo/labelerrors",
        "url": "https://github.com/albertleonardo/labelerrors",
        "description": "Pervasive Label Errors in Seismological Machine Learning Datasets",
        "stars": 1,
        "forks": 0,
        "readme": "# Pervasive Label Errors in Seismological Machine Learning Datasets\nThis is an effort to improve the datasets used for training deep learning models for observational seismology.\nThere are performance imporvements from culled datasets beyond what can be gained from adding model complexity with a fixed dataset.\nSee for example and inspiration:\n[https://www.youtube.com/watch?v=06-AZXmwHjo] minute 5-10 \n\nFor analog studies in other fields and other datasets check this one out:\n[https://labelerrors.com/]\n\n\n\nWe documented a series of erros or faulty labels in a number of machine learning datasets, focused on local earthquake recordings\n\nThere is a csv file for every dataset analyzed that contains the names and reference to the examples in the datasets per their Seisbench metadata. This is with the goal of avoiding them when training and testing.\n\n## Examples\n\nSome examples are shown here, mostly earthquakes that are not labeled might hurt model performance, as they are trained to recognize the labeled earthquakes while ignoring the unlabeled ones, which is precisely what we don't want them to do.\n\n![Image Alt text](/images/aq2009_0000080.png)\n\n## Noise samples \nWe quantified the number of noise samples that do contain unlabeled earthquakes, here some examples from the four datasets that contain noise samples. \n\n![Image Alt text](/images/txed_172405.png)\n![Image Alt text](/images/stead_0022054.png)\n![Image Alt text](/images/pnw_000119.png)\n![Image Alt text](/images/instance_0002650.png)\n\n\n## Other errors\nWe found other forms of errors, for which their prevalence was not quantified (yet)\nThe data does not contain the labeled arrivals, due to archival or instrumental issues\n![Image Alt text](/images/ceed_1001553.png)\n\nThe labels are inaccurate, for instance the S arrival label in this sample is too early\n![Image Alt text](/images/instance_0548673.png)\n\n\n\nManuscript is coming\n",
        "createdAt": "2025-10-20T06:05:06.000Z",
        "updatedAt": "2025-11-05T20:19:16.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/albertleonardo/labelerrors/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "calum-chamberlain/GPHS445_notebooks",
        "url": "https://github.com/calum-chamberlain/GPHS445_notebooks",
        "description": "Notebooks for the GPHS445 course at VUW",
        "stars": 25,
        "forks": 7,
        "readme": "# VUW GPHS445: Observational Earthquake Seismology\n## Jupyter notebooks for the GPHS445 course taught at Victoria University of Wellington\n\n[![Python 3.8](https://img.shields.io/badge/python-3.8-blue.svg)](https://www.python.org/downloads/release/python-380/)\n[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)\n![test](https://github.com/calum-chamberlain/GPHS445_notebooks/workflows/test/badge.svg)\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/calum-chamberlain/GPHS445_notebooks/master)\n\nThis repository contains relevant notebooks and links to notebooks to help students\nthrough practical elements of observational seismology. This is **NOT** an exhaustive\ntextbook for observational seismology.\n\nThese notebooks cover:\n1. [Introduction to handling seismic data with ObsPy](1_Intro_to_processing.ipynb)\n2. [Fundamentals of Fourier analysis for Seismic data](2_Fourier_analysis.ipynb)\n3. [Earthquake detection and seismic phase identification](3_Earthquake_detection_and_phase_analysis.ipynb)\n4. [Earthquake location](4_Earthquake_Location.ipynb)\n5. [Magnitude and focal mechanisms](5_Magnitudes_and_focal_mechanisms.ipynb)\n6. [Earthquake statistics](6_Earthquake_statistics.ipynb)\n\n    \n## How to use these notebooks\n1. If using github:\n    - Fork this repository (forking will allow you to keep your local changes seperate from changes here);\n    - Clone (`git clone ...` where ... is the url given by the *big green button*) to your local machine\n1. If just downloading:\n    - Download using the *big green button*\n2. Change into the newly created directory;\n3. Install the requirements (recommended to use [conda](https://conda.io/projects/conda/en/latest/user-guide/install/index.html#id2)):\n    ```bash\n    conda env create -f environment.yml  # Create an environment called gphs445\n    source activate gphs445  # Activate the environment - You will need to do this everytime you use the notebooks\n    ```\n4. Start jupyter-lab (run `jupyter lab`) in your repository directory and navigate to the notebook you \n   want to work on;\n5. Save any changes you make in the notebook;\n6. When you want to back-up your changes, or when you are happy with them, commit the\n   changes and push to your upstream repository \n   (check out the [github cheatsheet](https://services.github.com/on-demand/downloads/github-git-cheat-sheet.pdf) for more commands):\n   ```bash\n   git add <notebook you have been working on> # Replace with the filename you were working on\n   git commit -m \"Some memorable commit message\"\n   git push origin master\n   ```\n",
        "createdAt": "2019-02-26T04:10:06.000Z",
        "updatedAt": "2025-07-07T03:02:58.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/calum-chamberlain/GPHS445_notebooks/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "arjundatta23/SWRT",
        "url": "https://github.com/arjundatta23/SWRT",
        "description": "This is a suite of seismological codes (in Python) for semi-analytic solutions of surface wave propagation across sharp lateral discontinuities in 2-D media.",
        "stars": 3,
        "forks": 6,
        "readme": "# SWRT\nSWRT: Surface Wave Refelction and Transmission. This is a suite of codes for semi-analytic solutions of seismic surface wave propgation across sharp lateral discontinuities in 2-D media.  It accompanies Datta (2018):\n\nDatta, A.: SWRT: A package for semi-analytical solutions of surface wave propagation, including mode conversion, across transversely aligned vertical discontinuities, Geosci. Instrum. Method. Data Syst., 7, 101-112, https://doi.org/10.5194/gi-7-101-2018, 2018.\n\nRefer to the above for abbreviations/acronyms and references used in this README.\n\n**********************************************************************************************\nA. PACKAGE CONTENTS AND OVERVIEW\n\nSWRT consists of four main programs, implementing the three algorithms discussed in Datta (2018):\n\n1. \"method\\_alsop/implemen\\t_alsop.py\" - implements the Alsop method (Love waves only)\n2. \"methods\\_BodynGreen/implement\\_bodymethod.py\" - implements the GregAl method (Love waves only)\n3. \"methods\\_BodynGreen/implement\\_green\\_lov.py\" - implements the Green's Function (GF) method for Love waves\n4. \"methods\\_BodynGreen/implement\\_green\\_ray.py\" - implements the GF method for Rayleigh waves\n\nAll programs rely on the 'SW1D_earthsr' set of modules, which is available as a separate respository:\nhttps://github.com/arjundatta23/SW1D_earthsr\n\n**********************************************************************************************\nB. BASIC CODE USAGE\n\nSWRT requires NumPy, SciPy and matplotlib (for visualization). All four programs can be run from the command line.\n\nApart from the command line arguments, the user is required to input the frequency range (lower and upper bounds of frequency in Hz) in which calculations are to be performed.\n\n=======\nSimple command line usage:\n\npython <code_name> <mod\\_file\\_1> <eigen\\_file\\_1> <mod\\_file\\_2> <eigen\\_file\\_2>\n=======\n<mod\\_file\\_1> and <mod\\_file\\_2> are ASCII text files containing the layered-Earth model on either side of the vertical interface in the 2-D medium, corresponding to the incidence side and transmission side media respectively.\n<eigen\\_file\\_1> and <eigen\\_file\\_2> are ASCII files containing the corresponding (local) Love or Rayleigh wave eigenfunctions on the two sides.\n\nIf you want to use the code as is, all input files are in 'earthsr' format (see example dir) and are therefore read by modules in 'SW1D_earthsr'.\n\nPrompted user input: frequency range (lower and upper bounds in Hz) in which calculations are to be performed.\n\nFor example, if we consider the example of propagation from left to right in the demo model in Datta (2018, Figure 1),\nto run \"implement\\_alsop\" on this model in the frequency range 0.01 - 0.1 Hz, you would do:\n\n############  \npython implement_alsop.py <mod\\_file\\_1> <eigen\\_file\\_1> <mod\\_file\\_2> <eigen\\_file\\_2>\n<SOME CODE OUTPUT>\nEnter frequency range: 0.01 0.1  \n############  \n\n**********************************************************************************************\nC. RUNNING THE EXAMPLE \"example\\_modL\\_rhslyr7\".\n\nSWRT contains an example which corresponds to Figures 1 and 2 of Datta (2018). For Love wave propagation in the forward direction, <eigen\\_file\\_1> and <eigen\\_file\\_2> are \"eigen.xdist.0.lov.gz\" and \"eigen.xdist.400.lov.gz\" respectively. The GAl and GF methods can be run on this example (for Love waves) with the following commands:\n\nFor forward direction (medium 1 -> medium 2):\n\n###############  \npython ../methods_BodynGreen/implement_bodymethod.py mod.xdist.0.lov eigen.xdist.0.lov.gz mod.xdist.400.lov eigen.xdist.400.lov.gz  \npython ../methods_BodynGreen/implement_green_lov.py mod.xdist.0.lov eigen.xdist.0.lov.gz mod.xdist.400.lov eigen.xdist.400.lov.gz  \n###############  \n\nFor backward direction (medium 1 <- medium 2):\n\n###############  \npython ../methods_BodynGreen/implement_bodymethod.py mod.xdist.400.lov eigen.xdist.400.lov.gz mod.xdist.0.lov eigen.xdist.0.lov.gz  \npython ../methods_BodynGreen/implement_green_lov.py mod.xdist.400.lov eigen.xdist.400.lov.gz mod.xdist.0.lov eigen.xdist.0.lov.gz  \n###############    \n\nAfter entering the frequency range, this should produce the results needed to reproduce Figure 2 of Datta 2018.  \nNB: To do the Rayleigh case, simply replace the Love wave eigenfunction files (\"eigen.xdist.0.lov.gz\", \"eigen.xdist.400.lov.gz\") with Rayleigh ones (\"eigen.xdist.0.ray.gz\", \"eigen.xdist.400.ray.gz\").\n\n**********************************************************************************************\nD. VISUALIATION/PLOTTING SCRIPT\n\nSWRT makes use of the Python pickle module to store the results of any run of a program as a \"pickle\" which can be loaded later for visualization etc. The script \"view_pickles.py\" is provided for this purpose. If the result of any of the main programs is stored as {pickle name}, figures such as those in Datta (2018) can be made using:\n\n############  \npython \"view_pickles.py\" {pickle name 1} {pickle name 2} .... upto any number of stored pickles.  \n############\n",
        "createdAt": "2017-07-28T19:05:13.000Z",
        "updatedAt": "2025-03-05T16:09:35.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/arjundatta23/SWRT/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "UHVZ-CentralAmerica/Results-Seismology",
        "url": "https://github.com/UHVZ-CentralAmerica/Results-Seismology",
        "description": "The seismology results of modeling UHVZs using ScS precursors.",
        "stars": 0,
        "forks": 0,
        "readme": "\nBecause of repository dependencies, to download this repository, do:\n```\n$ git clone --recursive https://github.com/UHVZ-CentralAmerica/Results.git\n```\n\nIf `--recursive` is not added, the dependencies in Codes will not be downloaded.\n\n\n\n1. \"BinStacks\"\n\n    The files in this folder are weighted bin stack signals.\n    For each bin, there are two files, representing the data stack and the synthetic stack when modeling against the bin's best-fit model.\n\n    Each file is 2-column time series text file.\n    The first column represent time relative to ScS peak, in second.\n    The second column represent the amplitude relative to original ScS peak (before subtract).\n\n    These files can be used to create Extended Data Fig. 6.\n\n2. \"TableData\"\n\n    Files in this folder are text files.\n    The data are pulled from calculations.\n\n3. \"Codes\"\n\n    Selected primary C++ codes used for the seismic analysis part.\n    Dependencies are: SAC, MariaDB, GMT, fftw3\n\n\nQuestions about these materials can be sent to shuleyu@asu.edu\n",
        "createdAt": "2020-06-18T17:02:35.000Z",
        "updatedAt": "2020-06-18T17:13:35.000Z",
        "language": "C++",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/UHVZ-CentralAmerica/Results-Seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "silvioschwarz/TerremotoPi",
        "url": "https://github.com/silvioschwarz/TerremotoPi",
        "description": "seismological station based on the Raspberry Pi with live stream capabiities",
        "stars": 1,
        "forks": 0,
        "readme": "# Terremoto Pi\n![eqd](/images/chrome-capture.gif)\n\n![eqd](/images/chrome-capture3.gif)\nDisclaimer:\nThis project was first brought to life during the course Digitalseismologie at University Potsdam in 2013. It may have some strong resemblances with [raspberry shake]( https://raspberryshake.org/) which is unintentional. \nI am merely one nerdy guy :neckbeard: doing diy stuff as a hobby and the raspberry shakers are a team with expertise in seismological device development for 10+ years.\nStill, I am by magnitudes more affordable :smirk:\n",
        "createdAt": "2017-09-06T11:47:53.000Z",
        "updatedAt": "2022-06-26T20:13:25.000Z",
        "language": "Jupyter Notebook",
        "homepage": "https://silvioschwarz.github.io/TerremotoPi/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/silvioschwarz/TerremotoPi/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "PKU-Neo/Introduction_to_Seismology",
        "url": "https://github.com/PKU-Neo/Introduction_to_Seismology",
        "description": "This repository stores notes for the general elective course \"Introduction to Seismology\" in summer semester, 2023. The notes include TeX documents and PDF documents, welcome to download on demand.",
        "stars": 1,
        "forks": 0,
        "readme": "# Introduction_to_Seismology\nThis repository stores notes for the general elective course \"Introduction to Seismology\" in summer semester, 2023. The notes include TeX documents and PDF documents, welcome to download on demand.\n\nThe course is taught by Prof. Kechang Zhao from the School of Earth and Space Sciences, Peking University. The notes are compiled from his lectures and PowerPoint presentations. He has been working to popularize the basics of seismology among university students through this course.\n",
        "createdAt": "2023-07-28T10:58:13.000Z",
        "updatedAt": "2025-02-22T08:55:32.000Z",
        "language": "TeX",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/PKU-Neo/Introduction_to_Seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "jbmuir/SeismoTeaching",
        "url": "https://github.com/jbmuir/SeismoTeaching",
        "description": "Repository for python scripts for teaching observational seismology",
        "stars": 1,
        "forks": 1,
        "readme": "# SeismoTeaching\n\nA respository of python stuff for teaching observational seismology, primarily through obspy\n",
        "createdAt": "2017-02-26T18:44:24.000Z",
        "updatedAt": "2025-10-15T09:36:26.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/jbmuir/SeismoTeaching/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "pyrocko/lightguide",
        "url": "https://github.com/pyrocko/lightguide",
        "description": "Tools for distributed acoustic sensing data.",
        "stars": 61,
        "forks": 13,
        "readme": "# Lightguide\n\n*Tools for distributed acoustic sensing and modelling.*\n\n[![PyPI](https://img.shields.io/pypi/v/lightguide)](https://pypi.org/project/lightguide/)\n[![build](https://github.com/pyrocko/lightguide/actions/workflows/build.yml/badge.svg)](https://github.com/pyrocko/lightguide/actions/workflows/build.yml)\n<a href=\"https://github.com/psf/black\"><img alt=\"Code style: black\" src=\"https://img.shields.io/badge/code%20style-black-000000.svg\"></a>\n[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://pre-commit.com/)\n\nLightguide is a package for handling, filtering and modelling distributed acoustic sensing (DAS) data. The package interfaces handling and processing routines of DAS data to the [Pyrocko framework](https://pyrocko.org). Through Pyrocko's I/O engine :rocket: lightguide supports handling the following DAS data formats:\n\n- MiniSEED\n- Silixa iDAS (TDMS data)\n- ASN OptoDAS\n\nNumerical forward modelling of various dislocation sources in layered and homogeneous half-space towards DAS strain and strain-rate is employed through Pyrocko-Green's function package.\n\n> The framework is still in Beta. Expect changes throughout all functions.\n\n## Installation\n\nInstall the compiled Python wheels from PyPI:\n\n```sh\npip install lightguide\n```\n\n## Usage\n\n### Documentation\n\nFind the [documentation here](https://pyrocko.github.io/lightguide/).\n\n### Adaptive frequency filter\n\nThe adaptive frequency filter (AFK) can be used to suppress incoherent noise in DAS data sets.\n\n```py\nfrom lightguide.blast import Blast\n\nblast = Blast.from_miniseed(\"my-data.mseed\")\nblast.lowpass(corner_freq=60.0)\nblast.afk_filter(exponent=0.8)\n```\n\n\nThe filtering performance of the AFK filter, applied to an earthquake recording at an [ICDP](https://www.icdp-online.org/home/) borehole observatory in Germany. The data was recorded on a [Silixa](https://silixa.com/) iDAS v2. For more details see <https://doi.org/10.5880/GFZ.2.1.2022.006>.\n\n![AFK Filter Performance](https://user-images.githubusercontent.com/4992805/170084970-9484afe7-9b95-45a0-ac8e-aec56ddfb3ea.png)\n\n*The figures show the performance of the AFK filter applied to noisy DAS data. (a) Raw data. (b) The filtered wave field using the AFK filter with exponent = 0.6, 0.8, 1.0, 32 x 32 sample window size and 15 samples overlap. (c) The normalized residual between raw and filtered data. (d) Normalized raw (black) waveform and waveforms filtered (colored) by different filter exponents, the shaded area marks the signal duration. (e) Power spectra of signal shown in (d; shaded duration), the green area covers the noise band used for estimating the reduction in spectral amplitude in dB. The data are neither tapered nor band-pass filtered, the images in (a-c) are not anti-aliased.*\n\n## Citation\n\nLightguide can be cited as:\n\n> Marius Paul Isken, Sebastian Heimann, Christopher Wollin, Hannes Bathke, & Torsten Dahm. (2022). Lightguide - Seismological Tools for DAS data. Zenodo. <https://doi.org/10.5281/zenodo.6580579>\n\n[![DOI](https://zenodo.org/badge/495774991.svg)](https://zenodo.org/badge/latestdoi/495774991)\n\nDetails of the adaptive frequency filter are published here:\n\n> Marius Paul Isken, Hannes Vasyura-Bathke, Torsten Dahm, Sebastian Heimann, De-noising distributed acoustic sensing data using an adaptive frequency-wavenumber filter, Geophysical Journal International, 2022;, ggac229, <https://doi.org/10.1093/gji/ggac229>\n\n[![DOI](https://img.shields.io/badge/DOI-10.1093%2Fgji%2Fggac229-blue)](https://doi.org/10.1093/gji/ggac229)\n\n## Packaging\n\nTo package lightguit requires Rust and the maturin build tool. maturin can be installed from PyPI or packaged as well. This is the simplest and recommended way of installing from source:\n\n```sh\n# Install rust\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n\n# Install maturin and build\npip install maturin\nmaturin build\n```\n\n### Development\n\nLocal development through pip or maturin.\n\n```sh\ncd lightguide\npip3 install .[dev]\n```\n\nor\n\n```sh\ncd lightguide\nmaturin develop\n```\n\nThe project utilizes pre-commit for clean commits, install the hooks via:\n\n```sh\npip install pre-commit\npre-commit install\n```\n\n## License\n\nContribution and merge requests by the community are welcome!\n\nLightguide was written by Marius Paul Isken and is licensed under the GNU GENERAL PUBLIC LICENSE v3.\n",
        "createdAt": "2022-05-24T10:29:43.000Z",
        "updatedAt": "2025-11-19T07:36:06.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/pyrocko/lightguide/main/README.md",
        "mainPaper": {
            "doi": "",
            "title": "Lightguide",
            "dateReleased": "2022-07-01T00:00:00.000Z"
        },
        "repoDoi": "",
        "publications": [
            {
                "doi": "",
                "name": "Lightguide",
                "source": "",
                "authorNames": [],
                "publicationDate": "2022-07-01T00:00:00.000Z"
            }
        ]
    },
    {
        "source": "GitHub",
        "name": "JUNZHU-SEIS/latex_journal",
        "url": "https://github.com/JUNZHU-SEIS/latex_journal",
        "description": "Latex code for seismological journal",
        "stars": 1,
        "forks": 0,
        "readme": "",
        "createdAt": "2024-01-14T12:04:22.000Z",
        "updatedAt": "2025-07-26T20:55:33.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "srkim00/seismology_web",
        "url": "https://github.com/srkim00/seismology_web",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# [Hugo Research Group Theme](https://github.com/wowchemy/starter-hugo-research-group)\n\n[![Screenshot](./preview.png)](https://wowchemy.com/hugo-themes/)\n\nThe **Research Group Template** empowers your research group to easily create a beautiful website with a stunning homepage, news, academic publications, events, team profiles, and a contact form.\n\n️**Trusted by 250,000+ researchers, educators, and students.** Highly customizable via the integrated **no-code, widget-based Wowchemy page builder**, making every site truly personalized ⭐⭐⭐⭐⭐\n\n[![Get Started](https://img.shields.io/badge/-Get%20started-ff4655?style=for-the-badge)](https://wowchemy.com/hugo-themes/)\n[![Discord](https://img.shields.io/discord/722225264733716590?style=for-the-badge)](https://discord.com/channels/722225264733716590/742892432458252370/742895548159492138)  \n[![Twitter Follow](https://img.shields.io/twitter/follow/wowchemy?label=Follow%20on%20Twitter)](https://twitter.com/wowchemy)\n\nEasily write technical content with plain text Markdown, LaTeX math, diagrams, RMarkdown, or Jupyter, and import publications from BibTeX.\n\n[Check out the latest demo](https://research-group.netlify.app/) of what you'll get in less than 60 seconds, or [view the showcase](https://wowchemy.com/creators/).\n\nThe integrated [**Wowchemy**](https://wowchemy.com) website builder and CMS makes it easy to create a beautiful website for free. Edit your site in the CMS (or your favorite editor), generate it with [Hugo](https://github.com/gohugoio/hugo), and deploy with GitHub or Netlify. Customize anything on your site with widgets, light/dark themes, and language packs.\n\n- 👉 [**Get Started**](https://wowchemy.com/hugo-themes/)\n- 📚 [View the **documentation**](https://wowchemy.com/docs/)\n- 💬 [Chat with the **Wowchemy research community**](https://discord.gg/z8wNYzb) or [**Hugo community**](https://discourse.gohugo.io)\n- ⬇️ **Automatically import citations from BibTeX** with the [Hugo Academic CLI](https://github.com/wowchemy/hugo-academic-cli)\n- 🐦 Share your new site with the community: [@wowchemy](https://twitter.com/wowchemy) [@GeorgeCushen](https://twitter.com/GeorgeCushen) [#MadeWithWowchemy](https://twitter.com/search?q=%23MadeWithWowchemy&src=typed_query)\n- 🗳 [Take the survey and help us improve #OpenSource](https://forms.gle/NioD9VhUg7PNmdCAA)\n- 🚀 [Contribute improvements](https://github.com/wowchemy/wowchemy-hugo-themes/blob/main/.github/contributing.md) or [suggest improvements](https://github.com/wowchemy/wowchemy-hugo-themes/issues)\n- ⬆️ **Updating?** View the [Update Guide](https://wowchemy.com/docs/hugo-tutorials/update/) and [Release Notes](https://github.com/wowchemy/wowchemy-hugo-themes/releases)\n\n## We ask you, humbly, to support this open source movement\n\nToday we ask you to defend the open source independence of the Wowchemy website builder and themes 🐧\n\nWe're an open source movement that depends on your support to stay online and thriving, but 99.9% of our creators don't give; they simply look the other way.\n\n### [❤️ Click here to become a GitHub Sponsor, unlocking awesome perks such as _exclusive academic templates and widgets_](https://github.com/sponsors/gcushen)\n\n## Demo credits\n\nPlease replace the demo images with your own.\n\n- [Female scientist](https://unsplash.com/photos/uVnRa6mOLOM)\n- [2 Coders](https://unsplash.com/photos/kwzWjTnDPLk)\n- [Cafe](https://unsplash.com/photos/RnDGGnMEOao)\n- Blog posts\n  - https://unsplash.com/photos/AndE50aaHn4\n  - https://unsplash.com/photos/OYzbqk2y26c\n- Avatars\n  - https://unsplash.com/photos/5yENNRbbat4\n  - https://unsplash.com/photos/WNoLnJo7tS8\n",
        "createdAt": "2022-08-09T06:56:26.000Z",
        "updatedAt": "2022-08-09T06:57:29.000Z",
        "language": "TeX",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/srkim00/seismology_web/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "chenyk1990/MATseisdl",
        "url": "https://github.com/chenyk1990/MATseisdl",
        "description": "A Matlab package for dictionary learning applications in seismology",
        "stars": 10,
        "forks": 1,
        "readme": "**MATseisdl**\n======\n\n## Description\n\n**MATseisdl** is a Matlab package for different dictionary learning methods and their applications in seismology. DL has a variety of applications in seismology, including but not limited to seismic denoising, seismic reconstruction, seismic diffraction separation, constrained LSRTM, constrained FWI, etc.\n\n## Reference\n    Chen, Y. (2020). Fast dictionary learning for noise attenuation of multidimensional seismic data. Geophysical Journal International, 222(3), 1717-1727.\n    \n    Wang, H., Chen, W., Zhang, Q., Liu, X., Zu, S., & Chen, Y. (2020). Fast dictionary learning for high-dimensional seismic reconstruction. IEEE Transactions on Geoscience and Remote Sensing, 59(8), 7098-7108.\n\n    Chen, Y., Savvaidis, A., and Fomel, S., (2023). Dictionary learning for single-channel passive seismic denoising, Seismological Research Letters, 94, 2840-2851.\n\t\nBibTeX:\n\n\t@article{chen2020sgk,\n\t  title={Fast dictionary learning for noise attenuation of multidimensional seismic data},\n\t  author={Yangkang Chen},\n\t  journal={Geophysical Journal International},\n\t  volume={222},\n\t  number={3},\n\t  issue={3},\n\t  pages={1717-1727},\n\t  year={2020}\n\t}\n\n\t@article{wang2021sgk,\n\t  title={Fast dictionary learning for high-dimensional seismic reconstruction},\n\t  author={Hang Wang and Wei Chen and Quan Zhang and Xingye Liu and Shaohuan Zu and Yangkang Chen},\n\t  journal={IEEE Transactions on Geoscience and Remote Sensing},\n\t  volume={59},\n\t  number={8},\n\t  issue={8},\n\t  pages={7098-7108},\n\t  doi={10.1109/TGRS.2020.3030740},\n\t  year={2021}\n\t}\n\n\t@article{dl1dnoise,\n\t  author={Yangkang Chen and Alexandros Savvaidis and Sergey Fomel},\n\t  title = {Dictionary learning for single-channel passive seismic denoising},\n\t  journal={TBD},\n\t  year=2023,\n\t  volume={94},\n\t  number={6},\n\t  issue={6},\n\t  pages={2840-2851},\n\t  doi={10.1785/0220230169},\n\t}\n-----------\n## Copyright\n\tThe MATseisdl developing team, 2021-present\n-----------\n\n## License\n    MIT License  \n\n-----------\n\n## Install\nUsing the latest version\n\n    git clone https://github.com/chenyk1990/MATseisdl\n    cd MATseisdl\n    addpath(genpath('./')); #in Matlab command line\n\n-----------\n## Examples\n    The \"demo\" directory contains all runnable scripts to demonstrate different applications of MATseisdl. \n\n-----------\n## Gallery\nThe gallery figures of the pydrr package can be found at\n    https://github.com/chenyk1990/gallery/tree/main/matseisdl\nEach figure in the gallery directory corresponds to a DEMO script in the \"demo\" directory with the same file name.\n\n-----------\n## Dependence Packages\n* Matlab 2015 and later versions\n\n-----------\n## Development\n    The development team welcomes voluntary contributions from any open-source enthusiast. \n    If you want to contribute to this project, feel free to contact the development team. \n\n-----------\n## Contact\n    Regarding any questions, bugs, developments, or collaborations, please contact  \n    Yangkang Chen\n    chenyk2016@gmail.com\n\n-----------\n\n\n",
        "createdAt": "2022-08-04T00:32:48.000Z",
        "updatedAt": "2025-10-20T08:43:46.000Z",
        "language": "MATLAB",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/chenyk1990/MATseisdl/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "SITANSHUK182/seismology-",
        "url": "https://github.com/SITANSHUK182/seismology-",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2025-02-26T10:34:30.000Z",
        "updatedAt": "2025-03-05T09:59:58.000Z",
        "language": "C++",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ieferreira/Sismologia",
        "url": "https://github.com/ieferreira/Sismologia",
        "description": "Seismology, visualization of important earthquakes (3.5 Mw or above) in Colombia",
        "stars": 3,
        "forks": 1,
        "readme": "# Sismos de Colombia (Earthquakes from Colombia)\n**Visualización Sismos en Colombia usando cartopy y matplotlib (ES)**\n\n\nSe utiliza Cartopy y Matplotlib, se adjunta además los datos usados en la tabla de excel, a menos que se especifique, esta debe estar en la \nmisma carpeta que el codigo (.py). Se usa el reporte763.xlsx generado en la página de la Red Sismológica Nacional. Datos desde Agosto de 2011 a Febrero de 2018, fecha en la cual se paso de usar Seisan a usar Seiscomp3 para análisis de sismos en el servicio geológico, por lo cual el formato de datos cambió.\n\n**3D Visualization Earthquakes in Colombia, using python (EN)**\n\nCartopy and matplotlib are employed to visualize seismological data from the Colombian _Red Sismológica Nacional (RSN)_ (National Seismological Network) the data are recorded in the reporte763.xlsx file which was downloaded directly from their page (https://www2.sgc.gov.co/sismos/Paginas/default.aspx, as of 9/9/2020 unavailable). The data spans mainly from August 2011 to February 2018, on this date the RSN changed its seismological analysis software from Seisan to Seiscomp3.\n\nThis program is useful for visualizing the Seismic distribution in Colombia, you can see the Subduction Zone, the Caldas Tear (Vargas and Mann, 2013), the Bucaramanga Nest and the induced seismicity associated with oil extraction in Puerto Gaitán (cities of importance are plotted on the surface, being the Capital Bogotá, Los Santos where the Bucaramanga Nest is located and Quibdó, under which another seismic nest is located). \n\n![animacionVisualizacion](demoVisSismos.gif)\n",
        "createdAt": "2019-12-23T12:15:43.000Z",
        "updatedAt": "2021-06-25T06:07:56.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ieferreira/Sismologia/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "sean0921/sac_pkgbuild",
        "url": "https://github.com/sean0921/sac_pkgbuild",
        "description": "ArchLinux's PKGBUILD file for Requested Seismic Analysis Code source code by IRIS",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2020-07-04T20:19:52.000Z",
        "updatedAt": "2022-05-21T17:37:19.000Z",
        "language": "Shell",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "geozor-network/kiruna-blasting",
        "url": "https://github.com/geozor-network/kiruna-blasting",
        "description": "Seismography experiment - underground iron mine blasting detection by smartphones",
        "stars": 2,
        "forks": 0,
        "readme": "# Kiruna iron ore mine blasting location experiment\n\nSeismography experiment - underground iron mine blasting detection by smartphones. \n\nEvery night in Kiruna iron ore mine the tons of explosives are blasted betwen 1:15 and 1:40 local time. The most usual time for blasting is 1:30. \nIf you stand near to [Kiruna LKAB mine](https://en.wikipedia.org/wiki/Kiruna_mine) in that time you definitely feel the artificial earthquake generadet by explosion around one kilometer bellow surface. \n\nYou can hear the sound recording of the blasting by playing one of [these files](https://github.com/geozor-network/kiruna-blasting/tree/master/data/16092019/kakl). (But you definitely need a subwoofer to hear something) \n\nThe movements are so intensive that they could be measured by generic smartphone accelerometer sensor.\n\n\n![Blasting induced earthquake in Kiruna iron mine](/doc/img/seismograph.png \"Seismogram plotted from accelerometer sensor\")\n\nAs you can see from the plotted data above, explosion generated shaking is clearly visible. \n\nFor the data recording the AndroSensor application was used. The sampling period was configured to 20 ms.\n\nThe most challenging issue in the experiment is precise time synchronization between the smart-phones even in case of GPS fix (Android is unable to automatically correct system time). Situation is documented in following photo. \n\n\n![Time inconsistency of the phones](/data/16092019/Phone_time_unsync.JPG \"Different time at each phone used\")\n\n\nFull sound recording has around 1GB of data, which is out of github file size limit. Therefore are available upon request from [kaklik](https://github.com/kaklik). \n",
        "createdAt": "2019-09-17T09:53:00.000Z",
        "updatedAt": "2024-05-17T10:33:34.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/geozor-network/kiruna-blasting/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "aaspip/pyekfmm",
        "url": "https://github.com/aaspip/pyekfmm",
        "description": "A python package for 3D fast-marching-based traveltime calculation and its applications in seismology",
        "stars": 64,
        "forks": 12,
        "readme": "**Pyekfmm**\n======\n\n## Description\n\n**Pyekfmm** is python package for 3D fast-marching-based traveltime calculation and its applications in seismology. The initial version of this package was held at https://github.com/chenyk1990/pyekfmm, which is no longer maintained.\n\n\n## Reference\n    Chen Y., Chen, Y.F., Fomel, S., Savvaidis, A., Saad, O.M., Oboue, Y.A.S.I. (2023). A python package for 3D fast-marching-based traveltime calculation and its applications in seismology, Seismological Research Letters, 94, 2050-2059.\n    \nBibTeX:\n\n\t@article{pyekfmm,\n\t  title={Pyekfmm: a python package for 3D fast-marching-based traveltime calculation and its applications in seismology},\n\t  author={Yangkang Chen and Yunfeng Chen and Sergey Fomel and Alexandros Savvaidis and Omar M. Saad and Yapo Abol\\'{e} Serge Innocent Obou\\'{e}},\n\t  journal={Seismological Research Letters},\n\t  volume={94},\n\t  number={1},\n\t  issue={1},\n\t  pages={2050-2059},\n\t  year={2023}\n \t}\n\n-----------\n## Copyright\n    pyekfmm developing team, 2021-present\n\n-----------\n## License\n    MIT License \n\n-----------\n## Install\nUsing the latest version\n\n    git clone https://github.com/aaspip/pyekfmm\n    cd pyekfmm\n    pip install -v -e .\n\nor using Pypi\n\n    pip install pyekfmm\n\nor (recommended, because we update very fast)\n\n\tpip install git+https://github.com/aaspip/pyekfmm\n\n\n-----------\n## Verified runnable OS\nMac OS, Linux, Windows (need Microsoft C++ Build Tools)\n\n\n-----------\n## Examples\n    The \"demo\" directory contains all runable scripts to demonstrate different applications of pyekfmm. \n\n-----------\n## Notebook tutorials\nSome notebook tutorials are stored separately to ensure the minimal size of the pyekfmm package. They can be found at \n\n\thttps://github.com/aaspip/notebook/blob/main/pyekfmm\n\n-----------\n## Dependence Packages\n* scipy \n* numpy \n* matplotlib\n\n-----------\n## Development\n    The development team welcomes voluntary contributions from any open-source enthusiast. \n    If you want to make contribution to this project, feel free to contact the development team. \n\n-----------\n## Contact\n    Regarding any questions, bugs, developments, collaborations, please contact  \n    Yangkang Chen\n    chenyk2016@gmail.com\n\n-----------\n## Gallery\nThe gallery figures of the pyekfmm package can be found at\n    https://github.com/aaspip/gallery/tree/main/pyekfmm\nEach figure in the gallery directory corresponds to a DEMO script in the \"demo\" directory with the exactly the same file name. These gallery figures are also presented below. \n\nDEMO1 \nThe following figure shows an example of traveltime calculation for 2D isotropic media (a) and anisotropic media (b). Generated by [demos/test_pyekfmm_fig1.py](https://github.com/aaspip/pyekfmm/blob/main/demos/test_pyekfmm_fig1.py)\n<img src='https://github.com/aaspip/gallery/blob/main/pyekfmm/test_pyekfmm_fig1.png' alt='DEMO1' width=960/>\n\nDEMO2\nThe following figure shows an example of traveltime calculation for 3D isotropic media (a) and anisotropic media (b). Generated by [demos/test_pyekfmm_fig2.py](https://github.com/aaspip/pyekfmm/blob/main/demos/test_pyekfmm_fig2.py)\n<img src='https://github.com/aaspip/gallery/blob/main/pyekfmm/test_pyekfmm_fig2.png' alt='DEMO2' width=960/>\n\nDEMO3 \nThe following figure shows an example of traveltime calculation for 2D heterogeneous isotropic and anisotropic media. (a) Vertical velocity model. (b) Horizontal velocity model. (c) Anisotropic parameter η model. (d) Traveltime table in isotropic media. (e) Traveltime table in anisotropic media. Generated by [demos/test_pyekfmm_fig3.py](https://github.com/aaspip/pyekfmm/blob/main/demos/test_pyekfmm_fig3.py)\n<img src='https://github.com/aaspip/gallery/blob/main/pyekfmm/test_pyekfmm_fig3.png' alt='DEMO3' width=960/>\n\nDEMO4 \nThe following figure shows an ray tracing example in 2D (a) and 3D (b) media with vertically increasing velocities. Generated by [demos/test_pyekfmm_fig4.py](https://github.com/aaspip/pyekfmm/blob/main/demos/test_pyekfmm_fig4.py)\n<img src='https://github.com/aaspip/gallery/blob/main/pyekfmm/test_pyekfmm_fig4.png' alt='DEMO4' width=960/>\n\nDEMO5 \nThe following figure shows an example of traveltime calculation for the global earth. Generated by [demos/test_pyekfmm_fig5.py](https://github.com/aaspip/pyekfmm/blob/main/demos/test_pyekfmm_fig5.py)\n<img src='https://github.com/aaspip/gallery/blob/main/pyekfmm/test_pyekfmm_fig5.png' alt='DEMO5' width=960/>\n\nDEMO6\nThe following figure shows a location example and comparison with the NonLinLoc (NLL) result. \n<img src='https://github.com/aaspip/gallery/blob/main/pyekfmm/test_pyekfmm_fig6.png' alt='DEMO6' width=960/>\n\nDEMO7\nThe following figure shows a relocation example of the Spanish Springs, Nevada earthquake sequence. \n<img src='https://github.com/aaspip/gallery/blob/main/pyekfmm/test_pyekfmm_fig7.png' alt='DEMO7' width=960/>\n\nDEMO8\nThe following figure shows a surface-wave tomography test. (a) Ray path between a pair of virtual source (red star) and station (blue triangle). The background is the 5 sec group velocities of the Australian continent from ambient noise imaging. (b) Travel time field. (c) Ray paths of all 25,899 pairs. (d)-(f) The same as (a)-(c) but for the initial model with a constant velocity.\n<img src='https://github.com/aaspip/gallery/blob/main/pyekfmm/test_pyekfmm_fig8.png' alt='DEMO8' width=960/>\n\nDEMO9\nThe following figure shows the traveltime misfit in the surface-wave tomography test. (a) Group velocities inverted from the travel time residuals using the kernel constructed from the initial model. (b) Travel time misfits estimated from the initial and final models.\n<img src='https://github.com/aaspip/gallery/blob/main/pyekfmm/test_pyekfmm_fig9.png' alt='DEMO9' width=960/>\n\n# Below are new examples in addition to the results in the original paper\nDEM10\nThe following figure shows an example of traveltime calculation of two shots for 3D isotropic media\n Generated by [demos/test_pyekfmm_fig2-multishots.py](https://github.com/aaspip/pyekfmm/blob/main/demos/test_pyekfmm_fig2.py)\n<img src='https://github.com/aaspip/gallery/blob/main/pyekfmm/test_pyekfmm_fig2-multishots.png' alt='DEMO2' width=960/>\n\nDEM11\nThe following figure shows an example of traveltime calculation comparison between Pyekfmm and skfmm (scikit-fmm)\n Generated by [demos/test_pyekfmm_fig1.py](https://github.com/aaspip/pyekfmm/blob/main/demos/test_pyekfmm_fig1.py)\n<img src='https://github.com/aaspip/gallery/blob/main/pyekfmm/test_pyekfmm_fig1-comp.png' alt='DEMO2' width=960/>\n\nThe following figure shows an example of traveltime calculation comparison between Pyekfmm and pykonal (if pykonal is installed)\n Generated by [demos/test_pyekfmm_fig1.py](https://github.com/aaspip/pyekfmm/blob/main/demos/test_pyekfmm_fig1.py)\n<img src='https://github.com/aaspip/gallery/blob/main/pyekfmm/test_pyekfmm_fig1-comp2.png' alt='DEMO2' width=960/>\n\n\nDEM12\nThe following figure shows an example of computing traveltime, takeoff angle (dip and azimuth)\n Generated by [demos/test_pyekfmm_takeoff_dip_and_azim.py](https://github.com/aaspip/pyekfmm/blob/main/demos/test_pyekfmm_takeoff_dip_and_azim.py)\n<img src='https://github.com/aaspip/gallery/blob/main/pyekfmm/test_pyekfmm_takeoff_dip_and_azim-middle.png' alt='DEMO2' width=960/>\n\nThe following figure shows a slightly changed example of computing traveltime, takeoff angle (dip and azimuth) (from a source at the corner)\n Generated by [demos/test_pyekfmm_takeoff_dip_and_azim.py](https://github.com/aaspip/pyekfmm/blob/main/demos/test_pyekfmm_takeoff_dip_and_azim.py)\n<img src='https://github.com/aaspip/gallery/blob/main/pyekfmm/test_pyekfmm_takeoff_dip_and_azim-corner.png' alt='DEMO2' width=960/>\n\n\nDEM13\nThe following figure shows an example of computing rays in models with different sizes\n Generated by [demos/test_pyekfmm_raytracing3d.py](https://github.com/aaspip/pyekfmm/blob/main/demos/test_pyekfmm_raytracing3d.py)\n<img src='https://github.com/aaspip/gallery/blob/main/pyekfmm/test_pyekfmm_raytracing3d.png' alt='DEMO2' width=960/>\n\nDEM14\nThe following figure shows an example of computing reciprocal rays \n Generated by [demos/test_pyekfmm_raytracing3d_reciprocal.py](https://github.com/aaspip/pyekfmm/blob/main/demos/test_pyekfmm_raytracing3d_reciprocal.py)\n<img src='https://github.com/aaspip/gallery/blob/main/pyekfmm/test_pyekfmm_raytracing3d_reciprocal.png ' alt='DEMO2' width=960/>\n\nDEM15\nThe following figure shows an example of benchmarking Pyekfmm with bh_tomo package \n Generated by [demos/test_pyekfmm_raytracing2d_benchmarkWITHbhtomo.py](https://github.com/aaspip/pyekfmm/blob/main/demos/test_pyekfmm_raytracing2d_benchmarkWITHbhtomo.py)\n<img src='https://github.com/aaspip/gallery/blob/main/pyekfmm/test_pyekfmm_raytracing2d_benchmarkWITHbhtomo.png ' alt='DEMO2' width=960/>\n\n",
        "createdAt": "2023-02-14T21:20:22.000Z",
        "updatedAt": "2025-12-03T10:30:55.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/aaspip/pyekfmm/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "meowoodie/Compressed-Sensing-Delay-Estimation",
        "url": "https://github.com/meowoodie/Compressed-Sensing-Delay-Estimation",
        "description": "Proposed to develop a low-communication cost cross-correlation method with the idea of Compressed Sensing",
        "stars": 6,
        "forks": 4,
        "readme": "Delay Estimation with Compressed Sensing\n===\n\n### Test Results on Utah Dataset\n\nWe test our algorithm on [Utah dataset](http://home.chpc.utah.edu/~u0992976/Socal/). It contains 3 stations with good quality data, 3 stations with bad quality data, and a center station `001`. We compared the results of compressed sensing method with conventional cross-correlation method between center station and good/bad stations. \n\nAt first, we stacked data in one month (week) into 5 mins window.\n- Stack data in one month (week) into only one day, and average the stacked data bit by bit. \n- Stack averaged one day data into a 5 minutes window.\n\nYou can reach the result by visiting [Utah result]()\n\n",
        "createdAt": "2016-12-10T04:47:10.000Z",
        "updatedAt": "2023-05-15T06:06:43.000Z",
        "language": "Matlab",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/meowoodie/Compressed-Sensing-Delay-Estimation/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "wanghaoyucn/geophy-diy",
        "url": "https://github.com/wanghaoyucn/geophy-diy",
        "description": "Introduction and tips for geophysics research skills and knowledge (especially for seismology)",
        "stars": 2,
        "forks": 0,
        "readme": "# geophy-diy\n\nIntroduction and tips for geophysics research skills and knowledge (especially for seismology)\n",
        "createdAt": "2024-01-19T20:30:05.000Z",
        "updatedAt": "2024-07-18T05:52:07.000Z",
        "language": "HTML",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/wanghaoyucn/geophy-diy/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Domiko7/seismology-tools",
        "url": "https://github.com/Domiko7/seismology-tools",
        "description": "Some useful seismological tools mostly made in python",
        "stars": 0,
        "forks": 0,
        "readme": "# seismology-tools\nSome useful seismological tools mostly made in python\n",
        "createdAt": "2025-08-21T19:54:56.000Z",
        "updatedAt": "2025-08-21T19:55:00.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Domiko7/seismology-tools/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "farhanhamidlubis/computationalseismology",
        "url": "https://github.com/farhanhamidlubis/computationalseismology",
        "description": "Shared repository for Computational Seismology course at Bandung Institute of Technology",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2020-10-14T16:45:01.000Z",
        "updatedAt": "2022-09-12T02:16:41.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "nicholasfraser/Seismology",
        "url": "https://github.com/nicholasfraser/Seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# Seismology\n",
        "createdAt": "2022-09-09T06:52:44.000Z",
        "updatedAt": "2022-09-09T07:13:51.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/nicholasfraser/Seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ghfbsd/iasp",
        "url": "https://github.com/ghfbsd/iasp",
        "description": "Buland-Kennett tau-p travel time routines",
        "stars": 2,
        "forks": 1,
        "readme": "# Buland and Kennett Tau-P\n\nThe Buland and Kennett tau-p routines are a suite of programs that calculate\nseismic wave travel times through a radial earth model specified as velocity\nas a function of depth.\nThey use the methods described in Buland and Chapman (1983), BSSA 73, 1271-1302\nto calculate tables of tau to quickly determine all arrivals at a particular\nrange for an earthquake at a particular depth.\nThe original package provided a command line interface to the tables, and\ntables for one model, *iasp91* by Kennett and Engdahl (1991), GJI 105,\n429-465.\n\nThe package is useful and versatile, but lacked tables for other wavespeed\nmodels and a simple, well-documented subroutine interface to the tables so\nthat programs could take advantage of the rapid calculations the method\naffords.\nConsequently, I extended the package to include other earth models,\ndefined a simple subroutine interface to it, and built a configuration-based\ncompilation method that makes the program, subroutines and tables easy to\nbuild and install.\n\nThe general changes are:\n\n* do automatic source configuration;\n* remove Fortran language constructs that prevent it from compiling\n   on the GCC Fortran compilers;\n* to write table formats that are self describing (in size) and do\n   not require recompilation of previously-built travel time tables if\n   the table storage size changes;\n* adds additional phases to the set that is computed in the tables,\n   viz. depth phases for ScS and PcP and diffracted phases for PKP branches;\n* adds a subroutine-callable interface to the tables;\n* adds the capability to define models for planets lacking an inner core;\n* adds tables for different models:\n   ek137, iasp91, ak135, fakeprem, fakeprem20s, sp6, kghj, pemc (Earth), and\n   ss97a and r11dw (Mars);\n* adds a routine to extract slowness-range-travel time for each phase;\n* optionally, under configuration control, adds more depth phases\n   and core phases.\n\n\nTo configure and install:\n\n```\n./configure\nmake\nmake install\nmake clean\n```\n\nThis will build a default set of travel time tables for all models.  To build\na smaller set of tables by eliminating the depth phase arrivals for core\nreflections and multiple core phases, use the `--disable-depth` option:\n```\n./configure --disable-depth\n```\nTo enable generation of multiple core bounce phases (PmKP m=3,4 and SmKS\nm=3,4,5), configure with `--enable-core` option:\n```\n./configure --enable-core\n```\nThis can be used in conjunction with `--disable-depth`.\n\nDefault install location for travel time tables and linkable subroutine code\nis `/usr/local/lib`, and for the ttimes program is `/usr/local/bin`.  See\n```\n./configure --help=short\n```\nfor directions explaining how to change this.\n\n```\n                                 TAU\n   \n   The files in this anonymous FTP implement the travel time computation\nalgorithm described by Buland and Chapman (1983) in \"The Computation of\nSeismic Travel Times\", BSSA, v. 73, pp. 1271-1302, for the IASPEI phase\nset derived from the IASPEI travel-time model developed by Brian Kennett.\nThis phase set currently includes:  P, Pdiff, PKP, PKiKP, pP, pPdiff,\npPKP, pPKiKP, sP, sPdiff, sPKP, sPKiKP, PP, P'P', S, Sdiff, SKS, pS,\npSdiff, pSKS, sS, sSdiff, sSKS, SS, S'S', PS, PKS, SP, SKP, SKiKP, PcP,\nPcS, ScP, ScS, PKKP, PKKS, SKKP, and SKKS.  The IASPEI model was\ndeveloped specifically as a replacement for the Jeffreys-Bullen model for\nearthquake location work.  It incorporates a PEM core, a lower mantle \ndeveloped by Ken Toy, and an upper mantle constructed by Brian Kennett.\nThe phase set was chosen by requiring that the phases be well observed in\nsome distance range and that they be useful either for earthquake\nlocation or for studies of earth structure.\n\n   This implementation of the travel-time algorithm includes all\nrefracted arrivals of each phase listed, but only the partial\nreflections specifically noted in the phase list.  All total internal\nreflections (except at the free surface) have been supressed as they are\nalmost always tertiary arrivals.  This policy does result in no PS and\nSP arrivals in the distance range of about 25-50 degrees.  Phase naming\nis derived directly from the model layer in which the phase turns and is\nsometimes at odds with various seismological conventions.  In particular,\nthe lack of a low velocity zone in the upper mantle of the IASPEI model\nresults in Pn and Sn extending out to about 25 degrees.  Even without the\ntotal reflections, the upper mantle discontinuities and triplications\nwill typically result in two to four P and S arrivals each in the\ndistance range of about 17-30 degrees.\n\n   The tables are generated by a two step process which involves running\nfirst program REMODL and then program SETBRN.  REMODL generates the\nmodel by calling routines in EMIASP91.F.  In principle any model may be\nused.  However, a range of IASPEI models has actually been tested.\nREMODL examines the model and its implications, makes up a best guess at\nhow to sample slowness, and performs all of the raw integrals.  It writes\nits results into the REMODL.HED and REMODL.TBL files.  REMODL1.LIS and\nREMODL2.LIS show some of what goes on inside of REMODL and are useful\nwhen REMODL is being converted to a different computer.  SETBRN takes the\nresults of REMODL from (REMODL.HED AND REMODL.TBL) and constructs the\nspecific branches desired by the user.  While some attempt has been\nmade to make SETBRN ``programmable'', care must be taken that phases\nadded are within the assumptions made by the interpolation routines in\nLIBTAU.F.  SETBRN produces the tau-p tables IASP91.HED AND IASP91.TBL.\nThe access to these tables has been cleaned up so that they are quite\nmachine independent.  SETBRN also produces internal listing files\nSETBRN1.LIS, SETBRN2.LIS, and SETBRN3.LIS.  REMODL and SETBRN (and the\nroutines they call) use the include file LIMITS.INC.  It is assumed that\nthe include file is in the directory in which compilation is being done.\nThese programs are linked with the library LIBTAU.A (which should be\nconstructed from the source in LIBTAU.F).  REMODL and SETBRN also require\nmachine dependent routines from either LIBVAX.A or LIBSUN.A (which should\nbe constructed from the source in LIBVAX.F or LIBSUN.F) depending on the\ncomputer being used.  Note that REMODL and SETBRN are not actually needed\nto simply invoke the tau-p tables once generated.  They are only used for\nsetting up the tables the first time.\n\n   To invoke the travel times, a handful of routines also in the LIBTAU\nlibrary need to be called.  An example of how this is done is given in\nthe program TTIMES.  Note that a different include file, TTLIM.INC is\nneeded.  Playing with the print flags can give you a feeling for what\nTTIMES does, but one of them generates a lot of output (into file\nTTIM1.LIS).  The tau interpolation is usually only needed for debugging.\nHowever, the range summary can be very useful as it shows the distance\nrange over which each branch of each phase will appear.  The BRNSET\nroutine can be used interactively (as in TTIMES) or hardwired for a\nspecific set of phases.  It allows the user to select only the phases\nof immediate interest.  When running TTIMES, note that at each distance,\nall possible arrivals from all branches are given in time order.  The\ntravel-time is given twice (in seconds and in minutes and seconds).  The\nnumbers at the end of each line are various derivatives (travel-time\nwith respect to distance, travel-time with respect to depth and the\nsecond derivative of travel-time with respect to distance).\n\n                                           Ray Buland\n\nP.S. UNIX manual pages generated by Brian Kennett are provided in the file \n     TAU.MAN.\n\nLOCAL CHANGES:\n\n   A subroutine called tptt (code: tptt.f; binary tpttsub.o) is available\nto return travel time information (time, dt/ddelta, dt/ddepth, ddelta/dp)\nfrom these tables.  See the code for documentation on use.  (G. Helffrich\nCarnegie/DTM 28 Aug. 1991)\n\n   Include files limits.inc and libtau.inc were rationalized so that\n   dimensioning dependencies were made explicit.  All declarations changed\n   to use parameter values set in limits.inc and libtau.inc.\n      G. Helffrich/UB 27 Apr. 1993\n\n   libsun and libvax changed to add evget subroutine.\n   ttimes changed to take module name from environment variable if asked.\n      G. Helffrich/UB 27 Apr. 1993\n\n   emiasp91.f changed to flip data statements, making modification of parts\n   of model possible.\n      G. Helffrich/UB 5 June 1994\n   \n   Discontinuity size in remodl.f changed so that these aren't declared where\n   the mismatch is minor between velocities.  This causes trouble with the\n   iasp91 model and has been returned to its original size.\n      G. Helffrich/UB 5 June 1994\n   \n   Table writing rearranged so that table size can be checked by ttimes.\n   This is to ensure that tables that are too large or even a different\n   size may be read in correctly by the program.\n      G. Helffrich/UB 13 August 1995\n\n   Modified libtau.f so that PKPbc is also diffracted around the inner core.\n      G. Helffrich/UB 3 Dec. 1996\n   \n   Added code to check for table overflows of various sorts in remodl and\n   setbrn.  These mostly result in pause statements executed with cryptic\n   comments.\n      G. Helffrich/UB 20 Mar. 1998\n   \n   ***WARNING: This is not a backwards-compatible change.  If you make this\n   change, you have to either rebuild all your .hed files, or keep an old\n   version of ttimes around capable of reading the old table format. ***\n   Modified .hed format so that the ttimes program and friends can determine\n   whether the table it will read in is too big to fit.  This permits changes\n   to the size of built-in tables without having to change previous versions\n   of the tables that ttimes reads in.\n      G. Helffrich/UB 29 Mar. 1998\n   \n   Checked for overflow of temporary storage in fitspl (libtau.f).  Will\n   stop with an error message if number of branches exceeds bounds.\n      G. Helffrich/UB 1 Aug. 1998\n\n   remodl.f: Fixed bug in findrng which caused division by zero if any value\n   in the y table was equal to y(i) in inverse iteration.  Basically caused by\n   fact that xmax is real*4 and function arguments are real*8 - so you can get\n   no change in xmax for a change in its x value.\n      G. Helffrich/UB 6 Aug. 2003\n\n   remodl.f: Added pSKP and sSKP to tabulated phases.\n      G. Helffrich/UB 20 Aug. 2005\n\n   Makefile:  Tweak to prevent non-fatal build error messages and to install\n   with default model in ttimes.\n      G. Helffrich/UB 11 Jan. 2006\n\n   Makefile:  Tweak to fix builds on fast machines -- some models skipped.  Fix\n   install-all-models and behavior of simple 'install' with no previous models\n   built.\n      G. Helffrich/UB 3 Dec. 2006\n\n   tptt.f:  New routine tpttsv to return velocities at source.\n      G. Helffrich/UB 13 Oct. 2007\n\n   Makefile.in, configure.ac, utils/install-sh, utils/version, ttimes.f:  Make\n   configure-driven. (version 2)\n      G. Helffrich/UB 21 Apr. 2008\n\n   Makefile.in, configure.ac, utils/config.guess, utils/config.sub: Version 3\n      G. Helffrich/UB 3 Jun. 2008\n   1) Improve Mac installation defaults to be more intelligent about choosing\n      the man page locations.\n   2) Add gtt91 man page (not very useful); edit ttimes man page to be\n      consistent about name of program.\n   3) Add uninstall option (suggested by Arthur Snoke).\n   4) Add range in km option (sugg. by Art Snoke) and terse option.\n   5) Add take-off option (sugg. by Art Snoke).\n\n   limits.com, setbrn.f, ttimes.f, configure.ac/configure:  Version 4\n      G. Helffrich/UB 7 Jun. 2008\n   1) Add PcS, sScS, pScS, sPcP, pPcP phases -- increase limits.inc to\n      permit.\n   2) Include model name in even terse output (suggested by Arthur Snoke).\n   3) Eliminate odd configure complaint (reported by Arthur Snoke).\n   4) Correct conversion factors for slowness in km; fix output labels.\n\n   Makefile.in:  Version 5\n      G. Helffrich/UB 10 Jun. 2008\n   1) Create all directories in $(MANDIR) path (reported by Arthur Snoke).\n\n   Makefile.in:  Version 6\n      G. Helffrich/UB 21 Jan. 2010\n   1) ttimes.1, ttimes91.1 - fix typos, document behavior.\n      G. Helffrich/UB 21 Jan. 2010\n   2) ttimes.f, libtau.f, libvax.f, setbrn.f, remodl.f:  Eliminate dasign,\n      assign, retrns use; use Fortran constructs instead.  Add die() to\n      libvax.f\n   3) configure.ac, configure:  Add configure options to eliminate depth\n      phases and add extra core phases.  Automatic generation of limits.inc\n   4) tptt.f:  Document use of subroutine interface to tables better and the\n      tpmod and phgen routines.\n\n   Makefile.in:  Version 7\n      G. Helffrich/UB 2 Feb. 2010\n   1) libtau.f - enlarge temporary branch table, and check for small return\n      value array.\n\n   Makefile.in:  Version 8\n      G. Helffrich/UB 15 June 2013\n   1) libtau.f - lengthen msg variable.\n   2) remodl.f, setbrn.f - Allow for planetary models that lack an inner core.\n      Many hard-wired offsets into phase name tables and breakpoint tables\n      were changed to positions derived from model characteristics.\n   3) Add emss97a.f - Mars model of Sohl and Spohn (1997).\n\n   Makefile.in:  Version 9\n      G. Helffrich/ELSI 13 June 2017\n   1) libtau.f - Sanity check whether tables file exists before trying to read\n      it; report if not.\n   2) remodl.f, setbrn.f - Allow for planetary models where the core is a low\n      velocity zone for both P and S waves.\n   3) Add emr11dw.f - Mars model of Rivoldini et al. (2011) with Dreibus and\n      Wanke composition.\n\n   Makefile.in:  Version 10\n      G. Helffrich/ELSI 09 Apr. 2020\n   1) libtau.f - TXTB: New externally callable routine to return slowness,\n      range and travel time for specific branch.\n   2) emr11dw.f, emss97a.f, emak135.f - Eliminate compilation niggles.\n   3) tpttsub.3.in - New file to document subroutine interface to tau-p tables.\n   4) ttimes.1.in - New file to document command line interface to tau-p tables.\n\n   Makefile.in:  Version 11\n      G. Helffrich/ELSI 17 Nov. 2020\n   1) Add ek137 model by Kennett (2020)\n   2) ttimes.1.in - Add ek137 to model list.\n```\n",
        "createdAt": "2022-09-18T16:08:07.000Z",
        "updatedAt": "2025-10-13T02:08:55.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ghfbsd/iasp/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "akuhara/Syn_seis",
        "url": "https://github.com/akuhara/Syn_seis",
        "description": "Software to compute synthetic seismograms for an incidence P or SV wave to 1-D layered structure.",
        "stars": 12,
        "forks": 5,
        "readme": "# Syn_seis\n\nSoftware to compute synthetic seismograms for an incidence P or SV wave to 1-D layered structure.\n\n(c) Takeshi Akuhara 2020\n\nEarthquake Research Institute, The University of Tokyo \n\n<img src=\"./img/test_output.png\" width=\"400\">\n\n## Installation \n1. `cd src`\n2. `make`\n\nIt requires [FFTW](http://www.fftw.org/) and [SAC](http://ds.iris.edu/ds/nodes/dmc/software/downloads/sac/) libraries on your machine. The paths for these libraries must be specified in `src/Makefile`.\nIf the SAC commands are already available, `sac-config --cflags --libs sacio` will display the paths to the SAC library.\n\n## Test run\n`bin/syn_seis < sample/test.in`\n\n* Don't forget to put \"<\".\n* After this command, you will get output files `test.r` and `test.z`. \n\n## How to use\nThe program reads parameters and a velocity model from the standard input. The formats are:\n\n* Line 1: [output file index]\n* Line 2: [number of elements in output time series] [sampling interval (s)] [Time shift (s)]\n* Line 3: [number of layers] [ray parameter (s/km)] [phase type: 1 for P; -1 for S]\n* Lines 4...: [Vp (km/s)] [Vs (km/s)] [density (g/cm^3)] [thickness (km)]\n\n### Note: \n* For ocean setting, set Vs < 0 for the topmost layer. In this case, the receiver is assumed to be placed on the seafloor (i.e., layer 1-2 boundary).\n* Thickness of the bottom layer is just ignored.\n* Output is SAC format. t=0 corresponds to the timing when incidence wave passes through the deepest layer interface (i.e., top of the half-space). \n",
        "createdAt": "2020-09-02T02:13:31.000Z",
        "updatedAt": "2025-01-14T15:41:52.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/akuhara/Syn_seis/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "wal06seismo/Engineering_seismology",
        "url": "https://github.com/wal06seismo/Engineering_seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2022-06-07T15:57:09.000Z",
        "updatedAt": "2022-06-07T15:57:09.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "uafgeotools/mtuq_performance_tests",
        "url": "https://github.com/uafgeotools/mtuq_performance_tests",
        "description": null,
        "stars": 1,
        "forks": 1,
        "readme": "\nmtuq performance tests\n----------------------\n\ninspired by a similar package called github.com/johnnylee/python-numpy-c-extension-examples\n\n",
        "createdAt": "2019-08-06T00:03:47.000Z",
        "updatedAt": "2020-12-16T04:41:30.000Z",
        "language": "C",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/uafgeotools/mtuq_performance_tests/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "seisfwi/AnalyticalSolution",
        "url": "https://github.com/seisfwi/AnalyticalSolution",
        "description": "Analytical particle velocity/displacement/strain/strain rate solution for the 3D/2D wave equation due to a moment tensor source in a homogeneous, isotropic elastic medium based on Aki & Richards (2002).  ",
        "stars": 6,
        "forks": 0,
        "readme": "# AnalyticalSolution\nAnalytical particle velocity/displacement/strain/strain rate solution for the 3D/2D wave equation due to a moment tensor source in a homogeneous, isotropic elastic medium based on Aki & Richards (2002).  \n\nSee the notebook for the usage.\n",
        "createdAt": "2024-03-06T16:49:43.000Z",
        "updatedAt": "2025-05-15T01:24:41.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/seisfwi/AnalyticalSolution/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "fgallovic/RIKsrf",
        "url": "https://github.com/fgallovic/RIKsrf",
        "description": "RIK earthquake source model: Slip rate generator",
        "stars": 15,
        "forks": 7,
        "readme": "# RIKsrf\n-----------\nRuiz Integral Kinematic (RIK) earthquake source model: Slip rate generation and 1D full wavefield synthetics\n\nSuite of codes for earthquake strong ground motion simulations using an advanced kinematic source model based on the model of Ruiz et al. (2011).\nRIKsrf provides slip rate functions on a finely discretized source that result in synthetics with the desired\nomega-squared spectral decay in full (broadband) frequency range.\n\n####Main features of the RIK model:\n\n- The source is composed of randomly distributed overlapping subsources with fractal number-size distribution.\nPosition of the subsources can be constrained by prior knowledge of major asperities\n(stemming, e.g., from slip inversions), or can be completely random. This fractal composition of the source model implies that the slip decays as *k*<sup>-2</sup> at high wavenumbers *k*.\n\n- The rise time is considered to depend on subsource radius, i.e. there is a positive correlation between slip and rise time\nas observed in dynamic source modeling.\n\n- The latter two properties ensures omega-squared decay of resulting source time function. \n\n- Rupture velocity and rise time can follow local S-wave velocity profile, so that the rupture slows down and rise times increase close to the surface, avoiding unrealistically strong ground motions.\n\n- Rupture velocity can be either constant or with have random variations, which results in irregular rupture front while satisfying the\ncausality principle.\n\n- The generated slip rates can be simply incorporated in any numerical wave propagation code without requiring any cross-over filtering with stochastic Green's functions.\n\n------------\n\n###Content of directories:\n - `src-RIKsrf` - Source codes of the RIK slip rate generator.\n - `src-1Dsynthetics` - Calculation of 1D full wavefield synthetics using Axitra for Green's function calculations.\n - `examples` - Several examples for testing the code including graphics (requires Gnuplot).\n - `docs` - Documentation and papers related to the RIK model.\n - `src-graphics` - Codes for generating graphics such as slip rate snapshots (requires Gnuplot).\n",
        "createdAt": "2016-09-14T23:01:35.000Z",
        "updatedAt": "2025-04-03T10:50:06.000Z",
        "language": "Fortran",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/fgallovic/RIKsrf/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "grajh/const2grad",
        "url": "https://github.com/grajh/const2grad",
        "description": "Seismological python tool for converting constant velocity model to gradient velocity model.",
        "stars": 0,
        "forks": 0,
        "readme": "# const2grad\nSeismological Python tool for converting constant-layer velocity model (e.g. from 1-D hypocenter-velocity inversion) to gradient velocity model required for 3-D hypocenter-velocity inversion or any other purpose in seismology. Sometimes, the velocity model at hand is parameterized by layers with constant velocities (constant-velocity layers) and we need to transform this model into the best-fitting gradient model. For example, the former needs to be transformed in order to be used as the initial model for the 3-D inversion. Since this cannot be done exactly, we have approached the problem by implementing the algorithm that finds the best-fitting gradient model by successively calculating the slope between the newly constructed points. The resulting velocities are adjusted when velocity jumps occur that do not match the input velocity model.\n\nDependencies:\n\n    * Matplotlib\n    * NumPy\n",
        "createdAt": "2024-06-20T14:08:38.000Z",
        "updatedAt": "2024-06-20T14:16:28.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/grajh/const2grad/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "imrulkayes1998/Seismology",
        "url": "https://github.com/imrulkayes1998/Seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "cd # Seismology\nThis is my first repository\nThis just a learning practice",
        "createdAt": "2025-11-20T04:40:17.000Z",
        "updatedAt": "2025-11-20T10:02:44.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/imrulkayes1998/Seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "auliakhalqillah/qgislib",
        "url": "https://github.com/auliakhalqillah/qgislib",
        "description": "QGIS python library for seismic or seismology application",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2023-03-13T15:40:46.000Z",
        "updatedAt": "2023-03-14T01:17:29.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ISR-AIML/seismic_event_classifier",
        "url": "https://github.com/ISR-AIML/seismic_event_classifier",
        "description": "This is the implementation for Seismic-Event-Classification",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2024-03-14T09:31:21.000Z",
        "updatedAt": "2024-03-14T13:02:03.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "jaabell/ShakerMaker",
        "url": "https://github.com/jaabell/ShakerMaker",
        "description": "Easily create physically-realistic earthquake ground motions for engineering and seismology!",
        "stars": 21,
        "forks": 12,
        "readme": "![ShakerMaker](/docs/source/images/logo.png)\n\nShakerMaker is intended to provide a simple tool allowing earthquake engineers and seismologists to easily use the frequency-wavenumber method (FK) to produce ground-motion datasets for analysis using the Domain Reduction Method (DRM). DRM motions are stored directly into the H5DRM format.\n\nThe FK method, the core of ShakerMaker, is implemented in fortran (originally from http://www.eas.slu.edu/People/LZhu/home.html with several modifications), and interfaced with python through f2py wrappers. Classes are built on top of this wrapper to simplify common modeling tasks such as crustal model specification, generation of source faults (from simple point sources to full kinematic rupturespecifications), generating single recording stations, grids and other arrays of recording stations and stations arranged to meet the requirements of the DRM. Filtering and simple plotting tools are provided to ease model setup. Finally, computation of motion traces is done by pairing all sources and all receivers, which is parallelized using MPI. This means that ShakerMaker can run on simple personal computers all the way up to large supercomputing clusters. \n\nInstallation\n------------\n\nFor now, only though the git repo::\n\n\tgit clone git@github.com:jaabell/ShakerMaker.git\n\nUse the `setup.py` script, using setuptools, to compile and install::\n\n\tsudo python setup.py install\n\nIf you dont' have sudo, you can install locally for your user with::\n\n\tsudo python setup.py install --user\n\n\nDependencies\n------------\n\n- `h5py`\n- `f2py`\n- `numpy`\n- `scipy`\n- `mpi4py` (optional but highly recommended for parallel computing of the response)\n- `matplotlib` (optional, for plotting)\n\nYou can get all these packages with `pip`::\n\n\tsudo pip install mpi4py h5py f2py numpy scipy matplotlib\n\nor, for your user::\n\n\tsudo pip install --user mpi4py f2py h5py numpy scipy matplotlib\n\nQuickstart usage\n----------------\n\nUsing ShakerMaker is simple. You need to specify a :class:`CrustModel` (choose from the available\npredefined models or create your own), a :class:`SourceModel` (from a simple \n:class:`PointSource` to a complex fully-customizable extended source with :class:`MathyFaultPlane`) \nand, finally, a :class:`Receiver` specifying a place to record motions (and store them\nin memory or text format).\n\nIn this simple example, we specify a simple strike-slip (strike=90, that is due east) \npoint source at the origin and a depth of 4km, on a custom two-layer crustal model, \nand a single receiver 5km away to the north::\n\n\tfrom shakermaker.shakermaker import ShakerMaker\n\tfrom shakermaker.crustmodel import CrustModel\n\tfrom shakermaker.pointsource import PointSource \n\tfrom shakermaker.faultsource import FaultSource\n\tfrom shakermaker.station import Station\n\tfrom shakermaker.stationlist import StationList\n\tfrom shakermaker.tools.plotting import ZENTPlot\n\n\t#Initialize two-layer CrustModel\n\tmodel = CrustModel(2)\n\n\t#Slow layer\n\tVp=4.000\t\t\t# P-wave speed (km/s)\n\tVs=2.000\t\t\t# S-wave speed (km/s)\n\trho=2.600\t\t\t# Density (gm/cm**3)\n\tQp=10000.\t\t\t# Q-factor for P-wave\n\tQs=10000.\t\t\t# Q-factor for S-wave\n\tthickness = 1.0\t\t# Self-explanatory\n\tmodel.add_layer(thickness, Vp, Vs, rho, Qp, Qs)\n\n\t#Halfspace\n\tVp=6.000\n\tVs=3.464\n\trho=2.700\n\tQp=10000.\n\tQs=10000.\n\tthickness = 0   #Zero thickness --> half space\n\tmodel.add_layer(thickness, vp, vs, rho, Qp, Qs)\n\n\t#Initialize Source\n\tsource = PointSource([0,0,4], [90,90,0])\n\tfault = FaultSource([source], metadata={\"name\":\"single-point-source\"})\n\n\n\t#Initialize Receiver\n\ts = Station([0,4,0],metadata={\"name\":\"a station\"})\n\tstations = StationList([s], metadata=s.metadata)\n\n\nThese are fed into the shakermaker model class::\n\n\tmodel = ShakerMaker(crust, fault, stations)\n\nWhich is executed::\n\n\tmodel.run()\n\nResults at the station can be readily visualized using the utility function :func:`Tools.Plotting.ZENTPlot`::\n\n\tfrom shakermaker.Tools.Plotting import ZENTPlot\n\tZENTPlot(s, xlim=[0,60], show=True)\n\nYielding:\n\n![ShakerMaker](/examples/example0_fig1.png)\n",
        "createdAt": "2019-03-28T15:04:02.000Z",
        "updatedAt": "2025-07-31T12:12:51.000Z",
        "language": "Python",
        "homepage": "https://shakermaker.readthedocs.io/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/jaabell/ShakerMaker/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "3602kiva/seismic-deeplearning",
        "url": "https://github.com/3602kiva/seismic-deeplearning",
        "description": "Deep Learning for Seismology project",
        "stars": 0,
        "forks": 0,
        "readme": "# seismic-deeplearning\nDeep Learning for Seismology project\n",
        "createdAt": "2025-08-30T16:23:13.000Z",
        "updatedAt": "2025-09-07T16:13:54.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/3602kiva/seismic-deeplearning/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Elemento24/NTU-Seismology-Artificial-Intelligence",
        "url": "https://github.com/Elemento24/NTU-Seismology-Artificial-Intelligence",
        "description": "This repo consists of all the Code and Resources for my NTU Research Project titled \"Time Series Analysis With Deep Learning Techniques and Beyond\"",
        "stars": 4,
        "forks": 1,
        "readme": "# Time Series Analysis With Deep Learning Techniques and Beyond\n\n## Study Background\nTime series analysis has wide applications in many fields such as economics, finance, computer sciences, engineering, and earth sciences, where understanding patterns and trends is important to decision making and predicting future behaviours. For example, the ground motion caused by an earthquake is recorded as a course of time. Time-series data are always noisy and high-dimensional. There features increase the difficulty of deep and precise analysis. Recent development in deep learning provides some new techniques for efficient time-series analysis. In this project, we aim to create new models of unsupervised learning of features for time series analysis and prediction. Applications into seismic data for earthquake identification & prediction and for petroleum exploration will be explored.\n\n\n## Datasets\n- All the downloaded data is available as 4 different Kaggle datasets, and the links to the datasets can be found below:\n  - [Earthquakes Time-Series Analysis (NSFE | Part 1)](https://www.kaggle.com/datasets/elemento/ntu-rw)\n  - [Earthquakes Time-Series Analysis (NSFE | Part 2)](https://www.kaggle.com/datasets/elemento/earthquakes-timeseries-analysis-nsfe-part-2)\n  - [Earthquakes Time-Series Analysis (NSFE | Part 3)](https://www.kaggle.com/datasets/elemento/earthquakes-timeseries-analysis-nsfe-part-3)\n  - [Earthquakes Time-Series Analysis (NEFS)](https://www.kaggle.com/datasets/elemento/earthquakes-timeseries-analysis-nefs)\n- All the pre-processed data is available as 2 different Kaggle datasets, and the links to the datasets can be found below:\n  - [Earthquakes Time-Series Analysis (NSFE_SAC)](https://www.kaggle.com/datasets/elemento/earthquakes-timeseries-analysis-nsfe-sac)\n  - [Earthquakes Time-Series Analysis (NEFS_SAC)](https://www.kaggle.com/datasets/elemento/earthquakes-timeseries-analysis-nefs-sac)\n\n\n## Project Report\n- The rest of the details about the project can be found in my project report published at Notion. You can check it out [here](https://elemento24.notion.site/NTU-Project-Report-5f6ccd587b49461fb6124cbd139c82d6). It includes details such as:\n  - Study region\n  - Data types considered\n  - Distribution and statistics of the downloaded data\n  - Distribution and statistics of the retained data\n  - Differences beteween traditional and machine learning algorithms for automatical picking of phase \n  - And much more!\n\n",
        "createdAt": "2022-05-16T15:13:07.000Z",
        "updatedAt": "2024-12-15T14:44:04.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Elemento24/NTU-Seismology-Artificial-Intelligence/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ibrahimilhan35/ocean-bottom-seismology",
        "url": "https://github.com/ibrahimilhan35/ocean-bottom-seismology",
        "description": "crustal thickness",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2017-12-15T16:47:56.000Z",
        "updatedAt": "2017-12-15T16:47:56.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "dylanmikesell/GEOPH677",
        "url": "https://github.com/dylanmikesell/GEOPH677",
        "description": "Course material for GEOPH677 - Earthquake Seismology",
        "stars": 0,
        "forks": 5,
        "readme": "# GEOPH677\n\nThis repository contains course material for GEOPH677 - Earthquake Seismology.\n\n----\n\nDuring this course you may find the following tools useful:\n\n* [GISMO](https://geoscience-community-codes.github.io/GISMO/) - a MATLAB package for observational seismic data\n* [Obspy](https://github.com/obspy/obspy/wiki) - a Python framework for sesimology\n* [ObspyDMT](https://github.com/kasra-hosseini/obspyDMT) - a Python toolbox for retrieving and processing of large seismological datasets\n* [Obspy use cases](https://github.com/obspy/obspy/wiki#use-cases--applications-using-obspy) - a list of other tools built on the Obspy package\n\n#### Plotting\n\n* [GMT MATLAB API](http://gmt.soest.hawaii.edu/projects/gmt-matlab-octave-api/wiki) -- interface with GMT from within MATLAB\n* [M_Map](https://www.eoas.ubc.ca/~rich/map.html) -- a mapping package for MATLAB\n* [Basemap](http://matplotlib.org/basemap/) -- a part of the Python toolkit Matplotlib\n\n---\n\nThere are a number of other tools are there. If you already use something else, please email me and we can add it to the list.\n",
        "createdAt": "2017-01-10T16:48:31.000Z",
        "updatedAt": "2017-01-10T16:57:30.000Z",
        "language": "Matlab",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/dylanmikesell/GEOPH677/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "anowacki/SeisRequests.jl",
        "url": "https://github.com/anowacki/SeisRequests.jl",
        "description": "Get seismic data via web services in Julia",
        "stars": 4,
        "forks": 2,
        "readme": "# SeisRequests\n\nGather seismic data and metadata from web services using Julia.\n\n[![Build Status](https://github.com/anowacki/SeisRequests.jl/workflows/CI/badge.svg)](https://github.com/anowacki/SeisRequests.jl/actions)\n[![codecov](https://codecov.io/gh/anowacki/SeisRequests.jl/branch/master/graph/badge.svg?token=d0ePcA1m54)](https://codecov.io/gh/anowacki/SeisRequests.jl)\n\n## About SeisRequests\n\nSeisRequests allows you to easily create and submit a request for seismic data\nto any server round the world which supports either the FDSN\nor IRIS web services specifications.  Examples include [IRIS](https://iris.edu)\nitself and the EU's data repository [Orfeus](https://www.orfeus-eu.org).\n\nYou can search for seismic waveform data, earthquake locations and station\ninformation amongst other things.\n\n## Installing\n\nAdd SeisRequests like so:\n\n```julia\njulia> ] # Type ']' to enter pkg mode\n\npkg> add https://github.com/anowacki/Geodesics.jl https://github.com/anowacki/Seis.jl https://github.com/anowacki/SeisRequests.jl\n```\n\nSeisRequests supports the latest\n[long-term support release of Julia](https://julialang.org/downloads/#long_term_support_release),\nrequiring v1.6 or newer.\n\n## Using\n\n### High-level interface\nIf you just want to get earthquake parameters, station metadata or raw\nseismic data, use the high-level interface, which offers four main\nfunctions:\n- `get_events`, to return `Seis.Event`s with information about seismic\n  events in various catalogues.\n- `get_stations`, to return `Seis.Station`s with metadata about seismic\n  sensors.\n- `get_stations!`, to fill in missing metadata for a set of `Seis.Station`s.\n- `get_data`, to return `Seis.Trace`s with raw seismic data.\n\nEach of these accept keyword arguments to define the region of interest\nin which to search for the events or stations.  For details of all keywords\nwhich can be used, see the docstrings for `FDSNEvent`, `FDSNStation` and\n`FDSNDataSelect`.\n\n#### Servers\nSeisRequests knows about some servers with which it can communicate.  To\nsee the list, call `SeisRequests.server_list()`.\n\nThe server can be specified by either the server key (from `server_list`)\nor a full URL (like `\"http://service.iris.edu\"`) using the `server`\nkeyword argument to the `get_*` functions.\n\n#### Example\nFor example, let's try and find some data for the [Cwmllynfell event in\nSouth Wales on 17 February 2018](http://www.earthquakes.bgs.ac.uk/earthquakes/recent_events/20180217142554.html#page=summary):\n\n```julia\njulia> using SeisRequests, Dates\n\njulia> origintime = DateTime(2018, 02, 17, 14, 31, 06);\n\njulia> event = get_events(starttime=origintime-Second(10), endtime=origintime+Second(10), minmagnitude=4) |> first\nSeis.Event{Float64,Seis.Geographic{Float64}}:\n        lon: -3.8936\n        lat: 51.7074\n        dep: 10.42\n       time: 2018-02-17T14:31:04.750\n         id: smi:service.iris.edu/fdsnws/event/1/query?originid=28547804\n       meta: type => \"earthquake\"\n             quakeml => QuakeML.Event\n  description: Array{QuakeML.EventDescription}((1,))\n  comment: Array{QuakeML.Comment}((0,))\n  focal_mechanism: Array{QuakeML.FocalMechanism}((0,))\n  amplitude: Array{QuakeML.Amplitude}((0,))\n  magnitude: Array{QuakeML.Magnitude}((1,))\n  station_magnitude: Array{QuakeML.StationMagnitude}((0,))\n  origin: Array{QuakeML.Origin}((1,))\n  pick: Array{QuakeML.Pick}((0,))\n  preferred_origin_id: QuakeML.ResourceIdentifier\n  preferred_magnitude_id: QuakeML.ResourceIdentifier\n  preferred_focal_mechanism_id: Missing missing\n  type: QuakeML.EventType\n  type_certainty: Missing missing\n  creation_info: Missing missing\n  public_id: QuakeML.ResourceIdentifier\n\n             author => \"us\"\n             mag_type => \"mb\"\n             mag_author => \"us\"\n             mag => 4.3\n             description => \"UNITED KINGDOM (Flinn-Engdahl region)\"\n             server => \"IRIS\"\n```\n\nNow let's get the metadata about the station [JSA](http://www.earthquakes.bgs.ac.uk/data/station_book/stationbook_jsa.html)\nif it was active at the time:\n\n```julia\njulia> stations = get_stations(event, code=\"GB.JSA.*.BH?\")\n[ Info: Request status: Successful request, results follow\n3-element Vector{GeogStation{Float64}}:\n Station: GB.JSA..BHE, lon: -2.171698, lat: 49.187801, dep: 0.0, elev: 39.0, azi: 90.0, inc: 90.0, meta: 4 keys\n Station: GB.JSA..BHN, lon: -2.171698, lat: 49.187801, dep: 0.0, elev: 39.0, azi: 0.0, inc: 90.0, meta: 4 keys\n Station: GB.JSA..BHZ, lon: -2.171698, lat: 49.187801, dep: 0.0, elev: 39.0, azi: 0.0, inc: 0.0, meta: 4 keys\n```\n\nIf we want to get some data from here, we can ask how long before and\nafter the earthquake we want, then finally submit a request for some data.\nIn this case, let's ask for data starting 0 s (`Second(0)`) before and\n300 s (`Minute(6)`) after the earthquake.\n\n```julia\njulia> data = get_data(event, stations, Second(0), Minute(6))\n[ Info: Request status: Successful request, results follow\n3-element Vector{Trace{Float64, Vector{Float64}, Seis.Geographic{Float64}}}:\n Seis.Trace(GB.JSA..BHE: delta=0.02, b=0.015, nsamples=18000)\n Seis.Trace(GB.JSA..BHN: delta=0.02, b=0.005, nsamples=18000)\n Seis.Trace(GB.JSA..BHZ: delta=0.02, b=0.015, nsamples=18000)\n```\n\nIf we have Plots installed, we can now look at our lovely data!\n\n```julia\njulia> using Seis.Plot, Plots\n\njulia> plot(data)\n```\n\n![Cwmllynfell 2018-02-17 seismic event recorded at JSA, Jersey](docs/images/Cwmllynfell_JSA.png)\n\nTo request data windows based on predicted seismic travel times, install\n[SeisTau.jl](https://github.com/anowacki/SeisTau.jl); see the docstring\nfor `get_data` for details.\n\n\n### Low-level interface\nThe high-level functions work by calling the low-level interface, which\noperates in this way:\n1. Create a request using a constructor:\n  - Using the FDSN Web Services standard:\n    - `FDSNEvent`: Query for events\n    - `FDSNStation`: Look for stations\n    - `FDSNDataSelect`: Request waveform data\n  - Using the IRIS Web Services standard:\n    - `IRISTimeSeries`: Request waveform data with preprocessing done\n3. Send that request to your preferred server with `get_request`, and\n   get back a `HTTP.Message.Response`, containing the raw response in the\n   `.body` field.\n4. Process the raw output as needed.\n\nEach of the constructors has comprehensive documentation you can access\nin the REPL by typing, e.g., `?FDSNEvent`.\n\n",
        "createdAt": "2018-07-17T16:12:43.000Z",
        "updatedAt": "2025-11-25T12:10:50.000Z",
        "language": "Julia",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/anowacki/SeisRequests.jl/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "joeGermany/SpatialTemporalModel",
        "url": "https://github.com/joeGermany/SpatialTemporalModel",
        "description": "A Spatial Temporal Model for CTF4Science - Seismology data",
        "stars": 0,
        "forks": 0,
        "readme": "��#\u0000 \u0000S\u0000p\u0000a\u0000t\u0000i\u0000a\u0000l\u0000 \u0000T\u0000e\u0000m\u0000p\u0000o\u0000r\u0000a\u0000l\u0000 \u0000M\u0000o\u0000d\u0000e\u0000l\u0000\r\u0000\n\u0000",
        "createdAt": "2025-09-24T16:35:44.000Z",
        "updatedAt": "2025-11-19T13:19:23.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/joeGermany/SpatialTemporalModel/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "hemmelig/2021JB022655",
        "url": "https://github.com/hemmelig/2021JB022655",
        "description": "Repository of data and code to accompany Bacon et al., 2022 (DOI: 10.1029/2021JB022655)",
        "stars": 2,
        "forks": 1,
        "readme": "# Supplementary analysis/visualisation code\nData and code required to reproduce the visualisations presented in:\n\nBacon, C.A., Johnson, J.H., White, R.S. and Rawlinson, N., 2022. On the origin of seismic anisotropy in the shallow crust of the Northern Volcanic Zone, Iceland. Journal of Geophysical Research: Solid Earth, 127(1), p.e2021JB022655.\n\nArticle: [![DOI](https://img.shields.io/badge/DOI-10.1029/2021JB022655-blue)](https://doi.org/10.1029/2021JB022655)\n\nSupplementary datafiles: [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5007022.svg)](https://doi.org/10.5281/zenodo.5007022)\n\nAnalysis and visualisation: [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5636924.svg)](https://doi.org/10.5281/zenodo.5636924)\n\n## Steps to reproduce figures\n1. Clone this repository and navigate to it, e.g.:\n\n```\ngit clone https://github.com/hemmelig/2021JB022655\ncd 2021JB022655\n```\n\n2. Install the packages listed in the environment.yml file, either manually, or using (for example) conda:\n\n```\nconda env create\n```\n\n3. Add `mfast_summary.mplstyle` (a matplotlib stylesheet) to your `mpl_configdir` (usually found at `~/.config/matplotlib`)\n\n4. Optional: Install Helvetica font for Matplotlib\n\n5. Navigate to each figure directory and run the `.gmt` (as `bash <script>.gmt`) or `.py` (as `python <script>.py`) scripts.\n\n## Notes\nThese figures were prepared using Linux 20.04.\n",
        "createdAt": "2021-10-26T17:43:56.000Z",
        "updatedAt": "2022-11-16T14:58:41.000Z",
        "language": "Python",
        "homepage": "https://doi.org/10.1029/2021JB022655",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "10.1029/2021JB022655)",
            "openCitations": "10.5281/zenodo.5007022",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/hemmelig/2021JB022655/main/README.md",
        "mainPaper": {
            "doi": "10.1029/2021JB022655)",
            "title": "",
            "journal": "",
            "dateReleased": null,
            "abstract": "",
            "citationsArray": []
        },
        "repoDoi": "10.1029/2021JB022655)",
        "publications": [
            {
                "doi": "10.1029/2021JB022655)",
                "name": "",
                "source": "",
                "authorNames": [],
                "abstract": "",
                "publicationDate": null,
                "journal": ""
            }
        ]
    },
    {
        "source": "GitHub",
        "name": "anakapallichandrasekhar2003-cloud/Seismology-Project",
        "url": "https://github.com/anakapallichandrasekhar2003-cloud/Seismology-Project",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2025-08-13T16:33:25.000Z",
        "updatedAt": "2025-08-13T16:34:44.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "speedshi/Seis_Matlablib",
        "url": "https://github.com/speedshi/Seis_Matlablib",
        "description": "Matlab library which contains various functions and modules for seismology.",
        "stars": 22,
        "forks": 9,
        "readme": "# Seis_Matlablib\nMatlab library which contains various functions and modules for seismology studies.\n\n## Installation\nSimply add the whole folder including all subfolders to you Matlab path.\n\n## Function list\n### travel_times\n**计算特定地震相位的旅行时**\n1. tvtcalrt_ly: 计算层状介质中的直达波旅行时和take-off angle\n2. tvtcalrt_homo: 计算均匀介质中的直达波旅行时和take-off angle\n\n### model_build_show\n**用于构建和显示模型的函数**\n1. plotgeo: 用于显示三维层状介质模型，并在模型中同时显示震源、接收器阵列（包括表面阵列和垂直井接收阵列）的位置\n2. plotmpsd: 用于绘制层状介质模型的速度、密度和衰减因子剖面\n3. intpmodel3: 对输入的三维模型进行三维插值，得到符合期望(维度)大小的三维模型。注意输入三维模型的第一维为X，第二维为Y，第三维为Z\n4. modelgeo_show: 根据输入参数，显示出相应模型和成像区域\n\n### iofile\n**用于输入输出文件的函数**\n1. equihomo: 计算并输出层状介质的均方根速度\n2. wrtasgeo: 输出震源，接收器位置信息\n3. wrtmdf: 输出层状介质的速度、密度和衰减因子等信息，输出文件格式为fk软件输入格式\n4. outputcatalogue: 输出地震目录，包括定位结果，地震时间和各台站到时\n5. outputesgcsv: 按照ESG CSV 的格式输出地震信号\n6. fdmodres: 读入fdmodeling合成的各道地震记录\n7. rdfdmodres: 读入fdmodeling合成的各道地震记录，使用fdmodeling软件的模拟参数文件名和接收点文件名作为函数输入参数\n8. wtraces2segy: 调用SegyMAT将合成地震记录输出为segy格式\n9. read_seish5: 读入HDF5格式的地震数据，对HDF5文件的格式有固定的要求\n10. read_stations: 读入IRIS text 格式的地震台站信息，获取台站的名称、位置信息\n11. read_velocity: 读入均匀或层状介质的速度、层厚度信息\n12. read_seissac: 读入SAC格式的地震数据\n13. read_catalog: 读入IRIS text格式的地震目录\n14. read_seis: main function to read seismic data of different formats (by calling different sub-functions)\n15. read_staname: 读取HDF5数据文件中的台站名和台站数目\n16. read_seismat: 读取MAT格式的地震数据\n17. output_cataext1: 输出text格式catalog文件，适用于GMT画图\n\n\n### seismic_modeling\n**用于地震正演模拟的函数**\n1. gsynwihl: 均匀介质或层状介质中的波形模拟。可以根据模型参数，自动选择调用均匀介质或层状介质的相关正演函数进行正演模拟。模型参数文件（震源文件、速度模型文件、接收器文件）的格式参照“file_format_description”。\n2. rdmodelf: 读入均匀或层状介质模型参数文件(速度、密度、层厚度、衰减、起始深度)\n3. rdreceiverf: 读入接收器参数文件\n4. rdsourcef: 读入震源参数文件\n5. plotmodel: 显示模型\n6. example_build_input_files: 生成模拟所需的输入文件的示例代码\n7. example_main_generate_waveforms: 调用模拟程序合成数据的示例代码\n8. example_model.dat: 模型文件的示例\n9. example_receiver.dat: 台站信息文件的示例\n10. example_source.dat: 震源和模拟参数文件的示例\n\n(1) homogeneous 均匀介质中的波场模拟  \n1. calninum: 计算Aki & Richards Eq 4.29 的解析解中近场的积分项，使用数值积分法\n2. calnint:  计算Aki & Richards Equation 4.29 中的近场积分项，使用解析雷克子波\n3. gsynwhomo: 利用Aki & Richards Eq 4.29 (解析解)计算均匀介质中的波场\n4. gsynwhomo_rickerw: 利用Aki & Richards Eq 4.29 (解析解)计算均匀介质中的波场，使用解析雷克子波\n5. homogreenf:  利用Aki & Richards Eq 4.29 (解析解)计算均匀介质中的波场,以脉冲为震源时间函数，计算出的是 the Green's function\n6. homogreenfne:  利用Aki & Richards Eq 4.29 (解析解)计算均匀介质中的近场波场,以脉冲为震源时间函数，计算出的是近场 the Green's function (近场+中间场)\n7. homogreenffa: 利用Aki & Richards Eq 4.29 (解析解)计算均匀介质中的远场波场,以脉冲为震源时间函数，计算出的是远场 the Green's function\n\n(2) layer 层状介质中的波场模拟  \n1. gsynwavefk/wavefk: 调用fk函数，实现层状介质中的反射率法正演模拟, 两个函数的输入参数略有不同。gsynwavefk函数更通用一些，更推荐使用该函数。\n\n\n\n### general_math_func\n**一般数学函数**\n1. corrcoefn: 计算一个输入矩阵的n-维相关系数，n为矩阵的列数\n2. corrcoefnv: 计算一个输入矩阵的多维相关系数\n3. mycorrcoef: 计算输入矩阵的相关系数矩阵\n4. mycovn: 计算输入矩阵的归一化协方差矩阵\n5. mycroscorn: 计算输入矩阵的归一化互相关矩阵\n6. my_kurtosis: 计算输入数据沿特定滑动时窗的kurtosis\n7. my_stalta: 计算输入数据沿特定滑动时窗的STA/LTA\n8. deltam: Dleta 函数\n9. dnormlz: 将输入数据线性归一化到特定区间\n10. gtwin: 生成特定窗函数的加权系数\n11. intder: 计算输入数据的数值积分或微分\n12. mtnorm: 将输入矩张量归一化\n13. trdis2vel: 将地震数据由质点的位移分量转化为质点的速度分量\n14. trvel2dis: 将地震数据由质点的速度分量转化为质点的位移分量\n15. geod2cart: 将经纬度、海拔高度转化为笛卡尔坐标，使用wgs84Ellipsoid地理坐标系统\n16. callyifdp: 计算层状介质的每一层界面（包括介质自由表面—起始深度）的深度\n17. dnorm_mdn: 去除和获取数据的整体趋势，通过减去滑动时窗中的中位数来实现\n18. datatransf: 对输入数据进行转换，如取绝对值，开方，取对数等\n\n\n### display\n**画图及显示相关函数**\n1. disp_3dslice: 显示三个相互正交的剖面\n2. migmaxplt: 显示一个输入4D 数据的最大值剖面和沿各维度的投影剖面，4D数据格式：T-X-Y-Z\n3. disprs: 显示波剖面，即record section, 以震源接收器的水平距离为准排列道集\n4. dispwfsc: 显示波剖面，类似record section，不同的是以震源接收器的直线距离为准排列道集，在记录的波形上标记P/S波到时\n5. dispwfscn: 显示波剖面，类似record section，不同的是以震源接收器的直线距离为准排列道集，在每条记录的底线上标记P/S波到时\n6. dispwfscn_2se: 同时显示两个波剖面，类似record section，不同的是以震源接收器的直线距离为准排列道集，在每条记录的底线上标记P/S波到时\n7. particlemotion: 显示质点的振动轨迹\n8. quiver3c: 显示三维矢量图（箭头），类似quiver3，矢量可以自由设置颜色\n9. seisrsdisp: 显示地震剖面，按地震记录的顺序依次排列，每一道记录最大值归一化为1\n10. seisrsdispk: 显示地震剖面，按地震记录的顺序依次排列，根据所有记录的最大值统一归一化为1\n11. wigb: 显示地震波形记录\n12. dispwflstk: 叠加并显示一定时窗内，波形的线性叠加结果\n13. show_spectrogram: display the spectrogram of seismic data\n14. ispectrogram/ispectrogram_1: display the seismogram and spectrogram of seismic data\n15. dispwfscn_m:  显示地震波剖面,将台站名同时标注显示\n16.  dispwfscn_mn: 显示地震波剖面,将台站名同时标注显示,台站按震源接收器的直线距离依次排列,间距为1\n17. plot_evesta: 显示地震台站和地震事件的平面分布图\n\n### seismic_location\n**地震定位方法**  \n(1) waveform_migration: 基于波形偏移的地震定位方法\n1. stack_kernelf: 计算输入数据沿特定滑动时窗的特征函数\n2. wavefmstk: 计算特征函数的叠加结果\n3. mgrsprofdisp: 显示定位结果的XYZ剖面，并于地震目录中的结果对比\n4. event_optm: 寻找偏移结果中的地震事件，采用时间、空间间隔的方式\n5. extractevt: 寻找偏移结果中的地震事件，提取距离地震目录中的事件一定时间范围内的偏移最大值\n6. locreson: 寻找偏移结果中的地震事件，提取在一定空间范围内持续一段时间的事件\n7. findefmg: 寻找偏移结果中的地震事件，采用阈值和间隔时间的方式\n8. gchkrs: 显示定位结果的记录剖面，帮助确认是否为明显的真实地震事件\n9. profdisppw: 显示定位结果的XYZ剖面，并与地震目录中的地震事件一一对应\n10. gpltlocrs: 在偏移记录上显示对应的定位结果（对应局部峰值）\n11. mcm_genei: 读入各种数据，生成MCM Fortran程序所需的输入文件，并运行相应MCM程序\n12. gene_soup: 生成偏移成像点的位置信息，并输出MCM需要的对应二进制文件\n13. gene_traveltime: 生成旅行时表，并根据需要决定是否输出旅行时表二进制文件\n14. gene_wavetime: 根据输入的波形数据生成MCM需要的波形二进制文件，并提取其对应的旅行时表并输出相应二进制文件\n15. gene_migpara: 生成MCM所需的文本格式参数文件\n16. runmcm_matlab_test: 根据输入的地震位置运行MCM matlab测试版本，会显示偏移剖面及记录剖面，用于判断偏移结果的好坏，可用于测试参数(如频率和时窗)的选择\n17. waveform_migration_kernel: MCM偏移定位核心程序, use P+S phases\n18. waveform_migration_kernel_x: MCM偏移定位核心程序, use only one phase\n19. mcm_test_para.m: run MCM on a single position (soure location) to obtain the stacking trace to test the MCM parameters, such as frequency band, window size and seismic phases\n20. get_earthquake: obtain the specified earthquake information from the catalog\n21. mcm_test_freqband: test mcm results on different frequency bands\n22. stkcorrcoef: calculate correlation coefficient matrix and stack the Ccs\n23. waveforms_show: 显示定位结果的记录剖面，帮助确认是否为明显的真实地震事件,台站名和时间同时显示\n24. gene_mcmifiles: 生成MCM Fortran 程序需要的各种参数文件（e.g. traveltime table, waveform file, soupos file, migpara）\n25. detmst0: 生成搜索的orgin times 序列\n\n### colormaps\n**各种色标**\n1. mycolor1.mat: 64*3, 蓝-黄-红\n2. cmapmtrdneg.mat: 256*3, 蓝-黄\n3. cmapmtdpos2: 256*3, 兰-黄-红\n4. cmapmtrdpos: 64*3, 兰-黄-红\n5. cmapmtrdp2: 256*3, 蓝-兰-黄-红\n6. cmapmtrdp: 64*3, 蓝-兰-黄-红\n7. cmapmtv: 64*3, 蓝-兰-黄-正红\n8. cmapmtv2: 64*3, 蓝-兰-黄-正红, 兰黄占比增大\n9. cmapmtv3: 64*3, 蓝-兰-黄-正红, 兰黄占比增大\n10. cmapmtv4: 64*3, 蓝-兰-黄-正红, 兰黄占比增大\n\n### downloads\n**下载的各种函数和函数库**\n1. borders: 显示世界各个国家的边界\n2. color_map: 显示红蓝色标(地震剖面常用色标)\n3. filter1: 对输入信号进行滤波\n4. IPGP-sac_matlab-c67a67e: 对SAC文件进行读写\n5. topotoolbox-master: 地形工具箱\n6. XKCD_RGB: 获取不同颜色的RGB值\n7. deg2utm: 将经纬度转化为UTM笛卡尔坐标\n8. irisFetch: 链接IRIS，获取地震数据\n9. freezeColors_v23_cbfreeze: 对不同子图使用不同的色标\n10. segymat-1.6: 输入、输出和编辑segy格式的文件\n11. fun_for_piero: clustering according to the input row and column indices of the upper triangular part of the correlation-coefficient matrix\n\n### noise\n**噪音有关函数**\n1. addnoinsr: 按照噪信比（振幅比）向数据中加入指定噪音\n2. pnoise: 向数据中加入一定信噪比的高斯随机噪音，信噪比以能量比表示\n3. pnoisem: 向数据中加入一定噪信比的高斯随机噪音，噪信比以振幅比表示\n\n### wavelet\n**子波相关函数**\n1. rickerw: 生成雷克子波，子波时延1.1/f+t0\n2. rickerwd: 生成雷克子波导数（解析），子波时延1.1/f+t0\n3. rickerwi: 生成雷克子波积分（数值），子波时延1.1/f+t0\n4. waveldely: 将震源时间函数延迟\n5. wavlintp: 将输入的震源时间函数插值加密，缩短时间间隔\n\n### seismic source\n**震源相关函数**\n1. fgeom2mt: 由断层参数(走向、倾向、倾角)生成地震矩张量\n\n### source radiation pattern\n**震源辐射模式相关函数**\n1. mtrdpfas: 画图相关函数，在画震源辐射模式图时，将坐标轴原点置于中心\n2. mtrdpfax: 画图相关函数，在画震源辐射模式图时，将坐标轴原点置于中心；根据要画图像的数值自动选择合适的色标。\n3. mtradiationv: 画远场P, S波的震源辐射模式，以矢量图(箭头)的形式展现\n4.  mtradiationvbkv: 画远场P, S波的震源辐射模式，以矢量图(箭头)的形式展现，控制P波图中的矢量位置，使其图形更符合球形分布。\n5. mtradiationb: 绘制远场P波的beach ball, 传统黑白beach ball, 三维球及使用stereographic projection的平面图（二维）\n6. mtradiationb_prof: 绘制远场P波的beach ball, 蓝红色标, 三维球及其沿三个反坐标轴方向(-x, -y, -z)的三个二维平面视图\n7. mtradiationp: 绘制远场P波，S波，SV波和SH波的震源辐射模式，三维图\n8. mtradiationifps: 绘制中间场S波，远场P波，远场S波的震源辐射模式，三维图\n9. mtradiationifps2: 绘制中间场S波，远场P波，远场S波在某个特定方位的震源辐射模式，相当与三维图的一个切片\n\n\n### data_process\n**一般性数据处理函数**\n1. sltordotpsta: 按照与特定点的距离排列地震台站\n2. stanamnum: 计算在输入的HFD5文件里，在特定station文件中的台站的数目和名称\n3. wave_extract: extract waveforms along arrival times\n4. ireplace_zeros: 使用随机的极小值（~eps）替换数据中的0值\n5. decluster: 去除检测到的信号中连续存在的相同的事件，see code for detail\n6. seisext: 根据输入的参数提取地震数据\n\n### seismology\n**地震学相关函数**\n1. check_stations: 检测地震台站在一年中某个时间段是否有数据\n2. catana_dist: 计算并显示地震目录中的地震时间距某点的距离\n3. runFMF_mdata: 调用FMF的wrapper，注意其特定的数据输入模式\n4. getpicks_fromNLLobs: 读取NonLinLoc格式的P/S 波的拾取文件\n5. gene_FMFtemplate: 根据P/S波的拾取结果为FMF准备template\n6. save_FMFdata: 保存FMF得到的检测结果，包括检测到的事件的事件和叠加相关系数\n\n## Reference\nIf you use this package in your work, please cite the fellowing papers in your documents.  \n\nShi, P., Angus, D., Nowacki, A., Yuan, S., & Wang, Y. (2018). Microseismic full waveform modeling in anisotropic media with moment tensor implementation. Surveys in Geophysics, 39(4), 567-611.  \n\nShi, P., Angus, D., Rost, S., Nowacki, A., & Yuan, S. (2019). Automated seismic waveform location using multichannel coherency migration (MCM)–I: theory. Geophysical Journal International, 216(3), 1842-1866.  \n\nShi, P., Nowacki, A., Rost, S., & Angus, D. (2019). Automated seismic waveform location using Multichannel Coherency Migration (MCM)—II. Application to induced and volcano-tectonic seismicity. Geophysical Journal International, 216(3), 1608-1632.\n\n```\n@article{shi2018microseismic,\n  title={Microseismic full waveform modeling in anisotropic media with moment tensor implementation},\n  author={Shi, Peidong and Angus, Doug and Nowacki, Andy and Yuan, Sanyi and Wang, Yanyan},\n  journal={Surveys in Geophysics},\n  volume={39},\n  number={4},\n  pages={567--611},\n  year={2018},\n  publisher={Springer}\n}\n@article{shi2019automated,\n  title={Automated seismic waveform location using multichannel coherency migration (MCM)--I: theory},\n  author={Shi, Peidong and Angus, Doug and Rost, Sebastian and Nowacki, Andy and Yuan, Sanyi},\n  journal={Geophysical Journal International},\n  volume={216},\n  number={3},\n  pages={1842--1866},\n  year={2019},\n  publisher={Oxford University Press}\n}\n@article{shi2019automated,\n  title={Automated seismic waveform location using Multichannel Coherency Migration (MCM)—II. Application to induced and volcano-tectonic seismicity},\n  author={Shi, Peidong and Nowacki, Andy and Rost, Sebastian and Angus, Doug},\n  journal={Geophysical Journal International},\n  volume={216},\n  number={3},\n  pages={1608--1632},\n  year={2019},\n  publisher={Oxford University Press}\n}\n```\n\n## Licence (GPLv3)\nThis program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. For more details, see in the license file.\n\n## Contributing\nI'm happy for anybody who wants to use and contribute to this repo. If you would like to contribute to the project as a developer, please feel free to contact me and create Pull Requests.  \nIf you have any questions and suggestions about the functions in this package, please also feel free to contact me.\n\n## Contact information \nCopyright(C) 2021 Peidong Shi  \nAuthor: Peidong Shi  \nEmail: peidong.shi@sed.ethz.ch or speedshi@hotmail.com\n\n",
        "createdAt": "2019-09-30T09:52:27.000Z",
        "updatedAt": "2025-11-13T01:57:34.000Z",
        "language": "MATLAB",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/speedshi/Seis_Matlablib/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "MarineSeismologySymposium/SIG-ATaCR",
        "url": "https://github.com/MarineSeismologySymposium/SIG-ATaCR",
        "description": "Special Interest Group for the processing of broadband OBS data at the 2021 Marine Seismology Symposium",
        "stars": 3,
        "forks": 1,
        "readme": "## Special Interest Group on OBS Data Processing at the 2021 Marine Seismology Symposium\n\n\n**Instructors**: [Helen Janiszewski](https://helenjaniszewski.squarespace.com) and [Pascal Audet](https://www.uogeophysics.com/authors/admin/)\n\n**When**: Wednesday, March 10, 2021 at 9 PM (GMT). *Note*, this is 11 AM in Honolulu (HST = GMT - 10:00) and 4 PM in Ottawa (EST = GMT - 5:00)\n\n**Where**: On zoom (link available via Marine Seismology Symposium pathables registration)\n\n**What**: This SIG will provide hands-on tutorials to process broadband ocean-bottom seismic (BBOBS) data using the Automatic Tilt and Compliance Removal (ATaCR, pronounced \"attacker\") software. This software is designed to automate, as best as possible, the process of characterizing and removing tilt and compliance noise from vertical component BBOBS data. The software is available in two scripting languages: Matlab and Python. Both versions are open source, and installation instructions are described below. The session will first introduce common practices and pitfalls when working with noisy BBOBS data and perform basic quality control, then break out into two separate rooms where participants will follow live tutorials using ATaCR. \n\n**How to prepare**: To ensure that the live tutorial sessions run seamlessly, we ask participants to pre-register to either the Matlab or Python session via pathables and install ATaCR with demo data. Details are outlined below.\n\n---\n\n### Installing the Matlab version\n\n- Git repository: [ATaCR](https://github.com/helenjanisz/ATaCR)\n\n- Documentation can be found [here](https://github.com/helenjanisz/ATaCR/blob/master/ATaCR_Manual.pdf)\n\nTo install the Matlab version of ATaCR, navigate on the command line to a path where the software will be installed\n\n- Clone the ATaCR repository ([fork](https://docs.github.com/en/github/getting-started-with-github/fork-a-repo) it first, if you consider contributing):\n\n```bash\ngit clone https://github.com/helenjanisz/ATaCR\ncd ATaCR\n```\n\n### Getting and preparing the demo data\n\nDownload the demo data provided on this github repository by navigating from a terminal to some work folder and typing:\n\n```bash\ngit clone https://github.com/MarineSeismologySymposium/SIG-ATaCR\ncd SIG-ATaCR\n```\n\nThe `DATA`, `EVENTS`, and `Matlab_Setup` folders should now be on your computer. Prior to the start of the workshop, move these and the contents of Matlab_Setup into your main ATaCR directory, and run `ATaCRSIG_matlabsetup.m` to set up the file and folder structure appropriately.\n\nYou are ready to go!\n\n---\n\n### Installing the Python version\n\n***Note!! See the video showing how to install `OBStools` [here](https://www.dropbox.com/s/fsxbzvob8o2vekq/MSS2021_SIG_Python_Installation.mp4?dl=0)***\n\nATaCR is implemented as a separate module in the open-source Python package OBStools:\n\n- Git repository: [OBStools](https://github.com/nfsi-canada/OBStools)\n\n- Documentation can be found [here](https://nfsi-canada.github.io/OBStools/)\n\nTo install `obstools`, we strongly recommend installing and creating a `conda` environment (either from the [Anaconda](https://anaconda.org) distribution or [mini-conda](https://docs.conda.io/en/latest/miniconda.html)) where the code can be installed alongside its dependencies. This **significantly reduces** the potential conflicts in package versions. In a bash (or zsh) terminal, follow these steps:\n\n- Create a conda environment (here we call it `mss` for the name of the symposium) and install `python=3.8` and `obspy`:\n\n```bash\nconda create -n mss python=3.8 obspy -c conda-forge\n```\n\n- Activate the environment:\n\n```bash\nconda activate mss\n```\n\n- Install the required [`stdb`](https://github.com/schaefferaj/StDb) package using `pip`:\n\n```bash\npip install stdb\n```\n\nNow you're ready to install `obstools`. You might consider one of two options: 1) you want to look at the source code and are considering contributing (awesome!!); 2) you are only interested in using the software and are not interested in the source code.\n\n##### 1) Developer mode: Installing from source\n\n- Navigate on the command line to a path where the software will be installed\n\n- Clone the OBStools repository ([fork](https://docs.github.com/en/github/getting-started-with-github/fork-a-repo) it first, if you are serious about contributing):\n\n```bash\ngit clone https://github.com/paudetseis/OBStools.git\ncd OBStools\n```\n\n- Install using `pip`:\n\n```bash\npip install -e .\n```\n\n##### 2) User mode: Installing from the Python Package Index (PyPI):\n\n```bash\npip install obstools\n```\n\n### Getting the demo data\n\nFinally, download the demo data provided on this github repository by navigating to some work folder (where the data and results of the processing will be located) and typing:\n\n```bash\ngit clone https://github.com/MarineSeismologySymposium/SIG-ATaCR\ncd SIG-ATaCR\n```\n\nThe `DATA` and `EVENTS` folders should now be on your computer and you are ready to start the tutorial.\n\n### Testing your installation\n\nIf you want to make sure everything is installed properly, make sure your conda environment has been activated and open a python window by typing in a terminal:\n\n```bash\npython\n```\n\nwhich will produce something like:\n\n```bash\nPython 3.8.5 (default, Sep  4 2020, 02:22:02) \n[Clang 10.0.0 ] :: Anaconda, Inc. on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> \n```\n\nThen type:\n\n```bash\n>>> import stdb\n>>> import obstools\n```\n\nIf nothing happens, you're good to go! If you run into a problem, let us know by [raising an issue](https://github.com/MarineSeismologySymposium/SIG-ATaCR/issues). \n\n",
        "createdAt": "2021-02-12T13:39:15.000Z",
        "updatedAt": "2024-02-15T06:04:04.000Z",
        "language": "MATLAB",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/MarineSeismologySymposium/SIG-ATaCR/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "jahearn3/ringseis",
        "url": "https://github.com/jahearn3/ringseis",
        "description": "Functions related to ring seismology: spherical harmonics, interior models, resonance calculation, etc. ",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2022-04-04T18:27:58.000Z",
        "updatedAt": "2022-04-04T18:28:54.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "nwhoffman/DataCamp_seismology",
        "url": "https://github.com/nwhoffman/DataCamp_seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# DataCamp Template Course\n<a href=https://www.datacamp.com//teach/repositories/113355078/go target=\"_blank\"><img src=\"https://s3.amazonaws.com/assets.datacamp.com/img/github/content-engineering-repos/course_button.png\" width=\"150\"></a>\n<a href=https://www.datacamp.com//teach/repositories target=\"_blank\"><img src=\"https://s3.amazonaws.com/assets.datacamp.com/img/github/content-engineering-repos/dashboard_button.png\" width=\"150\"></a>\n\nThis an automatically generated <a href=https://www.datacamp.com target=\"_blank\">DataCamp</a> course. You can start from these template files to create your own course.\n\nChanges you make to this GitHub repository are automatically reflected in the linked DataCamp course. This means that you can enjoy all the advantages of version control, collaboration, issue handling ... of GitHub.\n\n## Workflow\n\n1. Edit the markdown and yml files in this repository. You can\n   * Use DataCamp's <a href=\"https://www.datacamp.com/teach/documentation#tab_teach_editor\">Teach Editor</a>\n   * Use GitHub's online editor\n   * Use <a href=https://git-scm.com/ target=\"_blank\">git</a> locally and push your changes\n2. Check out your build attempts on the <a href=https://www.datacamp.com//teach/repositories target=\"_blank\">Dashboard</a>.\n3. Check out your automatically updated <a href=https://www.datacamp.com/teach/repositories/113355078/go target=\"_blank\">course on DataCamp</a>\n\n## Getting Started\n\nA DataCamp course consists of two types of files:\n\n- `course.yml`, a <a href=http://docs.ansible.com/ansible/YAMLSyntax.html target=\"_blank\">YAML-formatted file</a> that's prepopulated with some general course information.\n- `chapterX.md`, a markdown file with:\n   - a YAML header containing chapter information.\n   - markdown chunks representing DataCamp Exercises.\n\nTo learn more about the structure of a DataCamp course, check out the <a href=https://www.datacamp.com//teach/documentation#tab_course_structure target=\"_blank\">documentation</a>.\n\nEvery DataCamp exercise consists of different parts, read up about them <a href=https://www.datacamp.com//teach/documentation#tab_code_exercises target=\"_blank\">here</a>. A very important part about DataCamp exercises is to provide automated personalized feedback to students. In R, these so-called Submission Correctness Tests (SCTs) are written with the <a href=https://github.com/datacamp/testwhat target=\"_blank\">`testwhat`</a> package. SCTs for Python exercises are coded up with <a href=https://github.com/datacamp/pythonwhat target=\"_blank\">`pythonwhat`</a>. Check out the GitHub repositories' wiki pages for more information and examples.\n\nWant to learn more? Check out the <a href=https://www.datacamp.com//teach/documentation target=\"_blank\">documentation</a> on teaching at DataCamp.\n\n*Happy teaching!*\n",
        "createdAt": "2017-12-06T18:52:10.000Z",
        "updatedAt": "2018-05-14T23:00:37.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/nwhoffman/DataCamp_seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "marcopovitch/sl2influxdb",
        "url": "https://github.com/marcopovitch/sl2influxdb",
        "description": "Fetch seedlink data and store them into InfluxDB",
        "stars": 30,
        "forks": 13,
        "readme": "# Seedlink to InfluxDB\n\nDump seedlink (seismological) time series into [InfluxDB](https://influxdata.com). Use\n[Grafana](http://grafana.org) to plot waveforms, real time latency delay, etc. Maps uses\nthe grafana [worldmap-panel plugin](https://github.com/grafana/worldmap-panel).\n\nDockerfile, docker-compose.yml are available [here](#using-docker).\n\n## Install\n\n```bash\npip install .\n```\n\n## Usage\n\n```bash\n~$ seedlink2influxdb -help\nUsage: seedlink2influxdb [options]\n\nOptions:\n  -h, --help            show this help message and exit\n  --dbserver=DBSERVER   InfluxDB server name\n  --dbport=DBPORT       InfluxDB server port\n  --slserver=SLSERVER   seedlink server name\n  --slport=SLPORT       seedlink server port\n  --fdsnserver=FDSN_SERVER[:PORT]\n                        fdsn station server name\n  --streams=STREAMS     streams to fetch (regexp): [('.*','.*','.*Z','.*')]\n  --flushtime=NUMBER    when to force the data flush to influxdb\n  --db=DBNAME           InfluxDB name to use\n  --dropdb              [WARNING] drop previous database !\n  --keep=NUMBER         how many days to keep data\n  --recover             use seedlink statefile to save/get streams from last\n```\n\nExample :\n\n```bash\nseedlink2influxdb\n    --dbserver localhost \\\n    --dbport 8086 \\\n    --slserver rtserve.resif.fr \\\n    --fdsnserver http://ws.resif.fr \\\n    --db eost2 \\\n    --keep 1\n```\n\n> **Note**\n>\n> Fdsnserver request (`--fdsnserver` option) is optional. It is only used and performed\n> at the begining and *could be slow* (if too much stations info are requested) ! Data\n> collected are only used to get station coordinates and are converted to geohash,\n> needed to plot measurements on a map.\n\n## Screenshot\n\n### Delay/Latency\n\nMap plugin|Geomap plugin\n--------- | ------------\n<img src=\"https://cloud.githubusercontent.com/assets/4367036/22286118/6a4fa65e-e2ee-11e6-93ae-ae1b4f68a7a2.png\" width=\"350\"> | <img src=\"https://user-images.githubusercontent.com/4367036/157223621-29a7d92b-4c7d-40be-8fce-c8a2794e1b8a.png\" width=\"350\">\n\n\n### Dealy Latency board\n\n<img src=\"https://user-images.githubusercontent.com/4367036/157223937-fce074a2-2500-4f63-b5e7-7adb0342d12e.jpg\" width=\"350\">\n\n### Waveform, RMS, latency plots for a given station\n\n<img src=\"https://cloud.githubusercontent.com/assets/4367036/12712707/95e9f498-c8ca-11e5-8115-cabb66dbf692.png\" width=\"350\">\n\n### Traces for multiple stations\n<img src=\"https://user-images.githubusercontent.com/4367036/157225061-275e1f09-5ed6-48bb-95d8-7e7b1ce2f0db.png\" width=\"350\">\n\n\n## InfluxDB\n\nInluxDB data representation (measurements, tags, fields, timestamps).\n\nMeasurements:\n\n* **queue**: internal messages producer queue (seedlink thread) and consumer queue (influxdb exporter thread)\n  * tags\n    * **type**=(consumer|producer)\n  * field\n    * **size**=queue size\n  * timestamp\n* **count** : amplitude in count (waveforms)\n  * tags\n    * **channel** = channel name (eg. FR.WLS.00.HHZ)\n  * field\n    * **value** = amplitude\n  * timestamp\n* **latency** : seedlink packet propagation time from station to datacenter\n  * tags\n    * **channel** = channel name\n  * field\n    * **value** = latency value\n  * timestamp\n* **delay** : time since last seedlink packet was received\n  * tags\n    * **channel** = channel name (eg. FR.WLS.00.HHZ)\n    * **geohash** = station coordinates geohash formated\n  * field\n    * **value** = latency value\n  * timestamp\n\n## Dependencies\n\n* [obspy](https://github.com/obspy/obspy/wiki)\n* [python InfluxDB](https://github.com/influxdata/influxdb-python)\n* [geohash](https://pypi.org/project/python-geohash/)\n* [grafana](http://grafana.org) (with [worldmap-panel plugin](https://github.com/grafana/worldmap-panel))\n\n## Using docker\n\nA `docker-compose` is available to quickly setup influxdb and grafana.\nUse `docker-compose build` to make docker images.\n\n### Data storage\n\nIf you are running this project for the first time, you have to create a\n*influxdb data docker volume* in order to keep your measurements between restarts:\n\n```bash\ndocker volume create --name=sl2influxdb_influxdb_data\n```\n\n### Start services\n\n#### For RaspberryShake\n\nThis configuration is ready to be run, assuming your raspeberryshake is in you local\nnetwork and reachable using *raspberryshake.local* address.\n\nTo start all the containers (influxdb, seedlink fetcher and grafana):\n\n```bash\ndocker-compose up -d rshakegrafana\n```\n\nCheck the logs to see if seedlink data is fetched without problem:\n\n```bash\ndocker-compose logs -f sl2raspberryshake\n```\n\n#### For Generic Seedlink Server\n\nYou need to customize the docker-compose.yml file to set properly this environement\nvariables:\n\n* SEEDLINK_SERVER\n* FDSN_WS_STATION_SERVER\n* SEEDLINK_STREAMS (without space !!)\n\nThen, starts the container:\n\n```bash\ndocker-compose up -d sl2generic\n```\n\nTo check the logs if seedlink data is fetched well:\n\n```bash\ndocker-compose logs -f sl2generic\n```\n\n### Acces to grafana interface\n\n```bash\ndocker-compose up -d grafana\n```\n\nSome time it may be required to wait for grafana to start since some modules will be installed or upgraded.\nHave a look to the log file using :\n\n```bash\ndocker-compose logs -f grafana\n```\n\nWhen upgrading grafana (eg: version 5 to 5.1 or later) it may be necessary to remove  and create again grafana volumes :\n```bash\ndocker volume rm _grafana_volume_name_\ndocker volume create --name=_grafana_volume_name_\n```\nAt least removes `_grafana_volume_name_`, when starting containers `docker-compose` will tell you which volume is missing and it will give you the command line to create it.\n\n\nThen launch you preferred browser and go to\n[http://localhost:3000](http://localhost:3000), with:\n\n* user: admin\n* passwd : admin\n\n### Stop services\n\n```bash\ndocker-compose down -v\n```\n",
        "createdAt": "2016-02-01T07:54:35.000Z",
        "updatedAt": "2024-11-23T21:27:20.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/marcopovitch/sl2influxdb/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "csmcgs/eqseis_container",
        "url": "https://github.com/csmcgs/eqseis_container",
        "description": "Jupyter lab container for the earthquake seismology class",
        "stars": 0,
        "forks": 0,
        "readme": "# Jupyter lab container for the earthquake seismology class\n\nBuild using:\n\n```console\n$ docker build -t csmcgs/eqseis:2022 .\n```\n",
        "createdAt": "2022-11-27T13:52:45.000Z",
        "updatedAt": "2022-11-27T13:53:22.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/csmcgs/eqseis_container/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "akuhara/SEIS_FILO",
        "url": "https://github.com/akuhara/SEIS_FILO",
        "description": "SEISmological transdimensional inversion tools for Flat and Isotropic Layered structure in the Ocean",
        "stars": 31,
        "forks": 11,
        "readme": "# SEIS_FILO \n\n__SEISmological transdimensional inversion tools for Flat and Isotropic Layered structures in the Ocean__ \n\n![LOGO](./img/SEIS_FILO_LOGO.png)\n\n[![Build Status](https://app.travis-ci.com/akuhara/SEIS_FILO.svg?branch=main)](https://app.travis-ci.com/akuhara/SEIS_FILO)\n[![codecov](https://codecov.io/gh/akuhara/SEIS_FILO/branch/main/graph/badge.svg?token=97D1SQ2VAV)](https://codecov.io/gh/akuhara/SEIS_FILO)\n![GitHub](https://img.shields.io/github/license/akuhara/SEIS_FILO)\n![Docker Cloud Build Status](https://img.shields.io/docker/cloud/build/akuhara/seis-filo)\n[![Documentation Status](https://readthedocs.org/projects/seis-filo/badge/?version=latest)](https://seis-filo.readthedocs.io/en/latest/?badge=latest)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4082670.svg)](https://doi.org/10.5281/zenodo.4082670)\n\nCopyright (C) 2019-2023 __Takeshi Akuhara__[![ORCID](https://orcid.org/sites/default/files/images/orcid_16x16.png)](https://orcid.org/0000-0002-6129-8459)\n\n___\n\n \n\nThe SEIS_FILO program package aims to carry out transdimensional joint inversion of surface waves and receiver functions for ocean-bottom observatories. The main features are: \n\n* __Transdimensional MCMC__\n* __Parallel tempering__\n* __Ocean layer__\n* __Supported input types__\n    * Dispersion curves of the fundamental and higher mode Rayleigh waves\n    * Rayleigh wave ellipticity\n    * Rayleigh wave admittance\n    * P receiver functions (or Green's functions)\n    * S receiver functions (or Green's functions)\n* __Model parameters__\n    * Absolute Vp & Vs\n    * Vp and Vs anomalies relative to the reference\n    * Number of layers\n    * Layer depths\n    * Standard deviation of noise\n\n---\n\n## Requirements\n* [FFTW library](http://fftw.org/)\n* [LAPACK library](http://www.netlib.org/lapack/)\n* [Open MPI](https://www.open-mpi.org/)\n* Some Python modules listed in [requirements.txt](https://github.com/akuhara/SEIS_FILO/blob/main/requirements.txt)\n\n  \n## Install\nType `make` in the `src` directory. Please edit the [Makefile](https://github.com/akuhara/SEIS_FILO/tree/main/src/Makefile) in accordance with your environment (i.e., compiler type and libarary paths). \n\n\n## Quick Guidance\n### Forward problem\n* [__disper_fwd__](https://github.com/akuhara/SEIS_FILO/tree/main/sample/disper_fwd): Surface wave forward computation (phase and group velocities, ellipticity and admittance)\n* [__recv_func_fwd__](https://github.com/akuhara/SEIS_FILO/tree/main/sample/recv_func_fwd): Receiver function forward computation\n\n### Inverse problem\n* [__joint_inv__](https://github.com/akuhara/SEIS_FILO/tree/main/sample/joint_inv): Surface wave and receiver function joint inversion by RJMCMC\n\nSee [online documentation](https://seis-filo.readthedocs.io/) for more details.\n\n## Publications\n\n### Rayleigh wave dispersion curve and P receiver functions\n* Akuhara, T., Yamashita, Y., Ohyanagi, S., Sawaki, Y., Yamada, T., & Shinohara, M. (2023). Shallow Low-Velocity Layer in the Hyuga-Nada Accretionary Prism and Its Hydrological Implications: Insights From a Passive Seismic Array. Journal of Geophysical Research: Solid Earth, 128(4). https://doi.org/10.1029/2022JB026298\n\n### Rayleigh wave dispersion curve, ellipticity, and S receiver functions\n* Ai, S., Akuhara, T., Morishige, M., Yoshizawa, K., Shinohara, M., & Nakahigashi, K. (2023). Layered Evolution of the Oceanic Lithosphere Beneath the Japan Basin, the Sea of Japan. Journal of Geophysical Research: Solid Earth, 128(2). https://doi.org/10.1029/2022JB025581\n\n### Multimode Rayleigh wave dispersion curves\n* Yamaya, L., Mochizuki, K., Akuhara, T., Nishida, K. (2021). Sedimentary structure derived from multi-mode ambient noise tomography with dense OBS network at the Japan Trench. _Journal of Geophysical Research: Solid Earth_, 126(6), e2021JB021789. https://doi.org/10.1029/2021JB021789\n\n### S receiver functions\n* Akuhara, T., Nakahigashi, K., Shinohara, M., Yamada, T., Shiobara, H., Yamashita, Y., et al. (2021). Lithosphere–asthenosphere boundary beneath the Sea of Japan from transdimensional inversion of S-receiver functions. Earth, Planets and Space, 73(1), 171. https://doi.org/10.1186/s40623-021-01501-5\n\n___\n\n\n",
        "createdAt": "2019-11-21T02:35:37.000Z",
        "updatedAt": "2025-09-19T14:39:34.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.4082670",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.4082670",
            "dataCite": "10.5281/zenodo.4082670",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/akuhara/SEIS_FILO/main/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.4082670",
            "title": "SEIS_FILO",
            "journal": "Zenodo",
            "dateReleased": "2022-03-05T00:00:00.000Z",
            "abstract": "<strong>SEISmological transdimensional inversion tools for Flat and Isotropic Layered structures in the Ocean</strong> The SEIS_FILO program package aims to carry out transdimensional joint inversion of surface waves and receiver functions for ocean-bottom observatories. The main features are: <strong>Transdimensional MCMC</strong> <strong>Parallel tempering</strong> <strong>Ocean layer</strong> <strong>Supported input types</strong> Dispersion curves of the fundamental and higher mode Rayleigh waves Rayleigh wave ellipticity P receiver functions S receiver functions <strong>Model parameters</strong> Absolute Vp &amp; Vs Vp and Vs anomalies relative to the reference Number of layers Layer depths Standard deviation of noise",
            "citationsArray": []
        },
        "publications": [
            {
                "doi": "10.1186/s40623-021-01501-5",
                "name": "Lithosphere–asthenosphere boundary beneath the Sea of Japan from transdimensional inversion of S-receiver functions",
                "source": "Zenodo",
                "authorNames": [
                    "Akuhara, Takeshi",
                    "Nakahigashi, Kazuo",
                    "Shinohara, Masanao",
                    "Yamada, Tomoaki",
                    "Shiobara, Hajime",
                    "Yamashita, Yusuke",
                    "Mochizuki, Kimihiro",
                    "Uehira, Kenji"
                ],
                "url": [
                    null,
                    "http://doi.org/10.1186/s40623-021-01501-5",
                    "http://ui.adsabs.harvard.edu/#abs/2021EP&S...73..171A"
                ]
            }
        ]
    },
    {
        "source": "GitHub",
        "name": "flysamc/seismology_lab",
        "url": "https://github.com/flysamc/seismology_lab",
        "description": "college seismology lab codes",
        "stars": 0,
        "forks": 0,
        "readme": "# seismology_lab\n",
        "createdAt": "2017-03-01T17:19:24.000Z",
        "updatedAt": "2025-10-09T19:01:09.000Z",
        "language": "Shell",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/flysamc/seismology_lab/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "hemmelig/2022GC010564",
        "url": "https://github.com/hemmelig/2022GC010564",
        "description": "Repository of data and code to accompany Bacon et al., 2022 (DOI: 10.1029/2022GC010564)",
        "stars": 2,
        "forks": 1,
        "readme": "# Supplementary analysis/visualisation code\nData and code to accompany:\n\nBacon, C. A., Rawlinson, N., Pilia, S., Gilligan, A., Wehner, D., Cornwell, D. G., & Tongkul, F. (2022). The signature of lithospheric anisotropy at post-subduction continental margins: New insight from XKS splitting analysis in northern Borneo. Geochemistry, Geophysics, Geosystems, 23, e2022GC010564. \n\nOpen-access paper: [![DOI](https://img.shields.io/badge/GGG-10.1029/2022GC010564-blue)](https://doi.org/10.1029/2022GC010564)\n\nSupplementary datafiles: [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6461787.svg)](https://doi.org/10.5281/zenodo.6461787)\n\nAnalysis and visualisation: [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6480581.svg)](https://doi.org/10.5281/zenodo.6480581)\n\n## Steps to reproduce figures\n1. Clone this repository and navigate to it, e.g.:\n\n```\ngit clone https://github.com/hemmelig/2022GC010564\ncd 2022GC010564\n```\n\n2. Install the packages listed in the environment.yml file—either manually, or using (for example) conda—then activate the new environment:\n\n```\nconda env create\nconda activate Bacon2022_2022GC010564\n```\n\n3. Add `2022GC010564.mplstyle` (a matplotlib stylesheet) to your `mpl_configdir` (usually found at `~/.config/matplotlib/stylelib` or `~/.matplotlib/stylelib`)\n\n4. Optional: Install Helvetica font for Matplotlib\n\n5. Move into the `figures` directory and run `download_grd_datafiles.gmt` script to download the DEM data used in Figure 1:\n\n```\ncd figures\nbash download_grd_datafiles.gmt\n```\n\n6. Navigate to each figure directory and run the `.gmt` (as `bash <script>.gmt`) or `.py` (as `python <script>.py`) scripts.\n\n## Notes\nThese figures were prepared using Linux 20.04. A limited number were produced using Affinity Designer for iPad, a licensed piece of software for graphic design. `.afdesign` files are provided.\n\nFor Supplementary Figure S6, you will need `AnisotroPy`, an open-source package for the analysis of seismic anisotropy. This can be downloaded from GitHub at https://github.com/hemmelig/AnisotroPy, where you will also find instructions on how to install the package.\n",
        "createdAt": "2022-04-24T00:08:26.000Z",
        "updatedAt": "2022-11-16T13:56:07.000Z",
        "language": "Jupyter Notebook",
        "homepage": "https://doi.org/10.1029/2022GC010564",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "10.1029/2022GC010564)",
            "openCitations": "10.5281/zenodo.6461787",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/hemmelig/2022GC010564/main/README.md",
        "mainPaper": {
            "doi": "10.1029/2022GC010564)",
            "title": "",
            "journal": "",
            "dateReleased": null,
            "abstract": "",
            "citationsArray": []
        },
        "repoDoi": "10.1029/2022GC010564)",
        "publications": [
            {
                "doi": "10.1029/2022GC010564)",
                "name": "",
                "source": "",
                "authorNames": [],
                "abstract": "",
                "publicationDate": null,
                "journal": ""
            }
        ]
    },
    {
        "source": "GitHub",
        "name": "chongjia7/Seismology",
        "url": "https://github.com/chongjia7/Seismology",
        "description": "Source Mechanism Inversion of Induced Seismic Activity around Central Oklahoma, U.S.A. (Tools: Python, Obspy,  gCAP & FK package, SAC Processing tools))",
        "stars": 2,
        "forks": 0,
        "readme": "",
        "createdAt": "2018-07-04T07:37:40.000Z",
        "updatedAt": "2021-07-09T04:57:55.000Z",
        "language": null,
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "nimanzik/pocketseis",
        "url": "https://github.com/nimanzik/pocketseis",
        "description": "Python modules, scripts and utilities to facilitate repetitive tasks in seismological projects",
        "stars": 2,
        "forks": 0,
        "readme": "# PocketSeis\n\n[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)\n[![Python](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)\n\n**PocketSeis** is a Python toolkit for seismological analysis and computation. It provides a collection of utilities and functions to facilitate common tasks in seismological research, including moment tensor operations, coordinate transformations, waveform processing, and specialized plotting capabilities.\n\n## Installation\n\n```bash\ngit clone https://github.com/nimanzik/pocketseis.git\ncd pocketseis\npip install .\n```\n\nFor development installation:\n\n```bash\npip install --editable .\n```\n\n## License\n\nThis project is licensed under the GNU General Public License v3.0 - see the [LICENSE](LICENSE) file for details.\n\n## Author\n\n**Nima Nooshiri** - [nima.nooshiri@gmail.com](mailto:nima.nooshiri@gmail.com)\n\n## Acknowledgments\n\nPocketSeis builds upon the excellent [Pyrocko](https://pyrocko.org/) framework and integrates with the broader Python scientific computing ecosystem.\n",
        "createdAt": "2021-11-17T12:43:05.000Z",
        "updatedAt": "2025-11-14T07:35:39.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/nimanzik/pocketseis/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "nahuelmol/japan_data_hinet",
        "url": "https://github.com/nahuelmol/japan_data_hinet",
        "description": "CMD tool for seismological data analysis. Utilizing Japan's HI NET database",
        "stars": 0,
        "forks": 0,
        "readme": "## Japan Data Hi-Net\n\n<div>\n  <img src=\"https://img.shields.io/github/last-commit/nahuelmol/japan_data_hinet\"/>\n  <img src=\"https://img.shields.io/github/languages/code-size/nahuelmol/japan_data_hinet\"/>\n  <img src=\"https://img.shields.io/github/languages/top/nahuelmol/japan_data_hinet\"/>\n    <img src=\"https://img.shields.io/github/languages/count/nahuelmol/japan_data_hinet\"/>\n</div>\n\nContent: most of the documentation is tentative. It's not built yet but it will be soon.\nI should mention that data source is K-NET (Kyoshin Network) in specific.\n\n## Index\n\n1.  [Plan](#Plan)\n3.  [Extracting Data](#Extracting-data)\n4.  [Script cleaner process: from raw data](#Script-cleaner-process:-from-raw-data)\n5.  [Proceeding](#Proceeding)\n6.  [CMD tool](#CMD-tool)\n\n    5.1 [Asking](#Asking)\n    \n    5.2 [Checking](#Checking)\n    \n    5.3 [Display](#Display)\n\n     5.3.1 [Waves](#Waves)\n    \n     5.3.2 [Maps](#Maps)\n    \n8.  [Seeding](#Seeding) \n9.  [Building](#Building)       \n\n### Plan\nThe following stages are established in order to perform a proper data analysis;\n\n* Data collection\n* Storage\n* Cleanup\n* Transformation\n* Analytics\n* Insights\n\nData collection, storage, cleanup, and transformation range from the direct handling of data to its shaping, being finally processed and analized. The final part is focused on understanding and giving a meaning to the raw data.\n\nThe main analytical method is Principal Component Analysis (PCA). This approach makes it possible to identify patterns within the large amount of information provided.\n\n### Extracting data\nFirst, data is available in the NIED site's database. Specificaly, the K-NET (Kyoshin Network) will be our great provider on this journey. Look at: https://www.bosai.go.jp/e/research/database/earthquake.html\n\nThen, I choose the K-NET tool. Once an earthquake is selected, I have two options\n* \"Download All Channels Data\"\n* \"Download All Data\" (ASCII or binary)\nThe second option in ASCII is the choosen one.\n\n### Script cleaner process: from raw data\nFiles will be in ASCII or binary format, then I must check and process them in such a way to get them ready for storage in the database.\n\nFirst, we select an earthquake. In our case, the well known March-11-2011 Tohoku eartquake. \n\n![March-11](capture1.png)\n\nThe browser will download a tar file which contains our data. The following is just decomprise it.\nNIED gives us, a bunch of data, placed in different files. Per station there will a file for each components; UD (up-down), the NS(north-south) and EW(east-west). It's possible to identify what we have by the name of the file.(capture2.png)\n\n![March-11](capture2.png)\n\nIn our example, you can see (capture1.png) that there are 1.227 sites, 1.227 stations and each one contains UD, NS and EW wave components as time series, along with other information.\nUsing an .UD file, we can see (capture3.png) how the ASCII file looks like, the called K-NET ASCII file. It starts with 17 lines of metadata and from the 18 line a time-series begins which represents the wave in its UD component.\n\n![March-11](capture3.png)\n\n### Proceeding\nThere will be a table that contains the studied earthquakes and its most important metadata.\n\n* Geiyo Islands -> March-24 (2001)\n* Chuetsu -> October-23 (2004)\n* Chuetsu -> July-16 (2007)\n* Iwate-Miyagi -> June-14 (2008)\n* Miyagi -> April-7 (2011)\n* Tokyo -> March-11 (2011)\n* Fukushima -> April-17 (2013)\n* Fukushima -> April-19 (2013)\n* Hokkaido -> September-6 (2018)\n* Fukushima -> Febraury-13 (2021)\n* Noto -> January-1 (2024)\n\nAt the beginning, are considered; depth, magnitude, prefecture, year, etc. This will allow an accurate control over data, paying attention to its integrity.\n\nThe best is to include more and more earthquakes. This process will be done using a c++ software for examining each file through a command line tool. This allows scalability, and possibly continuos integration in a future server-type project.\n\nFrom each time-series (UD at first) we can get:\n* Mean and peak values\n* Some type of pattern durgin the shaking like, decaing, attenuance, code, aparent velocity (Cx) role, surface waves presence\n\nOther table to include is the prefecture/subprefecture-site name. Each site (or station) has an specific code-name. Then it will be created a table containing those data for an easier querying cycle.\n\n|  prefecture  |  prefecture-code  |  siten  | \n|  ----  |  ----  |----  \n|  Miyagi  |  MYG  |  1  |\n|  Fukushima  |  FKS  |  1  |\n\nThe number of sites will inscrease as long as a site is added.\n\n## CMD tool\nThe main goal is the building of a good CMD tool for managing the constant flow to our database, modifying tables, columns and records. \n\nThe commands listed here are just previsualizations, they are not built yet. The name of the tool is SS.\n\n### Asking\n\nASK command will give us data about an specific area/zone being a prefecture, subprefecture, etc. \n\nThe following command will show us the number of sites within an specific prefecture.\n```\nSS ask -p Miyagi sitesn\n```\n\nThis one, returns data about an specific site if exists or ERR if not.\n```\nSS ask -st MYG002\n```\n\nThis print out all earthquakes registered in the database\n```\nSS ask eqs\n```\n\nThis add a new earthquake. The format is year-month-day-hour-minutes-seconds without dash.\n```\nSS add 20110311144600\n```\nFor this is neccesary to have the .knt folder already unzipped in the \"data\" directory.\n\n### Checking\n\nCHK command works like ASK, it returns data focused on a site. The following gives us which earthquakes were registered by MYG002 as a list ordered by their years. Each earthquake has a unique name, being the complete origin time (from year to second).\n\n```\nSS check MYG002 -years\n```\nThis shows a table of two columns, the earthquake name in the first and the requested property on second column. If there's not a property specified then it just returns names. Besides years, can be specified:\n\n* magnitude\n* lat\n* lon\n\n### Display\n\n### Maps\n\nUsing bitmap images it is possible to create an image of Japan and over it paint circles at each site's position and colour it with tone following a value. For example, the max acceleration can be taken. \n\nFirst a range of values is established, from minimum to maximum. Later, following the RGB spectra, a given value of acceleartion is translated to a color. Then a map is of values is printed out. The next step could be to paint a map of Japan below the point cloud.\n\nThe idea is to replicate what the NIED sied does.\n\nValues used for the mentioned map are scalars extracted from the header of the site ASCII file. However, what matters is getting new values from the wave. Then is needed to add that functionality.\n\n### Waves\nIt is just a wave plotter. A station and a particular earthquake should be specified. The following command uses the March 11 earthquake wave recorded by MYG002 is used.\n\n```\nSS print MYG002 M11-2011\n```\n\nGNUplot may possibly be used to generate the time series plots. It is a command line base tool that takes pre shaped data as an input and produces a plot.\n\n### Building\n\nBefore the data analysis project begins, structure must be ready. This means, tables should exists previous to its usage. On tables.cpp are specified global tables like earthquakes and stations, and their fields can be modified there. Once they're ready, the following command starts the building process on database. \n\n```\nSS db:build\n```\n\nIn the same script, any other table considered global can be included.\n\nThe mentioned global earthquake table could be used to build stations tables for each earthquake. Then, it's mandatory to have earthquakes table built and seeded. Being those requirements satisfied, type the following command:\n\n```\nSS db:build:eqs\n```\n\nEach table produced by this command contains information about an specific earthquake, and its effect on each station. From this, it is easier to build Japan's max acceleration distribution map for each events.\n\n### Seeding\n\nAll data living in our data directory can be saved into the database. It is important to keep in mind how data is organized as tables. An initial aproach follows that:\n* Each earthquake will have its own sites table (having just station activity specifically for that earthquake).\n* There will be a global station table, just as a register. This can be discarded. However, each station can have its own group of earthquakes that were detected by that station. Then it's mandatory to have them as a unique record asociated to different earthquakes on different times.\n\nThe command for this process is:\n```\nSS db:seed\n```\n\n### Db:tasks\n\nTasks are basic directives that gives specific information about the database. The following displays all earthquakes related to a specific station.\n\n```\nSS db:stats -story <statname>\n```\n\n\n### Building\n\nThis simple command will compile the code using Makefile\n\n```\nmingw32-make\n```\n\n",
        "createdAt": "2025-05-07T00:46:31.000Z",
        "updatedAt": "2025-10-04T20:46:42.000Z",
        "language": "C++",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/nahuelmol/japan_data_hinet/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "adjiyusuf/Seismology",
        "url": "https://github.com/adjiyusuf/Seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2024-06-18T04:40:01.000Z",
        "updatedAt": "2024-06-20T10:25:56.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "eqcorrscan/RT_EQcorrscan",
        "url": "https://github.com/eqcorrscan/RT_EQcorrscan",
        "description": "Real-Time implementation of EQcorrscan methods.",
        "stars": 14,
        "forks": 6,
        "readme": "# RT EQcorrscan\n## Real-time implementation of EQcorrscan's matched-filter earthquake detection\n\n[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)\n![test](https://github.com/eqcorrscan/RT_EQcorrscan/workflows/test/badge.svg?branch=master)\n[![codecov](https://codecov.io/gh/eqcorrscan/RT_EQcorrscan/branch/master/graph/badge.svg)](https://codecov.io/gh/eqcorrscan/RT_EQcorrscan)\n[![Documentation Status](https://readthedocs.org/projects/rt-eqcorrscan/badge/?version=latest)](https://rt-eqcorrscan.readthedocs.io/en/latest/?badge=latest)\n[![Python 3.6+](https://img.shields.io/badge/python-3.6+-blue.svg)](https://www.python.org/downloads/release/python-360/)  \n\n# Installation\n\n[![Anaconda-Server Badge](https://anaconda.org/conda-forge/rt-eqcorrscan/badges/installer/conda.svg)](https://conda.anaconda.org/conda-forge)\n\nEither install EQcorrscan from conda-forge (recommended) using something like:\n```bash\nconda install -c conda-forge RT-EQcorrscan\n```\n\nor from pypi using something like:\n```bash\npip install RT-EQcorrscan\n```\n\nor from source by cloning this repository and running pip install:\n```bash\ngit clone https://github.com/eqcorrscan/RT_EQcorrscan.git\ncd RT_EQcorrscan\npip install .  # This should install the required dependencies.\n```\n\n# Usage\n\nHave a peruse of the [docs](https://rt-eqcorrscan.readthedocs.io/en/latest/)\nto see more information on how to use RT-EQcorrscan. \n\nFeel free to edit the source code and add/change how RT-EQcorrscan works to make it fit\nyour application.  If you find any bugs, or have a feature that you want to add, then\nplease do! It would be really valuable to make this project better! See the section \nbelow on contributing.\n\n# Contributing\n\nRT_EQcorrscan is ready to play with, but don't expect it to be stable yet. If\nyou have any contributions they would be appreciated! Please fork the repository\nand create a pull-request on Master.\n\n# Funding\n\nThe creation of this project was funded by the Earthquake Commission of New Zealand (EQC).\nCurrently the maintainence of this project is unfunded.\n\n# Citation\n\nIf you use our software in your research please cite \n[our paper on the RT-EQcorrscan package](https://pubs.geoscienceworld.org/ssa/srl/article/doi/10.1785/0220200171/590814/RT-EQcorrscan-Near-Real-Time-Matched-Filtering-for).\nThese citations help to keep the developers in work and keep maintaining these software!\n\n> Chamberlain, C. J., J. Townend, and M. C. Gerstenberger (2020). \n> RT-EQcorrscan: Near-Real-Time Matched-Filtering for Rapid Development\n> of Dense Earthquake Catalogs, Seismol. Res. Lett. XX, 1–11, \n> doi: 10.1785/0220200171.\n\n# Funding\n\n![RCET](docs/images/RCET_logo_transparent.png)\n\nContinued development of the RT-EQcorrscan package is directly supported by the \n[RCET](https://www.rcet.science/), Rapid Characterisation of Earthquakes and Tsunami\nprogramme funded by the New Zealand Ministry of Business, Innovation and Employment\nEndeavour fund.",
        "createdAt": "2018-04-06T03:53:22.000Z",
        "updatedAt": "2025-10-06T20:46:39.000Z",
        "language": "C++",
        "homepage": "https://rt-eqcorrscan.readthedocs.io/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/eqcorrscan/RT_EQcorrscan/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "sstaehler/dailyspec",
        "url": "https://github.com/sstaehler/dailyspec",
        "description": "Plot clean spectrograms of seismic time series",
        "stars": 12,
        "forks": 6,
        "readme": "# dailyspec\n\n[![DOI](https://zenodo.org/badge/449205341.svg)](https://zenodo.org/badge/latestdoi/449205341)\n\nPlot clean spectrograms of longer seismic time series\n\n## Installation:\n\nAs usual, it's easiest to use anaconda. If you already have an environment with the typical seismology packages\n\n```\n    matplotlib\n    numpy\n    obspy \n    python \n    scipy\n    cartopy\n```\n\ninstalled, you're set.\nOtherwise, create yourself an environment with\n\n```shell\nconda env create -f dailyspec.yml\nconda activate dailyspec\npip install -e .\n```\n\n## Examples:\n\n### Single seismogram\n\nThis command\n\n    python -m dailyspec.plot_spec \\\n    --w0 24 -d ./S1022a/XB.ELYSE.02.BHZ \\       \n    --winlen 500  --fmax 10 --plot_ratio 0.6  --kind cwt \\  \n    --tstart 2022-05-04T22:00 --tend 2022-05-05T08:00 -h\n\nwill give you the spectrogram of the S1222a magnitude 5 marsquake, which was featured in NASA's press release on the\nevent.\n![spectrogram of a marsquake](https://mars.nasa.gov/system/news_items/main_images/9185_1-PIA25044-web.jpg)\nhttps://mars.nasa.gov/news/9185/nasas-insight-records-monster-quake-on-mars/?site=insight\n\n(the seismogram data will be released on October 1st 2022)\n\n### Whole directory\n\nDailyspec can also process a full directory of waveform data that is in a SeisComp3 directory structure and produce a\nsimple web catalog from it.\n\n```shell\npython -m dailyspec.process_dir \n       -d /data/sc3data/op/data/waveform/  # Path to SC3 directory with waveforms\n       -c events.xml                       # Path to QuakeML file with events to mark\n       -i inventory.xml                    # StationXML file \n       --year 2020                         # Restrict to a given year\n       --jday_start 1 --jday_end 30        # Restrict to days of year\n       -l channels.txt                     # List of SEED channel IDs\n       --fmax 10                           # Maximum frequency for HF plot\n       --winlen 3600                       # Window length for LF plot (1/fmin)\n       --kind cwt                          # Use spectrogram (fast) or CWT (slower)\n       --dBmin -70 --dBmax -10             # Dynamic range of colorscale\n       -r 0.8                              # Percentage of plot space for the LF plot\n```\n\nThis produces 2 directories `by_channel` and `by_day`, in which you find spectrograms for each channel on each day\nsorted,\nwith a HTML file to parse them quickly.\n\n### Add event markers\n\nYou can make the spectrogram easier to interpret by adding markers for known earthquakes. For that, downlad a QuakeML\nfile first and hand it to `dailyspec.process_dir` or `dailyspec.plot_spec` with argument `-c`.\n\nDownload a QuakeML file with minimum magnitude increasing in circles around a given station (or lat/lon pair) using\n`dailyspec.get_events`\n\n```shell\npython -m dailyspec.get_events \n       -s CH.HASLI..HHZ          # Channel snippet to center event set around\n       -i swiss_station/CH.xml   # StationXML file to get channel lat/lon from\n       --tstart 20220101         # Start of catalog\n       --tend 20221231           # End of catalog\n       --dists 5 30 180          # Maximum distance of bins, here 5°, 30° and 180°\n       --min_mags 1.5 5.0 7.0    # Download all events >1.5 until 5°, all >5 until 30° and all >7 anywhere\n```\n\nThis produces a file events.xml, which you can pass to the plotting codes.",
        "createdAt": "2022-01-18T08:41:57.000Z",
        "updatedAt": "2025-08-21T06:43:32.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/sstaehler/dailyspec/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "realcodestudio/Seismic-Stations-Viewer",
        "url": "https://github.com/realcodestudio/Seismic-Stations-Viewer",
        "description": "A seismic stations viewer for Wolfx SeisJS API",
        "stars": 12,
        "forks": 2,
        "readme": "<div align=\"center\">\n  <image width=\"256em\" src=\"https://github.com/user-attachments/assets/84003231-98c7-4b10-a761-f5bcf01a3adb\" />\n </div>\n<h1 align=\"center\">Wolfx SSV 🌏</h1>\n    <h4 align=\"center\">Seismic Stations Viewer </h4><br>\n\n<h3 align=\"center\">Seismic Stations Viewer is a project that uses the Wolfx SeisJS API. The project allows you to easily view data from stations connected to the Wolfx SeisJS API.</h3>\n\n<h4 align=\"center\">\n<a href=https://wolfx.jp>To Wolfx</a> |\n<a href=https://x.com/realcodestudio>Follow @RCBS</a> |\n<a href=zh.md>简体字中国语</a> |\n<a href=zht.md>繁體字中國語</a> |  \n<a href=ja.md>日本語</a>\n\n<div align=\"center\">\n<h4 align=\"center\"> 🚨 Wolfx SeisJS is an open-source earthquake detection and monitoring application developed by the <a href=https://github.com/WolfxProject>Wolfx Project</a>, a nonprofit organization. </h3>\n\n </div>\n<div align=\"center\">\n  <image src=\"https://github.com/user-attachments/assets/8bf5b723-5976-4c0a-895a-56ce34550f38\" />\n</div>\n\n## Demo Sites\n\n- https://wolfx.jp/ssv\n- https://ssv.bousai.cn\n\n## How to run?\n\n- Download the codes from Releases\n- Open in VSCode\n- Install “Vite” Extension.(optional)\n- Run these cmds in the terminal in turn.\n  > `cd Seismic-Stations-Viewer`<br> > `npm install -g pnpm`<br> > `pnpm install`<br> > `pnpm run dev`<br>\n\n<br>\n \n## How to build?\n- Open in VSCode\n- Run these cmds in the terminal in turn.\n  > `cd Seismic-Stations-Viewer`<br>\n  > `pnpm install`<br>\n  > `pnpm run build`<br>\n",
        "createdAt": "2024-11-16T00:39:45.000Z",
        "updatedAt": "2025-09-12T14:31:04.000Z",
        "language": "Vue",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/realcodestudio/Seismic-Stations-Viewer/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ShiroHirano/StochasticSTF",
        "url": "https://github.com/ShiroHirano/StochasticSTF",
        "description": "fortran90+, julia, matlab, and python function to generate stochastic source time functions proposed in seismology.",
        "stars": 0,
        "forks": 0,
        "readme": "# StochasticSTF\n\n<address><a href=\"https://interfacial.jp/\">Shiro Hirano</a></address>\n\n## Summary\n\nIn fortran90+, julia, matlab, and python code, \n```\nx = StochasticSTF(n)\n```\nreturns a stochastic Source Time Function (STF) of length `n` that satisfies the following properties:\n+ The STF starts from and terminates at zero (i.e., `x(1) = x(n) = 0`).\n+ The STF amplitude is non-negative.\n+ The Fourier amplitude spectrum follows $\\omega^{-2}$-model.\n+ The moment function $M_0(t) = \\displaystyle\\int_0^t \\textrm{STF}(s) \\ ds$ is propotional to $t^3$.\n\nRunning\n```\nx = StochasticSTF(n, r)\n```\nwith a floating-point number `r` $(> 1.0)$ results in more complicated STFs as in the following figure.\n\n![img/SSTFs.png](img/SSTFs.png)\n\n## Arguments and behavior\n\nThe function `StochasticSTF(n,r,d)` returns a stochastic Source Time Function (STF) of length `n`, where `r` and `d` are optional paramters.\nThe floating-point number `r` $(\\ge 1.0)$ is the ratio of two corner frequencies and determines roughness of the STF, where the default value (`r=1.0`) results in STF with the $\\omega^{-2}$-type spectrum, and STFs get rougher as `r` increases.\nInteger `d` $(\\ge 2)$, dimension of the Bessel bridge, is optional; the default value is `d=2`, and STFs get smoother as `d` increases.\nThe result is normalized so that `sum(STF) = 1.0` holds.\nThe algorithm has been modified after Hirano(2022; 2023).\nThis code generates STFs with arbitrary lengths by using Bessel bridges, while the original model has probabilistic lengths following the Gutenberg-Richter law.\n\n## References\n+ Hirano, S. (2022), \"Source time functions of earthquakes based on a stochastic differential equation\", Scientific Reports, 12:3936, [https://doi.org/10.1038/s41598-022-07873-2](https://doi.org/10.1038/s41598-022-07873-2)\n+ Hirano, S. (2023), \"Stochastic source time functions with double corner frequencies\", AGU23 Fall Meeting, S13F-0407, [https://agu.confex.com/agu/fm23/meetingapp.cgi/Paper/1299761](https://agu.confex.com/agu/fm23/meetingapp.cgi/Paper/1299761)\n\n\n## Fortran90+: Usage and Compilation\n\nSimply after compiling `m_stochasticSTF.f90`, the function `StochasticSTF(n,r)` will be callable.\n\nFor example, \n```bash\ngfortran m_stochasticSTF.f90 main.f90\n./a.out\n```\nresults in `STF.svg`, and the second column therein is the amplitude of a stochastic source time function. \nIgnore the first and last line of the file to get numerics.\nSee comments in the source codes for usage.\n\nTo specity precision, see the comments in `m_stochasticSTF.f90`.\n\n## Julia, Matlab, python: Usage\n\nRunning `main.jl`, `main.m`, or `main.py` calculates and plots a stochastic STF.\n\nLinearAlgebra, DSP, and Plots are required for julia.\n",
        "createdAt": "2024-06-02T13:02:07.000Z",
        "updatedAt": "2025-08-27T10:53:22.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ShiroHirano/StochasticSTF/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "QuentinBrissaud/AIDE",
        "url": "https://github.com/QuentinBrissaud/AIDE",
        "description": "AIDE allows you to scan vTEC records to detect co-seismic ionospheric perturbations. AIDE software consists of a Machine-Learning based detector, arrival-time picker, and an associator across GNSS satellite networks.",
        "stars": 7,
        "forks": 2,
        "readme": "# Artificial intelligence for Ionospheric perturbations DEtections (AIDE)\n\n## Summary\nAIDE allows you to scan vTEC records to detect co-seismic ionospheric perturbations. AIDE software consists of a Machine-Learning based detector, arrival-time picker, and an associator across GNSS satellite networks. \n<p align=\"center\">\n<img width=\"421\" alt=\"image\" src=\"https://user-images.githubusercontent.com/6717390/230471273-a553ba77-6860-4752-8100-3eb6956b4a40.png\" />\n</p>\nFigure: (from Brissaud and Astafyeva, 2022) (d) hand-picked arrival times for satellites G05 and G26 along with the epicentre location\n(yellow star), and surface projection of the fault slip (in m) as green to yellow patches, (e) RF-based arrival-time predictions for each confirmed detection\nfor satellites G05, G26 and G27 with an inset plot showing a newly detected CID arrival (red vertical line) for satellite G27 and station 0167 which was not\nreported by human analyst and (f) association classes determined from confirmed detections, along with an inset plot showing the vTEC data for satellite G26,\nstation 0155.\n\n## Requirements\n- Python3.7.9\n- Install AIDE in new conda environment with requirements.txt\n\n## Usage\nSee Python notebook \"AIDE_nb.ipynb\"\n\n## Data\nYou can collect the vTEC data and random forest models here: https://doi.org/10.6084/m9.figshare.19661115\n\n## Known issues\n- The installation of Basemap can be challenging due to GEOS-related errors: https://github.com/matplotlib/basemap/issues/437. No clear fix yet. This only affects the plotting using routines in \"utils_paper.py\"\n\n## Paper\nTsunamis generated by large earthquake-induced displacements of the ocean floor can lead to tragic consequences for coastal communities. Ionospheric measurements of Co-Seismic Disturbances (CIDs) offer a unique solution to characterize an earthquake’s tsunami potential in Near-Real-Time (NRT) since CIDs can be detected within 15 min of a seismic event. However, the detection of CIDs relies on human experts, which currently prevents the deployment of ionospheric methods in NRT. To address this critical lack of automatic procedure, we designed a machine-learning based framework to (1) classify ionospheric waveforms into CIDs and noise, (2) pick CID arrival times, and (3) associate arrivals across a satellite network in NRT. Machine-learning models (random forests) trained over an extensive ionospheric waveform dataset show excellent classification and arrival-time picking performances compared to existing detection procedures, which paves the way for the NRT imaging of surface displacements from the ionosphere.\nhttps://doi.org/10.1093/gji/ggac167\n\n## Citation\nBrissaud, Q., & Astafyeva, E. (2022). Near-real-time detection of co-seismic ionospheric disturbances using machine learning. Geophysical Journal International, 230(3), 2117-2130. doi: 10.1093/gji/ggac167\n```\n@article{brissaud2022near,\n  title={Near-real-time detection of co-seismic ionospheric disturbances using machine learning},\n  author={Brissaud, Quentin and Astafyeva, Elvira},\n  journal={Geophysical Journal International},\n  volume={230},\n  number={3},\n  pages={2117--2130},\n  year={2022},\n  publisher={Oxford University Press}\n}\n```\n",
        "createdAt": "2021-08-02T14:08:54.000Z",
        "updatedAt": "2025-05-26T05:30:01.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/QuentinBrissaud/AIDE/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "VioletaSeo/Papers",
        "url": "https://github.com/VioletaSeo/Papers",
        "description": "Papers that I read or planning to read. Topics related to seismology, mathematics, physics and miscellaneous interests.",
        "stars": 0,
        "forks": 0,
        "readme": "# Papers\nPapers that I read or planning to read. Topics related to seismology, mathematics, physics and miscellaneous interests.\n",
        "createdAt": "2020-04-03T08:55:09.000Z",
        "updatedAt": "2020-04-03T08:55:13.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/VioletaSeo/Papers/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "FranGuinez/Seismology_VisualApps",
        "url": "https://github.com/FranGuinez/Seismology_VisualApps",
        "description": "Applications that visualize ray paths and focal mechanisms.",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2024-05-23T20:17:19.000Z",
        "updatedAt": "2024-05-24T16:28:19.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "rhezazz/Seismology",
        "url": "https://github.com/rhezazz/Seismology",
        "description": "Several matlab program to determine hypocenter and epicenter using various algorithm.",
        "stars": 0,
        "forks": 0,
        "readme": "# Seismology\nSeveral matlab program to determine hypocenter and epicenter using various algorithm.\n",
        "createdAt": "2022-04-04T13:45:04.000Z",
        "updatedAt": "2022-04-04T14:00:24.000Z",
        "language": "MATLAB",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/rhezazz/Seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "SophieButcher/phasenet_demo",
        "url": "https://github.com/SophieButcher/phasenet_demo",
        "description": "PhaseNet demo and files for the ML in Seismology Summer Group 2022",
        "stars": 0,
        "forks": 0,
        "readme": "# phasenet_demo\nDemonstration of PhaseNet for volcanic seismicity, presented as part of the ML in Seismology Summer Group 2022. \n\nhttps://github.com/wayneweiqiang/PhaseNet\n\nZhu, W. and Beroza, G.C., 2019. PhaseNet: a deep-neural-network-based seismic arrival-time picking method. Geophysical Journal International, 216(1), pp.261-273.\n\nIf you git clone the original PhaseNet repository and install in the virtual environment as recommended, then the materials here can be included in the `phasenet/demo` directory to execute. \n\n### 1) Download data\nTo run this notebook, you'll need some waveform data. PhaseNet takes in data in csv, hdf5 or miniseed file formats. If you do not have any local data to access then you can download a sample of data to run these notebooks. I use the ObsPy client to download data, and there is an example of how to do this in `demo/prepare_mseed_files`.\n\nMy notebook is using seismic data from the IGUANA network at Sierra Negra volcano, Galapagos. This is freely available for download from IRIS. Within the data download notebook, you can change the variables to any network and any stations and any time interval (minutes, hours, seconds) that you are interested in. This notebook is adapted from a demo jupyter notebook available in the PhaseNet repository. It will download and create the correct file names and types to run the examples here and in the PhaseNet repository. \n\nFor more information on the IGUANA network, please see here: http://www.fdsn.org/networks/detail/8G_2018/\n\nYou can see information about any other networks here: http://www.fdsn.org/networks/\n\n\n### 2) Sample notebook\n\nI have provided a walk through of the PhaseNet methods at Sierra Negra on IGUANA stations SN07 and SN14. I have demonstrated two methods to execute PhaseNet:\n- Through the QuakeFlow API\n- Through your terminal\n\n### 3) Work in progress\nThis was very much in the **experimenting** stage when presented so there's a few things that aren't working yet. Please feel free to edit and bug fix at leisure! For instance, this notebook uses the pretrained model provided by PhaseNet and I'm still a little unsure about how to train a network. \n",
        "createdAt": "2022-07-21T12:55:13.000Z",
        "updatedAt": "2022-07-21T12:55:13.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/SophieButcher/phasenet_demo/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Mostafa-ebrahimi/Passive-Seismic",
        "url": "https://github.com/Mostafa-ebrahimi/Passive-Seismic",
        "description": null,
        "stars": 4,
        "forks": 0,
        "readme": "",
        "createdAt": "2018-03-02T21:45:26.000Z",
        "updatedAt": "2025-03-24T07:14:22.000Z",
        "language": "Matlab",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "geodynamics/axisem",
        "url": "https://github.com/geodynamics/axisem",
        "description": "AxiSEM is a parallel spectral-element method to solve 3D wave propagation in a sphere with axisymmetric or spherically symmetric visco-elastic, acoustic, anisotropic structures.",
        "stars": 71,
        "forks": 33,
        "readme": "# AxiSEM 1.4 [![Build Status](https://travis-ci.org/geodynamics/axisem.svg?branch=master)](https://travis-ci.org/geodynamics/axisem)\n\n## Axially symmetric Spectral Element Method\n\nCopyright 2018, Tarje Nissen-Meyer, Martin van Driel, Simon Stähler, Kasra Hosseini, Stefanie Hempel, Lion Krischer, Alexandre Fournier\n\nWebpage and distribution: http://www.axisem.info\nContact and information:  info@axisem.info\n\nApril, 13, 2018 \n\n## Citation\nIf you are publishing results obtained with this code, please cite this paper:\n\nT. Nissen-Meyer, M. van Driel, S. C. Staehler, K. Hosseini, S. Hempel, L. Auer, A. Colombi and A. Fournier:\n**\"AxiSEM: broadband 3-D seismic wavefields in axisymmetric media\"**, *Solid Earth*, 5, 425-445, 2014\ndoi:10.5194/se-5-425-2014 http://www.solid-earth.net/5/425/2014/\n\n## Content of the repository\n`manual_axisem_1.4.pdf` - PDF manual\n\n`MESHER` - The program to generate 2D meshes for the SEM forward solver\n\n`SOLVER` - the SEM forward solver itself\n\n`submit.py` - a Python script to create an Instaseis database\n\n`make_axisem.macros` - macro file to set compiler options\n\n`copytemplates.sh` - reset all input files to default templates \n\n`COPYING` - The GNU General Public License\n\n`HISTORY` - changelog\n\n`README` - this file\n\n## Basic instructions for running:\n\nMore details are found in the manual. For a quick start:\n\n - The mesher has to be run first to generate a SEM mesh for the solver. \n - Any changes in the resolution, spherically symmetric background model, or number \n   of processors requires a new mesh. \n - Changes in the source-receiver settings or 2.5D heterogeneities only need a new solver run.\n - Changes on the moment tensor, receiver components, or filtering only need a new postprocessing run.\n - Using Instaseis, seismograms for any depth or moment tensor can be calculated from the wavefield of one force source at the surface.\n\nGeneral settings and explanations for parameters needed in MESHER and SOLVER \nare found in the `inparam_*` files in the respective directories. \n\nTo create Instaseis databases quickly, run the script submit.py in the main directory.\n\n1) Run `copytemplates.sh` to set up a generic run with pre-set parameters\n\n2) Go into the MESHER directory, run `./submit.csh`. This compiles the code using\ngfortran and mpif90 as default compilers, and then submits a job on a single node. \nFor high resolution (seismic period below 3s), this requires significant amounts \nof RAM, see manual.\n\n3) Check `OUTPUT`; if finished, then run `./movemesh.csh <MESH_NAME>`\n\n4) go into `SOLVER`, check the vtk files in `/MESHES/<MESH_NAME>` with Paraview, if you want\n\n5) Edit `inparam_basic` to the desired `<MESH_NAME>`\n\n6) Run `./submit.csh <RUN_NAME>` , this compiles and then submits a parallel run.\n\n7) Check `<RUN_NAME>/OUTPUT*`, which will record the progress of the run and any problems.\n\n8) All data-related output is in `<RUN_NAME>/Data/`\n\n9) Convolution with a STF and summation for a moment source is done by running\n   `postprocessing.csh` in `<RUN_NAME>`\n\n10) A more modern and efficient way of seismogram retrieval is using Instaseis,\n    a Python toolbox to retrieve seismograms for arbitrary depths and moment\n    tensors from the stored wavefield of one AxiSEM run (http://www.instaseis.net)\n\nDetailed instructions can be found in the file `manual_axisem_1.4.pdf`\n\n## Installation\nGenerally, AxiSEM needs\n- a C and a Fortran90 compiler (typically gcc and gfortran)\n- MPI (typically OpenMPI)\n- NetCDF (not absolutely necessary, but needed to create an Instaseis database)\n\n### MacOS X\nThe easiest way to install the necessary requirements is *homebrew* (https://brew.sh/). If you have not installed homebrew itself yet, do so as described on the homepage. The preferred way (as of May 2022) is \n```\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n```\nand follow the steps therein.\n\nAfterwards, install gfortran, openmpi and NetCDF with \n```\nbrew install gfortran gcc openmpi netcdf hdf5-mpi\n```\n\n### Linux\nThe necessary packages can easily be installed using APT\n```\nsudo apt install gfortran gcc libopenmpi-dev openmpi-bin libnetcdff-dev \n```\n",
        "createdAt": "2014-02-28T00:47:00.000Z",
        "updatedAt": "2025-11-01T13:27:42.000Z",
        "language": "Fortran",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/geodynamics/axisem/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "shakesomuch/seismo_vibe",
        "url": "https://github.com/shakesomuch/seismo_vibe",
        "description": "Repository of different data processing tools for seismology",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2025-04-25T14:37:57.000Z",
        "updatedAt": "2025-04-25T14:37:57.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "huangkobehe123/huanghe",
        "url": "https://github.com/huangkobehe123/huanghe",
        "description": "Seismology, earthquake, interior physics of earth.",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2019-11-20T01:43:17.000Z",
        "updatedAt": "2019-11-20T01:43:17.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "pyrocko/pyrocko-notebooks",
        "url": "https://github.com/pyrocko/pyrocko-notebooks",
        "description": "An official read-only mirror of https://git.pyrocko.org/pyrocko/pyrocko-notebooks. Pyrocko Jupyter Notebook Examples",
        "stars": 5,
        "forks": 3,
        "readme": "# Pyrocko Jupyter Notebooks\n\nHands-on examples from the Pyrocko universe orbiting Jupyter.\n\n## Double-Couple Waveform Inversion\n\nA straight-forward inversion for a double couple source model using scipy's optimization strategies is excersized in `Double-Couble-Waveform-Inversion.ipynb`.\n\n\n## FDSN Data Download\n\nDownload private waveform data through an FDSN server using a cryotographic certificate.\n",
        "createdAt": "2017-09-13T08:30:50.000Z",
        "updatedAt": "2025-02-08T17:53:24.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/pyrocko/pyrocko-notebooks/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "hemmelig/AnisotroPy",
        "url": "https://github.com/hemmelig/AnisotroPy",
        "description": "Toolkit for the study of seismic anisotropy.",
        "stars": 26,
        "forks": 5,
        "readme": "<p align=\"center\">\n  <!-- DOI -->\n  <a href=\"https://doi.org/10.5281/zenodo.5931586\">\n    <img src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.5931586.svg\" />\n  </a>\n  <!-- ReadTheDocs -->\n  <a href=\"https://seismicanisotropy.readthedocs.io/en/latest\">\n    <img src=\"https://readthedocs.org/projects/seismicanisotropy/badge/?version=latest\" />\n  </a>\n  <!-- License -->\n  <a href=\"https://www.gnu.org/licenses/gpl-3.0\">\n    <img src=\"https://img.shields.io/badge/License-GPLv3-blue.svg\" />\n  </a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://seismicanisotropy.readthedocs.io/en/latest/index.html\">AnisotroPy</a> is a cross-platform Python package for the study of <a href=\"https://en.wikipedia.org/wiki/Seismic_anisotropy\">seismic anisotropy</a>.\n</p>\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/hemmelig/AnisotroPy/main/docs/img/AnilogoBig.png\" width=\"80%\" />\n</p>\n\nKey features\n------------\nThe goal of AnisotroPy is to provide straightforward access to a suite of routines and utilities for the study of seismic anisotropy. The package is primarily designed for both scripting purposes, but also includes a number of easy-to-use command-line utilities. Currently, we support:\n\n- Effective media modelling\n- Anisotropic layer fitting to observations\n- Visualisation of shear-wave splitting measurements\n- Shear-wave splitting analysis\n\nA sibling project—[AnisotropIO](https://github.com/hemmelig/AnisotropIO)—provides a standardised file format for shear-wave splitting measurements, as well as a range of parsers for other existing codes. The aim here is to make such analyses reproducible and straightforward to ingest into, for example, meta-studies.\n\nDocumentation\n-------------\nDocumentation for AnisotroPy is hosted [here](https://seismicanisotropy.readthedocs.io/en/latest/index.html).\n\nInstallation\n------------\nAnisotroPy requires Python version 3.8 and above. Installation of AnisotroPy, including all dependencies, can be done using pip:\n\n```console\npip install anisotropy\n```\n\nFor further information regarding installation—including virtual environment management and installation from source—please consult [our documentation](https://seismicanisotropy.readthedocs.io/en/latest/installation.html).\n\nUsage\n-----\nWe are working on tutorials covering how each individual aspect of the package works.\n\nThis is a work in progress - [see our documentation for full details](https://seismicanisotropy.readthedocs.io/en/latest/tutorials.html).\n\nFor a more comprehensive demonstration of the options available, see the [template scripts](examples/template_scripts).\n\nCitation\n--------\nIf you use AnisotroPy in your work, please cite the following:\n\nAnisotroPy Developers (2022). AnisotroPy: v0.0.1 (v0.0.1). Zenodo. https://doi.org/10.5281/zenodo.5931586\n\nContributing to AnisotroPy\n--------------------------\nContributions to AnisotroPy are welcomed. The first stop should be to reach out, either directly or—preferably—via the GitHub Issues panel, to discuss the proposed changes. Next, simply fork the AnisotroPy repository, make your changes/add your new contribution, then make a [pull request](https://help.github.com/articles/about-pull-requests/). All contributors to AnisotroPy will be listed as authors on the releases.\n\nBug reports, suggestions for new features and enhancements, and even links to projects that have made use of AnisotroPy are most welcome.\n\nSee our [contributions page](https://github.com/hemmelig/AnisotroPy/blob/main/.github/CONTRIBUTING.md) for more information.\n\nContact\n-------\nAny comments/questions can be directed to:\n* **Conor Bacon** - cbacon [ at ] ldeo.columbia.edu\n\nLicense\n-------\nAnisotroPy is **free** and **open source**, distributed under the GPLv3 License. Please see the [LICENSE](LICENSE) file for a complete description of the rights and freedoms that this provides the user.\n",
        "createdAt": "2022-01-29T04:38:30.000Z",
        "updatedAt": "2025-05-22T18:42:28.000Z",
        "language": "Python",
        "homepage": "https://seismicanisotropy.readthedocs.io",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.5931586",
            "openAlex": "10.5281/zenodo.5931586",
            "openCitations": "10.5281/zenodo.5931586",
            "dataCite": "10.5281/zenodo.5931586",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/hemmelig/AnisotroPy/main/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.5931586",
            "title": "AnisotroPy: v0.0.1",
            "journal": "Zenodo",
            "dateReleased": "2022-01-31T00:00:00.000Z",
            "abstract": "",
            "citationsArray": []
        },
        "repoDoi": "10.5281/zenodo.5931586",
        "publications": [
            {
                "doi": "10.5281/zenodo.5931586",
                "name": "AnisotroPy",
                "source": "",
                "authorNames": [],
                "abstract": "",
                "publicationDate": "2022-01-31T00:00:00.000Z"
            }
        ]
    },
    {
        "source": "GitHub",
        "name": "seiscomp-macOS/.github",
        "url": "https://github.com/seiscomp-macOS/.github",
        "description": "Welcome to the macOS port of SeisComP, a seismological software for data acquisition, processing, distribution and interactive analysis",
        "stars": 0,
        "forks": 0,
        "readme": "# SeisComP for macOS\n\nWelcome to the macOS port of SeisComP, a seismological software for data acquisition, processing, distribution and interactive analysis.\nThe seiscomp-macOS fork is synced/updated regularly once a week or month.\n\nPlease note that this is a forked repository of SeisComP developed by the GEOFON Program at Helmholtz Centre Potsdam GFZ German Research Centre for Geosciences and gempa GmbH,\nso no support is provided from GFZ or gempa GmbH.\n\nCheck SeisComP official site:\nhttps://www.seiscomp.de\n\nOriginal SeisComP Github repository:\nhttps://github.com/seiscomp/\n\n## SeisComP for macOS compilation instructions\n\n## About\n\nSeisComP is a seismological software for data acquisition, processing,\ndistribution and interactive analysis that has been developed by the\nGEOFON Program at  Helmholtz Centre Potsdam, GFZ German Research Centre\nfor Geosciences and gempa GmbH.\n\n## License\n\nSeisComP is primarily released under the AGPL 3.0. Please check the [license agreement](doc/base/license.rst).\n\n## Asking Questions\n\nPlease ask questions in the [forums](https://forum.seiscomp3.org) and\nuse appropriate topics to get help on usage or to discuss new features.\n\nIf you found a concrete issue in the codes or if you have code related\nquestions please use the Github issue tracker of the corresponding\nrepository,\ne.g. [GitHub issue tracker of this repository](https://github.com/SeisComP/seiscomp/issues).\n\n## Checkout the repositories with script clone_seiscomp-macos.sh\n\nThe SeisComP software collection is distributed among several repositories.\nThis repository only contains the build environment, the runtime framework\n(seiscomp control script) and the documentation.\n\nTo checkout all repositories to build a complete SeisComP distribution for macOS the following\nscript can be used.\n\nCopy/paste the following content to file: `clone_seiscomp-macos.sh`\n\n```\n#!/bin/bash\n\ntarget_dir=\"seiscomp-macOS\"\nrepo_path=https://github.com/gilcel/\n\nWORKDIR=$(pwd)\n\necho \"Cloning seiscomp base repository into $target_dir\"\ngit clone $repo_path/seiscomp.git $target_dir\n\necho \"Cloning base components\"\ncd $target_dir/src/base\ngit clone $repo_path/seedlink.git\ngit clone $repo_path/common.git\ngit clone $repo_path/main.git\ngit clone $repo_path/extras.git\n\necho \"Cloning external base components\"\ngit clone $repo_path/contrib-gns.git\ngit clone $repo_path/contrib-ipgp.git\ngit clone $repo_path/contrib-sed.git\n\necho \"Cloning SeisComP MeRT repo into ${target_dir}/src/base/extras/\"\n/bin/cd \"${target_dir}/src/extras/\" \ngit clone $repo_path/scmert.git\n\necho \"Done cloning seiscomp-macOS\"\n\ncd ../../\n```\n\nTo keep track of the state of each subrepository, [mu-repo](http://fabioz.github.io/mu-repo/)\nis a recommended way.\n\n\n## Build\n\n### Linux Prerequisites (not required for macOS compilation)\n\nThe following packages should be installed to compile SeisComP:\n\n- g++\n- git\n- cmake + cmake-gui\n- libboost\n- libxml2-dev\n- flex\n- libfl-dev\n- libssl-dev\n- crypto-dev\n- python-dev (optional)\n- python-numpy (optional)\n- libqt4-dev (optional)\n- qtbase5-dev (optional)\n- libmysqlclient-dev (optional)\n- libpq-dev (optional)\n- libsqlite3-dev (optional)\n- ncurses-dev (optional)\n\nThe Python development libraries are required if Python wrappers should be\ncompiled which is the default configuration. The development files must\nmatch the used Python interpreter of the system. If the system uses Python3\nthen Python3 development files must be present in exactly the same version\nas the used Python3 interpreter. The same holds for Python2.\n\nPython-numpy is required if Numpy support is enable which is also\nthe default configuration.\n\n### macOS Prerequisites\n\nThis will compile SeisComP natively on macOS for both Mac INTEL or Mac Silicon architectures (M1, M2, M3).\nTested on macOS Ventura 13.x and Sonoma 14.x on Mac INTEL and Mac Silicon.\n\n- Install Xcode Development Tools\n\nFirst we need to install the Development tools (Command Line Tools).\nNote that the full Xcode Development Tools from \"Mac App Store\" is *not* required.\n\nOpen your \"Terminal.app\" and install Xcode command line tools with command:\n\n`xcode-select --install`\n\n- Install Homebrew for macOS\n\nInstall Homebrew 'brew' command with the following one-liner:\n\n`/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"`\n\nOn INTEL Mac the default Homebrew directory location is in: `/usr/local/`\nOn Apple Silicon Mac the default Homebrew directory location is in: `/opt/homebrew/opt/`\n\nPython 3.11 is recommended since Python 3.12 has compatibility issues with seedlink.\n\nFirst install Python v3.11 with NumPy, which needs to be installed as a site-package with pip3.\n\n```\nbrew install python@3.11\nbrew install numpy\npip3.11 install numpy\n```\n\nContinue installing macOS dependencies with:\n\n```\nbrew install boost \nbrew install cmake\nbrew install fftw \nbrew install flex\nbrew install gfortran\nbrew install hdf5\nbrew install mysql #mariadb can also be installed as an alternative\nbrew install ncurses\nbrew install openssl\nbrew install qt5\nbrew install swig\n```\n\nNote: If you need a more specific version of Python with NumPy, e.g. Python 3.10:\n\n`brew install python@3.10`\n`pip3.10 install numpy`\n\nAfter that check or update your PATH to include Homebrew paths:\n\nThe Homebrew shell path for INTEL Mac /usr/local/bin/ and /usr/local/sbin and /opt/homebrew/bin resp. /opt/homebrew/sbin/\nshould be in your PATH. Edit your `~/.bashrc` accordingly\n\n`echo $PATH`\n\nOn INTEL Mac, your `~/.bashrc`should look like (note the `/usr/local/bin:/usr/local/sbin:` before `/bin/:/usr/bin`)\n\n`PATH=/usr/local/bin/:/usr/local/sbin/:/bin:/usr/bin:/usr/sbin:/usr/X11/bin:$PATH`\n\nOn Apple Silicon Mac, your `~/.bashrc`should look like (note the `/opt/homebrew/bin:/opt/homebrew/sbin:` before `/bin/:/usr/bin`)\n\n`PATH=/opt/homebrew/bin:/opt/homebrew/sbin:/bin:/usr/bin:/usr/sbin:$PATH`\n\n\n### Clone the Github repositories  from https://github.com/gilcel/\n\nNote that the script `clone_seiscomp-macos.sh` uses the repo from https://github.com/gilcel/ and not from https://github.com/seiscomp\nUse the script  `clone_seiscomp-macos.sh` to git-clone all the repos.\n\nHere's how to proceed:\n\n1. Create directory seiscomp-macos inside your Downloads directory:\n\n```\nmkdir ~/Downloads/seiscomp-macos\ncd ~/Downloads/seiscomp-macos\n```\nMove the script `clone_seiscomp-macos.sh` to `~/Downloads/seiscomp-macos`\n\nChange script to executable - do this once:\n\n`chmod u+x clone_seiscomp-macos.sh`\n\nNow clone the seiscomp-macOS repos inside `~/Downloads/seiscomp-macos`\n `./clone_seiscomp-macos.sh`\n \nAfter this you will see a the source-code directory named: `seiscomp` inside `~/Downloads/seiscomp-macos`\n\n### Compile seiscomp on macOS \n\nAfter succesful git-cloning with the script `clone_seiscomp-macos.sh`, compile SeisComP on your Mac with command: `cmake`\n\nStill inside `~/Downloads/seiscomp-macos` do the following:\n\n```\nmkdir build-seiscomp\ncd build-seiscomp\ncmake -DCMAKE_INSTALL_PREFIX=${HOME}/seiscomp ../seiscomp\n```\n\nNote 1: if you need to use a specific Python version, e.g \"Python 3.10\" (don't forget to set your PATH accordingly):\n`cmake -DCMAKE_INSTALL_PREFIX=${HOME}/seiscomp ../seiscomp/ -DPython_VERSION_REQUIRED=3.10`\n\nCompile SeisComP for macOS in the `build-seiscomp` directory:\n\n`make -j4`\n\nInstall with command:\n\n`make install`\n\nIf compilation was succesful it will install the binaries and libraries in ${HOME}/seiscomp (the MAKE_INSTALL_PREFIX).\nLaunch (test) e.g 'scmv' or 'scrttv' with command:\n\n`/Users/<YOUR_USER_NAME>/seiscomp/bin/`\n\n\nNote 1: After compilation the seedlink plugins directory contains compiled libraries e.g. libreftek.a libutil.a etc and objects .o\nYou should clean up the \"seedlink/plugins\" directory to be sure to recompile the latest versions (not necessary but should help compilation errors).\nAlso if you copy your \"seiscomp\" directory to another platform (Apple Silicon) or INTEL the compiled libraries are still there, so better do a `make clean`.\n\nJust go to `seiscomp/src/base/seedlink/plugins` and do a `make clean`\n\n```\ncd seiscomp/src/base/seedlink/plugins\nmake clean\n```\n\n### Configure MySQL on macOS for better performance\n\nCopy default MYSQL configuration file to /etc/my.cnf with command:\n\n`sudo cp $(brew --prefix mysql)/support-files/my-default.cnf /etc/my.cnf`\n\nFor better performance with the MySQL database, adjust the following parameters in /etc/my.cnf\nIf you have more than 8GB of RAM, increase `innodb_buffer_pool_size` (default is 128MB):\n \n```\ninnodb_buffer_pool_size = 8G\ninnodb-buffer-pool-instances=16\ninnodb_flush_log_at_trx_commit = 2\n```\n\n### macOS Troubleshooting\n\nIf you get the following error when compiling:\n\n`\"_Python3_NumPy_INCLUDE_DIR-NOTFOUND\"`\n\nThen you forgot to install NumPy with `pip3` (NumPy site-package).\nTo fix, do this:\n\n```\n#brew install numpy\n#pip3 install numpy\n```\n\nThe NumPy site-package will then be installed to:\n\n`/usr/local/lib/python3.<VERSION_NUMBER>/site-packages`\n\n### Configuration\n\nThe SeisComP build system provides several build options which can be\ncontrolled with a cmake gui or from the commandline\npassing `-D[OPTION]=ON|OFF` to cmake.\n\nIn addition to standard cmake options such as `CMAKE_INSTALL_PREFIX`\nthe following global options are available:\n\n|Option|Default|Description|\n|------|-------|-----------|\n|SC_GLOBAL_UNITTESTS|ON|Whether to build unittests or not. If enabled then use `ctest` in the build directory to run the unittests.|\n|SC_GLOBAL_PYTHON_WRAPPER|ON|Build Python wrappers for the C++ libraries. You should not turn off this option unless you know exactly what you are doing.|\n|SC_GLOBAL_PYTHON_WRAPPER_NUMPY|ON|Add Numpy support to Python wrappers. If enabled then all SeisComP arrays will provide a method `numpy()` which returns a Numpy array representation.|\n|SC_ENABLE_CONTRIB|ON|Enable inclusion of external contributions into the build. This includes all directories in `src/extras`.|\n|SC_GLOBAL_GUI|ON|Enables compilation of GUI components. This requires the Qt libraries to be installed. Either Qt4 or Qt5 are supported. The build will prefer Qt5 if found and will fallback to Qt4 if the Qt5 development libraries are not installed on the host system.|\n|SC_GLOBAL_GUI_QT5|ON|If SC_GLOBAL_GUI is enabled then Qt5 support will be enabled if this option is active. Otherwise only Qt4 will be supported.|\n|SC_DOC_GENERATE|OFF|Enable generation of documentation|\n|SC_DOC_GENERATE_HTML|ON|Enable generation of HTML documentation|\n|SC_DOC_GENERATE_MAN|ON|Enable generation of MAN pages|\n|SC_DOC_GENERATE_PDF|OFF|Enable generation of PDF documentation|\n\n### Compilation\n\n1. Clone all required repositories (see above)\n2. Run ```make```\n3. Configure the build\n4. Press 'c' as long as 'g' appears\n5. Press 'g' to generate the Makefiles\n6. Enter the build directory and run ```make```\n\n### Installation\n\n1. Enter the build directory and run ```make install```\n   to install SeisComP\n\n## Contributing improvements and bug fixes\n\nPlease consider [contributing](CONTRIBUTING.md) to the code.\n",
        "createdAt": "2024-01-23T11:12:20.000Z",
        "updatedAt": "2024-01-23T13:48:58.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/seiscomp-macOS/.github/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "estebanarivasv/DS-2020-SeismologyAngular",
        "url": "https://github.com/estebanarivasv/DS-2020-SeismologyAngular",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# :earth_americas: Seismology Proyect\n\nSubject: \"Diseño de sistemas\"\n<br><br>\nThis is a mirrored repository of the project I did for \"Programación I\" (Programming) at Universidad de Mendoza. For more information, go to https://github.com/estebanarivasv/Seismology-Flask\n\nThe aim of this repository is emphasize software design patterns and best software practices.\n<br><br>\n\n# :deciduous_tree: Project's tree structure diagram\n\nGiven the task, I've designed my project structure with the Model–view–controller (MVC) software design pattern.\n\n### REST Api on Flask\n```\n.\n├── app.py                                                  -- App instance\n├── database                                                -- SQLite db \n│   └── data.db\n├── insomnia_requests.json\n├── main\n│   ├── __init__.py\n│   ├── controllers                                         -- HTTP Requests jsons handling\n│   │   ├── __init__.py\n│   │   ├── seism.py\n│   │   ├── sensor.py\n│   │   └── user.py\n│   ├── extensions                                          -- Main components initialization\n│   │   ├── __init__.py\n│   ├── mapping                                             -- JSON dictionaries mapping for db\n│   │   ├── __init__.py\n│   │   ├── seism.py\n│   │   ├── sensor.py\n│   │   └── user.py\n│   ├── models                                              -- Database models\n│   │   ├── __init__.py\n│   │   ├── seism.py\n│   │   ├── sensor.py\n│   │   └── user.py\n│   ├── repositories                                        -- CRUD Methods for the controllers interacting with the db\n│   │   ├── __init__.py\n│   │   ├── main.py\n│   │   ├── seism.py\n│   │   ├── sensor.py\n│   │   └── user.py\n│   ├── resources\n│   │   ├── authentication\n│   │   │   ├── decorators.py                               -- Controllers restriction\n│   │   │   └── routes.py                                   -- Auth routes\n│   │   ├── functions.py                                    -- get_near_seisms() - Pandas\n│   │   ├── __init__.py\n│   │   ├── pagination.py                                   -- Model used in repositories\n│   │   └── validators.py                                   -- Objects validation used in repositories\n│   ├── services\n│   │   ├── __init__.py\n│   │   ├── jobs\n│   │   │   ├── functions.py                                -- get_seisms_from_api(), get_ids_to_delete()\n│   │   │   ├── __init__.py\n│   │   │   └── tasks.py                                    -- seisms_achiever, data_persistance\n│   │   └── mail_sending                                    -- Email admins of not working sensors\n│   │       ├── controller.py\n│   │       └── resources.py\n│   └── templates                                           -- Email templates\n│       └── mail\n│           ├── sensors_status.html\n│           └── sensors_status.txt\n└── requirements.txt\n```\n\n### Web client on Angular\n```\n.\n├── app\n│   ├── app.component.html                                  -- ROUTER IMPLEMENTATION, Bootstrap CDN\n│   ├── app.constants.ts                                    -- API URL\n│   ├── app.module.ts                                       -- Dependencies, Bootstrap CDN\n│   ├── app-routing.module.ts                               -- ROUTING, Bootstrap CDN\n│   ├── app-sorting.directive.ts\n│   ├── alerts                                              \n│   │   ├── alerts.service.ts\n│   ├── authentication                                      \n│   │   ├── authentication.component.html                   -- Login template\n│   │   ├── authentication.component.ts\n│   │   ├── authentication.service.ts                       -- Api authentication interaction\n│   │   ├── guards                                          -- Limit routing\n│   │   └── interceptors\n│   ├── guards\n│   ├── header\n│   │   └── header.component.html\n│   ├── home\n│   │   └── home.component.html\n│   ├── pagination.model.ts\n│   ├── seisms\n│   │   ├── seisms-filter.model.ts                          -- Seisms pagination, sorting and filter model\n│   │   ├── seisms.model.ts                                 -- Seisms model for requests mapping\n│   │   ├── seisms.service.ts                               -- Seisms components interaction with HTTP requests\n│   │   ├── unverified-seisms\n│   │   │   ├── edit-unverified\n│   │   │   ├── unverified-seisms.component.html\n│   │   │   ├── unverified-seisms.component.ts\n│   │   │   └── view-unverified\n│   │   └── verified-seisms\n│   │       ├── verified-seisms.component.html\n│   │       ├── verified-seisms.component.ts\n│   │       └── view-verified\n│   ├── sensors\n│   │   ├── add-sensor\n│   │   ├── check-sensor\n│   │   ├── delete-sensor\n│   │   ├── edit-sensor\n│   │   ├── sensors.component.html\n│   │   ├── sensors.component.ts\n│   │   ├── sensors-filter.model.ts                         -- Sensors pagination, sorting and filter model\n│   │   ├── sensors.model.ts                                -- Sensors model for requests mapping\n│   │   ├── sensors.service.ts                              -- Sensors component interaction with HTTP requests\n│   │   └── view-sensor\n│   ├── upper-body                                          -- BREADCRUMBS, TITLE\n│   │   ├── upper-body.component.html\n│   │   ├── upper-body.component.ts\n│   │   └── upper-body.interfaces.ts\n│   └── users\n│       ├── add-user\n│       ├── delete-user\n│       ├── edit-user\n│       ├── users.component.html\n│       ├── users.component.ts\n│       ├── users.model.ts                                  -- Users model for requests mapping\n│       └── users.service.ts                                -- Users component interaction with HTTP requests\n├── assets\n│   ├── img\n│   └── videos\n├── environments\n├── favicon.ico\n├── index.html\n├── main.ts\n├── polyfills.ts\n├── styles.scss\n└── test.ts\n```\n# Software design patterns and principles applied in the project \n\n- DRY, KISS, SOLID\n- Command -> Repositories\n- Observer -> .suscribe() Angular services (async)\n\n# :computer: Developing stages\n\nAt the beginning, I had the REST Api from the base repository from where I started. Here it is the full description of what I did:\n\n### Backend\n#### 1 - Mapping \nDB Models Schemas creation (flask-marshmallow). to_json(), from_json() deletion.\n#### 2 - Controllers and repositories \nModularity.   Controllers -> HTTP Requests\n#### 3 - Filtering, sorting and pagination \nGeneral Pagination class\n#### 4 - Repositories\nModularity.   Repositories -> DB interaction\n#### 5 - Resources\nModularity.   Reorganizing: auth, various functions for services, validators, pagination\n#### 6 - Services\nJobs: data persistance, data obtention\n\n### Frontend\n#### 1 - Components \nViews design: home, seisms, sensors, users\n#### 2 - Services \nHTTP Requests\n#### 3 - Forms and validations \nReactive forms on add and edit views\n#### 4 - Alerts\nService that displays alerts\n#### 5 - Filtering, sorting, pagination\nTemplate-driven forms. Filter models, NgbdSortableHeader, ngb-pagination\n#### 6 - Auth\nAuth service, HttpRequestsInterceptor, Guard\n\n#### What's left to do?\n- CSV download\n- Email sending integration\n- Near seisms from input location\n- Api validation\n- Guards\n\n# Problems\n- New framework, new languaje\n- Time\n\n# :information_source: Installation and usage for both API and Web client\nSteps to follow in order to get the Flask app and Angular client up and running\n\n#### 1 - Define the environment variables in .env and app.constants.ts files\nYou can rename the .env-example file to .env\n\n:exclamation: Remember you need to declare all the variables including the database path. \n\n#### 2 - Install dependencies\nTo begin the instalation of libraries and the frameworks needed: `./install.sh` and `npm install`\n\n#### 3 - Launch Flask application\nTo get the app running: `./boot.sh` and `ng serve`\n\n##### 3.1 - Import requests file for the api in Insomnia\n",
        "createdAt": "2020-08-19T22:25:21.000Z",
        "updatedAt": "2021-12-16T04:37:31.000Z",
        "language": "HTML",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/estebanarivasv/DS-2020-SeismologyAngular/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "swiss-seismological-service/pyrtdd",
        "url": "https://github.com/swiss-seismological-service/pyrtdd",
        "description": "Double-Difference Earthquake Location",
        "stars": 4,
        "forks": 1,
        "readme": "[![DOI](https://zenodo.org/badge/246001157.svg)](https://zenodo.org/badge/latestdoi/246001157)\n\nPlease cite the code as:\n\n\"Luca Scarabello & Tobias Diehl (2021). swiss-seismological-service/scrtdd. Zenodo doi: 10.5281/zenodo.5337361\"\n\n# pyrtdd\n\nPython wrapper for the C++ double-difference relocation library from [scrtdd](https://github.com/swiss-seismological-service/scrtdd).\n\n# Installation\n\n`pyrtdd` is tested with `gcc-11`, `gcc-12`, and `clang-14`. Before starting the installation process, please ensure that you have one of these compilers, along with [Anaconda](https://www.anaconda.com/products/distribution), installed on your system.\n\n1. Clone this repository, ensuring that `scrtdd` is also pulled as a submodule:\n\n    ```\n    git clone --recursive https://github.com/swiss-seismological-service/pyrtdd.git\n    ```\n\n2. Create a `pyrtdd` Anaconda environment where you will install pyrtdd:\n\n    ```\n    conda create -n pyrtdd\n    ```\n\n3. Activate the `pyrtdd` anaconda enviroment:\n\n    ```\n    conda activate pyrtdd\n    ```\n\n4. Install `pyrtdd` from source:\n\n    ```\n    pip install -v .\n    ```\n    Incase a particular compiler version is required:\n    ```\n    CC=gcc-10 CXX=g++-10  SKBUILD_CONFIGURE_OPTIONS=\"-DCMAKE_C_COMPILER:STRING=gcc-10 -DCMAKE_CXX_COMPILER:STRING=g++-10\" pip install -v .\n    ```\n\n\n# Example\n\nPlease note that the code is in development stage. The documentation is not ready yet and you should use the [scrtdd](https://github.com/swiss-seismological-service/scrtdd) manual as a temporary reference. In particular, have a look at the [catalog format](https://docs.gempa.de/scrtdd/current/base/multievent.html#event-catalog-plain-csv-files) and the [relocation process](https://docs.gempa.de/scrtdd/current/base/multievent.html#relocation-process) paragraphs.\n\n\n\n```python\nfrom pyrtdd.hdd import (\n        Catalog,\n        Config,\n        ConstantVelocity,\n        NLL,\n        ObspyWaveformProxy,\n        UTCClock,\n        DD,\n        SolverOptions,\n        ClusteringOptions,\n        NoWaveformProxy,\n    )\n\ncfg = Config()\n\n#\n# Defines a priority list of accepted P and S phases. Phases not in the list will be discarded from the catalog.\n# If multiple phases exist for the same event at a station, the first one in the list will be used\n# \ncfg.validPphases = ['Pg', 'P']\ncfg.validSphases = ['Sg', 'S']\n\n#\n# Here we specify the input catalog. We use the test catalong for this example\n#\ncat = Catalog('./package/test/py/data/starting-station.csv',\n              './package/test/py/data/starting-event.csv',\n              './package/test/py/data/starting-phase.csv',\n              False)\n\n#\n# Select the velocity model. There are two options available\n#\nttt = ConstantVelocity(5.8, 3.36) # P/S velocity\n\n# alternatively we can use NonLinLoc grids\n#ttt = NLL('path/model/iasp91.PHASE.mod',\n#          'path/time/iasp91.PHASE.STATION.time',\n#          'path/time/iasp91.PHASE.STATION.angle',\n#          False, # swap bytes\n#          255)   # maximum number of files to keep open (performance stuff)\n\n#\n# Main class used for the relocation\n#\ndd = DD(cat, cfg, ttt, NoWaveformProxy())\n\n#\n# Define clustering options\n# These options control which events and phases are used in the double-difference equation system. \n#\ncluster_cfg = ClusteringOptions()\n\n#\n# Quality settings\n# Allow to drop poorly connected events or bad phases\n#\ncluster_cfg.minNumNeigh = 4 # min neighbors required for an event\ncluster_cfg.minDTperEvt = 8 # min differential times per event pair required (i.e. how many P+S phases)\ncluster_cfg.minWeight = 0. # min weight of phases required (0-1). Uncertainties have to be included in the catalog\n\n#\n# Performance settings:\n#  limit maxDTperEvt only if the relocation is too slow, otherwise keep them all \n#  maxNumNeigh doesn't usually improve results above 30-40\ncluster_cfg.maxNumNeigh = 40 # max neighbors allowed. 0 -> disable\ncluster_cfg.maxDTperEvt = 0 # max differential times per event pair required (Including P+S) 0 -> disable\n\n#\n# Station filtering\n#\ncluster_cfg.minEStoIEratio = 0. # min hypocenter-station to interevent distance ratio required\ncluster_cfg.minESdist = 0. # min hypocenter-station distance required\ncluster_cfg.maxESdist = -1 # max hypocenter-station distance allowed (-1 -> disable)\n\n# Neighbours selection\n# This option controls how neighbouring events are selected. In the simpliest form 'numEllipsoids'\n# is set to 0 and 'maxNumNeigh' neighbours are selected on the nearest neighbour basis within a search\n# distance of 'maxEllipsoidSize'. This is the default choice for multi-event mode.\n# When 'numEllipsoids' is > 0, the ellipsoid selection algorithm from Waldhauser 2009: to assure a\n# spatially homogeneous subsampling, reference events are selected within each of `numEllipsoids`\n# concentric ellipsoidal layers of increasing thickness. Each layer is split up into its 8 quadrants\n# (or cells), and the neighboring events are selected from each ellipsoid/quadrant combination in a\n# round robin fashion until 'maxNumNeigh' is reached.\ncluster_cfg.numEllipsoids = 0\ncluster_cfg.maxEllipsoidSize = 5 # Km\n\n# There is no cross-correlation binding to python yet :(\ncluster_cfg.xcorrMaxEvStaDist = 0\ncluster_cfg.xcorrMaxInterEvDist = 0\ncluster_cfg.xcorrDetectMissingPhases = False\n\n#\n# Double-difference equations system solver configuration\n#\nsolver_cfg = SolverOptions()\n\nsolver_cfg.type = \"LSMR\" # Solver algorithm to use: either LSMR or LSQR\nsolver_cfg.algoIterations = 20 # how many iterations the solver performs\n\nsolver_cfg.absLocConstraintStart = 0.3 # 0 -> disable absolute location constraint\nsolver_cfg.absLocConstraintEnd = 0.3   # 0 -> disable absolute location constraint\nsolver_cfg.dampingFactorStart = 0.01   # 0 -> disable damping factor\nsolver_cfg.dampingFactorEnd = 0.01     # 0 -> disable damping factor\n\nsolver_cfg.downWeightingByResidualStart = 10. # 0 -> disbale downweighting\nsolver_cfg.downWeightingByResidualEnd = 3.    # 0 -> disbale downweighting\n\nsolver_cfg.usePickUncertainty = False # if True then phase uncertaintis must be populated\n\n# Air-quakes are events whose depth shift above the range of the velocity\n# model (typically 0, but confiurable) during the inversion. Possible actions are:\n# NONE - do nothing the event relocation will fail\n# RESET - reset quake location to previous iteration, that is before it became an air-quake\n# RESET_DEPTH - reset only quake depth to previous iteration\nsolver_cfg.airQuakes.action = SolverOptions.AQ_ACTION.NONE \nsolver_cfg.airQuakes.elevationThreshold = 0 # meters, threshold above which an event is\n                                            # considered an air-quake. Useful only if\n                                            # action is not NONE\n\n#\n# Perform the relocation\n#\ncat_new = dd.relocateMultiEvents(cluster_cfg, solver_cfg)\n\n#\n# Write relocated catalog\n#\ncat_new.writeToFile('relocated-event.csv',\n                    'relocated-phase.csv',\n                    'relocated-station.csv')\n\n```\n\n![reloc](https://user-images.githubusercontent.com/15273575/205635799-80128f78-be04-48dc-8c17-32887d929552.png)\n\n\n\n",
        "createdAt": "2022-10-12T15:07:02.000Z",
        "updatedAt": "2025-08-28T08:25:58.000Z",
        "language": "C++",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/swiss-seismological-service/pyrtdd/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "seisman/SAC_Docs_zh",
        "url": "https://github.com/seisman/SAC_Docs_zh",
        "description": "SAC参考手册中文版",
        "stars": 67,
        "forks": 32,
        "readme": "# SAC参考手册\n\n<img src=\"source/images/SAC_logo.png\" width=450/>\n\n[![Deploy](https://github.com/seisman/SAC_Docs_zh/actions/workflows/deploy.yml/badge.svg)](https://github.com/seisman/SAC_Docs_zh/actions/workflows/deploy.yml)\n[![GitHub release](https://img.shields.io/github/release/seisman/SAC_Docs_zh.svg)](https://github.com/seisman/SAC_Docs_zh/releases)\n[![License: CC BY-NC 4.0](https://img.shields.io/badge/License-CC%20BY--NC%204.0-blue.svg)](https://creativecommons.org/licenses/by-nc/4.0/deed.zh-hans)\n[![License (CC0 1.0)](https://img.shields.io/badge/license-CC0%201.0-blue.svg)](https://creativecommons.org/publicdomain/zero/1.0/)\n\n[在线阅读](https://seisman.github.io/SAC_Docs_zh/) |\n[PDF下载](https://seisman.github.io/SAC_Docs_zh/SAC_Docs.pdf) |\n[文档源码](https://github.com/seisman/SAC_Docs_zh)\n\n《SAC参考手册》是介绍地震学常用软件 [SAC](http://ds.iris.edu/ds/nodes/dmc/forms/sac/)\n用法的中文手册。\n\n## 参与维护\n\n本项目由 [seisman](https://github.com/seisman) 发起，目前由志愿者负责维护。\n\n欢迎更多SAC用户参与到本项目的维护中。具体的维护工作包括：\n\n- 校对错别字、病句等等\n- 校对排版问题\n- 完善部分未翻译的命令，以及个别命令中未翻译的部分\n- 随着SAC新版本的发布，更新手册中相应的内容\n- 补充命令示例以及脚本示例\n- 补充其他尚未包含在手册中的与SAC相关的知识点\n- 文档结构的调整\n- 项目源码的优化\n\n可以通过三种方式参与到项目的维护中：\n\n1. 发邮件发送给 [seisman.info@gmail.com]()\n2. 在项目主页提交Issue\n3. 在项目主页提交Pull Request，详情见 [Wiki](https://github.com/seisman/SAC_Docs_zh/wiki)\n\n## 许可协议\n\n本项目的文档部分采用[知识共享署名-非商业性使用 4.0 国际许可协议 (CC BY-NC 4.0)](https://creativecommons.org/licenses/by-nc/4.0/deed.zh-hans)。\n任何人都可以自由地分享、修改本作品，但必须遵循如下条件：\n\n- 署名：必须提到原作者，提供指向此许可协议的链接，表明是否有做修改\n- 非商业性使用：不能对本作品进行任何形式的商业性使用\n\n本项目中的脚本遵循 [CC0 1.0 通用 (CC0 1.0)](https://creativecommons.org/publicdomain/zero/1.0/deed.zh) 许可协议。\n即所有脚本放弃一切权利，全归公共领域。任何人可以自由无限制地使用、修改、共享本项目中的脚本。\n",
        "createdAt": "2015-05-09T13:41:50.000Z",
        "updatedAt": "2025-11-03T16:02:37.000Z",
        "language": "Shell",
        "homepage": "https://seisman.github.io/SAC_Docs_zh",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/seisman/SAC_Docs_zh/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Lchuang/Gallery",
        "url": "https://github.com/Lchuang/Gallery",
        "description": "A collection of codes for seismology related plotting",
        "stars": 1,
        "forks": 0,
        "readme": "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/Lchuang/Gallery/HEAD)\n# Gallery\nA collection of codes for seismology related plotting\n",
        "createdAt": "2023-03-19T22:48:05.000Z",
        "updatedAt": "2023-11-03T04:40:01.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Lchuang/Gallery/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "jsaul/flinnengdahl",
        "url": "https://github.com/jsaul/flinnengdahl",
        "description": "Lightweight, fast and configurable Python/C++ module to compute Flinn-Engdahl region numbers and names.",
        "stars": 1,
        "forks": 1,
        "readme": "# flinnengdahl.py\n\nStandalone, lightweight, fast and configurable Python/C++ module to compute\nFlinn-Engdahl region numbers and names.\n\n## Author\n\nJoachim Saul, GFZ Potsdam\n\n## Motivation\n\nThe goal is to have consistent and up-to-date region *naming* while\npreserving the original region *numbering* scheme. Names may change and some\nof the names in the original Flinn-Engdahl region naming are at least debatable.\nThe goal here is to follow Wikipedia as closely as possible. We added the\nability to use arbitrary translations. An example translation table\nis included for English, German and Spanish. The English region names are\nhardcoded as default and the C++ code defining the hardcoded names can easily\nbe (re)generated from the translation table whenever needed.\n\nThe translation table is provided as a *single file*, as it was found to be convenient\nto have translations to several languages on consecutive lines for comparison, with\ncomments added where needed to support a certain naming (e.g. \"Revillagigedo\" or\n\"Haida Gwaii\").\n\nThe codes are set up in a way that in theory allows defining new cells,\nprovided they fit in the 1x1-degree raster. But please note that such\nmodifications would be deviations from the \"standard\" Flinn-Engdahl\nregion numbering and should not be called \"Flinn-Engdahl regions\" any more.\n\n## Python\n\nSimple Python wrappers are automatically generated using SWIG.\n\nNote that there are other Python modules available to compute Flinn-Engdahl regions,\ne.g. in [SeisComP](https://github.com/SeisComP) and [ObsPy](https://github.com/obspy).\nThere is no reason not to use those, except in situations where the installation of\na large software package is not feasible or if the translation feature is of interest.\n\n## Examples\n\n```\n>>> import flinnengdahl\n>>> fe=flinnengdahl.FlinnEngdahl()\n>>> fe.name(50, 10)\n'Germany'\n>>> fe.number(50, 10)\n543\n>>> fe.read(\"numbered-names-intl.txt\")\n>>> fe.name(50, 10, \"de\")\n'Deutschland'\n>>> fe.name(50, 10, \"es\")\n'Alemania'\n>>> fe.name(50, 10, \"en\")\n'Germany'\n>>> fe.name(50, 10, \"\")\n'Germany'\n>>> fe.setCategory(\"de\")\n>>> fe.name(50, 10)\n'Deutschland'\n```\n\nYou get the picture.\n\n\n## Issues\n\nThere are still some names that may create confusion or discomfort, like\n\"Kashmir-India border region\".  Ideas on how to resolve this (and at the same\ntime keep the region numbering!) are welcome but probably outside the scope of\nthis module.\n\n\n## Reference\n\nJ.B. Young, B.W. Presgrave, H. Aichele, D.A. Wiens, E.A. Flinn (1996),\nThe Flinn-Engdahl Regionalisation Scheme: The 1995 revision.\nPhysics of the Earth and Planetary Interiors, 96, pp 223-297.\nhttps://doi.org/10.1016/0031-9201(96)03141-X\n",
        "createdAt": "2022-06-12T08:29:50.000Z",
        "updatedAt": "2025-03-17T07:31:34.000Z",
        "language": "C++",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/jsaul/flinnengdahl/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "michaelgrund/stacksplit",
        "url": "https://github.com/michaelgrund/stacksplit",
        "description": "Multi-event processing plugin for the MATLAB shear-wave splitting toolbox SplitLab",
        "stars": 17,
        "forks": 4,
        "readme": "\n# StackSplit                            [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5802051.svg)](https://doi.org/10.5281/zenodo.5802051) [![View michaelgrund/stacksplit on File Exchange](https://www.mathworks.com/matlabcentral/images/matlab-file-exchange.svg)](https://de.mathworks.com/matlabcentral/fileexchange/62402-michaelgrund-stacksplit)\n\n### A plugin for multi-event shear wave splitting analyses in SplitLab\n\nStackSplit is a plugin for the MATLAB toolbox [SplitLab](http://splitting.gm.univ-montp2.fr/) ([**_Wüstefeld et al., 2008_**](https://www.sciencedirect.com/science/article/pii/S0098300407001859)) which allows applying multi-event techniques for shear wave splitting measurements (SWS) directly within the main program.\n\nFor details regarding installation and usage, see the [UserGuide](https://github.com/michaelgrund/stacksplit/blob/main/StackSplit/Doc/StackSplit_userguide.md).\n\nCitation\n--------\n\nIf you make use of StackSplit in your work, please acknowledge my paper in which the program is described:\n\n- **_Grund, M. (2017)_**, StackSplit - a plugin for multi-event shear wave splitting analyses in SplitLab, *Computers & Geosciences*, 105, 43-50, https://doi.org/10.1016/j.cageo.2017.04.015.\n\nOptionally, you can also cite the [Zenodo DOI](https://zenodo.org/record/5802051#) given above, which refers to the latest version of this GitHub repository.\n\nWhich stacking methods are available?\n-------------------------------------\n\nStackSplit grants easy access to four stacking schemes with which single SWS measurements made with SplitLab can be processed:\n\n1. **WS**: stacking of error surfaces, normalized on the minimum/maximum (depending on input) of each single surface ([**_Wolfe & Silver, 1998_**](https://doi.org/10.1029/97JB02023))\n\n2. **RH**: modified WS method with weight depending on the SNR of each measurement and normalization regarding the available backazimuth directions ([**_Restivo & Helffrich, 1999_**](https://doi.org/10.1046/j.1365-246x.1999.00845.x))\n\n3. **no weight**: stacking of error surfaces without weighting following the PhD thesis of [**_Wüstefeld (2007)_**](http://splitting.gm.univ-montp2.fr/)\n\n4. **SIMW**: simultaneous inversion of multiple waveforms in the time domain ([**_Roy et al., 2017_**](https://doi.org/10.1093/gji/ggw470))\n\n![fig4github](https://user-images.githubusercontent.com/23025878/56716351-6d3d2a80-673a-11e9-8b34-2191c119d780.png)\n\nCompatibility with SplitLab and MATLAB versions\n-----------------------------------------------\n\n|StackSplit|SplitLab|MATLAB|\n|---|---|---|\n|dev ([main branch](https://github.com/michaelgrund/stacksplit))|[1.2.1](https://robporritt.wordpress.com/software/), [1.0.5](http://splitting.gm.univ-montp2.fr/) (not tested)|>= [2020a](https://mathworks.com/help/releases/R2020a/index.html) (< 2020a might work, but not tested yet)|\n|[v3.0](https://github.com/michaelgrund/stacksplit/releases/tag/v3.0) (latest release)|[1.2.1](https://robporritt.wordpress.com/software/), [1.0.5](http://splitting.gm.univ-montp2.fr/) (not tested)|>= [2020a](https://mathworks.com/help/releases/R2020a/index.html) (< 2020a might work, but not tested yet)|\n|[v2.0](https://github.com/michaelgrund/stacksplit/releases/tag/v2.0)|[1.2.1](https://robporritt.wordpress.com/software/), [1.0.5](http://splitting.gm.univ-montp2.fr/)|>= [2014b](https://mathworks.com/company/newsroom/mathworks-introduces-new-features-in-matlab-and-simulink.html) (tested up to and including [2018b](https://mathworks.com/help/releases/R2018b/index.html))|\n|[v1.0](https://github.com/michaelgrund/stacksplit/releases/tag/v1.0)|[1.2.1](https://robporritt.wordpress.com/software/), [1.0.5](http://splitting.gm.univ-montp2.fr/)|<= [2014a](https://mathworks.com/company/newsroom/mathworks-announces-release-2014a-of-the-matlab-and-simulink-product-families.html)|\n\nFor details regarding the different StackSplit versions, see the [Changelog](https://github.com/michaelgrund/stacksplit/blob/main/changelog.md).\n\nContributing\n------------\n\nDid you find a bug or have suggestions for improvements? Simply open a new [issue](https://github.com/michaelgrund/stacksplit/issues) or [pull request](https://github.com/michaelgrund/stacksplit/pulls) here on GitHub.\n\nRelated topics\n--------------\n\n- SplitLab [1.2.1](https://robporritt.wordpress.com/software/): Updated and expanded version (recommended to use with StackSplit)\n\n- SplitLab [1.0.5](http://splitting.gm.univ-montp2.fr/): Original version (most likely works with StackSplit)\n\n- SplitLab [1.9.0](https://github.com/IPGP/splitlab): Most recent version (not compatible with StackSplit yet)\n\n- [SplitPy](https://github.com/paudetseis/SplitPy): Shear wave splitting analysis in Python (based on SplitLab)\n\n- [SWSPy](https://github.com/TomSHudson/swspy): A Python package for performing shear wave splitting in an automated manner\n  \n- [SeisSplit.jl](https://github.com/anowacki/SeisSplit.jl): Shear wave splitting analysis in Julia\n\nReferences\n----------\n\n- **_Restivo, A. & Helffrich, G. (1999)_**, Teleseismic shear wave splitting measurements in noisy environments, Geophysical Journal International 137, 821-830, https://doi.org/10.1046/j.1365-246x.1999.00845.x.\n- **_Roy, C., Winter, A., Ritter, J. R. R., Schweitzer, J. (2017)_**, On the improvement of SKS splitting measurements by the simultaneous inversion of multiple waveforms (SIMW), Geophysical Journal International, 208, 1508–1523, https://doi.org/10.1093/gji/ggw470.\n- **_Wolfe, C. J. & Silver, P. G. (1998)_**, Seismic anisotropy of oceanic upper mantle: Shear wave splitting methodologies and observations, Journal of Geophysical Research 103(B1), 749-771, https://doi.org/10.1029/97JB02023.\n- **_Wüstefeld, A. (2007)_**, Methods and applications of shear wave splitting: The East European Craton. Ph.D. thesis, Univ. de Montpellier, France, http://splitting.gm.univ-montp2.fr/.\n- **_Wüstefeld, A., Bokelmann, G., Zaroli, C., Barruol, G. (2008)_**, SplitLab: A shear-wave splitting environment in Matlab, Computers & Geosciences 34, 515–528, https://doi.org/10.1016/j.cageo.2007.08.002.\n",
        "createdAt": "2016-12-24T14:32:13.000Z",
        "updatedAt": "2025-12-01T12:06:41.000Z",
        "language": "MATLAB",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.5802051",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.5802051",
            "dataCite": "10.5281/zenodo.5802051",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/michaelgrund/stacksplit/main/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.5802051",
            "title": "StackSplit - A plugin for multi-event shear wave splitting analyses in SplitLab",
            "journal": "Zenodo",
            "dateReleased": "2021-12-23T00:00:00.000Z",
            "abstract": "Multi-event processing plugin for the MATLAB shear wave splitting toolbox SplitLab.",
            "citationsArray": []
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "git-taufiq/Advanced-Computational-Seismology-2016",
        "url": "https://github.com/git-taufiq/Advanced-Computational-Seismology-2016",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2016-11-09T07:31:23.000Z",
        "updatedAt": "2018-02-11T17:09:45.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "hs-furtwangen/iris-ws-request",
        "url": "https://github.com/hs-furtwangen/iris-ws-request",
        "description": "Data Requests to the IRIS (Incorporated Research Institutions for Seismology) Web Services",
        "stars": 0,
        "forks": 1,
        "readme": "",
        "createdAt": "2019-06-03T09:58:31.000Z",
        "updatedAt": "2019-11-23T23:54:40.000Z",
        "language": "JavaScript",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "geotechno/rotational-seismology",
        "url": "https://github.com/geotechno/rotational-seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# rotational-seismology\n",
        "createdAt": "2018-01-11T12:53:32.000Z",
        "updatedAt": "2018-01-11T12:53:32.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/geotechno/rotational-seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "himmetgencer/ObsPy_Beaglebone",
        "url": "https://github.com/himmetgencer/ObsPy_Beaglebone",
        "description": "ObsPy on Beaglebone (A Python Framework for Seismology)",
        "stars": 0,
        "forks": 0,
        "readme": "# ObsPy_Beaglebone\nObsPy on Beaglebone (A Python Framework for Seismology)\n\nAM3358 Debian 10.3 2020-04-06 4GB Bone imajı yüklenir.(Debian 10 Buster)\n\nOBSPY Kurulumu\n\nMethod-1(apt)\n```sh\necho \"deb http://deb.obspy.org buster main\" >> /etc/apt/sources.list\nwget --quiet -O - https://raw.githubusercontent.com/obspy/obspy/master/misc/debian/public.key | sudo apt-key add -\nsudo apt-get update\nsudo apt-get install python3-obspy\n```\n\nMethod-2(PyPi Online)\n```sh\nsudo apt-get update\nsudo apt-get install libatlas-base-dev\noptional: sudo pip3 install --upgrade numpy \nsudo pip3 install obspy\n```\n\nMethod-3(PyPi zip file)[https://pypi.org/project/obspy/#files]\n```sh\nsudo apt-get update\nsudo apt-get install libatlas-base-dev\noptional: sudo pip3 install --upgrade numpy \nwget https://files.pythonhosted.org/packages/b8/8c/eef47074a1884c73bc4f2ba7b2961a79fc54952edadeff4b998de86dcb20/obspy-1.2.2.zip\nsudo pip3 install obspy-1.2.2.zip\n```\n\nRING SERVER Kurulumu\n```sh\nwget https://github.com/iris-edu/ringserver/archive/v2020.075.tar.gz\ntar -xzvf v2020.075.tar.gz\nmake\n```\nExample of ring.conf:\n```sh\nRingSize 100M\nRingDirectory /root/ringDir\nDataLinkPort 16000\nSeedLinkPort 18000\nServerID \"TDG_ET_HG\"\nTransferLogRX 0\nMSeedScan /root/ringScanDir StateFile=/root/ringDir/scan.state InitCurrentState=y\n```\n\n```sh\n./ringserver ring.conf\n./slinktool -Q 192.168.2.26:18000\n./slinktool -I 192.168.2.26:18000\n./slinktool -L 192.168.2.26:18000\n```\n\nslinktool kurulumu\n```sh\ngit clone https://github.com/iris-edu/slinktool.git\nezxml make file'da aşağıdaki komutu ekle\n\tCFLAGS = -std=c99\nmake\n```\n\nEk Notlar:\nKullanımı zorunlu değildir.\n\nSSH Root İzin\n```sh\nsudo passwd -d root\nnano /etc/ssh/sshd_config\n\tPermitRootLogin yes\n\tPermitEmptyPasswords yes\n\tUsePAM no\n/etc/init.d/ssh restart\t\n```\nPython 3.8 Kurulumu [https://tecadmin.net/install-python-3-8-ubuntu/]\n```sh\nsudo apt-get install build-essential checkinstall\nsudo apt-get install libreadline-gplv2-dev libncursesw5-dev libssl-dev \\\n    libsqlite3-dev tk-dev libgdbm-dev libc6-dev libbz2-dev libffi-dev zlib1g-dev\n\t\ncd /opt\nsudo wget https://www.python.org/ftp/python/3.8.3/Python-3.8.3.tgz\n\nsudo tar xzf Python-3.8.3.tgz \n\ncd Python-3.8.3\nsudo ./configure --enable-optimizations\nsudo make altinstall\n\npython3.8 -V\n\ncd /opt\nsudo rm -f Python-3.8.3.tgz\n```\nPython Remove\n```sh\nsudo apt list --installed | grep \"python\"\n\n#lists all versions of python installed in your system\n\nsudo apt remove python3.5 (or the version the above command returned)\n\n#the above Command removes python3.5 from the system.\n\nsudo apt purge python3.5\n\n#the above command removes all the folders and files generated or created by python3.5\n```\n\nInstalled Packages Query\n```sh\nsudo dpkg-query -l | less\n```\n\nPasswd Remove\n```sh\nsudo passwd -d root\n```\nInstall specific version\n```sh\npip install numpy==1.10.1\n```\nReinstall specific version\n```sh\nsudo python3.7 -m pip install 'numpy>1.0, <1.15' --force-reinstall\n```\nAdd User To Root Group\n```sh\nsudo usermod -a -G root john\n```\nDelete User With Root Privileges\n```sh\nsudo userdel john\n```\n",
        "createdAt": "2020-07-10T09:46:01.000Z",
        "updatedAt": "2020-10-18T12:18:47.000Z",
        "language": null,
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/himmetgencer/ObsPy_Beaglebone/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "miili/dcube-conv",
        "url": "https://github.com/miili/dcube-conv",
        "description": "Convert DataCube records to miniSEED and StationXML",
        "stars": 0,
        "forks": 0,
        "readme": "# dcube-conv\n",
        "createdAt": "2023-11-26T11:23:30.000Z",
        "updatedAt": "2025-11-25T16:04:52.000Z",
        "language": "Python",
        "homepage": "https://pyrocko.org",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/miili/dcube-conv/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "LLNL/mttime",
        "url": "https://github.com/LLNL/mttime",
        "description": "Time Domain Moment Tensor Inversion in Python",
        "stars": 70,
        "forks": 23,
        "readme": "\nMTtime\n======\n\nMTtime (Time Domain Moment Tensor Inversion in Python) is a python package developed for time domain inversion of complete seismic waveform data\nto obtain the seismic moment tensor. It supports deviatoric and full moment tensor inversions,\nand 1-D and 3-D basis Green's functions.\n\nRequirements\n------------\nThe package was developed on python 3.7 and 3.8, and is running and tested on Mac OSX.\n\n* ObsPy and its dependencies\n* pandas\n* cartopy (for plotting maps)\n\nInstallation\n------------\n\n* Create a Python environment\n* Install ObsPy and pandas\n* Make sure you have `cloned the repository <https://github.com/LLNL/mttime>`_\n* Install mttime\n\nI recommend installing Python via `Miniconda <https://docs.conda.io/en/latest/miniconda.html>`_\nor `Anaconda <https://docs.anaconda.com/anaconda/install/>`_. Choose Miniconda for a lower footprint.\nThen follow the instructions on their sites to install\n`ObsPy <https://github.com/obspy/obspy/wiki/Installation-via-Anaconda>`_\nand `pandas <https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html>`_\nfor your given platform.\n\nDownload mttime and install it from source. If you installed Python via conda make sure you activate\nthe environment where ObsPy and pandas are installed.\n\n.. code-block:: bash\n\n   # Activate environment\n   conda activate your_environment\n\n   # Build and install mttime\n   git clone https://github.com/LLNL/mttime\n   cd mttime\n   pip install .\n\n\nFinally, if you want to run the tutorials you will need to install `Jupyter Notebook <https://jupyter.org/install>`_.\n\nUsage\n-----\n\nExecuting the package from command line will launch the inversion,\nsave and plot the result to file:\n\n.. code-block:: bash\n\n   mttime-run mtinv.in\n\nThe equivalent in the Python console:\n\n.. code-block:: python\n\n   import mttime\n   config = mttime.Configure(path_to_file=\"mtinv.in\")\n   mt = mttime.Inversion(config=config)\n   mt.invert()\n   mt.write()\n\nResources\n---------\n\n* `Documentation for mttime <https://mttime.readthedocs.io/en/latest/index.html>`_\n* `A working example <https://github.com/LLNL/mttime/tree/master/examples/notebooks>`_\n\nLicense\n-------\n`mttime` is distributed under the terms of LGPL-3.0 license. All new contributions must be made under the LGPL-3.0 license.\n\nSPDX-License-Identifier: LGPL-3.0\n\nLLNL-CODE-814839\n",
        "createdAt": "2020-02-27T21:33:25.000Z",
        "updatedAt": "2025-11-30T15:33:07.000Z",
        "language": "Python",
        "homepage": "https://mttime.readthedocs.io/en/latest/index.html",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/LLNL/mttime/master/README.rst",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "uafgeotools/capuaf",
        "url": "https://github.com/uafgeotools/capuaf",
        "description": "seismic moment tensor inversion",
        "stars": 9,
        "forks": 2,
        "readme": "",
        "createdAt": "2015-07-23T23:40:22.000Z",
        "updatedAt": "2025-05-11T16:49:09.000Z",
        "language": "PostScript",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "christophersanborn/Radiative3D",
        "url": "https://github.com/christophersanborn/Radiative3D",
        "description": "Radiative transport in 3D Earth models",
        "stars": 11,
        "forks": 4,
        "readme": "# Radiative3D\nRadiative transport in 3D Earth models\n\nRadiative3D is a 3D radiative transport software tool being developed by Christopher Sanborn and the Solid Earth Geophysics Research Group at the University of Connecticut. Radiative3D can be used to produce synthetic waveforms, travel-time curves, or volumetric visualizations of energy propagation through three-dimensional Earth models. Radiative3D uses ray tracing to simulate propagation dynamics in large-scale structure, and uses a stochastic multiple scattering process to simulate the effects of statistically-described small-scale structure. Radiative3D simulates realistic source events described by moment tensor elements, allowing it to be used to simulate a variety of focal mechanisms, including explosions, double-couple earthquakes, CLVD's, etc.\n\n#### Publications:\n\n* [Modelling Lg blockage, GJI, 2018](https://academic.oup.com/gji/article/214/2/1426/5000173)\n* [Simulations with Radiative3D, 2017](https://opencommons.uconn.edu/dissertations/1460/)\n* [Combined effects of deterministic and statistical structure, GJI, 2017](https://academic.oup.com/gji/article/210/2/1143/3833065)\n\n## What's New:\n\n##### Janaury 2020:\n\n* Added support for model architecture composed of concentric spherical shells, in which the elastic velocity profiles vary quadraticaly with the radial coordinate (_v<sub>P,S</sub> = a r^2 + c_).  The grid defines the velocities at the top and bottom of each spherical layer. This allows for whole-Earth models, efficiently defined in terms of a reasonably small number of layers.\n\n* Raspbian Buster on the Raspberry Pi 4 is now my primary development and testing environment.  Radiative3D is fast, efficient, and runs on inexpensive hardware!  Of course, it still runs great on Intel and AMD-based workstations as well.\n\n## Build Process\n\nRadiative3D builds with GCC on MacOS (OS X), Linux, and Raspbian.  (And perhaps also Windows.)\n\n```\n$ git clone https://github.com/christophersanborn/Radiative3D.git\n$ cd Radiative3D\n$ make\n```\n\nResults in a binary named `main`.\n\nRun with:\n\n```\n$ ./main [args]\n```\n\nUser Manual here: [Radiative3D Manual Page](doc/MANUAL.md)\n\nThere are also supporting scripts (e.g. `do-crustpinch.sh`) to help with managing command line options and organizing the various output files and post-processing of data.  The \"do-scripts\" are writtin in BASH and may depend on the installation of additional command line tools.  (See Manual Page.)\n",
        "createdAt": "2018-07-23T17:08:24.000Z",
        "updatedAt": "2025-06-10T13:29:22.000Z",
        "language": "C++",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/christophersanborn/Radiative3D/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "lauralaurenti/FM_seismology_AutoEncoder",
        "url": "https://github.com/lauralaurenti/FM_seismology_AutoEncoder",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# Testing audio compression autoencoders for seismology: moving toward foundation models \n\nAuthors: Laura Laurenti, Christopher W. Johnson, Daniele Trappolini, Elisa Tinti, Fabio Galasso, Chris Marone. \n\nThe code will be released soon.\n\n",
        "createdAt": "2025-05-12T10:06:41.000Z",
        "updatedAt": "2025-05-12T10:09:04.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/lauralaurenti/FM_seismology_AutoEncoder/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "gcambiotti/seismology",
        "url": "https://github.com/gcambiotti/seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2023-02-28T09:06:55.000Z",
        "updatedAt": "2023-02-28T09:08:17.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "cran/eseis",
        "url": "https://github.com/cran/eseis",
        "description": ":exclamation: This is a read-only mirror of the CRAN R package repository.  eseis — Environmental Seismology Toolbox  ",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2018-06-14T21:56:49.000Z",
        "updatedAt": "2025-03-25T10:41:02.000Z",
        "language": "R",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Githbhatia/ReadV2",
        "url": "https://github.com/Githbhatia/ReadV2",
        "description": "Read and plot CISMIP formatted ,v2 or COSMOS .V2c  seismic ground motion records",
        "stars": 7,
        "forks": 4,
        "readme": "# ReadV2\n\nRead and plot CISMIP formated v2 OR COSMOS formated V2c seismic ground motion records such as posted on CESMD website (Center of Strong Motion Data https://www.strongmotioncenter.org/). The code can also be used to view building instrument records on the CESMD site and those also available at the HCAI website (https://hcai.ca.gov/construction-finance/facility-detail/ - navigate to a hospital that has instrumented buildings and look under the Instrumented Buildings Tab). Python code reads a .v2 or V2c files that contains one or three channels (Free-Field instruments have 3 channels, Instrumented Buildings have records for individual channels.)\n\nUser interface uses tkinter.\n\nThis code functions as a viewer for the ground motion data contained in the v2 file (1 file containing 3 channels with acceleration, velocity or displacment) or V2c files (9 containing each containing acceleration, velocity or displacement for 3 channels). Zip files containing the v2 or V2c files can be directly read including when they are double zipped as is the case when downloaded from the CESMD website (Center of Strong Motion Data https://www.strongmotioncenter.org/).\n\nCode shows location of seismic instrument that originated the record on a map. Plots acceleration, integrated velocity and displacement time history for each component. Plot response spectra in SA vs Time Period or ADRS format - computes energy content of each component. Plots 3D orbit plots. Plots rotated resultant in the maximum acceleration, velocity or displacement directions. Create RotD50, RotD00, RotD100 response spectra. Compare to Geomean Spectra. Plot resultant spectra in a Tripartite format. Compare to ASCE 7-22 design spectra using any coordinates in the US - default is the location of the instrument (that is, using the latitude and longitude of the instrument). \n\nSave time vs. acceleration in a text format.\n\nExample input v2 file from the Ferndale earthquake is included.  See png files to see sample outputs.\n\nNeeds many Python packages: numpy matplotlib itertools tkintner scipy\n\n\nChanges 5/3/2023\n  *Added option to plot Arias Intensity\n  *Changed ASCE7-22 url per changes by USGS\n  \nChanges 5/19/2023\n *Added option to rotate to a specified angle\n\n Changes 10/17/2024\n *Added D5-75 and D5-95 calculations reported on the Arias Intensity plots - easily changed to any interval in code.\n\n Changes 12/7/2024\n *Revised the 3D-orbit plot to have equal axis in all three directions.  Added option to create response spectra for velocity and displacement in addition to acceleration as requested by a user.\n\nChanges 12/8/2024\n*Revised 3D-orbit plot to have colors based on z displacements (up-down).  Changed the main gui window to be scrollable for users with lower resolution monitors.\n\nChanges 12/22/2024\n*Animated 3D-orbit plot  with play, stop and reverse buttons with accompanying side plots that can be acceleration, velocity or displacement.  Circular gridlines added to orbit plots for rotated directions.  v2 files can be zip archives as downloaded from CESMD or CSMIP sites. Other usability improvements.\n\nChanges 12/23/2024\n*As some downloaded CESMD records are zipped once and others zipped twice, added code to accept either.\n\nChanges 12/23/2024\n*Added markers for animations.  Fixed all the window titles with record time and date.  Other usability improvements. Standardize directions for 3D orbit plot as in other orbit plots.\n\nChanges 12/29/2024\n*Added option to read COSMOS V2c files.  Other improvements to speed up animation and spectra calculations (now uses only selected portion of the record).\n\nChanges 3/29/2025\n*Added revised file name schema used by USGS for v2C files for some internation stations.\n",
        "createdAt": "2023-03-31T18:05:43.000Z",
        "updatedAt": "2025-11-20T16:28:20.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Githbhatia/ReadV2/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "RoyMag/Computational-Seismology",
        "url": "https://github.com/RoyMag/Computational-Seismology",
        "description": "Codes for a few of the things in computational seismology I did during my Master Thesis",
        "stars": 0,
        "forks": 0,
        "readme": "# Computational-Seismology\nCodes for a few of the things in seismology\n\nHere I'll be adding some scripts for seismic tomography and in general computational seismology related topics.\nHope it might help you!\n\nAll programs are written using Shell-Scripting, AWK and Python. All graphs are plotted using GMT.\n",
        "createdAt": "2019-03-17T05:45:13.000Z",
        "updatedAt": "2020-06-11T15:45:45.000Z",
        "language": "Shell",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/RoyMag/Computational-Seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "marcopovitch/gr-mseed",
        "url": "https://github.com/marcopovitch/gr-mseed",
        "description": "An Out-of-Tree (OOT) module for gnuradio to handle seismological waveform",
        "stars": 2,
        "forks": 0,
        "readme": "# gr-mseed\n\n\nAn Out-of-Tree (OOT) module for **gnuradio** to handle **seismological waveform** (mseed).\n\n*It was just a Christmas project to have fun*. At least, it's possible \nto connect to a [seedlink](http://ds.iris.edu/ds/nodes/dmc/services/seedlink/) server or open a [mseed](https://ds.iris.edu/ds/nodes/dmc/data/formats/) file (seismological data standard) to plot spectrogram, waterfall, *etc.* using existing gnuradio modules. If you find it useful, please let me know.\n\n![gnuradio interface with gr-mseed](https://cloud.githubusercontent.com/assets/4367036/15627672/9311e104-24eb-11e6-8907-6a3a7fe34a00.png)\n\n![spectro&waterfall](https://cloud.githubusercontent.com/assets/4367036/15627689/2f4e555c-24ec-11e6-949c-a82a51027778.png)\n\n\n## Dependencies\n\n* [gnuradio](http://gnuradio.org) \n* [obspy](https://github.com/obspy/obspy/wiki)\n\n\n## Installation\nGo to the gr-mseed folder, then :\n<pre>\nmkdir build\ncd build\ncmake ../\nmake install\n</pre>\n\n## Usage\n\n### Demo\n\n* The gnuradio-compagnon grc file is localted in the `examples` directory. To start the demo :\n\n\t<pre>gnuradio-compagnon waterfall.grc</pre>\n\n* To try the standalone python app in the `examples` directory:\n\t<pre>./top_block.py</pre>\n\n## Links\n* [Out Of Tree Module Tutorial](https://wiki.gnuradio.org/index.php/OutOfTreeModules)\n",
        "createdAt": "2016-05-28T13:37:14.000Z",
        "updatedAt": "2021-06-22T07:13:53.000Z",
        "language": "CMake",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/marcopovitch/gr-mseed/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "sanjianke87/SeisCodes",
        "url": "https://github.com/sanjianke87/SeisCodes",
        "description": "Public version of code snippets for seismological researches",
        "stars": 0,
        "forks": 0,
        "readme": "# SeisCodes\n\nThis project contains code snippets for seismological researches.\n\n- `distaz`: Calculate great circle arc distance and azimuth/back-azimuth between two geographic coordinates\n- `GMT5_LaTeX_Highlight`: Highlight in LaTeX for GMT5 commands\n- [HinetScripts](https://github.com/seisman/HinetScripts): Python scripts for NIED continuous waveform data requesting and processing\n- `radpat`: Calcuate radiation pattern of P wave for double coupel or moment tensor\n- `nd2layer`: Convert Earth models in .nd format to homogeneous layered models\n",
        "createdAt": "2016-11-04T22:06:28.000Z",
        "updatedAt": "2017-11-07T16:57:42.000Z",
        "language": "Java",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/sanjianke87/SeisCodes/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "HMZ-03/DASPy",
        "url": "https://github.com/HMZ-03/DASPy",
        "description": "DASPy: A Python Toolbox for DAS (Distributed Acoustic Sensing) data processing.",
        "stars": 120,
        "forks": 21,
        "readme": "<img src=\"https://raw.github.com/hmz-03/daspy/main/website/logo.png\" height=\"200\" />\n\n[![Supported Python versions](https://img.shields.io/badge/python-3.9%20|%203.10%20|%203.11%20|%203.12-blue)](https://pypi.org/project/DASPy-toolbox/)\n[![License](https://img.shields.io/pypi/l/daspy-toolbox.svg)](https://opensource.org/license/mit)\n[![PyPI Version](https://img.shields.io/pypi/v/daspy-toolbox.svg)](https://pypi.org/project/DASPy-toolbox/)\n\n[![DOI](https://img.shields.io/badge/DOI-10.1785/0220240124-blue.svg)](https://doi.org/10.1785/0220240124)\n[![PyPI Downloads](https://img.shields.io/pypi/dm/daspy-toolbox.svg?label=pypi)](https://pypi.org/project/DASPy-toolbox/)\n[![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/daspy-toolbox?label=conda)](https://anaconda.org/conda-forge/daspy-toolbox)\n\nDASPy is an open-source project dedicated to provide a python package for DAS (Distributed Acoustic Sensing) data processing.\n\nThe goal of the DASPy project is to lower the bar of DAS data processing. DASPy includes:\n* Classic seismic data processing techniques, including preprocessing, filter, spectrum analysis, and visualization\n* Specialized algorithms for DAS applications, including denoising, waveform decomposition, channel attribute analysis, and strain-velocity conversion. \n\nDASPy is licensed under the MIT License. [An English version of DASPy tutorial](https://daspy-tutorial.readthedocs.io/en/latest/), [a Chinese version of DASPy tutorial](https://daspy-tutorial-cn.readthedocs.io/zh-cn/latest/) and [an example of Jupyter notebook](document/example.ipynb) is available. If you have any questions, please contact me via <hmz2018@mail.ustc.edu.cn>.\n\n## Installation\nDASPy runs on Linux, Windows and Mac OS and on Python 3.9 and up.\n\n### Pip\n```\npip install daspy-toolbox\n```\n\nInstall the latest version from GitHub:\n\n```\npip install git+https://github.com/HMZ-03/DASPy.git\n```\n\n### Conda\n\n```\nconda install daspy-toolbox\n```\n\nor\n\n```\nconda install conda-forge::daspy-toolbox\n```\n\n### Manual installation\n1. Install dependent packages: numpy, scipy >=1.13, matplotlib, geographiclib, pyproj, h5py, segyio, nptdms, tqdm\n\n2. Add DASPy into your Python path.\n\n## Getting started\n```\nfrom daspy import read\nsec = read()  # load example waveform\nsec.bandpass(1, 15)\nsec.plot()\n```\n<img src=\"./website/waveform.png\" height=\"500\" />\n\n### Contributing\n\nPlease see details on how to contribute to the project [here](CONTRIBUTING.md) and [here](CodingStyleGuide.md).\n\n### Reference\n\n  * Minzhe Hu and Zefeng Li (2024), [DASPy: A Python Toolbox for DAS Seismology](https://pubs.geoscienceworld.org/ssa/srl/article/95/5/3055/645865/DASPy-A-Python-Toolbox-for-DAS-Seismology), *Seismological Research Letters*, 95(5), 3055–3066, doi: `https://doi.org/10.1785/0220240124`.\n",
        "createdAt": "2024-03-27T10:47:41.000Z",
        "updatedAt": "2025-11-19T14:04:01.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/HMZ-03/DASPy/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "solldavid/TwistPy",
        "url": "https://github.com/solldavid/TwistPy",
        "description": "Toolbox for Wavefield Inertial Sensing Techniques",
        "stars": 36,
        "forks": 8,
        "readme": "![Twistpy](https://github.com/solldavid/TwistPy/blob/main/docs/source/_static/logo_adobe_title.svg)\r\n[![GithubAction Status](https://github.com/solldavid/twistpy/actions/workflows/build.yaml/badge.svg)](https://github.com/solldavid/twistpy/actions/workflows/build.yaml)\r\n![OS-support](https://img.shields.io/badge/OS-linux,win,osx-850A8B.svg)\r\n![Python39](https://img.shields.io/badge/python-3.9-yellow.svg)\r\n![Python310](https://img.shields.io/badge/python-3.10-yellow.svg)\r\n[![DOI](https://zenodo.org/badge/460458412.svg)](https://zenodo.org/badge/latestdoi/460458412)\r\n[![License: LGPL v3](https://img.shields.io/badge/License-LGPL_v3-red.svg)](https://www.gnu.org/licenses/lgpl-3.0)\r\n[![PEP8](https://img.shields.io/badge/code%20style-pep8-orange.svg)](https://www.python.org/dev/peps/pep-0008/)\r\n\r\nTwistPy is a small open-source Python package for seismic data processing. It includes routines for single-station polarization\r\nanalysis and filtering, as well as array processing tools.\r\n\r\nA special focus lies on innovative techniques to process spatial wavefield gradient data and, in particular, rotational\r\nseismic data obtained from dedicated rotational seismometers or small-aperture arrays of three-component sensors.\r\n\r\nSome of the tools available in TwistPy are:\r\n\r\n- Three-component polarization analysis and filtering (both time domain and S-transform).\r\n- Six-component polarization analysis and filtering (both time domain and S-transform).\r\n- Six-component wave type fingerprinting.\r\n- Single-station six-component Love- and Rayleigh-wave dispersion and Rayleigh wave ellipticity angle estimation.\r\n- Dynamic tilt corrections for seismometers using direct rotation measurements.\r\n- Beamforming (Bartlett, MVDR, and MUSIC algorithm).\r\n- Forward and inverse S-transform (Stockwell transform).\r\n\r\nCheck out the documentation at:\r\nhttps://twistpy.org/\r\n",
        "createdAt": "2022-02-17T13:58:42.000Z",
        "updatedAt": "2025-11-15T13:58:24.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/solldavid/TwistPy/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "mfaccenda/ECOMAN2.0-seismology.SKS-SPLIT",
        "url": "https://github.com/mfaccenda/ECOMAN2.0-seismology.SKS-SPLIT",
        "description": null,
        "stars": 0,
        "forks": 2,
        "readme": "# SKS-SPLIT\nSKS-SPLIT estimates the SKS splitting at a grid of virtual seismic stations placed at the top of the D-REX_M model as a function of the back-azimuth using the Fortran routines included in FSTRACK (Schulte-Pelkum and Blackmann, 2003; Becker et al., 2006). The routines have been adapted to load the D-REX_M output, stack the elastic tensors and densities in an upper mantle rock column beneath each virtual seismic station (Faccenda and Capitanio, 2013), and run in parallel using MPI.\n\n# COMPILATION\n\nUntar fstrack.tar.gz: tar -zxvf fstrack.tar.gz\n\nExecute the Makefile in fstrack/single_layer1: make\n\nExecute the Makefile in fstrack/multi_layer: make\n\nCopy the binary file anicake from /fstrack/bin directory to the SKS-SPLIT directory containing the bash file pbs_sks. Directory /fstrack can be now deleted, if wanted.\n\nExecute ./bash_compile in SKS-SPLIT\n\nDepending on the Intel Fortran Compiler and environment settings, F77=ifort, F90=ifort, CC=icc must be added to each of the two Makefile. This is done in the FSTRACK version present in this package\n\n# RUN \n\nSubmit the pbs_sks bash file which consecutively runs:\n\n./stack_calc args (generates a stack of horizontal layers with different elastic\ntensors)\n\nmpiexec -np nprocs ./split_calc args (computes splitting paramters aver-\naged over back-azimuth)\n\n\nMore detailed information and instructions are provided in the user [manual](https://newtonproject.geoscienze.unipd.it/wp-content/uploads/2024/02/ECOMAN2.0_manual.pdf). \n\n",
        "createdAt": "2024-01-31T09:26:30.000Z",
        "updatedAt": "2024-01-31T13:51:42.000Z",
        "language": "Fortran",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/mfaccenda/ECOMAN2.0-seismology.SKS-SPLIT/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "JaviNGD/seismology",
        "url": "https://github.com/JaviNGD/seismology",
        "description": "Aplicación que por medio de una task obtiene la data sismológica desde el sitio USGS, de los últimos 30 días (API) y persiste la información en una base de datos de acuerdo a ciertos parámetros.",
        "stars": 0,
        "forks": 0,
        "readme": "# Seismology\n\nAplicación que por medio de una task obtiene la data sismológica desde el sitio USGS, de los últimos 30 días <a href=\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_month.geojson\">(API)</a> y persiste la información en una base de datos de acuerdo a ciertos parámetros.\n\n## Correr el proyecto localmente\n\n### Clonar el proyecto\n```plaintext\nhttps://github.com/JaviNGD/seismology.git\n```\n\n### Ir al directorio del proyecto\n```plaintext\ncd seismology\n```\n\n### Instalar gemas\n```plaintext\nbundle install\n```\n\n### Crear en el directorio raíz un archivo .env para la conexión con la base de datos\n```plaintext\nDB_USERNAME=\nDB_PASSWORD=\nDB_HOST=\nDB_PORT=\n```\n\n### Crear la base de datos\n```plaintext\nrails db:create\n```\n\n### Migrar la base de datos\n```plaintext\nrails db:migrate\n```\n\n### Iniciar el servidor \n```plaintext\nrails server\n```\n\n### Se puede ingresar al servidor local desde:\n```plaintext\nhttp://127.0.0.1:3000/ ó http://localhost:3000/\n```\n*La dirección cambia respecto al valor ingresado en DB_HOST, del archivo .env*\n\n## Para correr la tarea ejecutar el siguiente comando\n```plaintext\nrails import_earthquake_data:earthquake_data\n```\n\n## Endpoint 1\n\n```plaintext\ncurl -X GET\n'http://127.0.0.1:3000/api/features'\n-H 'Content-Type: application/vnd.api+json' \n-H 'cache-control: no-cache'\n```\n\n### Filtrar por mag_type\n```plaintext\ncurl -X GET \\\n'http://localhost:3000/api/features?filters[mag_type]=valor' \\\n-H 'Content-Type: application/vnd.api+json' \\\n-H 'cache-control: no-cache'\n```\n\n### Paginación\n```plaintext\n'http://127.0.0.1:3000/api/features?page=valor'\n-H 'Content-Type: application/vnd.api+json' \n-H 'cache-control: no-cache'\n```\n\n🌍\n",
        "createdAt": "2024-04-15T17:47:04.000Z",
        "updatedAt": "2024-04-15T23:09:34.000Z",
        "language": "Ruby",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/JaviNGD/seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "maspy/maspy",
        "url": "https://github.com/maspy/maspy",
        "description": "Münster Array Seismology",
        "stars": 1,
        "forks": 1,
        "readme": "# maspy\nMünster Array Seismology\n",
        "createdAt": "2016-07-03T14:24:50.000Z",
        "updatedAt": "2019-10-18T08:34:54.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/maspy/maspy/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "pnsn/PULSE",
        "url": "https://github.com/pnsn/PULSE",
        "description": "Processing Utility for Live Seismic Events: Connecting emerging python seismology tools to live-streaming seismic data flows",
        "stars": 0,
        "forks": 0,
        "readme": "# PULSE  \n# ~~|^v~~~  \n## **P**rocessing (**P**ython) **U**tility for **L**ive **S**eismic **E**vents  \nIntegrating emerging python seismic analysis codes into live-streaming seismic data workflows.\n\n## About  \nPULSE is an open-source python project that provides an adaptable framework for integrating python-based seismic analysis tools into live-streaming seismic data analysis systems. This initial version of the project focuses on integrating machine-learning enhanced phase detection and labeling workflows hosted in [`SeisBench`](https://seisbench.readthedocs.io/en/stable/) into [`Earthworm`](http://www.earthwormcentral.org). In future releases we hope to incorporate hosting capabilities for computationally efficient phase association and denoising workflows.\n\nPULSE workflows center around sequences of single-task modules that progressively process data objects as they become available. This cascading data flow operates in discrete pulses (via the **pulse** class method for each module) that can be tuned Each module has a **pulse** method that conducts some number of actions \n\nPULSE uses and augments popular Python API's for routine seismic data analyses ([`ObsPy`](https://docs.obspy.org) and [`NumPy`](https://numpy.org)) to provide familar  and the key open-source package [`PyEarthworm`](https://github.com/Boritech-Solutions/PyEarthworm) for Python/Earthworm integration.\n\nIn future releases we plan to incorporate \n\n**We thank each of these development teams for their dedication to open-source scientific software.**  \n### License\nThis project is distributed under a GNU Affero General Public License (AGPL-3.0) to comform with licensing terms of its key dependencies and inspirations.  \n<a title=\"Affero General Public License\" href=\"https://en.wikipedia.org/wiki/GNU_Affero_General_Public_License\">\n    <img width=\"256\" alt=\"AGPLv3 Logo\" src=\"https://upload.wikimedia.org/wikipedia/commons/0/06/AGPLv3_Logo.svg\">\n</a>  \n\n# Getting Started\n\nPULSE consists of a collection of single-task-oriented module classes housed in `PULSE.module`,\nmodified ObsPy data classes housed in `PULSE.data`, and pre-composed sequences of modules housed\nin `PULSE.sequences`. Supporting python methods are housed in `PULSE.util`\n\n Supporting methods\n\n## For new users \nWe recommend installing `PULSE` and working through the **Pure-Python Tutorials** first to get familiar with the python-side aspects of the API.  \nOnce you're comfortable with these parts of the project, proceed with installing `Earthworm`, a Test Suite dataset, and `PyEarthworm` and try out the **Earthworm-Integrated Tutorials**\n\n### Installation Instructions  \n\n### Installing `PULSE`\nWe recommend creating a `conda` environment with clean installs of `pip` and `git` for the current distribution:  \n```\nconda create --name PULSE pip git\nconda activate PULSE\npip install git+https://github.com/pnsn/PULSE.git@develop\n``` \n\n#### Pure-Python Tutorials (No Earthworm Required)\n\n| Examples                        | Source Data  |  Notebook    | Reference                    |  \n| ------------------------------- | ------------ | ------------ | ---------------------------- |\n| Introduction to PULSE Data Classes | local | PLACEHOLDER | | \n| ObsPy Signal Processing | local        | PLACEHOLDER  |                              |\n| PhaseNet on One Station         | local        | PLACEHOLDER  | [Retailleau et al. (2022)](https://doi.org/10.1785/0220210279)   |\n| EQTransformer on Many Stations  | local        | PLACEHOLDER  | [Ni et al. (2023)](https://doi.org/10.26443/seismica.v2i1.368) | \n| Ensembling Model Predictions    | local        | PLACEHOLDER  | [Yuan et al. (2023)](https://doi.org/10.1109/TGRS.2023.3320148) | \n| PhaseNet + GaMMA Pick/Associate | local        | PLACEHOLDER  | |\n\n\n### Installing `Earthworm` and a Test Suite\nFollow  \n* Directions on Earthworm 7.10 installation can be found [here](https://gitlab.rm.ingv.it/earthworm/earthworm)  \n* The Univerity of Memphis Test Suite can be downloaded directly [here] (http://www.earthwormcentral.org/distribution/memphis_test.zip)\n\n**NOTE**: The PNSN is developing an PNW test suite to showcase PULSE' functionalities. Stay tuned!  \n\n### Installing `PyEarthworm`\n#### `pip` install from `main`\n**NOTE**: This is an abstraction from the PyEarthworm install instructions, refer to their repository for authoritative installation instructions  \n\nSource your `Earthworm` OS-specific environment (e.g., for the Memphis Test Suite example installed on a Mac)     \n```\nsource /usr/local/earthworm/memphis/params/ew_macosx.bash\n```\n\nInstall `PyEarthworm` from `main`  \n```\npip install git+https://github.com/Boritech-Solutions/PyEarthworm\n```  \n\n#### Earthworm Integrated Tutorials  \n\n| Examples                        | Source Data  |  Notebook    | \n| ------------------------------- | ------------ | ------------ |\n| PhaseNet RING2DISK Prediction   | Tankplayer   | PLACEHOLDER  | \n| ObsPy Picker RING2RING          | Tankplayer   | PLACEHOLDER  |\n| PhaseNet Picker RING2RING       | TankPlayer   | PLACEHOLDER  | \n| Ensemble Picker RING2RING       | TankPlayer   | PLACEHOLDER  |\n| GaMMA Association RING2RING     | TankPlayer   | PLACEHOLDER  |\n| PhaseNet + GaMMA RING2RING        | TankPlayer   | PLACEHOLDER  |\n\n\n## Installation In A Nutshell (For Experienced Users)\n```\nconda create --name PULSE pip git\n```\n```\nconda activate PULSE\n```\n```\npip install git+https://github.com/pnsn/PULSE@develop\n```\n```\nsource </path/to/your/ew_env.bash>\n```\n```\npip install git+https://github.com/Boritech-Solutions/PyEarthworm\n```\n\n## Adding Visualization Tools for `class DictStream` (For Experienced Users)  \nPULSE includes data visualization methods for the `DictStream` class that use elements of the [Pyrocko](https://pyrocko.org) project, namely `snuffler`. To add these tools to the environment described above, install the Pyrocko library following their instructions [here](https://pyrocko.org/docs/current/install/). These functionalities are not required for typical module operation with PULSE, but users may find them handy.  \n\n```\nconda install -c pyrocko pyrocko\n```\n\n# Documentation (Work In Progress)  \nSphinx documentation in ReadTheDocs formatting is under construction - stay tuned!  \nResource: https://sphinx-rtd-tutorial.readthedocs.io/en/latest/  \n\n\n<!-- ### Install with `conda`  \nThe same as above, but using a *.yaml  \n```\nwget https://github.com/pnsn/PULSE/conda_env_create.yaml\n``` -->\n\n\n\n# Additional Information\n\n## Primary Developer  \nNathan T. Stevens  \nemail: ntsteven (at) uw.edu  \norg: Pacific Northwest Seismic Network\n\n## Project Dependencies & Resources\n[`Earthworm`](http://www.earthwormcentral.org)  \n[`NumPy`](https://numpy.org)  \n[`ObsPy`](https://docs.obspy.org)  \n[`PyEarthworm`](https://github.com/Boritech-Solutions/PyEarthworm)  \n[`PyEarthworm Workshop`](https://github.com/Fran89/PyEarthworm_Workshop)  \n[`Pyrocko`](https://pyrocko.org)  \n[`SeisBench`](https://github.com/seisbench/seisbench)  \n\n## Branching Plan/Development Notes\n\nCurrent development version: ALPHA \n\nThe current developmental version of this code is hosted on the `develop` branch. Starting with version 0.0.1 the `main` branch will host deployment read code, `develop` will contain code that is in beta (debug only), and subsidiary `feature-*` branches will host new functionalities under development..  \n\nDeveloped with Python 3.1X, Apple M2 chipset, and Earthworm 7.10  \n\n\n<!-- ## Notes on the Initial Package Development\nThis initial version focuses on body wave detection and labeling tasks using the EarthquakeTransformer (EQT; Mousavi et al., 2018) and PhaseNet (Zhu et al., 2019) model architectures, along with the following pretrained model weights available through `SeisBench` (Woollam et al., 2020).\n\n| Model  | Weight   | Appeal                              | Reference               | DOI |\n|:------:| -------- | ----------------------------------- | ----------------------- | ------ |\n| EQT    | pnw      | PNSN Data Transfer Learning         | Ni et al. (2023)        | https://doi.org/10.26443/seismica.v2i1.368 |\n| EQT/PN | instance | Extensive Training Augmentation     | Michelini et al. (2021) | https://doi.org/10.13127/INSTANCE |\n| EQT/PN | stead    | \"Go-To\" Benchmark Training Dataset  | Mousavi et al. (2019)   | https://doi.org/10.1109/ACCESS.2019.2947848 |\n| EQT/PN | iquique  | Subduction Zone Aftershock Sequence | Woollam et al. (2019)   | https://doi.org/10.1785/0220180312 |\n| EQT/PN | lendb    | Local Seismicity                    | Magrini et al. (2020)   | https://doi.org/10.1016/j.aiig.2020.04.001; http://doi.org/10.5281/zenodo.3648232 |\n| PN     | diting   | Large mag range & event diversity   | Zhao et al. (2022)      | https://doi.org/10.1016/j.eqs.2022.01.022 |  \n\nAbstracted from `SeisBench` documentation: https://seisbench.readthedocs.io/en/stable/pages/benchmark_datasets.html#  \n -->\n",
        "createdAt": "2023-12-07T18:15:14.000Z",
        "updatedAt": "2025-01-29T00:03:52.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.3648232",
            "dataCite": "10.5281/zenodo.3648232",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/pnsn/PULSE/develop/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.3648232",
            "title": "LEN-DB - Local earthquakes detection: a benchmark dataset of 3-component seismograms built on a global scale",
            "journal": "Zenodo",
            "dateReleased": "2020-02-06T00:00:00.000Z",
            "abstract": "In this study ( The paper ) we present a large dataset of 1,249,411 3-component seismograms, recorded along the vertical, north, and east components of 1487 broad-band or very broad-band receivers distributed worldwide, including 631,105 3-component seismograms generated by 304,878 local earthquakes and labeled as earthquakes (EQ), and 618,306 ones labeled as noise (AN). The choice of collecting only local earthquake-data is motivated by the fact that small-magnitude events, which generate relatively small amplitudes and are easily attenuated, are often problematic to detect but provide valuable information about earthquake processes. The labeled data are split into HDF5-Groups: <em>EQ</em> and <em>AN</em>. Each of these groups contains as many HDF5-Datasets as the number of 3-component seismograms; these are labeled in accordance to the format <em>net_sta_starttime</em>, where <em>net</em>, <em>sta</em>, and <em>starttime</em> represent the seismic network, station, and start time of the seismograms. Each HDF5-Dataset (i.e. each triplet of seismograms) has an attribute, which allows accessing the respective metadata. In addition, the HDF5-Group <em>Stations</em> allows accessing stations’ metadata through as many HDF5-Datasets (which are labeled in accordance to the format <em>net_sta)</em> as the number of receivers employed for collecting the waveforms. This global dataset is intended to be used for carrying out a multitude of seismological and signal processing tasks on single-station recordings, and its size particularly suits machine learning (ML) applications.. Application of ML to this dataset shows that a simple Convolutional Neural Network of 67,939 parameters allows discriminating between earthquakes and noise single-station recordings with high accuracy (93.2%), even if applied in regions not investigated by the training set. We make the dataset publicly available as a unique file in HDF5 data format, intending to provide the seismological and broader scientific community with a benchmark for time-series to be used as a testing ground in seismology and signal processing.",
            "citationsArray": []
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "IshaLohan/Environmental-Seismology",
        "url": "https://github.com/IshaLohan/Environmental-Seismology",
        "description": "Summer 2021 IISc Bengaluru",
        "stars": 0,
        "forks": 0,
        "readme": "# Environmental-Seismology\nSummer 2021 IISc Bengaluru\n\nDuring my bachelor's summer internship at the Center for Earth Sciences, IISc, Bengaluru, in 2021, I did a literature review on Environmental Seismology.\nMainly Glacial, Fluvial, Volcano Seismology and Microseism were covered. A few of those ppts are added here.\nThe only purpose of sharing ppts is to share knowledge.\nI tried my best not to miss any references, but if I missed any I am sorry for it and would be happy to receive feedback.\n",
        "createdAt": "2023-08-25T07:06:54.000Z",
        "updatedAt": "2023-08-25T07:06:55.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/IshaLohan/Environmental-Seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "muteebmohsin/IRIS-2022-Seismology-Skill-Building-Workshop",
        "url": "https://github.com/muteebmohsin/IRIS-2022-Seismology-Skill-Building-Workshop",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# **Building the IRIS Docker Image and Running it in a Docker Container**\r\n\r\n## *Table of Contents*\r\n- [**Installation**](#installation)\r\n    - [Install WSL2 (Windows)](#install-wsl2)\r\n    - [Install Docker](#install-docker)\r\n    - [Install Git](#install-git)\r\n- [**Environment Set Up**](#environment-set-up)\r\n    - [Create a Case-Sensitive Disc Image (MacOS)](#create-a-case-sensitive-disc-image)\r\n    - [Before Cloning the Git Repository](#before-cloning-the-git-repository)\r\n        - [Windows](#windows)\r\n        - [MacOS](#macos)\r\n        - [Ubuntu](#ubuntu)\r\n    - [Cloning Repository](#cloning-repository)\r\n        - [ssh](#ssh)\r\n        - [https](#https)\r\n    - [Move to the IRIS Directory](#move-to-the-iris-directory)\r\n- [**Running IRIS**](#running-iris)\r\n    - [Image Build and Run](#image-build-and-run)\r\n        - [(Optional) Build the Image Without Running](#optional-build-the-image-without-running)\r\n        - [(Optional) Run Container Without Rebuilding](#optional-run-container-without-rebuilding)\r\n    - [IRIS Container](#iris-container)\r\n        - [Open Jupyter in Your Browser](#open-jupyter-in-your-browser)\r\n        - [Stop Your Container](#stop-your-container)\r\n        - [Run the Container Again](#run-the-container-again)\r\n- [**Migrating from OSL to Docker**](#migrating-from-osl-to-docker)\r\n    - [Compress Directories to a Zip File](#compress-directories-to-a-zip-file)\r\n    - [Download Zip File to Your Computer](#download-zip-file-to-your-computer)\r\n    - [Copy Zipped File to Docker](#copy-zipped-file-to-docker)\r\n    - [Unzip the Zipped File](#unzip-the-zipped-file)\r\n- [**Troubleshooting**](#troubleshooting)\r\n    - [Common Issues](#common-issues)\r\n    - [If You Encounter Issues](#if-you-encounter-issues)\r\n---\r\n\r\n\r\n\r\n# **Installation**\r\n\r\nIn this section, we will provide brief guides on how to set up an environment to run the IRIS Dockerfile.\r\n\r\n\r\nIn short, here are the list of things you will need to set up:\r\n- WSL2 (For Windows users only)\r\n- Docker\r\n- Git\r\n\r\n\r\n---\r\n## **Install WSL2**\r\n---\r\n\r\n_**Note: This only applies to Windows OS.**_\r\n\r\nWSL2 is a paired-down version of Linux running on Windows 10+.\r\n\r\n- [WSL 2 Installation Instructions]( https://docs.microsoft.com/en-us/windows/wsl/install-win10)\r\n    - Install the terminal as described in the final optional step on the page linked above\r\n    - Make sure to choose Ubuntu for the Linux kernel. While other Linux kernels may work, we have not tested and thus there is no guarantee that they will work.\r\n\r\n---\r\n\r\n## **Install Docker**\r\n\r\n- [Linux instructions](https://docs.docker.com/engine/install/ubuntu/) \r\n    - Select your Linux flavor from the left sidebar menu.\r\n- [Windows Instructions using WSL 2](https://docs.docker.com/desktop/windows/install/)\r\n    - Be sure to follow the WSL2 backend-specific instructions.\r\n- [Mac Instructions](https://docs.docker.com/desktop/mac/install/)\r\n    - *Only tested on x86 Mac support.\r\n\r\n_***Note: M1 Macs were not supported until recently, hence we cannot guarantee that it will work.**_\r\n\r\nTo make sure that Docker is properly installed, run the basic Docker command to test its validity. \r\n\r\n_Example:_ If you open and run `docker ps`, you should see something like this:\r\n![`docker ps` sample](img/docker_ps_sample.PNG)\r\n\r\nIf you get an error on basic commands, such as `docker ps`, you may need to troubleshoot.\r\n\r\n---\r\n    \r\n## **Install Git**\r\n\r\n- Install Git\r\n    - [Linux and Mac Instructions](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)\r\n    - Windows\r\n        - Follow the Linux instructions, running them in a WSL2 terminal\r\n- (_Optional_) Create an SSH key\r\n    - If you previously installed git outside of WSL2, you may need to generate a new ssh key.\r\n    - [Instructions](https://docs.github.com/en/github/authenticating-to-github/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent)\r\n- (_Optional_) Register your SSH key with GitHub\r\n    - If using WSL2 on Windows, note that your SSH keys will be stored in `/home/<username>/.ssh/`\r\n    - You can get `<username>` by using `whoami` command.\r\n    - [Instructions on how to set your ssh key to GitHub](https://docs.github.com/en/github/authenticating-to-github/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account).\r\n  \r\n\r\n_**Note: Setting `ssh` keys is optional since you can still run the IRIS Docker image by cloning with `https`. However, generating and adding an `ssh` key is more convenient in a long run.**_\r\n\r\n---\r\n\r\n# **Environment Set Up** \r\n\r\nAssuming you have completed the installation, this section will provide a general guideline on how to set the environment up. \r\n\r\nIn this section, you will:\r\n- Create a Case-Sensitive Disc Image (MacOS only)\r\n- Clone Git repository for IRIS\r\n\r\n---\r\n\r\n\r\n## Create a Case-Sensitive Disc Image\r\n\r\n_**Note: This only applies to MacOS.**_\r\n\r\n **Size Requirements**:\r\n\r\n    - Minimum 15 GB:\r\n        - 12 GB for Docker image \r\n        - Enough volume space for you to save progress.\r\n\r\n**Instruction**:\r\n\r\n\r\n- Open `Disc Utility`\r\n\r\n    ![Disk Utility](img/disc_utility.png)\r\n\r\n- Create a blank image\r\n    - Select `File -> New Image -> Blank Image...`\r\n    \r\n  ![blank image](img/blank_image.png)\r\n  \r\n    - Fill out the image details\r\n        - `Save As`\r\n            - name of the `.dmg` file the image will be saved to (IRIS)\r\n        - `Where`\r\n            - directory where `.dmg` will be saved (wherever you like)\r\n        - `Name`\r\n            - name of the image itself (IRIS)\r\n        - `Size`\r\n            - amount of space available on the image (should be large enough to hold your environment and data)\r\n        - `Format`\r\n            - `APFS (Case-sensitive)`\r\n        - Leave remaining fields set to their default values\r\n    - Click the `Save` button\r\n    \r\n    ![iris create image](img/iris_create_image.png)\r\n \r\n- Open a terminal\r\n  - change directories into your mounted volume (created from your new disc image)\r\n    - `cd /Volumes/IRIS`\r\n  - confirm case-sensitivity by creating two directories or files whose name differ only in capitalization\r\n    - `mkdir a`\r\n    - `mkdir A`\r\n    - if you are able to create both files or directories, the file system is case-sensitive\r\n    \r\n    ![iris confirm case sensitivity](img/iris_confirm_case_sensitive.png)\r\n     \r\n---\r\n## **Before Cloning the Git Repository**\r\n---\r\n\r\nThe process of cloning the repository differs based on the Operating System you are using.\r\n\r\n### **Windows**:\r\n---\r\n- Open WSL2 terminal\r\n- Move to your Linux home directory using `cd ~/`\r\n    - Run `pwd` to check that you are in `/home/<username>/`. At a minimum, make sure you don't have `/mnt/c/` when you use `pwd`.\r\n- Once you are on the Linux side of WSL, run the git clone command. Refer to the _cloning repository_ section for more detail.\r\n\r\n### **MacOS**\r\n---\r\n- In a terminal, move to the case-sensitive volume you created using the instructions above\r\n    - `cd /Volumes/IRIS`\r\n- Run the git clone command. Refer to the _cloning repository_ section for more detail.\r\n\r\n---\r\n**WARNING** - Windows (WSL2) and Mac OSX file systems are **case-insensitive** by default, which will cause an issue if the operating system-specific instructions are not followed.\r\n<br />\r\n\r\n---\r\n\r\n### **Ubuntu**\r\n---\r\n- Open your terminal\r\n- Run the git clone command. Refer to the _cloning repository_ section for more detail.\r\n\r\n---\r\n\r\n## **Cloning Repository**\r\n\r\n---\r\n\r\nYou will be cloning from ASF's [opensarlab-docker](https://github.com/ASFOpenSARlab/opensarlab-docker) repository. Make sure that you are in `main` branch. \r\n\r\n_**Note: When you clone this repository, you will only be using the `iris2022` directory. Ignore the `unavco2021/2022` directory.**_\r\n\r\nThere are two different ways of cloning a repository: You can clone by using `ssh` or using `https`.\r\n\r\n### **ssh**\r\n---\r\n\r\nBefore you begin, make sure that your `ssh` key is generated and attached to your GitHub account. To do this, refer to the _Install Git_ section.\r\n\r\n\r\nAssuming you have your `ssh` key setup, run the following command:\r\n\r\n```bash\r\ngit clone git@github.com:ASFOpenSARlab/opensarlab-docker.git\r\n```\r\n\r\nThis will clone the `opensarlab-docker` repository to where you are currently at.\r\n\r\n### **https**\r\n---\r\n\r\nIf you have not set your `ssh` key, you can clone your repository using `https`. \r\n\r\nSimply use the following command:\r\n\r\n```bash\r\ngit clone https://github.com/ASFOpenSARlab/opensarlab-docker.git\r\n```\r\n\r\nThis will also clone the `opensarlab-docker` repository to where you are currently without having to configure your `ssh` key.\r\n\r\n\r\n---\r\n\r\n## **Move to the IRIS Directory**\r\n---\r\n\r\nOnce you cloned your repository, run the following command to change into proper location:\r\n\r\n```bash\r\ncd opensarlab-docker/iris2022\r\n```\r\n\r\nIf you are in the right location, you should see a `Makefile`. You can verify this with the following command:\r\n\r\n```bash\r\nls\r\n```\r\n---\r\n\r\n# **Running IRIS**\r\n---\r\n\r\nThis seciton will introduce users on how to use the `Docker` to properly run the IRIS deployment on their local computer.\r\n\r\n---\r\n\r\n## **Image Build and Run**\r\n---\r\n\r\nBefore you begin, make sure that your Docker is running properly. \r\n\r\n\r\nOnce you installed Docker in your system, test that Docker works with few simple commands.\r\n\r\n_Example:_\r\n\r\n```docker\r\ndocker ps\r\n```\r\nShould display all containers that are currently running.\r\n\r\n\r\n```docker\r\ndocker images\r\n```\r\n\r\nShould display all images, including hidden ones, on to your terminal.\r\n\r\nIf you never ran a Docker before, it should not display any images or containers. However, you should not see any error as this indicates that there was something wrong with initial installation process.\r\n\r\n\r\nWe would also like to mention that when you are building a Docker image for the first time, it will take a long time, especially for big images like IRIS (30+ minutes).\r\n\r\nRunning the image for a second time should be instant as long as you don't make any changes to the `iris/dockerfile`.\r\n\r\n---\r\n\r\n\r\n### **Starting IRIS Deployment**\r\n---\r\n\r\nIn your terminal, run the following command:\r\n```bash\r\nmake\r\n```\r\n\r\nThis command will automatically `build` your Docker image and then `run` the image you just built. \r\n\r\nNote that we direct output to a log file\r\n- If your image build or container run fails, please send this log file when you reach out for support\r\n\r\n---\r\n### **(Optional) Build the Image Without Running**\r\n---\r\n\r\nIf you just want to build the image and not run it, use the following command:\r\n\r\n```bash\r\nmake build\r\n```\r\n\r\nNote that we direct output to a log file\r\n- If your image build or container run fails, please send this log file when you reach out for support\r\n\r\n---\r\n\r\n### **(Optional) Run Container Without Rebuilding**\r\n---\r\n\r\nAlternatively, if you already have a built image and just want to run the image without rebuilding, use the following command:\r\n\r\n```bash\r\nmake run\r\n```\r\n\r\nNote that we direct output to a log file\r\n- If your image build or container run fails, please send this log file when you reach out for support\r\n\r\n\r\n---\r\n## **IRIS Container**\r\n---\r\n\r\nNow that you have your Docker container running, you can follow these steps to navigate through your IRIS deployment.\r\n\r\n---\r\n\r\n### **Open Jupyter in Your Browser**\r\n\r\n- After successfully running the container, you will see some URLs in your terminal\r\n\r\n![Image showing jupyter url to open](img/jupyter_url.png)\r\n\r\n- Open the bottom URL in your browser\r\n- Do your work\r\n- Files you save in your home directory will be saved in your local `virtual_home/` directory and will still be accessible after the container is shut down\r\n\r\n---\r\n### **Stopping Your Container**\r\n---\r\n\r\n- In the terminal running your container and Jupyter Server\r\n    - In Linux and Windows, type `Ctrl + c` twice\r\n    - In Mac OS, type `control + c` twice \r\n\r\n- **NOTE:** If you are using WSL2 (i.e. Windows), you can close a terminal window without stopping any of its running processes. If you close the window where your container is running, it will stay alive and prevent you from running `make run` again until the container has been stopped. If this happens, complete the following steps to stop the container\r\n\r\n    1. Use `docker ps` to check if container the is running. If it is, copy the `CONTAINER ID`. ![docker_ps](./img/docker_ps.PNG)\r\n    1. Use `docker container stop <CONTAINER ID>` to stop your container. You should see your `CONTAINER ID` prompted when the container stops ![docker_stop](./img/docker_stop.PNG)\r\n\r\n    \r\n---\r\n### **Run the Container Again**\r\n---\r\n\r\n- run `make` again any time you wish to rerun the container\r\n    - If the docker image is not deleted or modified, a cached version will run\r\n    \r\n---\r\n\r\n# **Migrating from OSL to Docker**\r\n\r\n_**Note: This only applies to those who used the OSL version of IRIS deployment in the past.**_\r\n\r\nSince the IRIS deployment on OpenSARLab will no longer be available, you can follow these steps to migrate the files you wish to save on to your computer. \r\n\r\n\r\n---\r\n\r\n## **Compress Directories to a Zip File**\r\n\r\nFirst, you will need to compress the directories you want to be copied over to a zip file in OpenSARLab. You can do so with the following steps:\r\n\r\n1. Open up a terminal in the OSL, and make sure you are in the home directory. If you’re not sure, enter this command:\r\n\r\n``` bash\r\n(iris) jupyter-username: ~> cd ~\r\n```\r\n\r\n2. Now that you’re in your home directory, let’s take all the directories you made throughout the SSBW and compress them into one, downloadable zip file. The command you need will look similar to the following:\r\n\r\n``` bash\r\n(iris) jupyter-username: ~> zip -r OSLdata.zip directory1 directory2 directory3\r\n```\r\n\r\nWhere:\r\n\r\n- **OSLdata.zip**: Name of the zipped (compressed) file containing your directories.\r\n- **directory1...directoryN**: Names of the directories you made throughout the SSBW.\r\n- **-r**: Recursive flag; allows you to copy all of the contents within a directory.\r\n\r\n_**Note: The name of the zip file doesn't necessarily have to be `OSLdata.zip`, but we will use `OSLdata.zip` in this documentation.**_\r\n\r\n_**Example:**_\r\nAt this point, you should have the following directories in your OSL:\r\n\r\n- `focmec`\r\n- `geodesy`\r\n- `groupwork`\r\n- `ieb` \r\n- `irisdmc` \r\n- `jupyter` \r\n- `network`\r\n- `python`\r\n- `sac` \r\n\r\nSo the command to zip those directories should look like this:\r\n\r\n```bash\r\n(iris) jupyter-username: ~> zip -r OSLdata.zip focmec geodesy groupwork ieb irisdmc jupyter network python sac\r\n```\r\n\r\nFeel free to add other directories that you’d like to save or only copy over some of the directories! Also, don’t worry about the `iris_data` directory, as that will be part of the container you downloaded. If you look in the home folder you should now see a file called `OSLdata.zip`.\r\n\r\n---\r\n\r\n## **Download Zip File to Your Computer**\r\n\r\nNow that you have a zipped file, you should be able to download them from OSL to your home computer.\r\n\r\n1. On your OSL desktop, click on *Go to JupyterLab* in the top right corner next to the **shutdown** button.\r\n2. From the JupyterLab screen, find **OSLdata.zip** listed on the left panel. You should already be in the home directory but you may have to navigate around for the file. \r\n3. Once you find the **OSLdata.zip** file, right-click on **OSLdata.zip** (for a Mac: click with two fingers if you do not have a right-click)\r\n4. From there a drop-down menu will open up and you can click download. Refer to the image below:\r\n\r\n![osl_download_zip](img/osl_download_zip.png)\r\n\r\n5. The **OSLdata.zip** file will now be available on your computer in your Downloads folder. \r\n\r\n---\r\n\r\n## **Copy Zipped File to Docker**\r\n\r\nNow that you have downloaded a copy of the `OSLdata.zip` file, you should move them to your Docker container.\r\n\r\n1. Move the `OSLdata.zip` file to the `/Volumes/IRIS/opensarlab-docker/iris2022/virtual_home` directory.\r\n\r\nThis directory is connected to the Docker container, so any files you save there will also appear within your Docker. Likewise, any files you create within the Docker will appear in that directory.\r\n\r\nSee below image for more detail:\r\n![save_to_virtual_home](img/save_to_virtual_home.png)\r\n\r\n---\r\n\r\n## **Unzip the Zipped File**\r\n\r\nLastly, you will need to unzip the `OSLdata.zip` file that is located in your Docker container.\r\n\r\n1. Once the `OSLdata.zip` file is in the `virtual_home` directory, open the terminal on the Docker container.\r\n2. If the IRIS Dockerfile is not running already, follow the directions above to run the Docker. \r\n3. Open a terminal in the Docker as you have done with the OSL in the past.\r\n4. You should be in your home directory and if you type `ls` you should see the `OSLdata.zip` file. \r\n5. Run following command to unzip your file:\r\n``` bash\r\n(iris) jovyan@username:~$ unzip OSLdata.zip\r\n```\r\n\r\n\r\n\r\n\r\n---\r\n\r\n# **Troubleshooting**\r\n\r\n## **Common Issues**\r\n---\r\n\r\n### __Issue on Installing/Running Docker__\r\n---\r\nDocker is one of the common tools that users may encounter issues with. While we cannot troubleshoot every single problem, here are the common issues and some ways to solve them:\r\n\r\n- **Cannot run basic commands (e.g. `docker ps`)**\r\n\r\n    You may need to activate the Docker application on your computer. One possible way to solve this is by locating where the Docker application is and executing them. This process differs depending on your operating systems \r\n    \r\n    For instance, you may need to click the Docker app in the `applications` folder to execute. \r\n\r\n    Another possibility is that the Docker application does not have a proper permission setting (i.e. you will need to use `sudo` to run the `docker` command). To not use `sudo`, you will need to add yourself to the usergroup. Refer to the [official documentation](https://docs.docker.com/engine/install/linux-postinstall/) for more detail.\r\n\r\n- **Can run Docker, but build fails**\r\n\r\n    You may possibly run into following errors:\r\n    ``` bash\r\n    'docker build' error: \"failed to solve with frontend dockerfile.v0\r\n    ```\r\n\r\n    This is a common error, especially among those who are using WSL2. This happens due to the improper settings on `config.json`. \r\n    \r\n    One possible solution is to edit the `config.json` *located in `~/.docker/config.json`. Use any text editor, such as vim, and open the `config.json`. \r\n\r\n    **Note: This is the default location. If you moved it elsewhere or deleted the file, it may not be in this directory.*\r\n\r\n    If `config.json` has something like following:\r\n    ```bash\r\n    {\r\n        \"credsStore\": \"desktop.exe\"\r\n    }\r\n    ```\r\n    Then the automatically generated values are causing the issue. Try making `credsStore` to `credStore` (remove `s`) or remove the entire middle line (while keeping curly brackets). \r\n\r\n    Do note that this is one of the possible solutions.\r\n\r\n\r\n- **Docker slows down my computer**\r\n\r\n    This is a common issue for those that are using Windows/WSL2. Since WSL2 does not have any limit set on Docker, it may end up hogging up possible resources that your computer has. \r\n\r\n    One way to prevent Docker from hogging up your computer resources is to put a limit using `.wslconfig`. \r\n\r\n    If you are using Windows computer, here are the steps you'll need to take:\r\n\r\n    1. Use `WinKey + r` to open `run` window.\r\n    2. Type in `%UserProfile%` and hit enter.\r\n    3. If you do not see `.wslconfig`, then create one.\r\n    4. Edit your `.wslconfig` to set a resource limit.\r\n\r\n    For reference, our `.wslconfig` looks something like this:\r\n\r\n    ```\r\n    [wsl2]\r\n    memory=4GB\r\n    guiApplications=false\r\n    ```\r\n\r\n### __Permission Denied with `ssh`__\r\n---\r\n\r\nWhen you are trying to clone Git repository via `ssh`, you may come across erros that looks something like this:\r\n\r\n```bash\r\ngit@github.com:ASFOpenSARlab/opensarlab-docker.git\r\nCloning into 'opensarlab-docker'...\r\nThe authenticity of host 'github.com (xxx.xx.xxx.x)' cannot be established.\r\nECDSA key fingerprint is SHA256:xxxxxxxxxxxxxxxxxxxxxxxx/xxxxxxxx/xxxxxxxxx.\r\nAre you sure you want to continue connecting (yes/no/[fingerprint])? yes\r\nWarning: Permanently added 'github.com,xxx.xx.xxx.x' (ECDSA) to the list of known hosts.\r\ngit@github.com: Permission denied (publickey).\r\nfatal: Could not read from remote repository.\r\n\r\nPlease make sure you have the correct access rights\r\nand the repository exists.\r\n```\r\n\r\nThis indicates that you may be having an issue with `ssh` key.\r\n\r\n_Solution:_\r\n\r\nThere are a few ways to get around this:\r\n1. Clone using HTTPS\r\n1. Set the ssh key\r\n\r\nThe HTTPS method may be an easier workaround assuming you're not going to push anything to the Git repository. \r\n\r\nAlternatively, if you would like to clone using the ssh method, which is the recommended way for _developers_, then refer to the _Installing Git_ section of this README.\r\n\r\n### __run `make` in the terminal → Cannot connect to the Docker daemon__\r\n---\r\n\r\nWhen you try to run `make` command, you may see errors like this:\r\n\r\n```bash\r\ncd iris && bash build.sh 2>&1 | tee log\r\n+ IMAGE_NAME=iris2022\r\n+ '[' -e download.sh ']'\r\n+ bash download.sh\r\nCloning into 'TRAIN'...\r\nwarning: unable to access '/Users/<username>/.config/git/attributes': Permission denied\r\n+ cp dockerfile dockerfile.build\r\n++ date +%F-%H-%M-%S\r\n+ BUILD_TAG=2022-08-29-12-41-22\r\n++ git rev-parse --short HEAD\r\n+ COMMIT_HEAD=5437e1d\r\n+ docker build -f dockerfile.build --target testing .\r\nCannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\r\nbash start_container.sh 2>&1 | tee log\r\n++ pwd\r\n+ docker run -it --init --rm -p 8888:8888 -v /Volumes/IRIS/opensarlab-docker/iris2022/virtual_home:/home/jovyan iris2022:latest\r\ndocker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?.\r\nSee 'docker run --help'.\r\n```\r\n\r\n_Solution:_\r\n\r\nYou can do something like this to make sure that it works:\r\n\r\n- Open Docker app on your computer\r\n- Run make in the terminal again.\r\n\r\n\r\n---\r\n\r\n### __run `make` in the terminal → An error occurs at an intermediate step__\r\n\r\n_Solution:_\r\n\r\n- Close the Docker app and the terminal\r\n- Re-open both, and then run make in the terminal again (sometimes, closing and then re-opening things just works!) \r\n\r\n---\r\n## **If You Encounter Issues**\r\n---\r\n- Please reach out for support\r\n- Support contact: uaf-jupyterhub-asf+IRIS2022@alaska.edu\r\n",
        "createdAt": "2023-07-29T19:47:43.000Z",
        "updatedAt": "2023-07-29T19:53:27.000Z",
        "language": "Shell",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/muteebmohsin/IRIS-2022-Seismology-Skill-Building-Workshop/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "happjness/using-GMG-Mount-St.-Helens",
        "url": "https://github.com/happjness/using-GMG-Mount-St.-Helens",
        "description": "visualizing seismological data from Mount St. Helens with the GMG in julia ",
        "stars": 0,
        "forks": 0,
        "readme": "The goal was visualizing seismological data from Mount St. Helens, the a strato volcano in Washington state, using the [GeophysicalModelGenerator.jl](https://github.com/JuliaGeodynamics/GeophysicalModelGenerator.jl) \nWe processed geophysical data to generate **topographic** and **geological models**, exported to **ParaView**, and created a **side-view animation**.\n\n## 🗺️ Topographic Model \n\n    Created a topographic map from online data (e.g., SRTM, ETOPO).  \n    Defined spatial boundaries (lat/lon extent).  \n    Set depth = 0 as reference.  \n    Converted to Cartesian coordinates → exported as ParaView .vts file.\n<img width=\"694\" height=\"602\" alt=\"2Topo_screenshot\" src=\"https://github.com/user-attachments/assets/7f22f711-d1fb-4cf2-82e1-07dd3a9bae3a\" />\n\n## 🧭 Geological Model \n\n    Used PNG images as geological layers.  \n    Assigned depth = 0 for surface layer.  \n    Mapped to Cartesian grid → exported to ParaView.\n<img width=\"1439\" height=\"821\" alt=\"screenshot_615\" src=\"https://github.com/user-attachments/assets/d2726508-7bcb-4782-9435-c119917ba007\" />\n\n## 🎥 Animation (Side View) \n<img width=\"1600\" height=\"1237\" alt=\"beforeafter\" src=\"https://github.com/user-attachments/assets/16cb035b-f458-4f9e-bc0f-e507bece501b\" />\n-> Before and after-[image](https://geog361.blogspot.com/p/applied-hazards.html) of the mountain from top \n    Generated a movie from side-view slices.  \n    Dots represent features (details to be explained later).  \n    Exported via ParaView or Julia + FFMPEG.\n\n## Resources: \n- [GMG Docs](https://juliageodynamics.github.io/GeophysicalModelGenerator.jl/stable/)\n- [ParaView Export Guide](https://juliageodynamics.github.io/GeophysicalModelGenerator.jl/stable/visualization/) \n",
        "createdAt": "2025-10-29T16:10:09.000Z",
        "updatedAt": "2025-10-29T16:50:43.000Z",
        "language": "Julia",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/happjness/using-GMG-Mount-St.-Helens/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "andbrocode/SixDegreesOfFreedom",
        "url": "https://github.com/andbrocode/SixDegreesOfFreedom",
        "description": "Codes for Seismological Six Degree of Freedom Processing",
        "stars": 0,
        "forks": 0,
        "readme": "# SixDegreesOfFreedom\n\n## Basic Codes for 6-DoF Analysis\n\nThe codes in this package enable comprehensive seismological processing of 6 degree-of-freedom (DoF) data. A 6-DoF station ideally combines co-located observations of three components of translation and three components of rotation data. The package provides well-documented functions for array-derived rotation computation, backazimuth analysis, velocity estimation, and advanced visualization tools.\n\n## Installation\n\n### Using Conda (Recommended)\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/andbrocode/SixDegreesOfFreedom.git\ncd SixDegreesOfFreedom/\n```\n\n2. Create and activate the conda environment:\n```bash\nconda env create -f docs/environment.yml\nconda activate sixdegrees\n```\n\n3. Install the package in development mode:\n```bash\npip install -e .\n```\n\n### Dependencies\n\nThe package requires Python ≥3.9 and the following main dependencies:\n- numpy ≥1.20.0\n- scipy ≥1.7.0\n- matplotlib ≥3.4.0\n- obspy ≥1.3.0\n- pandas ≥1.3.0\n- scikit-learn ≥0.24.0\n- pyyaml ≥6.0\n- acoustics ≥0.2.3\n- cartopy ≥0.20.0 (optional, for map plotting)\n\nFor a complete list of dependencies, see `docs/environment.yml`.\n\n## Documentation\n\n### Project Structure\n\nThe project is organized as follows:\n- `sixdegrees/`: Core package code\n  - `sixdegrees.py`: Main implementation with comprehensive docstrings\n  - `seismicarray.py`: Seismic array processing with full documentation\n  - `plots/`: Visualization modules for data analysis and plotting\n  - `utils/`: Utility functions for data processing\n- `examples/`: Jupyter notebooks, example scripts, and workflows\n  - `workflows/`: Ready-to-use processing workflows\n    - `compute_adr/`: Array-derived rotation computation workflows\n    - `compute_baz/`: Backazimuth analysis workflows\n  - Various demonstration notebooks for different analysis types\n  - Example configuration files and sample data\n- `docs/`: Documentation and environment specifications\n- `tests/`: Comprehensive test suite\n\n### Example Usage\n\nThe package includes several Jupyter notebooks and workflows in the `examples/` directory:\n\n#### Jupyter Notebooks\n- Array-derived rotation computation from the Pinon Flats seismic array\n- Data acquisition examples for G-ring laser and BSPF station\n- Analysis examples for BSPF station, ROMY ring laser, and G-ring laser\n- Backazimuth analysis and visualization demos\n\n#### Processing Workflows\n- **Array-Derived Rotation (ADR)**: `examples/workflows/compute_adr/`\n  - Daily continuous 6-DoF data creation from array data\n  - SDS format storage and management\n- **Backazimuth Analysis**: `examples/workflows/compute_baz/`\n  - Automated backazimuth estimation and analysis\n  - Data merging and visualization tools\n\nSee individual workflow README files for detailed usage instructions.\n\n## Recent Updates\n\n### Version 0.1.1\n- **Enhanced Documentation**: Added comprehensive docstrings to all core functions in `sixdegrees.py`\n- **Improved Code Quality**: All functions now have proper type hints and detailed parameter descriptions\n- **Better Developer Experience**: Enhanced code readability and maintainability\n\n## Testing\n\n### Test Structure\n\nThe test suite is organized into three main categories:\n- `test_core/`: Tests for core functionality\n  - Basic initialization\n  - Data source handling\n  - Configuration validation\n  - Time and coordinate validation\n- `test_plots/`: Tests for plotting functions\n- `test_utils/`: Tests for utility functions\n\n### Running Tests\n\nTo run the test suite:\n\n```bash\n# Run all tests\npytest\n\n# Run specific test category\npytest tests/test_core/\npytest tests/test_plots/\npytest tests/test_utils/\n\n# Run with coverage report\npytest --cov=sixdegrees tests/\n```\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Make your changes\n4. Run the test suite\n5. Submit a pull request\n\n## License\n\nThe license information is found in the LICENCE file\n",
        "createdAt": "2025-03-03T16:38:03.000Z",
        "updatedAt": "2025-09-19T19:35:32.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/andbrocode/SixDegreesOfFreedom/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "shakelab/shakelab",
        "url": "https://github.com/shakelab/shakelab",
        "description": "Python Tools for Engineering Seismology",
        "stars": 6,
        "forks": 2,
        "readme": "<img alt=\"ShakeLab - Tools for Engineering Seismology\" class=\"right\" style=\"width: 100%\" src=\"https://raw.githubusercontent.com/klunk386/shakelab/master/logo/shakelab-header.png#1\" />\n\n[![AGPLv3](https://www.gnu.org/graphics/agplv3-88x31.png)](https://www.gnu.org/licenses/agpl.html)\n[![Build Status](https://travis-ci.org/shakelab/shakelab.svg?branch=master)](https://travis-ci.org/shakelab/shakelab)\n[![Supported Python versions](https://img.shields.io/pypi/pyversions/shakelab.svg)](https://pypi.python.org/pypi/shakelab)\n[![PyPI Version](https://img.shields.io/pypi/v/shakelab.svg)](https://pypi.python.org/pypi/shakelab)\n\n# ShakeLab\n\n**ShakeLab** is an open-source Python library that provides tools for engineering seismology.  \nIt is designed to assist researchers and practitioners in the analysis and interpretation of ground motion data, generation of shakemaps, site response calculations, and rapid seismic impact assessment.\n\n⚠️ **Note:** _The project is under active development and some functionalities may change._\n\n##  Installation\n\nYou can install ShakeLab in different ways depending on your needs.\n\n### 1️⃣ Standard Installation from PyPI\n\nInstall the latest stable version from [PyPI](https://pypi.org/project/shakelab/):\n\n```bash\npip install shakelab\n```\n\nor explicitly with Python 3:\n\n```bash\npip3 install shakelab\n```\n\n**Note:** The version available on PyPI may not include the latest features and fixes as the code is under active development. If you want the most up-to-date version, consider using one of the installation methods below.\n\n### 2️⃣ Installation from GitHub (latest code)\n\nTo install the most recent version directly from the GitHub repository:\n\n```bash\npip install git+https://github.com/shakelab/shakelab.git\n```\n\nTo upgrade an existing installation:\n\n```bash\npip install --upgrade git+https://github.com/shakelab/shakelab.git\n```\n\n### 3️⃣ Developer Installation from a Local Repository\n\nIf you want to modify the code and test it locally:\n\n```bash\ngit clone https://github.com/shakelab/shakelab.git\ncd shakelab\npip install -r requirements.txt -e .\n```\n\nThis will install the package in \"editable mode\", so any changes to the source code will be reflected immediately.\n\n> ✅ Optionally, create and activate a virtual environment before installation:\n> ```bash\n> python3 -m venv venv\n> source venv/bin/activate  # On Windows: venv\\Scripts\\activate\n> ```\n\n##  Dependencies\n\nShakeLab relies on the following Python packages:\n\n- [NumPy](https://numpy.org/) >= 1.24.0\n- [SciPy](https://scipy.org/) >= 1.10.0\n- [Shapely](https://pypi.org/project/Shapely/) >= 2.0.0\n- [Requests](https://pypi.org/project/requests/) >= 2.32.0\n- [cymseed3](https://pypi.org/project/cymseed3/) >= 0.1.4\n\n\n## Documentation\n\nOfficial documentation is available at: [shakelab.org](http://shakelab.org)\n\n##  License\n\nShakeLab is free software, released under the **GNU Affero General Public License v3.0**.  \nYou can redistribute it and/or modify it under the terms of the license.\n\nSee <https://www.gnu.org/licenses/agpl-3.0.html> for full details.\n\n## ⚠️ Disclaimer\n\nShakeLab is distributed in the hope that it will be useful, but **without any warranty**, including the implied warranty of merchantability or fitness for a particular purpose.\n\nThe authors assume no liability for any use of the software.\n",
        "createdAt": "2018-06-23T20:42:44.000Z",
        "updatedAt": "2025-10-19T14:08:04.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/shakelab/shakelab/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "njlindsey/Photonic-seismology-in-Monterey-Bay-Dark-fiber1DAS-illuminates-offshore-faults-and-coastal-ocean",
        "url": "https://github.com/njlindsey/Photonic-seismology-in-Monterey-Bay-Dark-fiber1DAS-illuminates-offshore-faults-and-coastal-ocean",
        "description": "Data used to generate the figures and support the findings of manuscript entitled, \"Photonic seismology in Monterey Bay: Dark fiber1DAS illuminates offshore faults and coastal ocean\" by Nathaniel J. Lindsey, T. Craig Dawe, and Jonathan B. Ajo-Franklin",
        "stars": 13,
        "forks": 1,
        "readme": "# Photonic seismology in Monterey Bay : Dark fiber DAS illuminates offshore faults and coastal ocean\nData used to generate the figures and support the findings of manuscript entitled, \"Photonic seismology in Monterey Bay: Dark fiber: DAS illuminates offshore faults and coastal ocean\" by Nathaniel J. Lindsey, T. Craig Dawe, and Jonathan B. Franklin\n\n- Bathymetry and faults from California Seafloor Mapping Program (https://pubs.usgs.gov/ds/781/)\n- Historical-Quarternary faults from USGS Fault & Fold Database (https://earthquake.usgs.gov/hazards/qfaults/)\n- MARS cable and DAS array geometry : mbari_cable_geometry_upload.csv\n- USGS earthquake catalog event retrieved from https://earthquake.usgs.gov/earthquakes/eventpage/nc72982551/executive\n- DAS earthquake \"event\" waveform data (\"event\\*.mssed\"): https://drive.google.com/drive/folders/157ot0vAW_UXrAGDfOsamfXOZjo6boriG?usp=sharing\n- Satellite data retrieved from Wavewatch III (https://polar.ncep.noaa.gov/waves/viewer.shtml?-multi_1-)\n- Buoy wave height data retrieved from www.ndbc.noass.gov : 2018_data_46042.txt\n- Buoy wave spectra data retrieved from www.ndbc.noass.gov : march_swd_data_46042.txt\n- Continuous DAS data for 10 - 17 km (\"noise\\*.mssed\") : https://drive.google.com/drive/folders/157ot0vAW_UXrAGDfOsamfXOZjo6boriG?usp=sharing\n- Continuous DAS strain spectrogram for 2 km (\"noiseMicroseism_1000_\\*.csv\") :\nhttps://drive.google.com/drive/folders/157ot0vAW_UXrAGDfOsamfXOZjo6boriG?usp=sharing\n\n",
        "createdAt": "2019-08-24T21:26:51.000Z",
        "updatedAt": "2025-09-10T09:47:14.000Z",
        "language": null,
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/njlindsey/Photonic-seismology-in-Monterey-Bay-Dark-fiber1DAS-illuminates-offshore-faults-and-coastal-ocean/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Yernazzar/seismology",
        "url": "https://github.com/Yernazzar/seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "Presentation with briefly view for our challenge and you can get started for our project with this presentation\n",
        "createdAt": "2024-10-05T11:42:55.000Z",
        "updatedAt": "2024-10-06T17:44:28.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Yernazzar/seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "JiarunZhou/PKIKP_Onset_Picker",
        "url": "https://github.com/JiarunZhou/PKIKP_Onset_Picker",
        "description": "This is the repo for article: Deep-Learning Phase-Onset Picker for Deep Earth Seismology: PKIKP Waves",
        "stars": 5,
        "forks": 0,
        "readme": "# Deep-Learning Phase-Onset Picker for Deep Earth Seismology: PKIKP Waves\nThis is the repo for article: Deep-Learning Phase-Onset Picker for Deep Earth Seismology: PKIKP Waves published on Journal of Geophysical Research: Solid Earth.\n\n## Content\n- Trained model;\n- Training datasets;\n- Examples of real PKIKP waveforms;\n- Functions for CNN training;\n- Functions for sliding-window picking;\n- Demo notebook.\n\n\n## Recommended Pre-processing\n- Original data: 150 s length, PKIKP onset predicted by ak135 located at 60 s\n- Format: .sac\n- Channel: Vertical channel\n- Waveform length input to the picker: 50 s around ak135 prediction\n- Sampling rate: 40 Hz\n- Frequency filtering: 0.5 - 2 Hz\n\n## Reference\nZhou, J., Phạm, T.‐S., & Tkalčić, H. (2024). Deep‐learning phase‐onset picker for deep Earth seismology: PKIKP waves. Journal of Geophysical Research: Solid Earth, 129, e2024JB029360. https://doi.org/10.1029/2024JB029360\n\n",
        "createdAt": "2024-08-28T01:36:59.000Z",
        "updatedAt": "2025-11-20T09:41:23.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/JiarunZhou/PKIKP_Onset_Picker/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "keurfonluu/disba",
        "url": "https://github.com/keurfonluu/disba",
        "description": "Numba-accelerated computation of surface wave dispersion",
        "stars": 168,
        "forks": 36,
        "readme": "disba\n=====\n\n|License| |Stars| |Pyversions| |Version| |Downloads| |Code style: black| |Codacy Badge| |Codecov| |Build| |Travis| |Awesome| |DOI|\n\n**disba** is a computationally efficient Python library for the modeling of surface wave dispersion that implements a subset of codes from `Computer Programs in Seismology (CPS) <http://www.eas.slu.edu/eqc/eqccps.html>`__ in Python compiled `just-in-time <https://en.wikipedia.org/wiki/Just-in-time_compilation>`__ with `numba <https://numba.pydata.org/>`__. Such implementation alleviates the usual prerequisite for a Fortran compiler needed by other libraries also based on CPS (e.g., `pysurf96 <https://github.com/miili/pysurf96>`__, `srfpython <https://github.com/obsmax/srfpython>`__ and `PyLayeredModel <https://github.com/harrymd/PyLayeredModel>`__) which often leads to further installation troubleshooting, especially on Windows platform.\n\n**disba** aims to be lightweight and portable without compromising on the performance. For both Rayleigh-wave and Love-wave, it is significantly faster than CPS's *surf96* program compiled with `f2py <https://np.org/devdocs/f2py/index.html>`__, noticeably for large number of layers.\n\n.. list-table::\n\n   *  - |Perf Rayleigh|\n      - |Perf Love|\n\nFeatures\n--------\n\nForward modeling:\n\n-  Compute Rayleigh-wave phase or group dispersion curves using *Dunkin's matrix* or *fast delta matrix* algorithms,\n-  Compute Love-wave phase or group dispersion curves using *Thomson-Haskell* method,\n-  Compute Rayleigh-wave ellipticity.\n\nEigenfunctions and sensitivity kernels:\n\n-  Compute Rayleigh- and Love- wave eigenfunctions,\n-  Compute Rayleigh- and Love- wave phase or group velocity, and Rayleigh-wave ellipticity sensitivity kernels with respect to layer thickness, P- and S- wave velocities, and density.\n\nInstallation\n------------\n\nThe recommended way to install **disba** and all its dependencies is through the Python Package Index:\n\n.. code:: bash\n\n   pip install disba[full] --user\n\nOtherwise, clone and extract the package, then run from the package location:\n\n.. code:: bash\n\n   pip install .[full] --user\n\nTo test the integrity of the installed package, check out this repository and run:\n\n.. code:: bash\n\n   pytest\n\nDocumentation\n-------------\n\nRefer to the online `documentation <https://keurfonluu.github.io/disba/>`__ for detailed description of the API and examples.\n\nAlternatively, the documentation can be built using `Sphinx <https://www.sphinx-doc.org/en/master/>`__:\n\n.. code:: bash\n\n   pip install -r doc/requirements.txt\n   sphinx-build -b html doc/source doc/build\n\nUsage\n-----\n\nThe following example computes the Rayleigh- and Love- wave phase velocity dispersion curves for the 3 first modes.\n\n.. code:: python\n\n   import numpy as np\n   from disba import PhaseDispersion\n\n   # Velocity model\n   # thickness, Vp, Vs, density\n   # km, km/s, km/s, g/cm3\n   velocity_model = np.array([\n      [10.0, 7.00, 3.50, 2.00],\n      [10.0, 6.80, 3.40, 2.00],\n      [10.0, 7.00, 3.50, 2.00],\n      [10.0, 7.60, 3.80, 2.00],\n      [10.0, 8.40, 4.20, 2.00],\n      [10.0, 9.00, 4.50, 2.00],\n      [10.0, 9.40, 4.70, 2.00],\n      [10.0, 9.60, 4.80, 2.00],\n      [10.0, 9.50, 4.75, 2.00],\n   ])\n\n   # Periods must be sorted starting with low periods\n   t = np.logspace(0.0, 3.0, 100)\n\n   # Compute the 3 first Rayleigh- and Love- wave modal dispersion curves\n   # Fundamental mode corresponds to mode 0\n   pd = PhaseDispersion(*velocity_model.T)\n   cpr = [pd(t, mode=i, wave=\"rayleigh\") for i in range(3)]\n   cpl = [pd(t, mode=i, wave=\"love\") for i in range(3)]\n\n   # pd returns a namedtuple (period, velocity, mode, wave, type)\n\n.. list-table::\n\n   *  - |Sample Rayleigh|\n      - |Sample Love|\n\nLikewise, ``GroupDispersion`` can be used for group velocity.\n\n**disba**'s API is consistent across all its classes which are initialized and called in the same fashion. Thus, eigenfunctions are calculated as follow:\n\n.. code:: python\n\n   from disba import EigenFunction\n\n   eigf = EigenFunction(*velocity_model.T)\n   eigr = eigf(20.0, mode=0, wave=\"rayleigh\")\n   eigl = eigf(20.0, mode=0, wave=\"love\")\n\n   # eigf returns a namedtuple\n   #  - (depth, ur, uz, tz, tr, period, mode) for Rayleigh-wave\n   #  - (depth, uu, tt, period, mode) for Love-wave\n\n.. list-table::\n\n   *  - |Eigen Rayleigh|\n      - |Eigen Love|\n\nPhase velocity sensitivity kernels (``GroupSensitivity`` for group velocity):\n\n.. code:: python\n\n   from disba import PhaseSensitivity\n\n   ps = PhaseSensitivity(*velocity_model.T)\n   parameters = [\"thickness\", \"velocity_p\", \"velocity_s\", \"density\"]\n   skr = [ps(20.0, mode=0, wave=\"rayleigh\", parameter=parameter) for parameter in parameters]\n   skl = [ps(20.0, mode=0, wave=\"love\", parameter=parameter) for parameter in parameters]\n\n   # ps returns a namedtuple (depth, kernel, period, velocity, mode,wave, type, parameter)\n\n.. list-table::\n\n   *  - |Kernel Rayleigh|\n      - |Kernel Love|\n\nEllipticity and ellipticity sensitivity kernels:\n\n.. code:: python\n\n   from disba import Ellipticity, EllipticitySensitivity\n\n   ell = Ellipticity(*velocity_model.T)\n   rel = ell(t, mode=0)\n\n   # ell returns a namedtuple (period, ellipticity, mode)\n\n   es = EllipticitySensitivity(*velocity_model.T)\n   ek = [es(20.0, mode=0, parameter=parameter) for parameter in parameters]\n\n   # es returns a namedtuple (depth, kernel, period, velocity, mode, wave, type, parameter)\n\n.. list-table::\n\n   *  - |Sample Ellipticity|\n      - |Kernel Ellipticity|\n\nContributing\n------------\n\nPlease refer to the `Contributing\nGuidelines <https://github.com/keurfonluu/disba/blob/master/CONTRIBUTING.rst>`__ to see how you can help. This project is released with a `Code of Conduct <https://github.com/keurfonluu/disba/blob/master/CODE_OF_CONDUCT.rst>`__ which you agree to abide by when contributing.\n\n.. |License| image:: https://img.shields.io/github/license/keurfonluu/disba\n   :target: https://github.com/keurfonluu/disba/blob/master/LICENSE\n\n.. |Stars| image:: https://img.shields.io/github/stars/keurfonluu/disba?logo=github\n   :target: https://github.com/keurfonluu/disba\n\n.. |Pyversions| image:: https://img.shields.io/pypi/pyversions/disba.svg?style=flat\n   :target: https://pypi.org/pypi/disba/\n\n.. |Version| image:: https://img.shields.io/pypi/v/disba.svg?style=flat\n   :target: https://pypi.org/project/disba\n\n.. |Downloads| image:: https://pepy.tech/badge/disba\n   :target: https://pepy.tech/project/disba\n\n.. |Code style: black| image:: https://img.shields.io/badge/code%20style-black-000000.svg?style=flat\n   :target: https://github.com/psf/black\n\n.. |Codacy Badge| image:: https://img.shields.io/codacy/grade/1d2218bb7d0e4e0fb2dec26fa32fe92e.svg?style=flat\n   :target: https://www.codacy.com/manual/keurfonluu/disba?utm_source=github.com&utm_medium=referral&utm_content=keurfonluu/disba&utm_campaign=Badge_Grade\n\n.. |Codecov| image:: https://img.shields.io/codecov/c/github/keurfonluu/disba.svg?style=flat\n   :target: https://codecov.io/gh/keurfonluu/disba\n\n.. |DOI| image:: https://zenodo.org/badge/DOI/10.5281/zenodo.3987395.svg?style=flat\n   :target: https://doi.org/10.5281/zenodo.3987395\n\n.. |Build| image:: https://img.shields.io/github/workflow/status/keurfonluu/disba/Python%20package\n   :target: https://github.com/keurfonluu/disba\n\n.. |Travis| image:: https://img.shields.io/travis/com/keurfonluu/disba/master?label=docs\n   :target: https://keurfonluu.github.io/disba/\n\n.. |Awesome| image:: https://img.shields.io/badge/awesome-yes-C6A4BF\n   :target: https://github.com/softwareunderground/awesome-open-geoscience\n\n.. |Perf Rayleigh| image:: https://raw.githubusercontent.com/keurfonluu/disba/e29865fb0b385b295bc55b733a138a741787879d/.github/perf_rayleigh.svg\n   :alt: perf-rayleigh\n\n.. |Perf Love| image:: https://raw.githubusercontent.com/keurfonluu/disba/5d23a8bb3967fd59c1a38b59ce1bf800749c7eb2/.github/perf_love.svg\n   :alt: perf-love\n\n.. |Sample Rayleigh| image:: https://raw.githubusercontent.com/keurfonluu/disba/5d23a8bb3967fd59c1a38b59ce1bf800749c7eb2/.github/sample_rayleigh.svg\n   :alt: sample-rayleigh\n\n.. |Sample Love| image:: https://raw.githubusercontent.com/keurfonluu/disba/5d23a8bb3967fd59c1a38b59ce1bf800749c7eb2/.github/sample_love.svg\n   :alt: sample-love\n\n.. |Sample Ellipticity| image:: https://raw.githubusercontent.com/keurfonluu/disba/5f9b95a144e3751ffa98b5860663af874c02ae1c/.github/sample_ellipticity.svg\n   :alt: sample-ellipticity\n\n.. |Eigen Rayleigh| image:: https://raw.githubusercontent.com/keurfonluu/disba/5f9b95a144e3751ffa98b5860663af874c02ae1c/.github/eigen_rayleigh.svg\n   :alt: eigen-rayleigh\n\n.. |Eigen Love| image:: https://raw.githubusercontent.com/keurfonluu/disba/5f9b95a144e3751ffa98b5860663af874c02ae1c/.github/eigen_love.svg\n   :alt: eigen-love\n\n.. |Kernel Rayleigh| image:: https://raw.githubusercontent.com/keurfonluu/disba/5f9b95a144e3751ffa98b5860663af874c02ae1c/.github/kernel_rayleigh.svg\n   :alt: kernel-rayleigh\n\n.. |Kernel Love| image:: https://raw.githubusercontent.com/keurfonluu/disba/5f9b95a144e3751ffa98b5860663af874c02ae1c/.github/kernel_love.svg\n   :alt: kernel-love\n\n.. |Kernel Ellipticity| image:: https://raw.githubusercontent.com/keurfonluu/disba/5f9b95a144e3751ffa98b5860663af874c02ae1c/.github/kernel_ellipticity.svg\n   :alt: kernel-ellipticity\n",
        "createdAt": "2020-06-18T04:57:57.000Z",
        "updatedAt": "2025-10-15T12:03:34.000Z",
        "language": "Python",
        "homepage": "https://github.com/keurfonluu/disba",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.3987395",
            "openAlex": "10.5281/zenodo.3987395",
            "openCitations": "10.5281/zenodo.3987395",
            "dataCite": "10.5281/zenodo.3987395",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/keurfonluu/disba/master/README.rst",
        "mainPaper": {
            "doi": "10.5281/zenodo.3987395",
            "title": "disba: Numba-accelerated computation of surface wave dispersion",
            "journal": "Zenodo",
            "dateReleased": "2024-12-20T00:00:00.000Z",
            "abstract": "",
            "citationsArray": []
        },
        "repoDoi": "10.5281/zenodo.3987395",
        "publications": [
            {
                "doi": "10.5281/zenodo.3987395",
                "name": "disba: Numba-accelerated computation of surface wave dispersion",
                "source": "",
                "authorNames": [],
                "abstract": "",
                "publicationDate": "2024-12-05T12:15:28.868Z"
            }
        ]
    },
    {
        "source": "GitHub",
        "name": "martanto/dsar",
        "url": "https://github.com/martanto/dsar",
        "description": "Displacement Seismic Amplitude Ratio (DSAR) value",
        "stars": 2,
        "forks": 0,
        "readme": "#  Displacement Seismic Amplitude Ratio (DSAR)\n\n## How to Use\n### Install using pip:\n```python\npip install dsar\n```\n\n### Import `dsar` module\n```python\nfrom dsar import DSAR, PlotDsar\n```\n\n### Initiate DSAR\n```python\ndsar = DSAR(\n    input_dir=\"G:\\\\Output\\\\Converted\\\\SDS\",\n    directory_structure='SDS',\n    start_date=\"2024-01-01\",\n    end_date=\"2024-04-22\",\n    station=\"RUA3\",\n    channel=\"EHZ\",\n    resample=\"10min\" # default\n)\n```\n\nSee https://github.com/martanto/magma-converter for supported `directory_structure`.\n\n### Run DSAR\n```python\ndsar.run()\n```\n\n### Results/Output directory\nOutput directory would be as the same folder where DSAR code is running. It will create `output` directory.\n\n### Plot DSAR\nInitiate DSAR plot\n```python\nplot = PlotDsar(\n    start_date=\"2024-01-01\",\n    end_date=\"2024-04-22\",\n    station=\"RUA3\",\n    channel=\"EHZ\"\n)\n```\n\n### Get combined dataframe to plot\n```python\ndf = plot.df\n```\nThe output of dataframe will be saved as CSV:\n```text\n✅ Saved to D:\\Project\\dsar\\output\\dsar\\VG.RUA3.00.EHZ\\combined_10min_VG.RUA3.00.EHZ.csv\n```\n\nPlot DSAR:\n```python\nplot.plot(\n    interval_day=7,\n    y_min=85,\n    y_max=225,\n    save=True,\n    file_type='jpg',\n)\n```\nOutput:\n```text\n✅ Saved to D:\\Project\\dsar\\output\\dsar\\VG.RUA3.00.EHZ\\combined_10min_VG.RUA3.00.EHZ.csv\n📷 Figure saved to: D:\\Project\\dsar\\output\\figures\\dsar\\VG.RUA3.00.EHZ\\VG.RUA3.00.EHZ_10min_2024-01-01-2024-04-22.jpg\n```\n\nFigures:\n![output.png](https://github.com/martanto/dsar/blob/master/images/output.png?raw=true)\n\n\n## References\n> Caudron, C., et al., 2019, Change in seismic attenuation as a long-term precursor of gas-driven\neruptions: Geology, https://doi.org/10.1130/G46107.1  \n> \n> Chardot, L., Jolly, A. D., Kennedy, B. M., Fournier, N., & Sherburn, S. (2015). Using volcanic tremor for eruption forecasting at White Island volcano (Whakaari), New Zealand. Journal of Volcanology and Geothermal Research, 302, 11–23. https://doi.org/10.1016/j.jvolgeores.2015.06.001\n",
        "createdAt": "2024-04-03T11:11:03.000Z",
        "updatedAt": "2025-07-28T09:10:22.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/martanto/dsar/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "gthompson/SUDS2SEISAN",
        "url": "https://github.com/gthompson/SUDS2SEISAN",
        "description": "Seismology codes for converting SUDS format and HYPO71 format to Seisan format",
        "stars": 0,
        "forks": 0,
        "readme": "# SUDS2SEISAN\nSeismology codes for converting SUDS format and HYPO71 format to Seisan format\n",
        "createdAt": "2019-12-02T05:29:49.000Z",
        "updatedAt": "2024-02-12T03:07:05.000Z",
        "language": "Rich Text Format",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/gthompson/SUDS2SEISAN/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "michaelgrund/sws_tools",
        "url": "https://github.com/michaelgrund/sws_tools",
        "description": "MATLAB tools for modeling and plotting of shear-wave splitting data",
        "stars": 14,
        "forks": 1,
        "readme": "# sws_tools\nMATLAB tools for modeling and plotting of shear-wave splitting data\n\n- 01_stereoplots: Generate stereoplots of splitting results based on output of **SplitLab** and **StackSplit**.\n- 02_data_handling: Read and visualize the shear-wave splitting data available from: https://doi.org/10.5445/IR/1000091427\n- 03_modeling: Model shear-wave splitting results using synthetic splitting parameters computed for single-layer, two-layer and dipping layer scenarios. \n\nIf you make use of the content in this repository please acknowledge our paper published in GJI and/or my PhD thesis in whose framework several of the scripts and functions were developed:\n\n- **_Grund, M. & Ritter, J.R.R. (2020)_**, Shear-wave splitting beneath Fennoscandia - Evidence for dipping structures and laterally varying multilayer anisotropy, Geophysical Journal International, 223, 1525–1547, https://doi.org/10.1093/gji/ggaa388\n\n- **_Grund, M. (2019)_**, Exploring geodynamics at different depths with shear wave splitting, Dissertation, Karlsruhe Institute of Technology (KIT), https://doi.org/10.5445/IR/1000091425 \n\nContent of the directories:\n\n## [01_stereoplots](https://github.com/michaelgrund/sws_tools/tree/master/01_stereoplots)\nGenerate stereoplots of splitting results based on output of **SplitLab** and **StackSplit**.\n\n- color-code bars of splits with respect to backazimuth (see below for supported colormaps)\n- shade full background or specific backazimuthal wedges\n- produce publication-ready plots in different formats (pdf, png, jpg...)\n\nSupported colormaps \n  1) standard MATLAB colormaps: parula, winter, summer, copper....\n\n  2) MatPlotLib 2.0 Colormaps: Perceptually Uniform and Beautiful \n    (see and download here: https://de.mathworks.com/matlabcentral/fileexchange/62729-matplotlib-2-0-colormaps-perceptually-uniform-and-beautiful)\n\n  3) Scientific colour-maps by F. Crameri (Zenodo. http://doi.org/10.5281/zenodo.1243862)\n    (see and download here: http://www.fabiocrameri.ch/colourmaps.php)\n    \nRun function `SWS_Analysis_BASICS_stereoplot` in the MATLAB command window to generate the stereoplots shown in the figure below using the provided test data set of station VAF.    \n\n\n![PLOT_res_ALL](https://user-images.githubusercontent.com/23025878/56903070-dfe03a00-6a9b-11e9-9cc0-606d9c2a4173.png)\n\n## [02_data_handling](https://github.com/michaelgrund/sws_tools/tree/master/02_data_handling)\n\nRead and visualize the shear-wave splitting data available from: https://doi.org/10.5445/IR/1000091427\n\n- generate individual stereoplots `SWS_read_evstruct('stereo')` \n- generate stereoplots of all 266 analyzed stations `SWS_read_evstruct('stereoall')`\n- generate histogram `SWS_read_evstruct('histo')`\n- generate output structs separated in split and null measurements `[SPLITS,NULLS]=SWS_read_evstruct`\n\n![PLOTS_merged](https://user-images.githubusercontent.com/23025878/57140316-c864c200-6db7-11e9-933b-b96c452f8349.png)\n\n## [03_modeling](https://github.com/michaelgrund/sws_tools/tree/master/03_modeling)\n\nModel shear-wave splitting results using synthetic splitting parameters computed for single-layer, two-layer and dipping layer scenarios. Results plots are generated as well. \n\nMSAT package of **_Walker & Wookey (2012)_** (https://github.com/andreww/MSAT) is required!\n\n- reproduce the modeling results presented in our paper **_Grund & Ritter (2020, https://doi.org/10.1093/gji/ggaa388)_** using the shear-wave splitting data available from: https://doi.org/10.5445/IR/1000091427\n- use your own measurements (**SplitLab**/**Stacksplit** output format, custom formatting is also supported)\n\nDescriptions on how to set up different models and how to handle individual data sets can be found in the _Supporting Information_ of **_Grund & Ritter (2020, https://doi.org/10.1093/gji/ggaa388)_**.\n\n![PLOTS_output](https://user-images.githubusercontent.com/23025878/85557282-ec244080-b627-11ea-8ca5-c2ae25e36176.png)\n",
        "createdAt": "2019-04-29T14:27:38.000Z",
        "updatedAt": "2025-12-01T12:05:49.000Z",
        "language": "MATLAB",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.1243862",
            "dataCite": "10.5281/zenodo.1243862",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/michaelgrund/sws_tools/main/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.1243862",
            "title": "Scientific colour maps",
            "journal": "Zenodo",
            "dateReleased": "2023-10-05T00:00:00.000Z",
            "abstract": "Suite of scientific, <strong>colour-vision deficiency friendly</strong> and <strong>perceptually-uniform colour maps</strong> (www.fabiocrameri.ch/colourmaps) that include all readers and significantly reduce visual errors. Book graphic design <strong>masterclasses</strong> on how to best use colour in a scientific context via www.fabiocrameri.ch/masterclasses. Commission <strong>professional graphic design</strong> support via Undertone.design. <strong>Support</strong> the underlying cause and the development of the Scientific colour maps via www.fabiocrameri.ch/products.",
            "citationsArray": [
                "10.1002/ange.202114910",
                "10.5194/se-13-583-2022",
                "10.5281/zenodo.12752995",
                "10.5281/zenodo.12752996",
                "10.5880/fidgeo.2024.028"
            ]
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "JanisHe/seisDAE",
        "url": "https://github.com/JanisHe/seisDAE",
        "description": "A Python module to train and apply a denoising autoencoder to seismological data",
        "stars": 23,
        "forks": 3,
        "readme": "# Denoising Autoencoder for seismological data\n\n## Integration into SeisBench\nThis package won't be maintained anymore, since it is included to SeisBench (https://github.com/seisbench/seisbench).\nThere, you can also find a Jupyter-Notebook how to train your own SeisDAE model.\n\n## Installation\nThis packages contains python scripts to train a neural network for the denoising of seismological data.\nThis package is based on the work by \n * Heuel, J. & Friederich, W. Suppression of wind turbine noise from seismological data using nonlinear thresholding and denoising autoencoder Journal of Seismology, 2022\n * Zhu, W.; Mousavi, S. M. & Beroza, G. C. Seismic signal denoising and decomposition using deep neural networks IEEE Transactions on Geoscience and Remote Sensing, IEEE, 2019, 57, 9476-9488\n * Tibi, R.; Hammond, P.; Brogan, R.; Young, C. J. & Koper, K. Deep Learning Denoising Applied to Regional Distance Seismic Data in Utah Bulletin of the Seismological Society of America, 2021 \n\nMoreover, the package `pycwt` is adapdted from https://github.com/regeirk/pycwt and modified.\n* Torrence, C. and Compo, G. P.. A Practical Guide to Wavelet Analysis. Bulletin of the American Meteorological Society, American Meteorological Society, 1998, 79, 61-78. \n\nBefore starting, run the following command, please have the following packages installed:\n * Numpy\n * Matplotlib\n * Scipy\n * Tensorflow >= 2.0\n * Obspy \n * Joblib\n * Pandas\n * tqdm\n \nOtherwise run the following command in your conda environment:\n```\nconda create -c conda-forge -n denoiser python=3.10 numpy scipy tqdm matplotlib obspy pandas joblib tensorflow\n```\n \nAfter the installation of all packages, you can train the example by running\n\n#### \n```\npython run_model_from_parfile.py\n```\n\nThe training of the example dataset will take a while. It depends whether you run it on CPU or GPU.\nThe trained model is saved in the directory `./Models` and is named `my_model.h5`. The config file is saved \nin `./config` und `my_model.config`. \nIn some cases, the training might be killed because of full memory. Then open `model_parfile` and set the parameters\n`workers` and `max_queue_size` to lower values.\n\nIf training was successfull, you can predict your first dataset by\n```\npython prediction.py\n```\nNow, you can start to train your first model.\n\n#### Training of the first own model\nCreate your own training dataset, that contains earthquake data with an high SNR and noise data. Both datasets\nare in two different directories, have the same length and sampling frequency. For the length and sampling frequency \n60 s windows and 100 Hz are recommended. \nFor earthquake data, the STanford EArthquake Dataset (STEAD) is recommended (https://github.com/smousavi05/STEAD).\nNote, each waveform is saved as a `.npz` file. If available, the earthquake data contain onsets of P- and S-arrivals \nin samples (`itp` and `its`). Save your data e.g. by the folllowing commands for earthquakes and noise, repectively:\n```\nnp.savez(data=earthquake_data, file=filename, its=0, itp=0, starttime=str(trace.stats.starttime))\nnp.savez(data=noise_data, file=filename)\n```\nAfterwards, adjust the parfile and start your training.\n\n#### Preparing a training dataset\nScripts to download e.g. STEAD or INSTANCE and to prepare these dataset will be added in future. Moreover,\nscripts to prepare noise samples also will be added in future versions.\nHowever, some data may be corrupted. Instead of going through all files, the script `datasets/check_datasets.py` can \nbe used  to check the files automatically. You can run the script directly from the command line by\n```commandline\npython datasets/check_datasets.py model_parfile\n```\nThe parameters for signal and noise files are read from the parameter file.\n\n#### Denoise data\nRun the function `predict` from file `prediction.py` with your created model and\nconfig file. The parameter data_list is a list with numpy arrays for denoising.\nUsing `prediction.py` only denoises time windows of the same length as for the training dataset,\nbut in many cases it is necessary to denoise longer time series (see next section).\n\nThe directory `./example_data/noisy_signals` contains nine noisy time series from the station RN.BAVN (for more \ninformation please have a closer look to the first mentioned paper). You can denoise these examples and compare\nthe result to different filter methods by running the script `./denoiser_comparison/comparison.py`. Note, this\nrepository does not include pre trained models as they need much memory.\n\n#### Denoise seismograms\nIf your seismogram is longer than your seismograms from the training dataset, the longer time series is split into\noverlapping segments, e.g. 60 s segments. Each of these segments is denoised and the overlapping segments are\nmerged to get one denoises time series. \nThe script `./denoiser/denoiser_utils.py` contains the function `denoising_stream` that removes the noise\nfrom all traces in a obspy stream object. For more details please read the function description.\nYou can use the pretrained model and config-file to suppress noise from your data. Try to run the following code:\n```\nfrom obspy import read, UTCDateTime\nfrom denoiser.denoise_utils import denoising_stream\n\nst = read(your data)  # Read your seismic data here\nst_de = st.copy()  # Create a copy of your obspy stream\nst_de = denoising_stream(stream=st, model_filename=\"Models/gr_mixed_stft.h5\",\n                         config_filename=\"config/gr_mixed_stft.config\")\n```\nCompare your original stream and the denoised stream whether some noise is removed from the data.\nThe pretrained model is trained with noise samples from several stations of the seismic network GR and\nwith high signal-to-ratio events from the Stanford Earthquake Dataset \n(STEAD, https://github.com/smousavi05/STEAD).\n\n#### Automatic denoiser\nIn many cases one needs real time denoising to analyse the denoised traces e.g. with Seiscomp.\nThe function `auto_denoiser` in `./denoiser.denoiser_utils.py` reads a list with all required parameters\nfrom a csv file (`./denoiser/auto_denoiser.csv`). By adding a date, it is possible to start a cron job\nthat denoises the defined stations regulary. \n\n#### Retrain a model\nIf you like to retrain an already trained denoising model, the file `model.py` contains the function\n`retrain`. The function loads a tensorflow model, takes new data and trains the model with the new data.\n",
        "createdAt": "2022-07-05T07:32:05.000Z",
        "updatedAt": "2025-09-17T13:18:05.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/JanisHe/seisDAE/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "RobertoGuardo/QcDataCleaning",
        "url": "https://github.com/RobertoGuardo/QcDataCleaning",
        "description": "Data selection for volcano tomography",
        "stars": 2,
        "forks": 1,
        "readme": "# QcDataCleaning\nData selection for volcano tomography\n",
        "createdAt": "2022-01-05T14:18:20.000Z",
        "updatedAt": "2022-01-06T01:24:47.000Z",
        "language": "MATLAB",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/RobertoGuardo/QcDataCleaning/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "lijunzh/ricker",
        "url": "https://github.com/lijunzh/ricker",
        "description": "Seismic synthetic signal using Ricker wavelet (Mexican hat) with controlable length, peak frequency, and peak location.",
        "stars": 9,
        "forks": 1,
        "readme": "# Ricker Wavelet Generator\n\n[![PyPI version](https://badge.fury.io/py/ricker.svg)](https://badge.fury.io/py/ricker) \n[![Build Status](https://travis-ci.org/gatechzhu/ricker.svg?branch=master)](https://travis-ci.org/gatechzhu/ricker)\n\n## Introduction\n[Ricker wavelet](http://wiki.seg.org/wiki/Dictionary:Ricker_wavelet) (Mexican hat signal) is widely used in synthetic seismic simulation. Although, [SciPy](https://github.com/scipy/scipy#id1) offers a nice [ricker](https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.signal.ricker.html) generator, it is very basic and limited in flexibility. After repeated writing similar code to generate a shifted Ricker wavelet, I decided to write a small tool for it.  \n\n## Dependancy\n- [NumPy](http://www.numpy.org/)\n\n## Installation\n### From PyPI\n```\npip install ricker\n```\n\n### From source file\nDownload srouce file from [releases page](https://github.com/gatechzhu/ricker/releases). Under the root directory, type:\n\n```\npython setup.py install\n```\n\n## Contact\n\nIn counter of any trouble, contact *gatechzhu@gmail.com*\n",
        "createdAt": "2017-02-13T21:11:40.000Z",
        "updatedAt": "2024-09-20T01:16:09.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/lijunzh/ricker/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Suchocki88/Slinky",
        "url": "https://github.com/Suchocki88/Slinky",
        "description": "Slinky Seismology Activity State Standards",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2025-08-12T23:01:32.000Z",
        "updatedAt": "2025-08-12T23:02:29.000Z",
        "language": "HTML",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "konstak98/Seismology_Tools",
        "url": "https://github.com/konstak98/Seismology_Tools",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2023-11-03T11:23:50.000Z",
        "updatedAt": "2023-11-03T11:27:21.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "jakewalter/easyQuake",
        "url": "https://github.com/jakewalter/easyQuake",
        "description": "Simplified machine-learning driven earthquake detection, location, and analysis.",
        "stars": 75,
        "forks": 27,
        "readme": "# easyQuake\n\n[![CI](https://github.com/jakewalter/easyQuake/actions/workflows/ci.yml/badge.svg)](https://github.com/jakewalter/easyQuake/actions/workflows/ci.yml)\n\nSimplified machine-learning driven earthquake detection, location, and analysis in one easy-to-implement python package.\n\n\nFor more details, see the documentation: https://easyquake.readthedocs.io/\n\nOn most systems you should be able to simply:\n```\npip install easyQuake\n```\nTo stay on the bleeding edge of updates:\n```\npip install easyQuake --upgrade\n```\n\nOr if you need to tweak something, like the number of GPUs in gpd_predict, you could:\n```\ngit clone https://github.com/jakewalter/easyQuake.git\ncd easyQuake\npip install .\n```\n\nIf you find this useful, please cite:\n\n```\nWalter, J. I., P. Ogwari, A. Thiel, F. Ferrer, and I. Woelfel (2021), easyQuake: Putting machine \nlearning to work for your regional seismic network or local earthquake study, Seismological Research \nLetters, 92(1): 555–563, https://doi.org/10.1785/0220200226\n```\n\n## Requirements\nThis code leverages machine-learning for earthquake detection with the choice of the GPD (https://github.com/interseismic/generalized-phase-detection), EQTransformer (https://github.com/smousavi05/EQTransformer), or PhaseNet (https://github.com/AI4EPS/PhaseNet) pickers. You should have suitable hardware to run CUDA/Tensorflow, which usually means some sort of GPU. This has been tested on servers with nvidia compute cards and modest multi-core desktop with consumer gaming nvidia card (e.g. Geforce 1050 Ti). The event-mode can be run efficiently enough on a laptop.\n\n* Most tested configuration includes nvidia-cuda-toolkit, obspy, keras, tensorflow-gpu==2.2, basemap\n* I've found that the the easiest way to install cuda, tensorflow, and keras is through installing Anaconda python and running ```conda install tensorflow-gpu==2.2```\n* Because tensorflow-gpu 2.2 requires python 3.7 (not the latest version), you might find an easier road creating a new environment:\n```\nconda create -n easyquake python=3.7 anaconda\nconda activate easyquake\nconda install tensorflow-gpu==2.2\nconda install keras\nconda install obspy -c conda-forge\npip install easyQuake\n```\n\n## Running easyQuake\n\nThe first example is a simple one in \"event mode\" - try it:\n\n```\nfrom easyQuake import detection_association_event\n\ndetection_association_event(project_folder='/scratch', project_code='ok', maxdist = 300, maxkm=300, local=True, machine=True, latitude=36.7, longitude=-98.4, max_radius=3, approxorigintime='2021-01-27T14:03:46', downloadwaveforms=True)\n```\n\nThis next example runs easyQuake for a recent M6.5 earthquake in Idaho for the 2 days around the earthquake (foreshocks and aftershocks). The catalog from running the example is in the examples folder: https://github.com/jakewalter/easyQuake/blob/master/examples/catalog_idaho_2days.xml\n\nIf you don't have a suitable computer, try it in Google Colab\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jakewalter/easyQuake/blob/master/examples/easyquake_demo.ipynb)\n\n```\nfrom easyQuake import download_mseed\nfrom easyQuake import daterange\nfrom datetime import date\nfrom easyQuake import combine_associated\nfrom easyQuake import detection_continuous\nfrom easyQuake import association_continuous\n\nfrom easyQuake import magnitude_quakeml\nfrom easyQuake import simple_cat_df\n\nimport matplotlib.pyplot as plt\nmaxkm = 300\nmaxdist=300\nlat_a = 42\nlat_b = 47.5\nlon_a = -118\nlon_b = -111\n\n\nstart_date = date(2020, 3, 31)\nend_date = date(2020, 4, 2)\n\nproject_code = 'idaho'\nproject_folder = '/data/id'\nfor single_date in daterange(start_date, end_date):\n    print(single_date.strftime(\"%Y-%m-%d\"))\n    dirname = single_date.strftime(\"%Y%m%d\")\n    download_mseed(dirname=dirname, project_folder=project_folder, single_date=single_date, minlat=lat_a, maxlat=lat_b, minlon=lon_a, maxlon=lon_b)\n    detection_continuous(dirname=dirname, project_folder=project_folder, project_code=project_code, single_date=single_date, machine=True,local=True)\n    #run it with EQTransformer instead of GPD picker\n    #detection_continuous(dirname=dirname, project_folder=project_folder, project_code=project_code, machine=True, machine_picker='EQTransformer', local=True, single_date=single_date)\n    #PhaseNet\n    #detection_continuous(dirname=dirname, project_folder=project_folder, project_code=project_code, machine=True, machine_picker='PhaseNet', local=True, single_date=single_date)\n    association_continuous(dirname=dirname, project_folder=project_folder, project_code=project_code, maxdist=maxdist, maxkm=maxkm, single_date=single_date, local=True)\n    ### IMPORTANT - must call the specific picker to create association and catalogs specific to that picker within each dayfolder!!\n    #association_continuous(dirname=dirname, project_folder=project_folder, project_code=project_code, maxdist=maxdist, maxkm=maxkm, single_date=single_date, local=True, machine_picker='EQTransformer')\n    #association_continuous(dirname=dirname, project_folder=project_folder, project_code=project_code, maxdist=maxdist, maxkm=maxkm, single_date=single_date, local=True, machine_picker='PhaseNet')\n\ncat, dfs = combine_associated(project_folder=project_folder, project_code=project_code)\n#cat, dfs = combine_associated(project_folder=project_folder, project_code=project_code, machine_picker='EQTransformer')\n#cat, dfs = combine_associated(project_folder=project_folder, project_code=project_code, machine_picker='PhaseNet')\ncat = magnitude_quakeml(cat=cat, project_folder=project_folder,plot_event=True)\ncat.write('catalog_idaho.xml',format='QUAKEML')\n\n\ncatdf = simple_cat_df(cat)\nplt.figure()\nplt.plot(catdf.index,catdf.magnitude,'.')\n```\n\n## Tips for successful outputs\n\nWithin your systems, consider running driver scripts as nohup background processes ```nohup python ~/work_dir/okla_daily.py &```. In this way, one could ```cat nohup.out | grep Traceback``` to understand python errors or ```grep nohup.out | Killed``` to understand when the system runs out of memory.\n\n## Video intros to easyQuake\n\nMost recent updates, recorded for the 2021 SSA Annual meeting: https://www.youtube.com/watch?v=bjBqPL9pD5w\n\nRecorded for the fall 2020 Virtual SSA Eastern Section meeting: https://www.youtube.com/watch?v=coS2OwTWO3Y\n\n## About EasyQuake\n\nStay up to date on the latest description of EasyQuake contents: https://easyquake.readthedocs.io/en/latest/About.html\n\n## Running easyQuake with SLURM\n\nIf you have access to shared computing resources that utilize SLURM, you can drive easyQuake by making a bash script to run the example code or any code (thanks to Xiaowei Chen at OU). Save the following to a drive_easyQuake.sh and then run it\n```\n#!/bin/bash\n#\n#SBATCH --partition=gpu_cluster\n#SBATCH --ntasks=1\n#SBATCH --mem=1024\n#SBATCH --output=easyquake_%J_stdout.txt\n#SBATCH --error=easyquake_%J_stderr.txt\n#SBATCH --time=24:00:00\n#SBATCH --job-name=easyquake\n#SBATCH --mail-user=user@school.edu\n#SBATCH --mail-type=ALL\n#SBATCH --chdir=/drive/group/user/folder\nconda init bash\nbash\nconda activate easyquake\npython idaho_example.py\n```\n## Version brief notes\n\nVersion 1.4 (9/30/2024) = Long overdue version update, including modules for PyOcto association conversion to QuakeML file and seisbench picker integration.\n\nVersion 1.3 (11/22/2022) = PhaseNet now included, in addition to GPD and EQTransformer pickers. Numerous other bugs squashed.\n\nVersion 1.2 (8/1/2022) - Rewrote the non-ML picker to be easier to work with (recursive_sta_lta from obpsy) and include input of those parameters within detection_continuous function.\n\nVersion 0.9 (2/23/2022) - Modules to cut easyQuake event waveforms from continuous data (cut_event_waveforms) and module for converting easyQuake catalog (or any QuakeML-formatted catalog) to HDF5 (quakeML_to_hdf5) for training new ML models\n\nVersion 0.8 (7/30/2021) - Several major bug fixes and improved controls for Hypoinverse location\n\nVerson 0.6 (2/24/2021) - Implemented choice of GPD or EQTransformer pickers for the picking stage\n\nVersion 0.5 (2/10/2021) - includes embedded hypoinverse location functionality, rather than the simple location with the associator.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details\n\n## Acknowledgments\n\n* code was used or otherwise changed to suit our purposes from obspy (https://github.com/obspy/obspy/wiki), GPD (https://github.com/interseismic/generalized-phase-detection), PhasePApy (https://github.com/austinholland/PhasePApy), EQTransformer (https://github.com/smousavi05/EQTransformer), PhaseNet (https://github.com/AI4EPS/PhaseNet), and others\n* would not be possible without the robust documentation in the obspy project\n* this work was developed at the Oklahoma Geological Survey\n\n",
        "createdAt": "2020-04-12T03:24:45.000Z",
        "updatedAt": "2025-11-15T01:01:13.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/jakewalter/easyQuake/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "compgeolab/szwillus-moho-depth",
        "url": "https://github.com/compgeolab/szwillus-moho-depth",
        "description": "Download the seismological Moho depth grid from Szwillus et al. (2019) and convert it to netCDF",
        "stars": 2,
        "forks": 0,
        "readme": "# Global seismological Moho depth estimate from Szwillus et al. (2019)\n\nDownload the seismological Moho depth grid from the supplementary material of\nthe paper and convert it to netCDF for easier manipulation and loading with GMT\nand xarray.\n\n## References\n\nSzwillus, W., Afonso, J. C. C., Ebbing, J., & Mooney, W. D. (2019). Global\ncrustal thickness and velocity structure from geostatistical analysis of\nseismic data. Journal of Geophysical Research: Solid Earth, 124, 1626– 1652.\nhttps://doi.org/10.1029/2018JB016593\n\n## License\n\nAll source code is made available under a MIT license.\nYou can freely use and modify the code, without warranty,\nso long as you provide attribution to the original authors.\nSee [LICENSE](LICENSE) for the full license text.\n",
        "createdAt": "2020-10-30T16:43:19.000Z",
        "updatedAt": "2022-02-24T16:25:15.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/compgeolab/szwillus-moho-depth/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ivgnk/Prognoz2021",
        "url": "https://github.com/ivgnk/Prognoz2021",
        "description": "Delphi 10.3 Seismic Prediction Program (Программа прогнозирования сейсмических явлений)",
        "stars": 0,
        "forks": 0,
        "readme": "# Prognoz2021\nSeismic Prediction Program, Delphi 10.3\n\nПрограмма прогнозирования сейсмических явлений, Delphi 10.3\n",
        "createdAt": "2021-06-29T11:23:59.000Z",
        "updatedAt": "2023-09-22T12:38:09.000Z",
        "language": null,
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ivgnk/Prognoz2021/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "LM2025/HooverDamProject",
        "url": "https://github.com/LM2025/HooverDamProject",
        "description": "All code, files, and data associated with the Hoover Dam Seismology Project",
        "stars": 0,
        "forks": 0,
        "readme": "# HooverDamProject\nAll code, files, and data associated with the Hoover Dam Seismology Project\n\n## Welcome to the main page for Luke Mazza's Hoover Dam project! Like exploring spectral ratios? Like subspace detection? Like exploring advanced seismological coding? You have found the Right Place!\n\n### In this repository, you will find the neccessary data, code, and instructions for runnind and exploring the Hoover Dam seismology project. \n### Cloning this repository into your local system is a valid option for exploring this documentation. \n\n### Note: In the main jupyter notebook for this project, the python module plotly is used to visualize some code outputs. Github does not currently support visualization through plotly. To see all the visualizations for this project and the notebook in its entirety, please visit https://nbviewer.org/github/LM2025/HooverDamProject/blob/main/HOVR_project.ipynb\n\n#### For any comments, questions, or concerns, please reach out to lmazza@mines.edu\n\n#### Geophysics rocks! :)\n",
        "createdAt": "2023-06-26T20:30:06.000Z",
        "updatedAt": "2023-06-26T21:17:29.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/LM2025/HooverDamProject/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "anowacki/SAC.jl",
        "url": "https://github.com/anowacki/SAC.jl",
        "description": "Process seismic data in SAC format with Julia",
        "stars": 13,
        "forks": 7,
        "readme": "# SAC.jl\n\n[![Build Status](https://img.shields.io/travis/anowacki/SAC.jl.svg?style=flat-square&label=linux)](https://travis-ci.org/anowacki/SAC.jl)\n[![Build status](https://img.shields.io/appveyor/ci/AndyNowacki/sac-jl.svg?style=flat-square&label=windows)](https://ci.appveyor.com/project/AndyNowacki/sac-jl/branch/master)\n[![Coverage Status](https://coveralls.io/repos/github/anowacki/SAC.jl/badge.svg?branch=master)](https://coveralls.io/github/anowacki/SAC.jl?branch=master)\n\n## Status\n\nSAC.jl is now **deprecated** in favour of\n[Seis.jl](https://github.com/anowacki/Seis.jl), and no new features will\nbe added to this package.\n\nAll SAC.jl functionality exists in Seis.jl, which also includes much\nbetter [documentation](https://anowacki.github.io/Seis.jl/stable/)\nand more IO options, and integrates with the wider Seis.jl\necosystem (including [SeisSplit](https://github.com/anowacki/SeisSplit.jl),\n[SeisTau](https://github.com/anowacki/SeisTau.jl) and\n[Beamforming](https://github.com/anowacki/Beamforming.jl)).\n\n## What is SAC.jl?\nA [Julia](http://julialang.org) package for dealing with seismic data in the\n[SAC](http://ds.iris.edu/files/sac-manual/manual/file_format.html) format, and\nprocessing that data in a similar way to the SAC program:\neither the [SAC/BRIS](http://www1.gly.bris.ac.uk/~george/sac-bugs.html) or\n[IRIS](http://ds.iris.edu/ds/nodes/dmc/software/downloads/sac/) versions.\n\n\n## How to install\nAlthough not registered as an official package, SAC.jl can be added to your\nJulia install like so:\n\nOn Julia v0.7 or v1.0 (press `]` to get to `pkg` mode first):\n\n```julia\n(v1.0) pkg> add https://github.com/anowacki/SAC.jl\n```\n\n(Or you can also do `import Pkg; Pkg.add(\"https://github.com/anowacki/SAC.jl\")`.)\n\nOn Julia v0.6, versions before 0.3 can be installed like so:\n\n```julia\nPkg.clone(\"https://github.com/anowacki/SAC.jl\")\n```\n\nThis will automatically install the depndencies you need.  You then need only do\n\n```julia\nusing SAC\n```\n\nand if that works, you're ready to go.\n\n\n## How to use\n### SACtr type\nSAC.jl represents SAC files with the `SACtr` type, which is exported.  Methods\nare written expecting and dispatched on this type.  Methods are also defined\nfor arrays of `SACtr`, `Array{SACtr}`, which allows for easy operations on\nmultiple traces.\n\nThe `SACtr` type has fields whose names and types correspond to SAC headers.\nThese are accessed via `Symbol`s which are the name of header.  (To get a `Symbol`,\njust write the name of the header with a colon in front.)\n\n```julia\njulia> t = SAC.sample()\nSAC.SACtr:\n    delta: 0.01\n   depmin: -1.56928\n   depmax: 1.52064\n        b: 9.459999\n\t\t  ⋮\n    kevnm: K8108838\n\njulia> typeof(t)\nSAC.SACtr\n\njulia> t[:delta]\n0.01f0\n\njulia> t[:delta] = 0.02\n0.02\n```\n\n(Note that SAC floating point headers are `Float32`s.)\n\nThe field `t` contains the trace as an `Array{Float32,1}`.  To change the trace,\njust alter the `:t` index:\n\n```julia\njulia> t[:depmax]\n1.52064f0\n\njulia> t[:t] += 1;\n\njulia> t.t\n1000-element Array{Float32,1}:\n 0.90272\n 0.90272\n 0.90144\n ⋮      \n 0.92832\n 0.9232 \n 0.9232 \n\njulia> t[:depmax]\n2.52064f0\n```\n\nYou can use the methods `+`, `-`, `*` and `/` to modify the traces without\nneeding to access `:t` directly, too:\n\n```julia\njulia> t == SAC.sample() + 1\ntrue\n\njulia> t == 1*t\ntrue\n```\n\nYou can also get or modify several header values at once:\n\n```julia\njulia> T = [SAC.sample() for _ in 1:5]\n5-element Array{SAC.SACtr,1}:\n SAC.SACtr(delta=0.01, b=9.459999, npts=1000, kstnm=CDV, gcarc=3.357463, az=88.14708, baz=271.8529)\n SAC.SACtr(delta=0.01, b=9.459999, npts=1000, kstnm=CDV, gcarc=3.357463, az=88.14708, baz=271.8529)\n SAC.SACtr(delta=0.01, b=9.459999, npts=1000, kstnm=CDV, gcarc=3.357463, az=88.14708, baz=271.8529)\n SAC.SACtr(delta=0.01, b=9.459999, npts=1000, kstnm=CDV, gcarc=3.357463, az=88.14708, baz=271.8529)\n SAC.SACtr(delta=0.01, b=9.459999, npts=1000, kstnm=CDV, gcarc=3.357463, az=88.14708, baz=271.8529)\n\njulia> typeof(T)\nArray{SAC.SACtr,1}\n\njulia> T[:t0] = 1:5 # Set time markers in t0\n1:5\n\njulia> T[:t0]\n5-element Array{Float32,1}:\n 1.0\n 2.0\n 3.0\n 4.0\n 5.0\n\njulia> T[:t0] += 2 # Move all time markers back by 2 s\n5-element Array{Float32,1}:\n 3.0\n 4.0\n 5.0\n 6.0\n 7.0\n\njulia> T[:kstnm] = [\"A1\", \"B2\", \"C3\", \"D4\", \"E5\"] # Set station names\n5-element Array{ASCIIString,1}:\n \"A1\"\n \"B2\"\n \"C3\"\n \"D4\"\n \"E5\"\n```\n\n### Reading files\nThe `read` function is not exported to avoid name clashes, so one must call\n`SAC.read()`.   For example, to load a single file, do\n\n```julia\nt = SAC.read(\"XM.A01E.HHZ.SAC\")\n```\n\nThis loads the file `XM.A01E.HHZ.SAC` into the `SACtr` object `t`.\n\n### Reading with wildcards\nAs with SAC, one can use wildcards to read a set of files, e.g.:\n\n```julia\nT, filenames = read_wild(\"*Z.SAC\")\n```\n\nAn array of `SACtr`, `T`, is returned as well as a list of matching file names\nin `filenames`.\n\n### Writing files\nUse the exported `write` method, passing a `SACtr` object and the file name, or\nan array of `SACtr` and filenames\n\n```julia\nwrite(t, \"file.SAC\")\nwrite(T, filenames)\n```\n\n### Processing\nA number of common processing steps are already implemented as methods, such as\n`lowpass!`, `taper!`, `envelope!` and so on.  In many cases, methods which have\nsimilar names to SAC commands can also be used with the SAC short forms.  For\ninstance, `bandpass!` and `bp!` are the same.\n\nNote that as is convention in Julia, these commands end with an exclamation\nmark (!) and modify the trace in-place.  Copying versions of these commands are\navailable and do not have an exclamation mark (e.g., `lowpass`, `taper`, etc.).\n\n### File endianness\n[SAC/BRIS](http://www1.gly.bris.ac.uk/~george/sac-bugs.html) expects files to\nalways be in big-endian format;\n[SAC/IRIS](http://ds.iris.edu/ds/nodes/dmc/software/downloads/sac/) expects them\nin the same endianness as the machine.  SAC.jl is agnostic and will both read\nand write in either endianness, but generally prefers to stick to big-endian,\nfor compatibilty with SAC/BRIS.\n\n### Plotting\nA companion repo, [`SACPlot`](https://github.com/anowacki/SACPlot.jl)\ncan be used to perform some of the plotting that SAC can do.\n\n\n## Getting help\nFunctions are documented, so at the REPL type `?` to get a `help?>` prompt,\nand type the name of the function:\n\n```julia\nhelp?> bandpass!\nsearch: bandpass! bandpass\n\n  bandpass!(s::SACtr, c1, c2; ftype=:butterworth, npoles=2, passes=1) -> s\n\n  Perform a bandpass filter on the SAC trace s, between frequency corners c1 and c2,\n  returning the modified trace.\n\n  Select type of filter with ftype: current options are: Symbol[:butterworth]. Set\n  number of poles with npoles.\n\n  passes may be 1 (forward) or 2 (forward and reverse).\n```\n\n### Documentation\nDocumentation is a work in progress, but all useful commands are documented.\nTo see the list of commands, check the code, or in the REPL type `SAC.` then\npress tab a couple of times to see all the module methods and variables.\nCalling up the interactive help will give a useful description of each.\n\n\n## Other software\n\n* If you use Fortran, then you should investigate the following modules:\n  - [`sacio90`](https://github.com/jwookey/sacio90)\n  - `f90sac` in the [seismo-fortran](https://github.com/anowacki/seismo-fortran)\n    repo, inspired by the James Wookey original above.\n* If you use Python, use [ObsPy](https://github.com/obspy/obspy/wiki).\n* If you use MATLAB, use [`msac`](https://github.com/jwookey/msac).\n",
        "createdAt": "2016-06-14T16:24:12.000Z",
        "updatedAt": "2024-06-13T07:23:58.000Z",
        "language": "Julia",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/anowacki/SAC.jl/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "geophystech/eqalert.ru",
        "url": "https://github.com/geophystech/eqalert.ru",
        "description": "One more information resource about earthquakes, seismic impacts and hazards",
        "stars": 10,
        "forks": 4,
        "readme": "<p align=\"center\">\n<a href=\"https://eqalert.ru\">\n    <img src=\"https://raw.githubusercontent.com/geophystech/eqalert.ru/master/src/assets/img/logos/eqalert.png\">\n</a>\n\n<br>\nThe real-time information resource about earthquakes, seismic impacts, and hazards.\n<br>\nBuilt on <a href=\"https://vuejs.org\">Vue.js 2.5</a>, <a href=\"https://webpack.js.org\">Webpack 4</a>, and <a href=\"https://getbootstrap.com/docs/4.0\">Twitter Bootstrap 4</a>\n<br>\n<br>\n\n<a href=\"https://vuejs.org\">\n    <img alt=\"vue.js-2.5.x\" src=\"https://img.shields.io/badge/vue.js-2.5-337ab7.svg?style=flat-square\">\n</a>\n\n<a href=\"https://getbootstrap.com/docs/4.0\">\n    <img alt=\"webpack-4\" src=\"https://img.shields.io/badge/webpack-4-337ab7.svg?style=flat-square\">\n</a>\n\n<a href=\"https://getbootstrap.com/docs/4.0\">\n    <img alt=\"bootstrap-4.0.0\" src=\"https://img.shields.io/badge/bootstrap-4-337ab7.svg?style=flat-square\">\n</a>\n\n<br>\n\n</p>\n\n\n# eqalert.ru [![Build Status](https://travis-ci.org/geophystech/eqalert.ru.svg?branch=master)](https://travis-ci.org/geophystech/eqalert.ru) [![codecov](https://codecov.io/gh/geophystech/eqalert.ru/branch/master/graph/badge.svg)](https://codecov.io/gh/geophystech/eqalert.ru/)\n\nEqalert.ru – the real-time information resource about earthquakes, seismic impacts and hazards\n\n## v3 (SSR version)\n\nThis README is deprecated. Please check `./nuxt2/README.md`.\n\nThere is a new application written using `Nuxt2` with `Server-Side Rendering (SSR)` to make SEO better for _Google_ and _Yandex_ robots\n\n---\n\n## Used technologies\n\nThis is a Single Page Application built with [VueJS](https://vuejs.org/) and [webpack](https://webpack.js.org/).\nData is received from an API host using [axios](https://github.com/axios/axios) library.\n\nMore about used technologies you can get from the `packages.json` file.\n\n## Preparing application\n\n```bash\n# Clone application from GitHub\ngit clone git@github.com:geophystech/eqalert.ru.git\n\ncd eqalert.ru\n\n# Install dependencies\nyarn install\n```\n\n\n## Development mode\n\nTo run application in development mode, just run:\n\n``` bash\nyarn dev\n```\n\nThe application will be compiled and run.\nIt will be available at http://localhost:8080\n\nBy default the application uses hot-reload mode which means any changes will be picked up and all open pages will be reloaded automatically.\nException is static markdown pages located in `static/markdown` directory.\n\nIf you changem them, you should update your browser pages manually (`F5`).\n\n## Run tests\n\nThe application has a few tests. Not very good coverage yet, but still.\n\nTo run tests, just run:\n\n```\n# Unit tests\nnpm run unit\n\n# E2E tests\nnpm run e2e\n\n# Or run all tests\nnpm test\n```\n\n## Build application\n\nTo run application in staging or production areas, it should be compiled first:\n\n```\nNODE_ENV=production yarn build\n```\n\nThe compiled application will be stored in the `dist/` directory.\nYou can deploy it to your target (staging, production, etc) using any tools you like.\n\nExample of deploying the application manually to a staging host:\n\n```bash\nrsync -avzr --delete -e ssh dist/ USERNAME@HOSTNAME:/opt/eqalert-frontend-test\n```\n\n## Run docker container with prebuild application\n\n```bash\ndocker run -p 8888:80 --rm --env API_URI=https://api-test.geophystech.ru/api --env AUTH_URI=https://oauth-client-test.geophystech.ru harbor.ju0.ru/geophystech/eqalert-front\n```\n\n## LICENSE\n\n   Copyright 2017-2019 GEOPHYSTECH LLC\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n\n## Vulnerabilities\n\n  - node-sass@8.0.0 >> [website](https://devhub.checkmarx.com/cve-details/CVE-2017-12964/?utm_source=jetbrains&utm_medium=referral&utm_campaign=pycharm&utm_term=python)\n  ```\n    There is a stack consumption issue in all versions of LibSass,\n    that is triggered in the function \"Sass::Eval::operator()\"\n    in \"eval.cpp\". It will lead to a remote denial of service attack.\n    This issue also affects all versions of other packages that use\n    the LibSass library, such as node-sass, libsass-python, sassc, jsass.\n  ```\n",
        "createdAt": "2017-08-12T07:09:51.000Z",
        "updatedAt": "2024-03-16T04:08:12.000Z",
        "language": "JavaScript",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/geophystech/eqalert.ru/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "thehalfspace/Seismology",
        "url": "https://github.com/thehalfspace/Seismology",
        "description": "A collection of small projects on earthquakes",
        "stars": 0,
        "forks": 0,
        "readme": "# Seismology\nA collection of small projects on earthquakes using GMT and Python. Look at final ppt.pdf for the results.\n\nCluster the earthquakes in space and compute b-values (https://www.wikiwand.com/en/Gutenberg%E2%80%93Richter_law) for each cluster.\n\nSteps:\n\n1. Download data from NCEDC for any specified time period. (In this repo, Parkfield_EQ_.htm, Parkfield_Time1.csv, etc.).\n2. Run Parkfield.sh. Additional information in the file. This is a shell script for projection of earthquake data along the fault. It uses generic mapping tools.\n3. Run the python program bValues.ipynb. This program clusters the projected data using k-means clustering and computes the b-values for each cluster.\n",
        "createdAt": "2018-04-15T14:34:54.000Z",
        "updatedAt": "2018-09-05T15:39:43.000Z",
        "language": "HTML",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/thehalfspace/Seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "lucyxkz/Seismology-II-group",
        "url": "https://github.com/lucyxkz/Seismology-II-group",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2019-03-03T20:19:00.000Z",
        "updatedAt": "2019-03-03T20:30:57.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "nadavwetzler/Stress-Inversion",
        "url": "https://github.com/nadavwetzler/Stress-Inversion",
        "description": "A Matlab code to calculate stress field from FMS data over a range of friction coefficients ",
        "stars": 7,
        "forks": 0,
        "readme": "# Stress-Inversion\nA Matlab code to calculate stress field from FMS data over a range of friction coefficients \n![Fig0](https://user-images.githubusercontent.com/88764899/129442006-29dcde44-8db3-42de-ade6-6d6e528b9889.png)\n\n\n# Features\nThe code calculates the principal axis of the stress state from focal mechannism information.\nUse FMS data at /data folder as a tamplate for your one dataset\n\nOne of the chalanges is to identify the fault plane from the two nodal planes of the double couple solution. We select the planes that best fit the general stress state of the population, constrained by the friction coheficient to provide a mechanical strategy to exclude the auxilary planes.\nTo identify the fault plane from the two nodal planes, determination of the faults frictional strength and calculate the local stress field we used stress-inversion approach developed to calculate the stress state associated with a set of focal-mechanisms (Reches et al., 1992; Busetti and Reches, 2014). This approach was customized into MATLAB environment to calculate the orientations and relative magnitudes of the three principal stress axes (3D stress tensor) for a group of FMSs under three assumptions: \n1) All the earthquakes in the group occurred under the same stress state. \n2) Slip along a fault occurs in the direction of maximum resolved shear stress \n3) The shear and normal stress on the faults satisfy the Coulomb failure criterion.\n\nEach pair of the nodal planes is tested with respect to the Principle Axes Misfit Angle (PAM), which is the angle between the ideal stress axes of each nodal plane and general stress axes of the entire group according to the optimal mechanical condition for faulting. The quality of the calculated stress tensor is represented by the confidence levels, calculated by bootstrapping method, for 500 random samples of the original FMS group. \nStress is inverted for each friction coefficient that ranges 0.1 to 0.8, weighted by the earthquake magnitude, and fault planes are selected according to smallest PAM between the two FMS nodal planes. To ensure coherent selection, two more criteria are set: \n1) PAM is smaller than 30 deg. \n2) The aperture between the PAM of the two nodal planes is larger than 10%.\n\nBusetti, S., Jiao, W., Reches, Z., 2014. Geomechanics of hydraulic fracturing microseismicity: Part 1. Shear, hybrid, and tensile events. Am. Assoc. Pet. Geol. Bull. 98, 2439–2457. https://doi.org/10.1306/05141413123\n\nReches, Z., Baer, G., Hatzor, Y., 1992. Constraints on the strength of the upper crust from stress inversion of fault slip data. J. Geophys. Res. 97, 12481. https://doi.org/10.1029/90JB02258\n\n# Run\nDownload the files\n\nAdd the contant of the folder to your Matalb path\n\nRun run_Stress_inv.m file and you should soon see the results\n\nDepending on the resolution of your screen, the lagend may overlap the bottom of subplot. you can shift it down\n\n# Input\nThe code loads an earthquake dataset composed of 18 columns in a matrix format (all numeric):\n1 - event id,\n2 - year,\n3 - month,\n4 - day,\n5 - hour,\n6 - minuet,\n7 - seconds,\n8 - magitude,\n9 - latitude,\n10 - longitude,\n11 - depth,\n12 - strike1,\n13 - dip1,\n14 - rake1,\n15 - strike2,\n16 - dip2,\n17 - rake2,\n18 - cluster id,\n\nif you dont knnow the auxilary use:\nhttps://github.com/g2e/seizmo/blob/master/cmt/auxplane.m\nto calculate: strike2, dip2, rake2\n\n#\tAcknowledgements \nDr Seth Busetti, Aramco Services, wrote the generic Matlab code for the stress-inversion (Stress_inv.m).\n\n",
        "createdAt": "2021-08-11T06:58:49.000Z",
        "updatedAt": "2025-04-23T17:12:05.000Z",
        "language": "MATLAB",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/nadavwetzler/Stress-Inversion/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "KhuzaimahAziz/thesis_seismology",
        "url": "https://github.com/KhuzaimahAziz/thesis_seismology",
        "description": null,
        "stars": 1,
        "forks": 0,
        "readme": "# Seisbench Training : A Configurable Seismic Phase Picking Pipeline\r\n\r\nPhasePicker is a fully **configurable, Hydra and Mlflow driven pipeline** for seismic phase picking built on top of **SeisBench**. It supports flexible dataset loading, augmentation, model configuration, not only training workflows but also the evaluation workflow using **Mlflow**.\r\n\r\n## 🚀 Features\r\n\r\n* **Hydra Configuration** for every component:\r\n\r\n  * Dataset (name, component_orders, dimension_orders, samplng_rate)\r\n  * Augmentations (probabilitic_labeller, normalize, randow_window)\r\n  * Training (lr, epochs, batch_size, n_workers, optimizer)\r\n  * Model (model_name)\r\n  * \r\n* **Reproducible experiments** with Hydra logging and config saving\r\n  \r\n* **MLflow Pipeline** for evaluation and saving best model.\r\n\r\n   * Setup Mlflow callback that evaluates model metrics during training from validation dataset and save metrics and plots in mlflow.\r\n   * Once the training is finished it saves the best_model as artifact with the details of the model and the config that it was trained on.\r\n  \r\n\r\n\r\n  \r\n\r\n\r\n\r\n",
        "createdAt": "2025-04-02T22:04:51.000Z",
        "updatedAt": "2025-11-18T20:03:58.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/KhuzaimahAziz/thesis_seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ikteran/Proyecto-ANS-Clustering-sismos",
        "url": "https://github.com/ikteran/Proyecto-ANS-Clustering-sismos",
        "description": "Aplicación de técnicas de aprendizaje no supervisado para agrupar eventos sísmicos ocurridos en Colombia entre 2009 y 2024",
        "stars": 1,
        "forks": 1,
        "readme": "<p align=\"center\">\n  <img src=\"https://industrial.uniandes.edu.co/sites/default/files/miad.jpg\" width=\"300\" height=\"100\" alt=\"Image\">\n</p>\n\n# Agrupación espacio-temporal de sismos en Colombia\n\n**¿Sabías que Colombia tiene más de 2000 sismos al mes?** Estos eventos pueden impactar significativamente en la infraestructura y la seguridad de sus habitantes. Por ello, nosotros, estudiantes del MIAD de Uniandes, hemos desarrollado este proyecto para analizar los catálogos de sismicidad. La agrupación de estos eventos puede ofrecer valiosas perspectivas para generar modelos de Machine Learning sobre la actividad sísmica, contribuyendo así a la gestión de riesgos y a la planificación preventiva.\n\n<p align=\"center\"> <img src=\"https://github.com/ikteran/Proyecto-ANS-Clustering-sismos/blob/main/data/mapa_sismicidad.png\" width=\"300\" height=\"400\" alt=\"Distribución espacial de los datos de sismicidad utilizados en el proyecto\"> <br> <small>Distribución espacial de los datos de sismicidad utilizados en el proyecto</small> </p>\n\n\n# Metodología\n\nEste estudio se enfoca en aplicar técnicas de aprendizaje no supervisado para agrupar los eventos sísmicos en Colombia entre 2009 y 2024. Nuestro objetivo es identificar patrones en la distribución de los sismos que puedan ayudar a modelar la actividad sísmica.\n\nUtilizamos datos proporcionados por el <a href=\"http://bdrsnc.sgc.gov.co/paginas1/catalogo/index.php\">Servicio Geológico Colombiano</a>. Optamos por el aprendizaje no supervisado debido a la naturaleza compleja y no etiquetada de los datos sísmicos. Este estudio se enmarca en el área de clustering. Al aplicar estas técnicas, esperamos descubrir estructuras y patrones significativos en los datos, aportando así soluciones prácticas y valiosas para la gestión del riesgo sísmico en Colombia.\n\n\n# Hablemos de datos\n\nLa base de datos contiene 96177 resgistros sísmicos superficiales (menor a 30km de profundidad), los cuales son interesantes ya que tienen un mayor potencial de causar daños siginificativos. A continuación se describen las variables:\n\n<table>\n    <tr>\n      <th>Variable</th>\n      <th>Tipo de Variable</th>\n      <th>Descripción</th>\n    </tr>\n    <tr>\n      <td>Fecha</td>\n      <td>DateTime</td>\n      <td>La fecha en la que ocurrió el evento sísmico</td>\n    </tr>\n    <tr>\n      <td>Hora UTC</td>\n      <td>DateTime</td>\n      <td>La hora exacta del evento sísmico en Tiempo Universal Coordinado (UTC).</td>\n    </tr>\n    <tr>\n      <td>Latitud</td>\n      <td>Numerica (Float)</td>\n      <td>La latitud geográfica del epicentro del sismo</td>\n    </tr>\n    <tr>\n      <td>longitud</td>\n      <td>Numérica (Float)</td>\n      <td>La longitud geográfica del epicentro del sismo</td>\n    </tr>\n    <tr>\n      <td>Profundidad</td>\n      <td>Numérica (Float)</td>\n      <td>La profundidad a la que ocurrió el sismo bajo la superficie terrestre, medida en km</td>\n    </tr>\n    <tr>\n      <td>Epicentro</td>\n      <td>String</td>\n      <td>El nombre del centro poblado más cercano al epicentro del sismo.</td>\n    </tr>\n    <tr>\n      <td>Magnitud</td>\n      <td>Numérica (Float)</td>\n      <td>La magnitud del evento sísmico. Este valor cuantifica la energía liberada</td>\n    </tr>\n    <tr>\n      <td>RMS</td>\n      <td>Numérica (Float)</td>\n      <td>Es la medida del error, comparando la diferencia promedio entre el tiempo de arribo teórico y el tiempo de arribo observado en segundos, utilizando las lecturas de los sismogramas.  Los valores menores reflejan buenas localizaciones.</td>\n    </tr>\n    <tr>\n      <td>GAP</td>\n      <td>Numérica (Float)</td>\n      <td>El ángulo en grados como medida de la distribución angular entre el epicentro del sismo y las estaciones sísmicas que registran el evento. Entre mayor, hubo una menor cobertura de la red para registrar el sismo.</td>\n    </tr>\n    <tr>\n      <td>GAP</td>\n      <td>Numérica (Float)</td>\n      <td>El ángulo en grados como medida de la distribución angular entre el epicentro del sismo y las estaciones sísmicas que registran el evento. Entre mayor, hubo una menor cobertura de la red para registrar el sismo.</td>\n    </tr>\n    <tr>\n      <td>Error-Lat</td>\n      <td>Numérica (Float)</td>\n      <td>El margen de error en la estimación de la latitud del epicentro, medido en km.</td>\n    </tr>\n    <tr>\n      <td>Error-Lon</td>\n      <td>Numérica (Float)</td>\n      <td>El margen de error en la estimación de la longitud del epicentro, medido en km</td>\n    </tr>\n    <tr>\n      <td>Error-Z</td>\n      <td>Numérica (Float)</td>\n      <td>El margen de error en la estimación de la profundidad del epicentro, medido en km</td>\n    </tr>\n</table>\n\n# Navegación\n\n- <b>data:</b> Se encuentra la base de datos utilizada en formato csv\n- <b>document:</b> Está la documentación de los hallazgos\n- <b>results:</b> Se encuentran los archivos e imagenes obtenidas a partir de los resultados\n- <b>scripts:</b> Visita nuestro documento de Jupyter donde verás el proceso a detalle.\n\n# Referencias\n\nA continuación, te compartimos algunos artículos científicos que exploramos para este proyecto:\n\n<ol>\n<li><strong>Gracia, M. D.</strong> (2017). <em>Seismotectonic characterization of the Colombian Pacific region: Identification of tectonic patterns through geostatistical analysis</em> (Undergraduate thesis). Universidad de los Andes, School of Sciences, Faculty of Geosciences.</li>\n\n<li><strong>Mato, F., & Toulkeridis, T.</strong> (2017). An unsupervised K-means based clustering method for geophysical post-earthquake diagnosis. <em>2017 IEEE Symposium Series on Computational Intelligence (SSCI)</em>, 1–8. <a href=\"https://doi.org/10.1109/SSCI.2017.8285216\" target=\"_blank\">https://doi.org/10.1109/SSCI.2017.8285216</a></li>\n\n<li><strong>Novianti, P., Setyorini, D., & Rafflesia, U.</strong> (2017). K-Means cluster analysis in earthquake epicenter clustering. <em>International Journal of Advances in Intelligent Informatics</em>, 3(2), 81. <a href=\"https://doi.org/10.26555/ijain.v3i2.100\" target=\"_blank\">https://doi.org/10.26555/ijain.v3i2.100</a></li>\n\n<li><strong>Yuan, R.</strong> (2021). An improved K-means clustering algorithm for global earthquake catalogs and earthquake magnitude prediction. <em>Journal of Seismology</em>, 25(3), 1005–1020. <a href=\"https://doi.org/10.1007/s10950-021-09999-8\" target=\"_blank\">https://doi.org/10.1007/s10950-021-09999-8</a></li>\n</ol>\n\n",
        "createdAt": "2024-08-26T15:37:47.000Z",
        "updatedAt": "2025-06-26T02:24:43.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ikteran/Proyecto-ANS-Clustering-sismos/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "EarthScope/ispaq",
        "url": "https://github.com/EarthScope/ispaq",
        "description": "Python command line script that uses R packages to calculate seismology data quality metrics.",
        "stars": 29,
        "forks": 12,
        "readme": "# ISPAQ - IRIS System for Portable Assessment of Quality\n\nISPAQ is a Python client that allows seismic data scientists and instrumentation operators to run data \nquality metrics on their own workstation, using much of same code as used in EarthScope's (formerly IRIS) \n[MUSTANG](https://service.earthscope.org/mustang/) data quality web service. It can be installed on Linux \nand macOS.\n\nUsers have the ability to create personalized _preference files_ that list combinations\nof station specifiers and statistical metrics of interest, such that they can be run\nrepeatedly over data from many different time periods. Alternatively, single station specifiers \nand metrics can be specified on the command line for simple runs or for use in shell scripting.\n\nISPAQ offers the option for access to [FDSN Web Services](https://www.fdsn.org/webservices/) to retrieve \nseismic data and metadata directly from selected data centers supporting the FDSN protocol. Users also \nhave the option to read local [miniSEED](https://ds.iris.edu/ds/nodes/dmc/data/formats/seed/) files and \nmetadata on their own workstations and construct on-the-spot data quality analyses on that data.\n\nOutput is optionally in CSV format or written to an internal SQLite database for tabular metrics. \nIn addition, Probability Density Functions (PDF) can be plotted to PNG image files. The PDF plots also \ninclude the New High Noise Model and the New Low Noise Model curves [Peterson,1993](https://doi.org/10.3133/ofr93322) \nand the minimum, maximum, and mode PDF statistics curves.\n\nThe business logic for MUSTANG metrics is emulated through [ObsPy](https://github.com/obspy/obspy/wiki) \nand custom Python code and the core calculations are performed using the same R packages as used by \nMUSTANG.\n\n\n# Background\n\nIn 2023, IRIS (Incorporated Research Institutions for Seismology) and UNAVCO merged\nto form EarthScope Consortium. IRIS (now EarthScope) webservices are unchanged but \ncan now be accessed at [https://service.earthscope.org](https://service.earthscope.org)\n as well as [https://service.iris.edu](https://service.iris.edu).\n\n[EarthScope](https://www.earthscope.org) has developed a \ncomprehensive quality assurance system called [MUSTANG](https://service.earthscope.org/mustang/).\n\nThe MUSTANG system was built to operate at EarthScope and is not generally portable. \nHowever, the key MUSTANG component is the Metric Calculators and these are publicly available. \nWhile the results of MUSTANG calculations are stored in a database and provided to users via web \nservices, ISPAQ is intended to carry out the process of calculating these metrics locally on the \nuser's workstation. This has the benefit of allowing users to generate just-in-time metrics on data \nof their choosing, whether stored an FDSN data center or on the user's own data store.\n\nEarthScope has over 40 MUSTANG metrics algorithms, most written in R, that are now available in the \n[CRAN](https://cran.r-project.org/) (Comprehensive R Archive Network) repository under the name \n[IRISMustangMetrics](https://cran.r-project.org/web/packages/IRISMustangMetrics/index.html). \nISPAQ comes with the latest version of these packages available in CRAN and ISPAQ has an update capability to \nallow users to seamlessly upgrade these R packages as new releases become available.\n\nISPAQ contains business logic similar to MUSTANG, such that the computed metrics produced are identical \n(or very similar) to the results you will see in MUSTANG. The end result is a lightweight and portable \nversion of MUSTANG that users are free to leverage on their own hardware.\n\n\n#### Questions or comments can be directed to the EarthScope Quality Assurance Group at <a href=\"mailto:data-help@earthscope.org\">data-help@earthscope.org</a>.\n\n\n# Installation\n\nISPAQ is distributed through _GitHub_, via EarthScope's public repository (_EarthScope_). You will use a ```git``` \nclient command to get a copy of the latest stable release. In addition, you will use the ```miniconda``` \npython package manager to create a customized Python environment designed to run ISPAQ properly. This will \ninclude a localized installation of ObsPy and R.\n\nIf running macOS, Xcode command line tools should be installed. Check for existence and install if \nmissing:\n```\nxcode-select --install\n```\n\nFollow the steps below to begin running ISPAQ.\n\n## Download the Source Code\n\nYou must first have ```git``` installed your system. This is a commonly used source code management system\nand serves well as a mode of software distribution as it is easy to capture updates. See the \n[Git Home Page](https://git-scm.com/) to begin installation of git before proceeding further.\n\nAfter you have git installed, you will download the ISPAQ distribution into a directory of your choosing \nfrom GitHub by opening a text terminal and typing:\n\n```\ngit clone https://github.com/EarthScope/ispaq.git\n```\n\nThis will produce a copy of this code distribution in the directory you have chosen. When new ispaq versions \nbecome available, you can update ISPAQ by typing:\n\n```\ncd ispaq\ngit pull origin master\n```\n\n## Install the Anaconda Environment\n\n[Anaconda](https://www.anaconda.com) is quickly becoming the *defacto* package manager for \nscientific applications written python or R. [Miniconda](https://conda.pydata.org/miniconda.html) is a trimmed \ndown version of Anaconda that contains the bare necessities without loading a large list of data science packages \nup front. With miniconda, you can set up a custom python environment with just the packages you need to run ISPAQ.\n\nProceed to the [Miniconda](https://conda.pydata.org/miniconda.html) web site to find the installer for your\noperating system before proceeding with the instructions below. If you can run ```conda``` from the command \nline, then you know you have it successfully installed.\n\nBy setting up a [conda virtual environment](https://docs.conda.io/projects/conda/en/latest/user-guide/concepts/environments.html), \nwe assure that our ISPAQ installation is entirely separate from any other installed software.\n\n\n### Creating the ispaq environment\n\nYou will go into the ispaq directory that you created with git, update miniconda, then create an \nenvironment specially for ispaq. You have to ```activate``` the ISPAQ environment whenever you \nperform installs, updates, or run ISPAQ.\n\n> _Note:_ If you are upgrading from ISPAQ 2.0 to ISPAQ 3.0+, you should create a new ispaq environment.\n\nInstructions for Linux or macOS (Intel chip)\n```\ncd ispaq   #top level directory\nconda update conda\nconda env remove --name ispaq  #if you are upgrading from an existing ISPAQ 2.0 installation to ISPAQ 3.0\nconda create --name ispaq -c conda-forge python=3.12\nconda activate ispaq\nconda install -c conda-forge --file ispaq-conda-install.txt\n```\n\nInstructions for macOS (Apple M1 or M2 chip):\n```\ncd ispaq   \nconda update conda\nconda env remove --name ispaq \nCONDA_SUBDIR=osx-64 conda create --name ispaq -c conda-forge python=3.12\nconda activate ispaq\nCONDA_SUBDIR=osx-64 conda install -c conda-forge --file ispaq-conda-install.txt\n```\n\nSee what is installed in our (ispaq) environment with:\n\n```\nconda list\n```\n\nNow install the EarthScope R packages for ISPAQ using the -I option:\n```\npython run_ispaq.py -I    #downloads latest packages from CRAN (https://cran.r-project.org)\n```\n\nOr alternatively, install the EarthScope R packages from local files: \n```\nR CMD INSTALL seismicRoll_1.1.5.tar.gz\nR CMD INSTALL IRISSeismic_1.7.0.tar.gz\nR CMD INSTALL IRISMustangMetrics_2.4.8.tar.gz\n```\n\nYou should run `./run_ispaq.py -U` after you update ISPAQ minor versions to verify that you have both the \nrequired minimum versions of anaconda packages and the most recent EarthScope R packages.\n\n> _Note:_ If you are using macOS and see the error: \"'math.h' file not found\" when compiling seismicRoll, then it is \nlikely that your command line tools are missing. Try running `xcode-select --install`.\n\n# Using ISPAQ\n\nEvery time you use ISPAQ you must ensure that you are running in the proper Anaconda\nenvironment. If you followed the instructions above you only need to type:\n\n```\ncd ispaq\nconda activate ispaq\n```\n\nafter which your prompt should begin with ```(ispaq) ```. You run ispaq using the ```run_ispaq.py``` \npython script. The example below shows how to get ISPAQ to show the help display.  A leading ```./``` \nis used to indicate that the script is in the current directory.\n\nA list of command-line options is available with the ```--help``` flag:\n\n```\n(ispaq) bash-3.2$ python run_ispaq.py -h\nusage: run_ispaq.py [-h] [-P PREFERENCES_FILE] [-M METRICS] [-S STATIONS]\n                    [--starttime STARTTIME] [--endtime ENDTIME]\n                    [--dataselect_url DATASELECT_URL] [--station_url STATION_URL]\n                    [--event_url EVENT_URL] [--resp_dir RESP_DIR]\n                    [--output OUTPUT] [--db_name DB_NAME] [--csv_dir CSV_DIR]\n                    [--psd_dir PSD_DIR] [--pdf_dir PDF_DIR] [--pdf_type PDF_TYPE]\n                    [--pdf_interval PDF_INTERVAL] [--plot_include PLOT_INCLUDE]\n                    [--sncl_format SNCL_FORMAT] [--sds_files] [--sigfigs SIGFIGS]\n                    [--log-level {DEBUG,INFO,WARNING,ERROR,CRITICAL}] [-A] [-V]\n                    [-I] [-U] [-L]\n\nISPAQ version 3.4.0\n\nsingle arguments:\n  -h, --help                       show this help message and exit\n  -A, --append                     append to TRANSCRIPT file rather than overwriting\n  -V, --version                    show program's version number and exit\n  -I, --install-r                  install CRAN EarthScope Mustang packages, and exit\n  -U, --update-r                   check for and install newer CRAN EarthScope Mustang packages \n                                   and/or update required conda packages, and exit\n  -L, --list-metrics               list names of available metrics and exit\n\narguments for running metrics:\n  -P PREFERENCES_FILE, --preferences-file PREFERENCES_FILE\n                                   path to preference file, default=./preference_files/default.txt\n  -M METRICS, --metrics METRICS    single Metrics alias as defined in preference file, or one or \n                                   more metric names in a comma-separated list, required\n  -S STATIONS, --stations STATIONS\n                                   single Station_SNCLs alias as defined in preference file, or \n                                   one or more SNCL[Q] in a comma-separated list, required.\n                                   notes: SNCL[Q] refers to Station.Network.Channel.Location.(optional)Quality\n                                          If using wildcarding, enclose in quotation marks\n  --starttime STARTTIME            starttime in ObsPy UTCDateTime format, required for webservice requests \n                                   and defaults to earliest data file for local data \n                                   examples: YYYY-MM-DD, YYYYMMDD, YYYY-DDD, YYYYDDD[THH:MM:SS]\n  --endtime ENDTIME                endtime in ObsPy UTCDateTime format, default=starttime + 1 day; \n                                   if starttime is also not specified then it defaults to the latest data \n                                   file for local data \n                                   examples: YYYY-MM-DD, YYYYMMDD, YYYY-DDD, YYYYDDD[THH:MM:SS]\n\noptional arguments for overriding preference file entries:\n  --dataselect_url DATASELECT_URL  FDSN webservice or path to directory with miniSEED files\n  --station_url STATION_URL        FDSN webservice or path to stationXML file\n  --event_url EVENT_URL            FDSN webservice or path to QuakeML file\n  --resp_dir RESP_DIR              path to directory with RESP files\n  --output OUTPUT                  write metrics to csv file (csv) or sqlite database file (db). Options: csv, db\n  --db_name DB_NAME                name of sqlite database file, if output=csv\n  --csv_dir CSV_DIR                directory to write generated metrics .csv files, if output=csv\n  --psd_dir PSD_DIR                directory to write/read existing PSD .csv files, if output=csv\n  --pdf_dir PDF_DIR                directory to write generated PDF files\n  --pdf_type PDF_TYPE              output format of generated PDFs - text and/or plot\n  --pdf_interval PDF_INTERVAL      time span for PDFs - daily and/or aggregated over the entire span\n  --plot_include PLOT_INCLUDE      PDF plot graphics options - legend, colorbar, and/or fixed_yaxis_limits, \n                                   or none\n  --sncl_format SNCL_FORMAT        format of SNCL aliases and miniSEED file names \n                                   examples:\"N.S.L.C\",\"S.N.L.C\"\n                                   where N=network code, S=station code, L=location code, C=channel code\n  --sds_files                      if set, ISPAQ will look for local data files with Seiscomp SDS naming format\n                                   NET.STA.LOC.CHAN.TYPE.YEAR.DAY where TYPE=D\n  --sigfigs SIGFIGS                number of significant figures used for output columns named \"value\"\n\nother arguments:\n  --log-level {DEBUG,INFO,WARNING,ERROR,CRITICAL}\n                                   log level printed to console, default=\"INFO\"\n\nIf no preference file is specified and the default file ./preference_files/default.txt cannot be found:\n--csv_dir, pdf_dir, and psd_dir default to \".\"\n--sncl_format defaults to \"N.S.C.L\"\n--sigfigs defaults to \"6\"\n--pdf_type defaults to \"plot,text\"\n--pdf_interval defaults to \"aggregated\"\n--plot_include defaults to \"colorbar,legend\"\n```\n\nFor those that prefer to run ISPAQ as a package, you can use the following invocation (using help example):\n```\n(ispaq) $ python -m ispaq.ispaq --help\n````\n\nWhen calculating metrics, valid arguments for `-M` and `-S` are required and must be provided.\nIf `-P` is not provided, ISPAQ uses the default preference file located at `ispaq/preference_files/default.txt`.\nHowever, all entries in the preference file can be overridden by command-line options. \nIf `--log-level` is not specified, the default log-level is `INFO`.\n\nWhen `--starttime` is invoked without `--endtime`, metrics are run for a single day. Metrics that are defined \nas day-long metrics (24 hour windows, see metrics documentation at \n[MUSTANG](https://services.iris.edu/mustang/measurements/1)) will be calculated for the time period \n`00:00:00-23:59:59.9999`. An endtime of `YYYY-DD-MM` is interpreted as  `YYYY-DD-MM 00:00:00` so that \ne.g., `--starttime=2016-01-01 --endtime=2016-01-02` will also calculate one day of metrics. When an end time \ngreater than one day is requested, metrics will be calculated by cycling through multiple single days to produce \na measurement for each day. Additionally, and only if using local data files, you can run metrics without specifying \na start time. In this case, ISPAQ will use a start time corresponding to the earliest file found that matches the \nrequested Station_SNCLs. If end time is also not specified, ISPAQ will use an end time corresponding to the latest file \nfound that matches the requested Station_SNCLs.\n\n### Preference files\n\nThe ISPAQ system is designed to be configurable through the use of *preference files*.\nThese are usually located in the ```preference_files/``` directory. Not surprisingly, the default\npreference file is ```preference_files/default.txt```. This file is self describing\nwith the following comments in the header:\n\n```\n# Preferences fall into five categories:\n#  * Metrics -- aliases for user defined combinations of metrics (Use with -M)\n#  * Station_SNCLs -- aliases for user defined combinations of SNCL patterns (Use with -S)\n#                     SNCL patterns are station names formatted as Network.Station.Location.Channel\n#                     wildcards * and ? are allowed. SNCL pattern format can be modified \n#                     using the Preferences sncl_format.          \n#  * Data_Access -- FDSN web services or local files\n#  * Preferences -- additional user preferences\n#  * PDF_Preferences -- preferences specific to PDF calculation\n#\n# This file is in a very simple format.  After each category heading, all lines containing a colon \n# will be interpreted as key:value and made available to ISPAQ.\n#\n```\n\n**Metric** aliases can be any of one of the predefined options or any user-created `alias_name: metric` combination, \nwhere *metric* can be a single metric name or a comma separated list of valid metric names. Aliases cannot be \ncombinations of other aliases. \nExample: `myMetrics: num_gaps, sample_mean, cross_talk`.\n\n**Station_SNCL** aliases are user created `alias_name: Network.Station.Location.Channel[.Quality]` combinations, where [ ] denotes an optional element. \nStation_SNCLs can be comma separated lists. `*` or `?` wildcards can be used in any of the network, station, location, channel, or quality elements. \n Example: `\"myStations: IU.ANMO.10.BHZ.M, IU.*.00.BH?.M, IU.ANMO.*.?HZ, II.PFO.??.*`. By default, aliases are formatted\nas `Network.Station.Location.Channel[.Quality]`. This format pattern can be modified using the `sncl_format`entry discussed below.\n\n> _Note:_ the use of the quality code is optional and is not fully utilized in this version of ISPAQ. Specifying a quality code will not guarantee that ISPAQ retrieves data with only that quality code; instead data will be of whatever quality the specified web services (or local data) provides. This is a known issue and will be addressed in a future release. \n\n> _Note:_ the PDF metric _will_ use the quality code specified, if there is one, as it retrieves PSDs. If no quality code is specified in the station SNCL, then it will look for any and all quality codes that might exist for that SNCL.\n\n> _Note:_ When directly specifying a SNCL pattern on the command line, SNCLs containing wildcards should be enclosed by quotes to avoid a possible error of unrecognized arguments.\n\n**Data_Access** has four entries describing where to find data, metadata, events, and optionally response files.\n\n* `dataselect_url:` should indicate a *miniSEED* data resource as one of the *FDSN web service aliases* used by ObsPy \n(e.g. `IRIS`), EARTHSCOPE (an alias of 'IRIS'), the EarthScope PH5 web service alias 'IRISPH5', \nan explicit URL pointing to an FDSN web service domain (e.g. `https://service.earthscope.org` ), or a file \npath to a directory containing miniSEED files (_See: \"Using Local Data Files\", below_).\n\n> _NOTE:_ When data is missing and it is marked as percent_availability=0, the quality code to assign to the target must be inferred. To do this, the current logic is to assign quality \"M\" for EarthScope (fdsnws) derived data, and quality \"D\" for all other data (IRISPH5, local data, or any other webservice). We are aware that this is too simplistic to truly capture the range of possible quality codes, and have it on our radar to improve with a later release. \n\n* `station_url:` should indicate a metadata location as an FDSN web service alias, EARTHSCOPE (an alias of 'IRIS'),\nthe EarthScope PH5 web service alias 'IRISPH5',\nan explicit URL, or a path to a file containing metadata in [StationXML](https://www.fdsn.org/xml/station/) format \n([schema](https://www.fdsn.org/xml/station/fdsn-station-1.0.xsd)). If both `dataselect_url` and `station_url` point to web services, they should point to the same location (e.g. `https://service.earthscope.org`). For local metadata, StationXML is read at the channel level and any \nresponse information is ignored. Local instrument response (if used) is expected to be in RESP file format and specified \nin the `resp_dir` entry (see below). If neither webservices or StationXML is available for metadata, the `station_url` entry \nshould be left unspecified (blank). In this case, metrics that do not require metadata will still be calculated. Metrics that \ndo require metadata information (cross_talk, polarity_check, orientation_check, transfer_function) will not be calculated \nand will return a log message stating \"No available waveforms\". \n\n    If you are starting from a *dataless SEED* metadata file, you can create StationXML from this using the \n[FDSN StationXML-SEED Converter](https://github.com/iris-edu/stationxml-seed-converter).\n\n* `event_url:` should indicate an *event catalog resource* as an FDSN web service alias (e.g. `USGS`), an \nexplicit URL (e.g. `https://earthquake.usgs.gov`), or a path to a file containing event information in \n[QuakeML](https://quake.ethz.ch/quakeml) format \n([schema](https://quake.ethz.ch/quakeml/docs/xml?action=AttachFile&do=get&target=QuakeML-BED-1.2.xsd)). \nOnly web service providers that can output text format can be used at this time. This entry will \nonly be used by metrics that require event information in order to be calculated (*cross_talk, polarity_check, \norientation_check*). The EarthScope event service is deprecated and event_urls directed at the EarthScope web services will \nredirect to the USGS event service at https://earthquake.usgs.gov.\n\n* `resp_dir:` should be unspecified or absent if local response files are not used. The default behavior\n is to retrieve response information from the EarthScope web service [Evalresp](https://service.earthscope.org/irisws/evalresp/1/). If the Earthscope PH5 service (IRISPH5) is the source for the `station_url`, then it will default to the [ph5 Evalresp] (https://service.earthscope.org/ph5ws/evalresp/1/) instead. \nTo use local instrument responses instead of [Evalresp](https://service.earthscope.org/irisws/evalresp/1/),\n this parameter should indicate a path to a directory containing response files \nin [RESP](https://ds.iris.edu/ds/nodes/dmc/data/formats/resp/) format. Local response files are expected to be \nnamed `RESP.network.station.location.channel` or `RESP.station.network.location.channel`. Filenames with extension `.txt` \nare also acceptable. E.g., `RESP.IU.CASY.00.BH1, RESP.CASY.IU.00.BH1, RESP.IU.CASY.00.BH1.txt.` \n\n    Response information is only needed when generating PSD derived metrics, PDF plots,\nor the transfer_function metric.    \n\n    If you are starting from a dataless SEED, you can create RESP files using [rdseed](https://ds.iris.edu/ds/nodes/dmc/manuals/rdseed/).\n\n**Preferences** has eight entries describing ispaq output.\n\n* `output:` either 'db' (write to SQLite database) or 'csv' (write to CSV files)\n* `db_name:` if writing to a database (output=db), the name of the database\n* `csv_dir:` of writing to CSV (output=csv), directory path for output of generated metric text files (CSV). \nIf the directory does not exist, then it attempts to create that directory.\n\n* `psd_dir:` should be followed by a directory path for writing and reading PSD csv files.\nIf the directory does not exist, then it defaults to the current working directory. PSD csv files generated\nby the 'psd_corrected' metric will be written to a directory structure within 'psd_dir' based on network code and\nstation code ('psd_dir'/NET/STA)\n\n* `pdf_dir:` should be followed by a directory path for output of PDF csv and png files. These files will be\nwritten to a directory structure within 'pdf_dir' based on network code and station code ('pdf_dir'/NET/STA).\n\n* `sigfigs:` should indicate the number of significant figures used for output columns named \"value\". Default is 6.\n\n* `sncl_format:` should be the format of sncl aliases and miniSEED file names, must be some combination of\n period separated `N`=network, `S`=station, `L`=location, `C`=channel (e.g., `N.S.L.C, S.N.L.C`).\nIf no `sncl_format` exists, it defaults to `N.S.L.C`.\n\n* `sds_files:` if set to 'True', ISPAQ will look for files using the SeisComp SDS file naming convention with type='D',\n(e.g. NET.STA.LOC.CHAN.D.YEAR.DAY) when using local data files\n\n**PDF_Preferences** has three entries describing PDF output.\n\n* `pdf_type:` should be followed by either \"text\",\"plot\", or \"text,plot\".  \n\"text\" will output PDF information in a csv format file with frequency, power, and hits columns, or to a database.  \n\"plot\" will output a PDF plot in a png format file.  \n\"text,plot\" will output both.  \n\n* `pdf_interval:` should be followed by either \"daily\",\"aggregate\", or \"daily,aggregate\".    \n\"daily\" will calculate separate PDFs for each day between the starttime and endtime.  \n\"aggregate\" will calculate one PDF spanning the starttime to endtime span.  \n\"daily,aggregate\" will calculate both.  \n\n* `plot_include:` should be followed by any of \"legend\",\"colorbar\",\"fixed_yaxis_limits\".  \n\"legend\" will include the legend for the minimum/maximum/mode PDF statistics curves.  \n\"colorbar\" will include the histogram legend for the PDF.  \n\"fixed_axis_limits\" will plot the PDF with y-axis limits of -25 to -225 dB (if not specified, the y-axis limits are determined by the data).  \n\"legend,colorbar,fixed_axis_limits\" will create a PDF plot with all three features.  \n\nAny of these preference file entries can be overridden by command-line arguments:\n`-M \"Metric name/alias\"`, `-S \"Station_SNCL\"`, `--dataselect_url`, `--station_url`, `--event_url`, `--resp_dir`, \n`--output`, `--db_name`, `--csv_output_dir`, `--plot_output_dir`, `--sigfigs`, `--sncl_format`,`--pdf_type`, \n`--pdf_interval`, `--plot_include`, `--sncl_format`, `--sigfigs`\n\nMore information about using local files can be found below in the section \"Using Local Data Files\".\n\n### Output files\n\nISPAQ will always create a log file named ```ISPAQ_TRANSCRIPT.log``` to record actions taken\nand messages generated during processing.\n\nIn addition, the metric calculations will write to either .csv files or to a SQLite database, depending on the \n`output` option selected.  \n\n#### CSV files\nResults of most metrics calculations will be written to .csv files using the following naming scheme:\n\n* `MetricAlias`\\_`Station_SNCLAlias`\\_`startdate`\\__`businessLogic`.csv\n\nwhen a single day is specified on the command line or\n\n* `MetricAlias`\\_`Station_SNCLAlias`\\_`startdate`\\_`enddate`\\_`businessLogic`.csv\n\nwhen multiple days are specified from the command line. End date in this context is inclusive of that day.\n\nIf specifying metrics and station_SNCLs from the command line instead of using preference file aliases,\nthe metric name and station_SNCL[Q] will be used instead of the MetricAlias and Station_SNCLAlias in the output\nfile name. In addition, any instances of command-line wildcards \"*\" or \"?\" will be replaced with the letter\n\"x\" in the output file name.\n\n_businessLogic_ corresponds to which script is invoked:\n\n| businessLogic | ISPAQ script | metrics |\n| ----------|--------------|---------|\n| simpleMetrics | simple_metrics.py | most metrics |\n| SNRMetrics | SNR_metrics.py | sample_snr   |\n| PSDMetrics | PSD_metrics.py, PDF_aggregator.py | pct_above_nhnm, pct_below_nlnm, dead_channel_{lin,gsn}, psd_corrected, pdf |\n| crossTalkMetrics | crossTalk_metrics.py | cross_talk |\n| pressureCorrelationMetrics | pressureCorrelation_metrics.py | pressure_effects | \n| crossCorrelationMetrics | crossCorrelation_metrics.py | polarity_check | \n| orientationCheckMetrics | orientationCheck_metrics.py | orientation_check | \n| transferMetrics | transferFunction_metrics.py | transfer_function |\n\nThe metric alias psdPdf in the default preference file (or any user defined set with metric 'psd_corrected') will \ngenerate corrected PSDs in files named:\n\n* `S.N.C.L.Q`\\_`startdate`\\_PSDcorrected.csv\n\nThe metric alias psdPdf in the default preference file (or any user defined set with metric 'pdf') will generate \nPDFs in files named:\n\n* `S.N.C.L.Q`\\_`startdate`\\_PDF.csv  (for daily PDF text)\n* `S.N.C.L.Q`\\_`startdate`\\_`enddate`\\_PDF.csv (for aggregate PDF text)\n* `S.N.C.L.Q`\\_`startdate`\\_PDF.png  (for daily PDF plot)\n* `S.N.C.L.Q`\\_`startdate`\\_`enddate`\\_PDF.png  (for aggregate PDF plot)\n\n> _Note:_ The metric 'pdf' requires that corrected PSDs exist.  If using `output` 'csv' then `S.N.C.L.Q`\\_`startdate`\\_PSDcorrected.csv files must exist in the `psd_dir` specified directory.  \nIf you run the metric 'pdf' alone and see the warning 'No PSD files found', then try running metric 'psd_corrected'\nfirst to generate the PSD files. You will also see the warning 'No PSD files found' if there is no data available for that day.\nThese two  metrics can be run simulataneously, as it will calculate the PSDs before calculating the PDFs. \n\n\n\n#### SQLite database\nUsing the 'db' `output` option will write to a SQLite database with the filename supplied in the `db_name` field. All metrics values, \nexcept for any .png PSD or PDFs that may be generated, will be inserted into the database. Tables within the datbase correspond to the \nmetric name. For example:  \n\n```\nsqlite> .tables\namplifier_saturation     max_range                sample_mean\ncalibration_signal       max_stalta               sample_median\nclock_locked             missing_padded_data      sample_min\ncross_talk               num_gaps                 sample_rate_channel\ndead_channel_gsn         num_overlaps             sample_rate_resp\ndead_channel_lin         num_spikes               sample_rms\ndigital_filter_charging  orientation_check        sample_snr\ndigitizer_clipping       pct_above_nhnm           sample_unique\nevent_begin              pct_below_nlnm           spikes\nevent_end                pdf                      suspect_time_tag\nevent_in_progress        percent_availability     telemetry_sync_error\nglitches                 polarity_check           timing_correction\nmax_gap                  psd_corrected            timing_quality\nmax_overlap              sample_max\n```\n  \n  \nThe majority of tables (metrics) will have the same set of columns. These include:  \n\n`target` - the network.station.location.channel.quality code that the measurement corresponds to  \n`value` - value of measurement  \n`start` - start time of the measurement  \n`end` - end time of the measurement  \n`lddate` - the load date, when the measurement was inserted (or updated) in the table  \n\nIn addition to those fields, these metrics have other columns as well:  \n\n* polarity_check: `snclq2`  \n* transfer_function: `gain_ratio`, `phase_diff`, `ms_coherence`  \n* orientation_check: `azimuth_R`, `backAzimuth`, `azimuth_Y_obs`, `azimuth_X_obs`, `azimuth_Y_meta`, `azimuth_X_meta`, \n`max_Czr`, `max_C_zr`, `magnitude`  \n* psd_corrected: `frequency`, `power`  \n* pdf: `frequency`, `power`, `hits`  \n\n> _Note:_  transfer_function, orientation_check, psd_corrected, and pdf metrics all lack the `value` column.  \n\n\nThe metric 'pdf' requires that corrected PSDs exist. If using `output` 'db' then the PSDs must exist in the database specified by `db_name`.  \nIf you run the metric 'pdf' and see the warning 'Unable to access PSD values', then try running metric 'psd_corrected' \nfirst to generate the PSD values. These two  metrics can be run simulataneously, as it will calculate the PSDs before calculating the PDFs. You will also see the warning 'Unable to access PSD values' if there is no data available for that day, \nor 'Unable to access table psd_corrected' if no the table does not exist, which may indicate that no PSDs have been calculated and added to the database \nyet.  \n\n\nFor those using [QuARG](https://github.com/EarthScope/quarg), a utility produced by EarthScope for generating quality assurance reports, it is possible \nto have QuARG read metrics from your local ISPAQ SQLite database rather than from the MUSTANG web services.  Simply point the `metric source` in the QuARG  preference file to the database file produced by ISPAQ and it will use your local metric values.  \n\n\nExamples of how to access and use the metrics are included as jupyter notebooks in the EXAMPLES/ directory. For more information on how to navigate a SQLite database, see [https://sqlite.org/cli.html](https://sqlite.org/cli.html).  Given your `ispaq` environment is activated, you should be able to run the jupyter notebooks if you have installed the conda environment using the provided ispaq-conda-install.txt file. But if you are having trouble you can go through installation steps here: [https://jupyter.org/install](https://jupyter.org/install).\n\n\n\n### Command line invocation\n\nExample invocations are found in the ```EXAMPLES``` section and at the end of this `README`. \n\nYou can modify the information printed to the console by modifying the ```--log-level```.\nTo see detailed progress information use ```--log-level DEBUG```. To hide everything other\nthan an outright crash use ```--log-level CRITICAL```. If `--log-level` is not invoked, the default is \nto print information at the `INFO` level. The other available levels are `WARNING` and `ERROR`.\n\nThe following example demonstrates what you should see. _Note:_ Please ignore the warning message from *matplotlib*. \nIt will only occur on first use. \n\n```\n(ispaq) $ ./run_ispaq.py -M basicStats -S basicStats --starttime 2010-04-20 --log-level INFO\n2017-05-26 13:58:12 - INFO - Running ISPAQ version 1.0.0 on Fri May 26 13:58:12 2017\n~/miniconda2/envs/ispaq/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is \nbuilding the font cache using fc-list. This may take a moment. \nwarnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n2017-05-26 13:58:22 - INFO - Calculating simple metrics for 3 SNCLs on 2010-04-20\n2017-05-26 13:58:22 - INFO - 000 Calculating simple metrics for IU.ANMO.00.BH1\n2017-05-26 13:58:24 - INFO - 001 Calculating simple metrics for IU.ANMO.00.BH2\n2017-05-26 13:58:25 - INFO - 002 Calculating simple metrics for IU.ANMO.00.BHZ\n2017-05-26 13:58:26 - INFO - Writing simple metrics to basicStats_basicStats_2010-04-20__simpleMetrics.csv\n2017-05-26 13:58:26 - INFO - ALL FINISHED!\n```\n\nAdditional information about running ISPAQ on the command line can be found by invoking `run_ispaq.py --help`.\n\n### Using Local Data Files\n\nLocal data files should be in *miniSEED* format and organized in *network-station-channel-day* files. By default, \nISPAQ recognizes the following file naming convention:\n\n```\nNetwork.Station.Location.Channel.Year.JulianDay.Quality\n```\nwhere `Quality` is optional (e.g., `TA.P19K..BHZ.2016.214.M` or `TA.P19K..BHZ.2016.214`).   \n\nThis naming convention can be modified by using the `sncl_format` entry in the preferences file or \nthe `--sncl_format` option on the command line. `sncl_format` allows you to specify a different order for \n`Network.Station.Location.Channel`, although all these elements must be present in the file name. \nFor example, sncl_format `S.N.L.C` will change the file naming convention that ISPAQ uses to:\n\n```\nStation.Network.Location.Channel.Year.JulianDay.Quality\n```\nwhere `Quality` is again optional (e.g. `P19K.TA..BHZ.2016.214.M` or `P19K.TA..BHZ.2016.214`).\n\nIf the `sds_files` parameter is set either as a command line flag (`--sds_files`) or set to 'True' in the preference\nfile, ISPAQ will look for files using the Seiscomp SDS file name format with type='D', e.g.,\n\n```\nNetwork.Station.Location.Channel.D.Year.JulianDay\n```\nThe sncl_format and optional quality code are also honored when using `sds_format`.\n\nISPAQ will search for miniSEED files in the directory specified by `dataselect_url` in the preferences file or \n`--dataselect_url` on the command line. Furthermore, it will recursively follow that directory structure and\nlook for miniSEED files in directories contained within the `dataselect_url` directory. If more than one file name \nis found that matches the same requested network, station, location, channel, year, and julian day, then the metrics \nwill be run on the first file that is found. To request all data files, use preference file *Station_SNCL* alias: \n`*.*.*.*`, or `-S \"*.*.*.*\"` from the command line\". Wildcarding every element is strongly discouraged when using \nFDSN webservices instead of local files.\n\n> _Note:_ All data is expected to be in the day file that matches its timestamp; if records do not break on the \nUTC day boundary, data that is not in the correct day file will not be used in the metrics calculation. This can \nlead to cases where, for example, a gap is calculated at the start of a day when the data for that time period \nis in the previous day file.\n\nIf your miniSEED files are not already split on UTC day boundaries, one tool that can be used for this task is the \n*dataselect* command-line tool available at [https://github.com/EarthScope/dataselect](https://github.com/EarthScope/dataselect). \nFollow the [releases](https://github.com/EarthScope/dataselect/releases) link in the README to download the latest \nversion of the source code. The following example reads the input miniSEED files, splits the records on day\nboundaries, and writes to files named `network.station.location.channel.year.julianday.quality`.\n\nExample: `dataselect -Sd -A %n.%s.%l.%c.%Y.%j.%q inputfiles`\n\n### Updating CRAN packages\n\nThe command-line argument `-U`, `--update-r` can be used to check CRAN for newer IRISSeismic, seismicRoll, and\nIRISMustangMetrics R packages.\n\n```\n(ispaq) bash-3.2$ ./run_ispaq.py -U\n2021-11-16 12:06:09 - INFO - Running ISPAQ version 3.0.0 on Tue Nov 16 12:06:09 2021\n2021-11-16 12:06:11 - INFO - Checking for recommended conda packages...\n2021-11-16 12:06:11 - INFO - Required conda packages found\n2021-11-16 12:06:11 - INFO - Checking for EarthScope R package updates...\n\n              package installed   CRAN  upgrade\n0         seismicRoll     1.1.4  1.1.4    False\n1         IRISSeismic     1.6.3  1.6.3    False\n2  IRISMustangMetrics     2.4.4  2.4.4    False\n\n2021-11-16 12:06:15 - INFO - No CRAN packages need updating.\n\nAlternatively, the command-line argument `-I`, `--install-r` will install the CRAN packages regardless of what\nversion is already installed\n\n```\n\nIf a newer CRAN package does exist, the `-U` option will then automatically download the package from CRAN and\ninstall it. ISPAQ code can be updated using `git pull origin master`. Sometimes it is necessary to update the ISPAQ \npython code in conjunction with the CRAN code.\n\n\n### List of Metrics\n\nThe command-line argument `-L` will list the names of available metrics.\n\n#### Brief Metrics Descriptions and Links to Documentation\n \n* **amplifier_saturation**:\nThe number of times that the 'Amplifier saturation detected' bit in the 'dq_flags' byte is set within a \nminiSEED file. This data quality flag is set by some dataloggers in the fixed section of the miniSEED header. \nThe flag was intended to indicate that the preamp is being overdriven, but the exact meaning is \ndatalogger-specific. [Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/amplifier_saturation/)\n\n* **calibration_signal**:\nThe number of times that the 'Calibration signals present' bit in the 'act_flags' byte is set within a miniSEED \nfile. A value of 1 indicates that a calibration signal was being sent to that channel.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/calibration_signal/)\n\n* **clock_locked**:\nThe number of times that the 'Clock locked' bit in the 'io_flags' byte is set within a miniSEED file. This \nclock flag is set to 1 by some dataloggers in the fixed section of the miniSEED header to indicate that its \nGPS has locked with enough satellites to obtain a time/position fix.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/clock_locked/)\n\n* **cross_talk**:\nThe correlation coefficient of channel pairs from the same sensor. Data windows are defined by seismic events. \nCorrelation coefficients near 1 may indicate cross-talk between those channels.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/cross_talk/)\n\n* **dead_channel_gsn**:\nA boolean measurement providing a TRUE or FALSE indication that the median PSD values of channel exhibit an\naverage 5dB deviation below the NLNM in the 4 to 8s period band as measured using a McNamara PDF noise matrix. \nThe TRUE condition is indicated with a numeric representation of '1' and the FALSE condition represented as a '0'.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/dead_channel_gsn/)\n    + channels = [BCDHLM][HX].  \n\n* **dead_channel_lin**:\nDead channel metric - linear fit. This metric is calculated from the mean of all the PSDs generated (typically 47 \nfor a 24 hour period). Values of the PSD mean curve over the band linLoPeriod:linHiPeriod are fit to a linear curve \nby a least squares linear regression of PSD mean ~ log(period). The dead_channel_lin metric is the standard deviation \nof the fit residuals of this regression. Lower numbers indicate a better fit and a higher likelihood that the mean \nPSD is linear - an indication that the sensor is not returning expected seismic energy.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/dead_channel_lin/)\n    + channels = [BCDHM][HX].\n\n* **digital_filter_charging**:\nThe number of times that the 'A digital filter may be charging' bit in the 'dq_flags' byte is set within a miniSEED \nfile. Data samples acquired while a datalogger is loading filter parameters - such as after a reboot - may contain \na transient.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/digital_filter_charging/)\n\n* **digitizer_clipping**:\nThe number of times that the 'Digitizer clipping detected' bit in the 'dq_flags' byte is set within a miniSEED file. \nThis flag indicates that the input voltage has exceeded the maximum range of the ADC.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/digitizer_clipping/)\n\n* **event_begin**:\nThe number of times that the 'Beginning of an event, station trigger' bit in the 'act_flags' byte is set within a \nminiSEED file. This metric can be used to quickly identify data days that may have events. It may also indicate \nwhen trigger parameters need adjusting at a station.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/event_begin/)\n\n* **event_end**:\nThe number of times that the 'End of an event, station detrigger' bit in the 'act_flags' byte is set within a \nminiSEED file. This metric can be used to quickly identify data days that may have events. It may also indicate \nwhen trigger parameters need adjusting at a station.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/event_end/)\n\n* **event_in_progress**:\nThe number of times that the 'Event in progress' bit in the 'act_flags' byte is set within a miniSEED file. This \nmetric can be used to quickly identify data days that may have events. It may also indicate when trigger \nparameters need adjusting at a station.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/event_in_progress/)\n\n* **glitches**:\nThe number of times that the 'Glitches detected' bit in the 'dq_flags' byte is set within a miniSEED file. This \nmetric can be used to identify data with large filled values that data users may need to handle in a way that they \ndon't affect their research outcomes.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/glitches/)\n\n* **max_gap**:\nIndicates the size of the largest gap encountered within a 24-hour window.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/max_gap/)\n\n* **max_overlap**:\nIndicates the size of the largest overlap in seconds encountered within a 24-hour window.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/max_overlap/)\n\n* **max_range**:\nThis metric calculates the difference between the largest and smallest sample value in a 5 minute rolling window and returns the largest value encountered within a 24-hour timespan.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/max_range/)\n\n* **max_stalta**:\nThe STALTAMetric function calculates the maximum of STA/LTA of the incoming seismic signal over a 24 hour period. \nIn order to reduce computation time of the rolling averages, the averaging window is advanced in 1/2 second \nincrements. [Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/max_stalta/)\n    + channels = [BHCDES][HPLX].\n\n* **missing_padded_data**:\nThe number of times that the 'Missing/padded data present' bit in the 'dq_flags' byte is set within a miniSEED file. \nThis metric can be used to identify data with padded values that data users may need to handle in a way that they \ndon't affect their research outcomes.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/missing_padded_data/)\n\n* **num_gaps**:\nThis metric reports the number of gaps encountered within a 24-hour window.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/num_gaps/)\n\n* **num_overlaps**:\nThis metric reports the number of overlaps encountered in a 24-hour window.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/num_overlaps/)\n\n* **num_spikes**:\nThis metric uses a rolling Hampel filter, a median absolute deviation (MAD) test, to find outliers in a timeseries. \nThe number of discrete spikes is determined after adjacent outliers have been combined into individual spikes.\nNOTE: not to be confused with the spikes metric, which is an SOH flag only.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/num_spikes/)\n    + channels = [BH][HX].  \n\n* **orientation_check**:\nDetermine channel orientations by rotating horizontal channels until the resulting radial component maximizes \ncross-correlation with the Hilbert transform of the vertical component. This metric uses Rayleigh waves from large, \nshallow events.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/num_spikes/)\n    + channels = [BCHLM][HX]. \n\n* **pct_above_nhnm**:\nPercent above New High Noise Model. Percentage of Probability Density Function values that are above the New \nHigh Noise Model. This value is calculated over the entire time period.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/pct_above_nhnm/)\n    + channels = [BCDHM][HX].  \n\n* **pct_below_nlnm**:\nPercent below New Low Noise Model. Percentage of Probability Density Function values that are below the New Low Noise \nModel. This value is calculated over the entire time period.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/pct_below_nlnm/)\n    + channels = [BCDHM][HX].  \n\n* **pdf**:\nProbability density function plots and/or text output (controlled by PDF_Preferences in the preference file; or by `--pdf_type`,\n`--pdf_interval`, `--plot_include` on the command line). You must have local PSD files written in the format produced by the \n'psd_corrected' metric (below) or run it concurrently with 'psd_corrected'. These files should be in a directory specified by the\n`psd_dir` entry in the preference file or by `--psd_dir` on the command line, or in the database specified by `db_name`.\n[Reference: Ambient Noise Levels in the Continental United States, McNamara and Buland, 2004](https://doi.org/10.1785/012003001)\n\n* **percent_availability**:\nThe portion of data available for each day is represented as a percentage. 100% data available means full coverage of \ndata for the reported start and end time.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/percent_availability/)\n\n> _NOTE:_ percent_availability will only be calculated for target-days that have metadata.  If metadata is available but no data can be retrieved, then it will be marked as percent_availability=0.  In this case, the quality code associated with that target cannot be determined from the data itself and must be inferred. ISPAQ will currently mark data from `EarthScope` (fdsnws) as quality \"M\" and data from all other sources as \"D\". We are aware that this may not be able to capture the complexity of possible quality codes and will work on improving the logic in a future release.\n\n* **polarity_check**:\nThe signed cross-correlation peak value based on the cross-correlation of two neighboring station channels in \nproximity to a large earthquake signal. A negative peak close to -1.0 can indicate reversed polarity.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/polarity_check/)\n    + channels = [BCFHLM][HX].   \n\n* **pressure_effects**:\nThe correlation coefficient of a seismic channel and an LDO pressure channel. Large correlation coefficients may \nindicate the presence of atmospheric effects in the seismic data.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/pressure_effects/)\n    + channels = LH., LDO  \n  \n* **psd_corrected**:\nPower spectral density values, corrected for instrument response, in text format (starttime, endtime, \nfrequency, power).\n[Documentation](https://service.earthscope.org/mustang/noise-psd/docs/1/help/)\n    + channels = .[HLGNPYXD].\n\n* **sample_max**:\nThis metric reports largest amplitude value in counts encountered within a 24-hour window.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/sample_max/)\n\n* **sample_mean**:\nThis metric reports the average amplitude value in counts over a 24-hour window. This mean is one measure of the \ncentral tendency of the amplitudes that is calculated from every amplitude value present in the time series. The mean \nvalue itself may not occur as an amplitude value in the times series.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/sample_mean/)\n\n* **sample_median**:\nThis metric reports the middle amplitude value in counts of sorted amplitude values from a 24-hour window. This median \nis one measure of the central tendency of the amplitudes in a time series when values are arranged in sorted order. \nThe median value itself always occurs as an amplitude value in the times series.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/sample_median/)\n\n* **sample_min**:\nThis metric reports smallest amplitude value in counts encountered within a 24-hour window.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/sample_min/)\n\n* **sample_rate_channel**:\nA boolean measurement that returns 0 if miniSEED and channel sample rates agree within 1%, or 1 if they disagree. \n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/sample_rate_channel/)\n\n* **sample_rate_resp**:\nA boolean measurement that returns 0 if miniSEED and response-derived sample rates agree within 15%, or 1 if they disagree. \nResponse-derived sample rates assume that the high-frequency amplitude rolloff is ~85% of the Nyquist frequency.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/sample_rate_resp/)\n\n* **sample_rms**:\nDisplays the RMS variance of trace amplitudes within a 24-hour window.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/sample_rms/)\n\n* **sample_snr**:\nA ratio of the RMS variance calculated from data 30 seconds before and 30 seconds following the predicted \nfirst-arriving P phase.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/sample_snr/)\n    + channels = .[HLGNPYX].\n\n* **sample_unique**:\nThis metric reports the number (count) of unique values in data trace over a 24-hour window. \n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/sample_unique/)\n\n* **spikes**:\nThe number of times that the 'Spikes detected' bit in the 'dq_flags' byte is set within a miniSEED file. This data \nquality flag is set by some dataloggers in the fixed section of the miniSEED header when short-duration spikes have \nbeen detected in the data. Because spikes have shorter duration than the natural period of most seismic sensors, \nspikes often indicate a problem introduced at or after the datalogger.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/spikes/)\n  \n* **suspect_time_tag**:\nThe number of times that the 'Time tag is questionable' bit in the 'dq_flags' byte is set within a miniSEED file. \nThis metric can be used to identify stations with GPS locking problems and data days with potential timing issues.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/suspect_time_tag/)\n\n* **telemetry_sync_error**:\nThe number of times that the 'Telemetry synchronization error' bit in the 'dq_flags' byte is set within a miniSEED \nfile. This metric can be searched to determine which stations may have telemetry problems or to identify or omit gappy \ndata from a data request.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/telemetry_sync_error/)\n\n* **timing_correction**:\nThe number of times that the 'Time correction applied' bit in the 'act_flags' byte is set within a miniSEED file. \nThis clock quality flag is set by the network operator in the fixed section of the miniSEED header when a timing \ncorrection stored in field 16 of the miniSEED fixed header has been applied to the data's original time stamp. \nA value of 0 means that no timing correction has been applied.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/timing_correction/)\n\n* **timing_quality**:\nDaily average of the SEED timing quality stored in miniSEED blockette 1001. This value is vendor specific and \nexpressed as a percentage of maximum accuracy. Percentage is NULL if not present in the miniSEED.\n[Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/timing_quality/)\n\n* **transfer_function**:\nTransfer function metric consisting of the gain ratio, phase difference and magnitude squared of two co-located \nsensors. [Documentation](https://service.earthscope.org/mustang/metrics/docs/1/desc/transfer_function/)\n    + channels = [BCFHLM][HX].\n\n### Access for restricted data\nAccess to restricted data from ISPAQ can be managed with a .netrc file that has valid credentials.\nTo set up ISPAQ for use with a .netrc file:\n\n```\ncd ispaq\nconda activate ispaq\ntouch $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\ntouch $CONDA_PREFIX/etc/conda/deactivate.d/env_vars.sh\n```\n\nEdit the $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh as follows:\n> #!/bin/sh     \n> export IrisClient_netrc='path-to-netrc-file'\n\nwhere 'path-to-netrc-file' is the file path for your .netrc.\n\nEdit the $CONDA_PREFIX/etc/conda/deactivate.d/env_vars.sh as follows:\n> #!/bin/sh    \n> unset IrisClient_netrc\n\nThen you'll need to re-install the CRAN IRISSeismic package:\n\n```\nconda deactivate\nconda activate ispaq\nRscript -e 'Sys.getenv(\"IrisClient_netrc\")'   # verify that your .netrc file path is correct\n./run_ispaq.py -I\n```\n\n### Examples Using preference_files/default.txt Preference File\n\n> _Note:_ not using `-P` in the command line is the same as specifying `-P preference_files/default.txt`\n\n```\ncd ispaq  # top-level directory\nconda activate ispaq\n./run_ispaq.py -M basicStats -S basicStats --starttime 2010-100             # starttime specified as julian day\n./run_ispaq.py -M stateOfHealth -S ANMO --starttime 2013-01-05              # starttime specified as calendar day\n./run_ispaq.py -M gaps -S ANMO --starttime 2011-01-01 --endtime 2011-01-08\n./run_ispaq.py -M psdPdf -S psdPdf --starttime 2013-06-01 --endtime 2013-06-04\n```\n\n### Examples Using Command-line Options to Override Preference File\n```\n./run_ispaq.py -M sample_mean -S II.KAPI.00.BHZ --starttime 2013-01-05 --dataselect_url ./test_data --station_url ./test_data/II.KAPI_station.xml --output csv --csv_dir ./test_out\n\n./run_ispaq.py -M psd_corrected,pdf -S II.KAPI.00.BHZ --starttime 2013-01-05 --endtime 2013-01-08 --dataselect_url ./test_data --station_url ./test_data/II.KAPI_station.xml --output csv --psd_dir ./test_out/PSDs --pdf_dir ./test_out/PDFs --pdf_type plot --pdf_interval aggregated\n```\n\n### Example Using SQLite database\n```\n./run_ispaq.py -M basicStats -S basicStats --starttime 2010-100 --output db --db_name ispaq_example.db\n```\nTo view values in sqlite database:\n```\nsqlite3 ispaq_example.db\n```\nAt sqlite prompt:\n```\n.tables\nselect * from sample_mean;\nselect * from sample_max;\n```\nCtrl + D to exit sqlite\n\n\n\n\n\n\n",
        "createdAt": "2016-10-05T22:30:34.000Z",
        "updatedAt": "2025-11-10T23:02:22.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/EarthScope/ispaq/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "fatiando/tremelique",
        "url": "https://github.com/fatiando/tremelique",
        "description": "Educational 2D elastic wave simulator in Python",
        "stars": 5,
        "forks": 0,
        "readme": "<h1 align=\"center\">Tremelique</h1>\n\n<p align=\"center\"><strong>Educational 2D elastic wave simulator in Python</strong></p>\n\n<p align=\"center\">\n<a href=\"https://www.fatiando.org/tremelique\"><strong>Documentation</strong> (latest)</a> •\n<a href=\"https://www.fatiando.org/tremelique/dev\"><strong>Documentation</strong> (main branch)</a> •\n<a href=\"https://github.com/fatiando/tremelique/blob/main/CONTRIBUTING.md\"><strong>Contributing</strong></a> •\n<a href=\"https://www.fatiando.org/contact/\"><strong>Contact</strong></a> •\n<a href=\"https://github.com/orgs/fatiando/discussions\"><strong>Ask a question</strong></a>\n</p>\n\n<p align=\"center\">\nPart of the\n<a href=\"https://www.fatiando.org\"><strong>Fatiando a Terra</strong></a>\nproject.\n</p>\n\n<p align=\"center\">\n<a href=\"https://pypi.python.org/pypi/tremelique\"><img src=\"http://img.shields.io/pypi/v/tremelique.svg?style=flat-square\" alt=\"Latest version on PyPI\"></a>\n<a href=\"https://github.com/conda-forge/tremelique-feedstock\"><img src=\"https://img.shields.io/conda/vn/conda-forge/tremelique.svg?style=flat-square\" alt=\"Latest version on conda-forge\"></a>\n<a href=\"https://pypi.python.org/pypi/tremelique\"><img src=\"https://img.shields.io/pypi/pyversions/tremelique.svg?style=flat-square\" alt=\"Compatible Python versions.\"></a>\n</p>\n\n## About\n\nTBD.\n\n## Project goals\n\nTBD.\n\n## Project status\n\n**Tremelique is in very early stages of development.**\n\n**We welcome feedback and ideas!** This is a great time to bring new ideas on\nhow we can improve the project.\n[Join the conversation](https://www.fatiando.org/contact) or submit\n[issues on GitHub](https://github.com/fatiando/tremelique/issues).\n\n## Getting involved\n\n🗨️ **Contact us:**\nFind out more about how to reach us at\n[fatiando.org/contact](https://www.fatiando.org/contact/).\n\n👩🏾‍💻 **Contributing to project development:**\nPlease read our\n[Contributing Guide](https://github.com/fatiando/tremelique/blob/main/CONTRIBUTING.md)\nto see how you can help and give feedback.\n\n🧑🏾‍🤝‍🧑🏼 **Code of conduct:**\nThis project is released with a\n[Code of Conduct](https://github.com/fatiando/community/blob/main/CODE_OF_CONDUCT.md).\nBy participating in this project you agree to abide by its terms.\n\n> **Imposter syndrome disclaimer:**\n> We want your help. **No, really.** There may be a little voice inside your\n> head that is telling you that you're not ready, that you aren't skilled\n> enough to contribute. We assure you that the little voice in your head is\n> wrong. Most importantly, **there are many valuable ways to contribute besides\n> writing code**.\n>\n> *This disclaimer was adapted from the*\n> [MetPy project](https://github.com/Unidata/MetPy).\n\n## License\n\nThis is free software: you can redistribute it and/or modify it under the terms\nof the **BSD 3-clause License**. A copy of this license is provided in\n[`LICENSE.txt`](https://github.com/fatiando/tremelique/blob/main/LICENSE.txt).\n",
        "createdAt": "2024-03-04T16:55:18.000Z",
        "updatedAt": "2025-11-26T17:49:32.000Z",
        "language": "Python",
        "homepage": "https://www.fatiando.org/tremelique",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/fatiando/tremelique/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "yvonnefroehlich/sws-visualization-and-modeling",
        "url": "https://github.com/yvonnefroehlich/sws-visualization-and-modeling",
        "description": "Visualization and Modeling of Shear Wave Splitting Observations in MATLAB and Python",
        "stars": 4,
        "forks": 0,
        "readme": "# Visualization and Modeling of Shear Wave Splitting\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7213156.svg)](https://doi.org/10.5281/zenodo.7213156)\n\nMATLAB (mainly) and Python (upcoming) functions for visualization and modeling of shear wave splitting observations:\n\n- Optimized for the output of _SplitLab_ ([**_Wüstefeld et al. 2008_**](https://doi.org/10.1016/j.cageo.2007.08.002)) and\n  [_StackSplit_](https://github.com/michaelgrund/stacksplit) ([**_Grund 2017_**](https://doi.org/10.1016/j.cageo.2017.04.015)).\n- The modeling routine is applicable for the _energy minimization_ method ([**_Silver, Chan 1991_**](https://doi.org/10.1029/91JB00899)).\n- Extended and strongly modified from [sws_tools](https://github.com/michaelgrund/sws_tools) by [Michael Grund](https://github.com/michaelgrund).\n\n\n## Citation\n\nIf you make use of this material, please acknowledge the relating publications in which framework these functions were implemented:\n\n**Peer-reviewed journal articles**\n- [**_Fröhlich Y, Grund M, Ritter J R R (2024)_**](https://doi.org/10.1093/gji/ggae245).\n  Lateral and vertical variations of seismic anisotropy in the lithosphere-asthenosphere system underneath Central Europe from long-term splitting measurements.\n  *Geophysical Journal International*, 239(1):112-135.\n  https://doi.org/10.1093/gji/ggae245.\n- [**_Ritter J R R, Fröhlich Y, Sanz Alonso Y, Grund M (2022)_**](https://doi.org/10.1007/s10950-022-10112-w).\n  Short-scale laterally varying SK(K)S shear wave splitting at BFO, Germany – implications for the determination of anisotropic structures.\n  *Journal of Seismology*, 26:1137-1156.\n  https://doi.org/10.1007/s10950-022-10112-w, correction https://doi.org/10.1007/s10950-023-10136-w.\n\n**Doctoral studies**\n- [**_Fröhlich Y (2025)_**](https://doi.org/10.5445/IR/1000183786).\n  Shear wave splitting analysis of long-term data: Anisotropy studies in the Upper Rhine Graben area, Central Europe.\n  Dissertation, *Karlsruhe Institute of Technology, Geophysical Institute*.\n  https://doi.org/10.5445/IR/1000183786.\n\n**Presentation**\n- [**_Fröhlich Y, Ritter J R R (2024)_**](https://dx.doi.org/10.5281/zenodo.14510993).\n  Vertical and Small-scale Lateral Varying Seismic Anisotropy in the Upper Mantle Underneath the Upper Rhine Graben, Central Europe.\n  *Annual Meeting of the American Geophysical Union*, Washington D.C..\n  Division Session Exploring Innovations and New Directions in Seismic Anisotropy and Attenuation: Observations, Models, and Experiments I Oral, DI21A-02.\n  [Abstract ID 1578275](https://agu.confex.com/agu/agu24/meetingapp.cgi/Paper/1578275).\n  https://dx.doi.org/10.5281/zenodo.14510993.\n\n\n## Content\n\n_Example figures_: Generated with the provided test data\n\n- **[001_stereoplot](https://github.com/yvonnefroehlich/sws-visualization-and-modeling/tree/main/001_stereoplot)**\n- **[002_visualization](https://github.com/yvonnefroehlich/sws-visualization-and-modeling/tree/main/002_visualization)**\n- **[003_modeling](https://github.com/yvonnefroehlich/sws-visualization-and-modeling/tree/main/003_modeling)**\n\n![](https://github.com/yvonnefroehlich/sws-visualization-and-modeling/raw/main/_images/000_repo_readme_image.png)\n\n\n## Requirements\n\n_Tested with_: R2022a, R2021a,b under Linux and Windows\n\n- **_MATLAB_**: Forward calculation\n  - Deep Learning Toolbox\n  - Mapping Toolbox\n  - [_MATLAB Seismic Anisotropy Toolkit_ (MSAT)](https://www1.gly.bris.ac.uk/MSAT/) ([**_Walker, Wookey 2012_**](https://doi.org/10.1016/j.cageo.2012.05.031))\n- **Data**: Shear wave splitting observations\n  - Output *.txt files (_nulls_, _splits_) of _SplitLab_ version 1.5.0 ([**_Wüstefeld et al. 2008_**](https://doi.org/10.1016/j.cageo.2007.08.002)) or 1.2.1 (**_Porritt 2014_**)\n  - Output *.mat structure and *.txt files (STACK, SIMW) of _StackSplit_ ([**_Grund 2017_**](https://doi.org/10.1016/j.cageo.2017.04.015))\n- **Colormaps** (optional): Color-coding of the fast polarization direction and the root mean square error\n  - [MatPlotLib Perceptually Uniform Colormaps](https://de.mathworks.com/matlabcentral/fileexchange/62729-matplotlib-perceptually-uniform-colormaps)\\\n    version v2.1.3, MATLAB File Exchange, last access 2022 June 26\n  - [crameri perceptually uniform scientific colormaps](https://de.mathworks.com/matlabcentral/fileexchange/68546-crameri-perceptually-uniform-scientific-colormaps)\\\n    version v1.09, MATLAB File Exchange, last access 2023 April 10; based on [**_Crameri (2021)_**](https://zenodo.org/record/5501399)\n  - [cmocean perceptually-uniform colormaps](https://de.mathworks.com/matlabcentral/fileexchange/57773-cmocean-perceptually-uniform-colormaps)\\\n    version v2.02, MATLAB File Exchange, last access 2022 June 18; based on [**_Thyng et al. (2016)_**](https://dx.doi.org/10.5670/oceanog.2016.66)\n\n\n## Releases\n\n| Release | Zenodo DOI | Publication | RADAR4KIT dataset |\n| --- | --- | --- | --- |\n| [dev](https://github.com/yvonnefroehlich/sws-visualization-and-modeling/tree/main) |  |  |  |\n| [v2.0](https://github.com/yvonnefroehlich/sws-visualization-and-modeling/releases/tag/v2.0) |  | [**_Fröhlich et al. (2024)_**](https://doi.org/10.1093/gji/ggae245) | https://dx.doi.org/10.35097/685 |\n| [v1.0](https://github.com/yvonnefroehlich/sws-visualization-and-modeling/releases/tag/v1.0) | [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7213157.svg)](https://doi.org/10.5281/zenodo.7213157) | [**_Ritter et al. (2022)_**](https://doi.org/10.1007/s10950-022-10112-w) | https://dx.doi.org/10.35097/684 |\n\nFor details of the individual releases as well as for changes and differences compared to [sws_tools](https://github.com/michaelgrund/sws_tools)\nby [Michael Grund](https://github.com/michaelgrund) see the [changelog](https://github.com/yvonnefroehlich/sws-visualization-and-modeling/blob/main/changelog.md).\n\n\n## Known Issues\n\n- Modeling of multi-event analysis: Only using either STACK or SIMW results is supported\n- Model parameter distribution for T1: Under development, not fully tested\n- Synthetic stereoplot for T1 and H2: Backazimuths of predicted nulls are partly wrong\n- Synthetic stereoplot for T1: Gray arrow is partly not exactly placed in the center\n\n\n## Contributing\n\nFor bug reports, suggestions or recommendations feel free to [open an issue](https://github.com/yvonnefroehlich/sws-visualization-and-modeling/issues) or\n[submit a pull request](https://github.com/yvonnefroehlich/sws-visualization-and-modeling/pulls) directly here on\n[GitHub](https://github.com/yvonnefroehlich/sws-visualization-and-modeling/tree/main).\n\n\n## Related topics\n\n| Software | Language | Description | Author |\n| --- | --- | --- | --- |\n| [MSAT](https://github.com/andreww/MSAT)                | MATLAB | Toolkit for the analysis of elastic and seismic anisotropy                | A&nbsp;M&nbsp;Walker,&nbsp;J&nbsp;Wookey |\n| [PyDRex](https://github.com/seismic-anisotropy/PyDRex) | Python | Simulate crystallographic preferred orientation evolution in polycrystals | L Bilton, T Duvernay |\n| [PyRaysum](https://github.com/paudetseis/PyRaysum)     | Python | Software for modeling ray-theoretical body-wave propagation               | W Bloch, P Audet     |\n\n\n## References\n\n- [**_Bowman J R, Ando M (1987)_**](https://doi.org/10.1111/j.1365-246X.1987.tb01367.x).\n  Shear-wave splitting in the upper-mantle wedge above the Tonga subduction zone.\n  *Geophysical Journal International*, 88(1):25-41.\n  https://doi.org/10.1111/j.1365-246X.1987.tb01367.x.\n- [**_Crameri F (2021)_**](https://zenodo.org/record/5501399).\n  Scientific colour maps, version 7.0.1. *Zenodo*. https://www.fabiocrameri.ch/colourmaps.php. https://zenodo.org/record/5501399.\n- [**_Grund M (2017)_**](https://doi.org/10.1016/j.cageo.2017.04.015).\n  StackSplit - a plugin for multi-event shear wave splitting analyses in SplitLab.\n  *Computers & Geosciences*, 105:43-50.\n  https://doi.org/10.1016/j.cageo.2017.04.015.\n- **_Porritt R W (2014)_**. SplitLab version 1.2.1.\n  available at https://robporritt.wordpress.com/software/.\n- [**_Restivo A, Helffrich G (1999)_**](https://doi.org/10.1046/j.1365-246x.1999.00845.x).\n  Teleseismic shear wave splitting measurements in noisy environments.\n  *Geophysical Journal International*, 137:821-830.\n  https://doi.org/10.1046/j.1365-246x.1999.00845.x.\n- [**_Roy C, Winter A, Ritter J R R, Schweitzer J (2017)_**](https://doi.org/10.1093/gji/ggw470).\n  On the improvement of SKS splitting measurements by the Simultaneous Inversion of Multiple Waveforms (SIMW).\n  *Geophysical Journal International*, 208:1508-1523.\n  https://doi.org/10.1093/gji/ggw470.\n- [**_Silver P G, Chan W W (1991)_**](https://doi.org/10.1029/91JB00899).\n  Shear wave splitting and subcontinental mantle deformation.\n  *Journal of Geophysical Research*, 96(B10):16429-16454.\n  https://doi.org/10.1029/91JB00899.\n- [**_Thyng K M, Greene C A, Hetland R D, Zimmerle H M, DiMarco S F (2016)_**](https://dx.doi.org/10.5670/oceanog.2016.66).\n  True colors of oceanography: Guidelines for effective and accurate colormap selection.\n  *Oceanography*, 29(3)9-13.\n  https://dx.doi.org/10.5670/oceanog.2016.66.\n- [**_Walker A M, Wookey J (2012)_**](https://doi.org/10.1016/j.cageo.2012.05.031).\n  MSAT — A new toolkit for the analysis of elastic and seismic anisotropy.\n  *Computer & Geosciences*, 49:81-90.\n  https://doi.org/10.1016/j.cageo.2012.05.031.\n  available at https://www1.gly.bris.ac.uk/MSAT/, https://github.com/andreww/MSAT.\n- [**_Wolfe C J, Silver P G (1998)_**](https://doi.org/10.1029/97JB02023).\n  Seismic anisotropy of oceanic upper mantle: Shear wave splitting methodologies and observations.\n  *Journal of Geophysical Research: Solid Earth*, 103(B1):749-771.\n  https://doi.org/10.1029/97JB02023.\n- [**_Wüstefeld A, Bokelmann G, Zaroli C, Barruol G (2008)_**](https://doi.org/10.1016/j.cageo.2007.08.002).\n  SplitLab: A shear-wave splitting environment in Matlab.\n  *Computers & Geosciences*, 34(5):515-528.\n  https://doi.org/10.1016/j.cageo.2007.08.002.\n\n\n## Funding\n\nThe presented research and YF received support from various sources:\n\n- [Graduate Funding from the German States](https://www.khys.kit.edu/english/graduate_funding.php) (scholarship)\n- [NSF grant EAR-1948602](https://www.nsf.gov/awardsearch/showAward?AWD_ID=1948602) (travel support for AGU24)\n- [DFG project 521545943](https://gepris.dfg.de/gepris/projekt/521545943?language=en) within the\n  [DFG Priority Program DeepDyn SPP 2404 – 500707704](https://www.geo.lmu.de/deepdyn/en/) (research assistant)\n",
        "createdAt": "2022-04-27T06:17:30.000Z",
        "updatedAt": "2025-10-17T18:38:59.000Z",
        "language": "MATLAB",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.7213156",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.7213156",
            "dataCite": "10.5281/zenodo.7213156",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/yvonnefroehlich/sws-visualization-and-modeling/main/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.7213156",
            "title": "Visualization and Modeling of Shear Wave Splitting",
            "journal": "Zenodo",
            "dateReleased": "2022-10-16T00:00:00.000Z",
            "abstract": "MATLAB functions for visualization and modeling of shear wave splitting observations:\n\n\n\nOptimized for the output of SplitLab (Wüstefeld et al. 2008) and StackSplit (Grund 2017)\n\nThe modeling routine is applicable for the energy minimization method (Silver & Chan 1991)\n\nExtended and strongly modified from sws_tools by Michael Grund",
            "citationsArray": [
                "10.1007/s10950-022-10112-w",
                "10.1093/gji/ggae245"
            ]
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "tjnewton/sigpig",
        "url": "https://github.com/tjnewton/sigpig",
        "description": "Seismology and Signal Processing Toolkit ",
        "stars": 4,
        "forks": 0,
        "readme": "# sigpig\n## Seismology and Signal Processing Toolkit \n### ⚠️ warning: this repo is a work in progress ⚠️\nThis repository contains data pipelines developed for research. These pipelines generally start by ingesting data and/or models, followed by an analysis to infer some property of the system. Results are plotted via the sigpig.figures module, which contains numerous functions to visualize and describe data, models, and analyses. \nSigpig is built on top of [matplotlib](https://github.com/matplotlib/matplotlib), [ObsPy](https://github.com/obspy/obspy), [GDAL](https://github.com/OSGeo/gdal), [Laspy](https://github.com/laspy/laspy), [NumPy](https://github.com/numpy/numpy), [pptk](https://github.com/heremaps/pptk), [EQcorrscan](https://github.com/eqcorrscan/EQcorrscan), and other libraries. \n\n### To get started:\nClone this repo in your preferred directory:  \n`git clone https://github.com/tjnewton/sigpig.git`  \nMove into the sigpig directory:  \n`cd sigpig`  \nUse conda to create a Python environment from the sigpig.yml file:  \n`conda env create -f sigpig.yml`  \nActivate the environment:  \n`conda activate sigpig`  \nLaunch Jupyter Lab within sigpig environment:  \n`jupyter lab`  \nBrowse the repo directories and notebooks within Jupyter Lab :)\n\n## Functionality:\n### sigpig.core.data fetches, formats, and analyzes data:  \nTODO: order and cleanup functions, move plot_trace_properties to core.figures  \nTODO: list functions for other modules  \nFunctions: snr, max_amplitude, get_Waveforms, get_Events, trim_Daily_Waveforms, process_gmap_file, project_stations, rattlesnake_Ridge_Station_Locations, dtm_to_grid, eqTransformer_Formatter, get_trace_properties, plot_trace_properties, top_n_autopicked_events, get_event_stream, get_network_stream, events_dict_to_snuffler, get_picked_uncertainties, process_autopicked_events, get_response_files, instantaneous_frequency, calculate_magnitude  \n\nDownload time series data from IRIS DMC  \n```\nnetwork = \"AK\"\nstations = [\"BAL\", \"BARN\", \"DHY\", \"DIV\", \"DOT\", \"GHO\", \"GLB\", \"K218\",\n            \"KLU\", \"KNK\", \"MCAR\", \"MCK\", \"PAX\", \"PTPK\", \"RIDG\", \"RND\",\n            \"SAW\", \"SCM\", \"VRDI\", \"WAT1\", \"WAT6\", \"WAT7\"]\nlocation = \"**\"\nchannels = [\"BHZ\", \"BNZ\", \"HNZ\"]\nstart_Time = UTCDateTime(\"2016-06-15T00:00:00.0Z\")\nend_Time =   UTCDateTime(\"2017-08-11T23:59:59.999999999999999Z\")\nget_Waveforms(network, stations, location, channels, start_Time, end_Time)\n```\n\nFetch the geophone station locations for the Rattlesnake Ridge experiment on a specified date  \n```\n# specify the date of interest\ndate = UTCDateTime(\"2018-03-13T00:04:00.0Z\")\n# get station locations coordinates in UTM meters easting and northing format\nformat = \"UTM\"\nstation_locations = rattlesnake_Ridge_Station_Locations(date, format=format)\n```\n\n### sigpig.core.figures generates various figures:  \nFigures are either generated from objects, like NumPy arrays and ObsPy streams, or from files via a list of file paths. Functions: plot_stalta, plot_stream, plot_Time_Series_And_Spectrogram, plot_Time_Series, plot_party_detections, plot_stack, plot_template_and_stack, plot_stream_absolute, plot_distribution, plot_event_picks, plot_trace_curvature, spectrogram, spectra.  \n\nTime series and spectrogram plotting from files:  \n```\n# define dates of interest\ndoi = UTCDateTime(\"2016-09-26T09:28:00.0Z\") # period start\ndoi_end = UTCDateTime(\"2016-09-26T09:30:00.0Z\") # period end\n\n# define time series files path\nfiles_path = \"/Users/human/Dropbox/Research/Alaska/build_templates/picked\"\n\n# bandpass filter from 1-15 Hz\nfilter = True\nbandpass = [1, 15]\n\n# plot time markers on specified stations\ntime_markers = {\"AK.GLB\":  [UTCDateTime(\"2016-09-26T09:28:40.0Z\"),\n\t\t\t    UTCDateTime(\"2016-09-26T09:28:54.0Z\")],\n\t\t\"AK.PTPK\": [UTCDateTime(\"2016-09-26T09:28:55.0Z\"),\n\t\t\t    UTCDateTime(\"2016-09-26T09:29:09.0Z\")],\n\t\t\"AV.WASW\": [UTCDateTime(\"2016-09-26T09:28:36.0Z\"),\n\t\t\t    UTCDateTime(\"2016-09-26T09:28:50.0Z\")]}\n\nfig = plot_Time_Series_And_Spectrogram(doi, doi_end, files_path,\n\t\t\t\t       filter=filter, bandpass=bandpass,\n\t\t\t\t       time_markers=time_markers)\n```\n![](doc/images/ts-spect.png?raw=true)\n\nTime series plotting from files:  \n```\nfig = plot_Time_Series(doi, doi_end, files_path, filter=filter,\n\t\t       bandpass=bandpass, time_markers=time_markers)\n```\n![](doc/images/ts.png?raw=true)\n\nSpectrogram plotting from an Obspy stream object:  \n```\nfig = spectrogram(stream)\n```\n\nSeparate spectra plotting for each trace in an Obspy stream object: \n```\nfig = spectra(stream)\n```\n\nCombined spectra plotting of each trace in an Obspy stream object: \n```\nfig = multi_spectra(stream)\n```\n![](doc/images/waveform_spectra.png?raw=true)\n\n### sigpig.core.lidar processes lidar data:  \n3D point cloud plotting  \n```\nfilename = \"/Users/human/Dropbox/Programs/lidar/RR_2019_10cm_NAD83_UTMz10.laz\"\nv = visualize_Point_Cloud(filename)\n```\n![](doc/images/point_cloud.png?raw=true)\n\nRetrieve elevations from raster at specified points    \n```\nraster_file = '/Users/human/Dropbox/Programs/lidar/yakima_basin_2018_dtm_43.tif'\nlongitudes = [-120.480, -120.480]\nlatitudes = [46.538, 46.519]\n\n# query raster at specified coordinates\nelevations = elevations_from_raster(raster_file, longitudes, latitudes)\n```\n\nQuery raster on a grid    \n```\nraster_file = '/Users/human/Dropbox/Programs/lidar/yakima_basin_2018_dtm_43.tif'\n# define limits of grid\nx_limits = [694.15, 694.45]\ny_limits = [5155.40, 5155.90]\n\n# query raster on a grid\nlongitude_grid, latitude_grid, elevation_grid = grids_from_raster(raster_file, x_limits, \n\t\t\t\t                                  y_limits, plot=False,\n                                                                  UTM=True)\n```\n\n### sigpig.core.lfe_finder detects LFEs using matched-filtering:  \nProcess arrival time picks from a Snuffler marker file into detections\n#FIXME: add single call function for detection\n```\n# define template length and prepick length (both in seconds)\ntemplate_length = 16.0\ntemplate_prepick = 0.5\n\n# build stream of all station files for templates\nfiles_path = \"/Users/human/Dropbox/Research/Alaska/build_templates/2016-09-26\"\ndoi = UTCDateTime(\"2016-09-26T00:00:00.0Z\")\ntemplate_files = glob.glob(f\"{files_path}/*.{doi.year}-{doi.month:02}\"\n\t\t\t      f\"-{doi.day:02}.ms\")\n\n# get templates and station_dict objects from picks in marker file\nmarker_file_path = \"lfe_template.mrkr\"\nprepick_offset = 11 # in seconds\ntemplates, station_dict, pick_offset = markers_to_template(marker_file_path, prepick_offset)\n\n# define path of files for detection\ndetection_files_path = \"/Volumes/DISK/alaska/data\"\n# define period of interest for detection\nstart_date = UTCDateTime(\"2016-06-15T00:00:00.0Z\")\nend_date = UTCDateTime(\"2018-08-11T23:59:59.9999999999999Z\")\n\n# run matched-filter detection\nparty = detect_LFEs(templates, template_files, station_dict,\n                    template_length, template_prepick,\n                    detection_files_path, start_date, end_date)\n\t\t    \n# inspect the party object detections\ndetections_fig = party.plot(plot_grouped=True)\nrate_fig = party.plot(plot_grouped=True, rate=True)\n```\n![](doc/images/lfe_detections.png?raw=true)\n\n![](doc/images/lfe_detection_rate.png?raw=true)\n\nStack template detections\n```\n# load previous stream list?\nload_stream_list = False\n# get the stacks\nstack_list = stack_waveforms(party, pick_offset,\n\t\t       \t     detection_files_path, template_length,\n\t\t\t     template_prepick, station_dict)\n```\n\n### sigpig.core.autopicker detects noise windows in time series:  \n#TODO: rename this to noise_miner, as autopicker is refactored into time_miner and noise_miner\n\n### sigpig.core.time_miner detects and associates signals in time series:  \n#TODO: rename to something more intuitive like event_miner and refactor internal names\n\nDetect phase arrival times and associate phases to events:  \n```\n# run time_miner.py from terminal and specify the start and end dates of the detection as parameters\n# this example specifies a 5 second window\n./time_miner.py 2018-03-13T00:10:00.0Z 2018-03-13T00:10:05.0Z\n\n# set parameters in time_miner.py for tunability, including plot_Association_Results=True to generate figures\n```\n![](doc/images/association_results.png?raw=true)  \n\nProcess an autopicked event catalog and display the results:  \n```\nfrom data import top_n_autopicked_events\nimport pickle\n\n# define the path to the autopicked events file in snuffler format\nautopicked_file_path = \"autopicked_events_03_13-07_09_2018.mrkr\"\n# define the desired number of events to get (-1 means all events)\nn = -1\n# extract a dictionary of events processed from the file\nevents = top_n_autopicked_events(autopicked_file_path, n)\n\n# save events dict to a file for future reference\nevents_dict_path = \"events_dict.pkl\"\noutfile = open(events_dict_path, 'wb')\npickle.dump(events, outfile)\noutfile.close()\n\n# generate a histogram to describe the events\nevents_dict_histogram(events_dict_path, save_fig=True)\n```\n![](doc/images/events_histogram.png?raw=true)  \n\nor inspect an autopicked event catalog directly without culling it:\n```\n# define the file path to the snuffler-format autopicked events file\nevent_filename = \"autopicked_events_03_13-07_09_2018.mrkr\"\n\n# generate a histogram to describe the temporal distribution of events\nevent_histogram(event_filename, save_fig=True)\n\n# generate a histogram to describe the temporal distribution of all phases of all events\nsignal_histogram(event_filename, save_fig=True)\n```\n\n### sigpig.core.stress inverts slip vectors for principal stress orientation:  \nxy\n",
        "createdAt": "2021-07-06T16:48:36.000Z",
        "updatedAt": "2023-08-26T13:27:05.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/tjnewton/sigpig/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "earthinversion/send-mseed-files-to-remote-server-socketio",
        "url": "https://github.com/earthinversion/send-mseed-files-to-remote-server-socketio",
        "description": "#socketio #senddata #socket #python #miniseed #seismology",
        "stars": 7,
        "forks": 2,
        "readme": "# send-mseed-files-to-remote-server-socketio\n\nThis repository contains client and server scripts that enable the transfer and merging of mseed files using the Socket.IO library in Python. The client script sends mseed files to the server, which merges the files into a single file.\n\n## Client side script\nThe client.py script is a Python script that uses the Socket.IO library to send mseed data files from the client to the server. It sends files to the server that are located in a specific directory on the client machine.\n\nThe client script uses asyncio for concurrency and also relies on the glob and os libraries for directory and file management. It also uses the cryptography library for encryption.\n\n### Dependencies\n- Python 3.x\n- Socket.IO client library\n- asyncio\n- glob\n- os\n- logging\n- cryptography.fernet\n\n### Run\nThe client.py script can be run from the command line using the following command:\n```\npython client.py\n```\n\n## Server side script\nThe `app.py` script is the server-side script that receives mseed files sent by the client and merges them into a single file. The server uses the Socket.IO library to handle communication with the client.\n\nThe script uses the os, glob, eventlet, and socketio libraries. It also uses the cryptography library for encryption.\n\nThe app.py script requires a passcode.rf and key.rf files to be present in the same directory. The passcode.rf file contains the passcode used to encrypt and decrypt messages between the client and server. The key.rf file contains the encryption key used to encrypt and decrypt the passcode.\n\nThe app.py script can be run from the command line using the following command:\n```\npython app.py\n```\n",
        "createdAt": "2021-06-03T10:27:47.000Z",
        "updatedAt": "2023-06-18T22:24:09.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/earthinversion/send-mseed-files-to-remote-server-socketio/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "shubheshanand/Seismology_Data_Processing",
        "url": "https://github.com/shubheshanand/Seismology_Data_Processing",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# Seismology_Data_Processing",
        "createdAt": "2019-10-24T12:31:04.000Z",
        "updatedAt": "2019-10-25T09:28:39.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/shubheshanand/Seismology_Data_Processing/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "andresacosta87/Seismology",
        "url": "https://github.com/andresacosta87/Seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# Seismology\nThis repository collect rotational earthquakes data registered by \nBWLARS, Wettzel for Chile seismics events from 2007 to 2918 \n\nThe data from BWLRAS are available at https://rotations-database.geophysik.uni-muenchen.de/\n",
        "createdAt": "2019-09-23T19:21:13.000Z",
        "updatedAt": "2020-07-15T16:51:15.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/andresacosta87/Seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "sean0921/sac_debian_packager",
        "url": "https://github.com/sean0921/sac_debian_packager",
        "description": "Debian/Ubuntu format SELF-packaging script for Seismic Analysis Code by IRIS",
        "stars": 5,
        "forks": 1,
        "readme": "- This package is no longer maintained before I come back to seismology world.  If someone is agreed with the concept of this repo, fork is [welcome](/LICENSE)!\n\n# SAC Debian/Ubuntu Packager\n\n* **NOTE: Because of LICENSE restriction, this repository does NOT provide IRIS' SAC source code or binary packages for you.  You have to APPLY and download sources tarball on your own if you have related qualifications!**\n    - Official software request page: http://ds.iris.edu/ds/nodes/dmc/forms/sac/\n\n![](https://github.com/sean0921/sean0921.github.io/raw/3ef1e32d61fc62c546c6ba31ef526ccc050cc7b2/images/demo.gif)\n\n## Benefits\n\n* Let installing and uninstalling steps more cleanly (NO MORE `sudo make install`!  It will mess up your system!)\n* After installation, you don't have to manually set environment variables or `source` anything by yourself!\n\n## Supported Linux Distribution\n\n* Debian Stretch (9/oldoldstable)\n* Debian Buster (10/oldstable)\n* Debian Bullseye (11/stable)\n* Debian Sid (bookworm/sid)\n* Ubuntu Xenial Xerus (16.04)\n* Ubuntu Bionic Beaver (18.04)\n* Ubuntu Focal Fossa (20.04)\n* Ubuntu Jammy Jellyfish (22.04)\n\n## Supported Linux Architecture\n\n* amd64\n* arm64 (aarch64)\n\n## How to use this script\n\n* If you know what [docker](https://www.docker.com/) is, it is suggested to use it or create new clean container/chroot to simplify your build environment.\n* If you are lazy to use docker/other chroot-like environment, please make sure your environment are not too messy (too many custom installation, i.e. `sudo make install`)\n\n### Download latest release of this script\n\nYou can download [current `*.zip` archive of this repository](https://github.com/sean0921/sac_debian_packager/archive/master.zip) and extract it.\n\n### Put downloaded SAC source tarball into cloned repo\n\n```bash\ncd sac_debian_packager-${VERSION_NUMBER}\ncp ${LOCATION_OF_DOWNLOADED_TARBALL} ./              ## For example, cp ~/Download/sac-101.6a-source.tar.gz ./\n```\n\n### Install build dependencies\n\n```bash\napt install build-essential libx11-dev libncurses-dev libreadline-dev autoconf automake autopoint autotools-dev libcurl4-openssl-dev zlib1g-dev libxml2-dev pkg-config ### with root\n```\n\n### Run build scripts\n\nchange your current directory to this source code repo and:\n\n```bash\n./build.bash                          ### with normal user, or you can type bash build.bash\n```\n\n### Install generated `*.deb` file\n\n```bash\napt install ./sac-iris-*_amd64.deb    #### with root, you can change * to specific version number\n```\n\n### Done!\n\n* You can directly type `sac` and run the SAC program, *without any shell profile installation*.\n\n### Remove installed SAC package\n\n```bash\napt remove sac-iris      ### with root\n```\n\n### Clean Build Directory before Rebuild\n\n```bash\n./build.bash --clean     ### with normal user\n```\n\n## To Do\n\n* Adding build dependencies package installing procedure to `build.bash`.\n* ~~fix shellcheck warning~~\n* ~~Add patch to fix wrong autoconf name (`configure.in` should be `configure.ac`), simplified duplicated patch.~~ (upstream fixed)\n* ~~Pass the compilaion with `-fno-common` mode~~ (upstream fixed)\n",
        "createdAt": "2020-06-26T13:25:39.000Z",
        "updatedAt": "2024-10-28T01:19:35.000Z",
        "language": "Shell",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/sean0921/sac_debian_packager/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "megies/obspyck",
        "url": "https://github.com/megies/obspyck",
        "description": "ObsPyck is a GUI application that is intended to cover the tasks in a standard analysis workflow for seismic events in seismological observatory practice.",
        "stars": 62,
        "forks": 31,
        "readme": "       ____  __         ____             __\n      / __ \\/ /_  _____/ __ \\__  _______/ /\n     / / / / __ \\/ ___/ /_/ / / / / ___/ //_/\n    / /_/ / /_/ (__  ) ____/ /_/ / /__/ ,<\n    \\____/_.___/____/_/    \\__, /\\___/_/|_|\n                          /____/\n\nObsPyck\n=======\n\nObsPyck is a GUI application that is intended to cover the tasks in a standard analysis workflow for seismic events in seismological observatory practice.\n\nIf you find ObsPyck useful for your research, please consider citing it using one of the [DOIs at Zenodo](https://zenodo.org/search?page=1&size=20&q=obspyck), e.g.:\n\nMegies, T. (2016). ObsPyck 0.4.1. Zenodo. http://doi.org/10.5281/zenodo.438638\n\nFor FAQ see the [list of issues with the \"question\" label](https://github.com/megies/obspyck/issues?q=is%3Aissue+label%3Aquestion+).\n",
        "createdAt": "2012-09-25T18:09:46.000Z",
        "updatedAt": "2025-11-11T04:43:07.000Z",
        "language": "Python",
        "homepage": "https://github.com/megies/obspyck/wiki",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.438638",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.438638",
            "dataCite": "10.5281/zenodo.438638",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/megies/obspyck/master/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.438638",
            "title": "ObsPyck 0.4.1",
            "journal": "Zenodo",
            "dateReleased": "2016-11-25T00:00:00.000Z",
            "abstract": "ObsPyck is a GUI application that is intended to cover the tasks in a standard analysis workflow for seismic events in seismological observatory practice.",
            "citationsArray": []
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Colinebouch/mammamia_alldata_processing",
        "url": "https://github.com/Colinebouch/mammamia_alldata_processing",
        "description": "The codes are combining the ploughmeter, seismology, runoff and velocity data together with the framework to analyse them",
        "stars": 0,
        "forks": 0,
        "readme": "# README\nThe codes are combining the ploughmeter, seismology, runoff and velocity data together with the framework to analyse them.\nToo cite the code, please go to: https://zenodo.org/badge/latestdoi/519147675\n\n# Reference\nThe codes are supporting the manuscript 'The MAMMAMIA project: A multi-scale multi-method approach to\nunderstand runoff-induced changes in the subglacial environment\nand consequences for surge dynamic in Kongsvegen glacier,\nSvalbard', Coline bouchayer, Ugo Nanni, Pierre-Marie Lefeuvre, John Hulth, Louise Schmidt, Jack Kohler, Francois Renard and Thomas V. Schuler. The mansucript is in prepartaion to be submitted to The Crysophere (@ Add DOI when submitted).\n\n# The mammamia project\nThe data and this study have been supported by the Research Council of Norway through the project MAMMAMIA (grant no. 301837). It stands for Multi-scAle-Multi-Method Analysis of Mechanisms causing Ice Acceleration. Sliding of ice is a major player in controlling the glacier contribution to sea-level rise, yet it is poorly understood. In the research project MAMMAMIA we will conduct a Multi-method-multi-scale analysis of mechanisms causing ice acceleration.\nFor more information, please visit the page of the project (https://www.mn.uio.no/geo/english/research/projects/mammamia/).\n\n# Data availability\nThe full data set including all the data used in the study at a 3H resolution can be found on Zenodo (DOI: 10.5281/zenodo.7648444).\n\n# Library required:\nimport numpy as np,\nimport pandas as pd,\nimport matplotlib.pyplot as plt,\nimport scipy.optimize,\nfrom scipy.optimize import minimize,\nfrom numpy.random import rand,\nfrom scipy import interpolate,\nimport itertools,\nimport time,\nfrom scipy import signal,\nimport datetime,\nfrom datetime import timedelta, date,\nimport xarray as xr,\n\n# Description of the files\nKGS01_spectrogram.ipynb: code to reproduce the spectrogram in the Figure E1 (Appendix E). The spectrogram is the frequency content of KSG01 (geophone at the surface of borehole)\n\nfrequency_data.ipynb: code to reproduce the figure I1 (Appendix I). It displays the frequency content of all the time series explored in this study. \n\nplough2Theta.ipynb: code to convert the data of the ploughmeter to force and azimuth (Figure 4d).\n\nproduce_df2122_clean.ipynb: code to combine all the data used in this study at a 3h time resoulution between the 6th June 2021 and 9th August 2022. The code is made to reproduce the Figure 4 + Figure D1\n\nvelocity_long_term.ipynb: code to produce Figure 1b showing the velocity at KNG6 from 2005 to 2022 (Credits: NPI/Jack Kohler).\n\nwork_flow_diurnal_3.ipynb: workflow to filter the data at a diurnal time scale, comparing P, R and S to Q with predictions and discriminate the events into class (Figure 8, 9, G1)\n\nwork_season_3D.ipynb: workflow to filter the data at a seasonal and 3D time scale, comparing P, R and S to Q with predictions and discriminate the events into class (Figure 5, 6, 7)\n",
        "createdAt": "2022-07-29T08:57:11.000Z",
        "updatedAt": "2023-02-17T07:29:47.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Colinebouch/mammamia_alldata_processing/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "saifulkhan/isc_scheduling_system",
        "url": "https://github.com/saifulkhan/isc_scheduling_system",
        "description": "Task scheduling system developed for International Seismological Centre",
        "stars": 1,
        "forks": 0,
        "readme": "# Task scheduling system\nAn example screenshot of the task Scheduling-System\n![alt tag](scheduling-system.png)\n",
        "createdAt": "2016-10-27T11:45:53.000Z",
        "updatedAt": "2021-01-25T13:24:31.000Z",
        "language": "Java",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/saifulkhan/isc_scheduling_system/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "geophystech/seismo-ml-models-integration",
        "url": "https://github.com/geophystech/seismo-ml-models-integration",
        "description": "seisan/earthworm integration scripts to process seismology data using machine learning models",
        "stars": 1,
        "forks": 3,
        "readme": "# seismo-ml-models-integration\nseisan/earthworm integration scripts to process seismology data using machine learning models\n\n## Quick Example\n```\ngit clone https://github.com/Syler1984/seismo-ml-models-integration\ncd seismo-ml-models-integration\npip install --user -r requirements.txt\npython archive_scan.py --start YYYYMMDD --end YYYYMMDD\n```\n\nNote: required `python == 3.8`\n\n## Quick setup guide\n\n### Install the programm\n```\ngit clone https://github.com/Syler1984/seismo-ml-models-integration\ncd seismo-ml-models-integration\npip install --user -r requirements.txt\n```\n\n### Specify archives to scan\nList of archive stations to scan is read from `MULPLT.DEF` file. Default search path is `data/MULPLT.DEF` but if\nfile is not found there, program will look up `$SEISAN_TOP/DAT/MULPLT.DEF`.\n\nTo set a custom list of archive channels for a scan, you can create a `data/MULPLT.DEF`. Example can be found\nin `data/EXAMPLE_MULPLT.DEF`.\n\n### Run `archive_scan.py`\n\nThis will run a scan using Seismo-Performer model for the October 1st 2021:\n```\npython archive_scan.py --start 20211001 --end 20211002\n```\n\nDate format is `YYYYMMDD` or `YYYYMMDDTHHmmss`, e.g. running a scan for a two hours window:\n```\npython archive_scan.py --start 20211001T10:00:00 --end 20211002T12:00:00\n```\n\n### Using different NN models\n\nThree models are supported by default and can be specified via command-line options:\n\n`--cnn` - Seismo CNN model. <br>\n`--gpd` - GPD (Generalized Phase Detection) or ConvNet. <br>\n*no model flag* or `--favor` - Seismo-Performer.\n\nIt is also possible to provide a custom model, see more at [Custom models](#custom-models) section. Or see at \n[Model configuration](#model-configuration) at how to set models through config file, including usage of different\nmodels for different stations.\n\n\n## Full setup guide\n\n### Configuring the stations \n#### MULPLT.DEF\nDefault search path is `/data/MULPLT.DEF`. You can supply different path through `--mulplt-def <path>` option, or\nby using configuration file option:\n```\nmulplt-def = <path>\n```\nIf no `MULPLT.DEF` file found, program will look for `$SEISAN_TOP/DAT/MULPLT.DEF` file instead.\n\nNote, that neither name `MULPLT.DEF` or extension `.DEF` are specifically required, you can provide any\nfile as long as it has station channels definitions like following:\n\n```\nDEFAULT CHANNEL NYSH EN Z\n```\n\n`DEFAULT CHANNEL` serves as an indicator for a channel to scan, followed by a station name and \na component information. Unlike standard `MULPLT.DEf`, precise character positions does not matter as long as words\nare separated by any amount of whitespaces.\n\n#### Channel order\n\nDefault Seismo-Performer, Seismo-CNN and GPD models require three-channel input. Channel order can be configured\nby using either `--channel-order <channels>` command line option, or through\n```\nchannel-order = <channels>\n```\n`<channels>` is a string (without whitespaces) of separate channel order arrangements, each  arrangement consists\nof components separated by a comma, e.g. `N,E,Z`. Arrangements are separated by a semi-column, e.g. \n`N,E,Z;1,2,Z`.\n\nLater, station archives are passed to a NN model in the first channel order, which can describe \nspecified station archives.\n\nFor example, with `MULPLT.DEF` being:\n\n```\nDEFAULT CHANNEL STAT1 SH Z\nDEFAULT CHANNEL STAT1 SH N\nDEFAULT CHANNEL STAT1 SH E\n\nDEFAULT CHANNEL STAT2 EH 1\nDEFAULT CHANNEL STAT2 EH 2\nDEFAULT CHANNEL STAT2 EH Z\n\nDEFAULT CHANNEL STAT3 EH Z\n```\n\nAnd `channel-order = N,E,Z;1,2,Z;Z,Z,Z`\n\nSTAT1 would be passed to NN model in order N, E and Z. STAT2 would be passed in order 1, 2 and Z.\nAnd a single STAT3 channel would be tripled and passed to a model (order Z, Z, Z).\n\nNote, that if you flip order of arrangements like that: `channel-order = Z,Z,Z;N,E,Z;1,2,Z`, than \nSTAT1, STAT2 and STAT3 all be passed as Z, Z and Z, because `Z,Z,Z` arrangement would have the highest priority and\nall three stations fit that arrangement.\n\nDefault channel-order is `N,E,Z;1,2,Z;Z,Z,Z`, so there is no need to specify it, unless extra arrangements \nare required.\n\nNote, that all default models trained on data with Z channal being the last one, so it is recommended to keep that\norder in custom arrangements.\n\n## Detailed configuration\n\nIn *archive_scan.py* .ini config file, provided via `--config PATH` option,\nstation-specific options (eg filtering, output, channel order, ...) might be written.\n\nIn order to configure individual station, simply write a new section with station name.\nExample of station section can be found in `data/config.ini`:\n\n```\n[ARGI]\nno-filter = true\nout = predictions_argi.txt\n```\n\n### Model configuration\nWIP\n\n## Custom models\nWIP\n\n## Advanced search\nWith flag `--advanced-search` (or option `advanced-search = true` in config file) enabled, \nfor every _detected event_ (_detected event_ is a combination of closely packed detection with number of events greater or equal \n`detections-for-event` parameter value) additional scan will be performed.\nThis scan could have different threshold and window shift to extract more detailed information about\nevent.\n\nNote: scan will be performed only on stations on which detections were found (you can change that behaviour).\n\nList of related command line options / parameters:\n- `--advanced-search` - enable advanced search\n- `--advanced-search-range <number>` - range of search (in seconds) around detected events, default: 30\n- `--advanced-search-threshold <threshold>` - threshold for advanced search, could be set just like regular `--threshold`:\nwith number from 0 to 1.0 or for _p_ and _s_ labels individually, default: 0.9\n- `--advanced-search-shift <number>` - window shift for advanced search (in samples), default: 2\n- `--advanced-search-combine` - if specified (or set in config: `advanced-search-combine = true`) \nwill combine all detections in advanced search as single event, otherwise will use normal event combination\nmethod. Without this option enabled, if `advanced-search-range` is larger than `combine-events-range`, advanced search\nfor a single event could potentially yeild multiple events.\n- `--advanced-search-all-stations` - with this option enabled the scan will be performed on full list\nof stations (same as for the original scan), otherwise, only stations with detected wave arrivals will be used for \nthe advanced search.",
        "createdAt": "2021-04-01T05:01:58.000Z",
        "updatedAt": "2023-08-26T14:00:31.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/geophystech/seismo-ml-models-integration/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "emile-dns/PyRFBI",
        "url": "https://github.com/emile-dns/PyRFBI",
        "description": "Bayesian joint inversion of teleseismic converted waves arrival times and polarities",
        "stars": 3,
        "forks": 0,
        "readme": "# PyRFBI\n\nA complete framework for the Bayesian joint inversion of teleseismic converted waves arrival times and polarities.\n\nReceiver Functions Bayesian Inversion (RFBI) is a tool written in Python to probabilistically invert teleseismic converted wave arrival times and polarities on the transverse component. It uses the Python package [PyRaysum](https://github.com/paudetseis/PyRaysum) to compute synthetic receiver functions for the forward model.\n\n## Installation and requirements\n\nYou will neeed a Python environment with the module [PyRaySum](https://paudetseis.github.io/PyRaysum/init.html#installation). I strongly recommend creating a custom conda environment where PyRaySum can be installed along with its dependencies. I define here a a minimal environment for running PyRFBI:\n\n```\nconda create -n prs python=3.8 fortran-compiler obspy pandas seaborn cmcrameri -c conda-forge\n```\n```\nconda activate prs\n```\n```\npip install pyraysum\n```\n\nThen, you can download PyRFBI code from github and add its location to your PATH variable:\n\n```\nexport PATH=\"$PATH:/absolute/path/to/PyRFBI/\"\n```\n\n## Usage and documentation\n\n### 0. Input data\n\nArrival times and polarities needs to be organized in 4 files:\n- pick_time.csv : arrival times\n- pick_time_error.csv : arrival times uncertainties\n- pick_polarity.csv : polarities\n- pick_polarity_error.csv : polarities uncertainties\n\nThe nomenclatura is based on PyRaySum\n\n```\nrfbi_gen_input.py\n```\n\nWith every command, you can use `-h` to have help about the usage of the command.\n\n### 1. Generate structure\n\nCreate working directories, copy the data and create a config file (rfbi.ini):\n\n```\nrfbi_make_wkdir.py wkdir datadir\ncd wkdir\n```\n\n### 2. Initiate \n\nInitiate the earth model (number of layers and target parameters). You will need to change the parameters_inversion.csv file.\n\n```\nrfbi_init_invstruct.py n_layers target_parameters\n```\n\nCheck that parameters_inversion.csv is correct.\n\n```\nrfbi_check_invstruct.py\n```\n\n### 3. Initiate \n\nSelect Metropolis or adaptative Metropolis algorithm and add parameters to config file. You will need to change the inversion and sampling parameters in the config file. \n\n```\nrfbi_init_inversion.py sampling_method\n```\n\n### 4. Plotting data \n\nPlot inversion data:\n\n```\nrfbi_plot_data.py\n```\n\n### 5. Run the inversion \n\nRun the inversion:\n\n```\nrfbi_invert.py\n```\n\n### 6. Plotting inversion results\n\nFinally, you can plot the results of the inversion. It will create figures ...\n\n```\nrfbi_plot_inv.py\n```\n\n## Contact\n\nFeel free to contact me and ask questions at [emile.denise@ens.psl.eu](mailto:emile.denise@ens.psl.eu). I'll try to reply as soon as possible.\n\n## Contributing\n\nAll constructive contributions are welcome, e.g. bug reports, discussions or suggestions for new features.",
        "createdAt": "2023-12-01T11:34:25.000Z",
        "updatedAt": "2025-01-22T11:56:20.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/emile-dns/PyRFBI/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ivemv/Seismology",
        "url": "https://github.com/ivemv/Seismology",
        "description": "Clase Sismología I UMDI",
        "stars": 0,
        "forks": 0,
        "readme": "# Seismology\nClase Sismología I UMDI\n",
        "createdAt": "2018-02-19T22:05:02.000Z",
        "updatedAt": "2018-11-23T21:41:33.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ivemv/Seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "oceanicdayi/Utaipei-Seismology",
        "url": "https://github.com/oceanicdayi/Utaipei-Seismology",
        "description": null,
        "stars": 0,
        "forks": 3,
        "readme": "# Utaipei-Seismology",
        "createdAt": "2020-07-02T14:54:55.000Z",
        "updatedAt": "2021-02-23T02:26:42.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/oceanicdayi/Utaipei-Seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "SKvr-46/seismology-ruby-react",
        "url": "https://github.com/SKvr-46/seismology-ruby-react",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# README\n\nThis README would normally document whatever steps are necessary to get the\napplication up and running.\n\nThings you may want to cover:\n\n* Ruby version\n\n* System dependencies\n\n* Configuration\n\n* Database creation\n\n* Database initialization\n\n* How to run the test suite\n\n* Services (job queues, cache servers, search engines, etc.)\n\n* Deployment instructions\n\n* ...\n\n# To Start Demo\n* rails s\n* npm start",
        "createdAt": "2023-05-21T15:26:26.000Z",
        "updatedAt": "2023-05-21T15:27:15.000Z",
        "language": "Ruby",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/SKvr-46/seismology-ruby-react/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Irrationalpunker/Ovsyannikov_seismology",
        "url": "https://github.com/Irrationalpunker/Ovsyannikov_seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2025-08-25T12:07:58.000Z",
        "updatedAt": "2025-10-21T08:04:44.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "SGCpy/sgc_seistools",
        "url": "https://github.com/SGCpy/sgc_seistools",
        "description": "Tools for SGC seismology data acquisition.",
        "stars": 0,
        "forks": 0,
        "readme": "![SGC](images/sgc_logo.png)<!-- .element width=\"700\"-->\n\n# sgc_seistools\nTools for SGC seismology data acquisition.\n",
        "createdAt": "2020-09-25T15:06:52.000Z",
        "updatedAt": "2020-12-04T18:52:31.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/SGCpy/sgc_seistools/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "uafgeotools/waveform_collection",
        "url": "https://github.com/uafgeotools/waveform_collection",
        "description": "Collect seismic/infrasound waveforms and metadata from IRIS/WATC/AVO servers, local miniSEED files, etc.",
        "stars": 15,
        "forks": 8,
        "readme": "waveform_collection\n===================\n\n[![](https://readthedocs.org/projects/uaf-waveform-collection/badge/?version=master)](https://uaf-waveform-collection.readthedocs.io/)\n\nPackage containing convenience functions to collect seismic/infrasound waveforms\nand metadata from IRIS/WATC/AVO servers or local files (miniSEED, etc.).\n\nInstallation\n------------\n\nIt's recommended that you install this package into a new or pre-existing\n[conda](https://docs.conda.io/projects/conda/en/latest/index.html) environment.\n(If you choose the latter option, ensure that your environment contains all of\nthe packages listed in the [Dependencies](#dependencies) section.)\n\nTo create a new conda environment for use with this and other _uafgeotools_\npackages, execute the following terminal command:\n```\n$ conda create -n uafinfra -c conda-forge obspy multiprocess\n```\nThis creates a new environment called `uafinfra` with ObsPy, multiprocess, and their dependencies\ninstalled.\n\nTo install _waveform_collection_, execute the following terminal commands:\n```\n$ conda activate uafinfra  # Or your pre-existing env\n$ git clone https://github.com/uafgeotools/waveform_collection.git\n$ cd waveform_collection\n$ pip install -e .\n```\nThe final command installs the package in \"editable\" mode, which means that you\ncan update it with a simple `git pull` in your local repository. This install\ncommand only needs to be run once.\n\nDependencies\n------------\n\nPython packages:\n\n* [ObsPy](http://docs.obspy.org/)\n* [multiprocess](https://multiprocess.readthedocs.io/)\n\n...and its dependencies, which you don't really have to be concerned about if\nyou're using conda!\n\nUsage\n-----\n\nDocumentation is available online\n[here](https://uaf-waveform-collection.readthedocs.io/).\n\nAccess the package's functions with (for example)\n```python\nfrom waveform_collection import gather_waveforms\n```\nand so on. For a usage example, see\n[`example.py`](https://github.com/uafgeotools/waveform_collection/blob/master/example.py).\n\nAuthors\n-------\n\n(_Alphabetical order by last name._)\n\nDavid Fee<br>\nJulia Gestrich<br>\nLogan Scamfer<br>\nDarren Tan<br>\nLiam Toney<br>\nAndrew Winkelman\n",
        "createdAt": "2019-09-20T15:36:44.000Z",
        "updatedAt": "2025-09-22T09:57:46.000Z",
        "language": "Python",
        "homepage": "https://uaf-waveform-collection.readthedocs.io/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/uafgeotools/waveform_collection/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "openeew/openeew-sensor",
        "url": "https://github.com/openeew/openeew-sensor",
        "description": "Hardware for an OpenEEW sensor",
        "stars": 48,
        "forks": 13,
        "readme": "# OpenEEW Sensor\nThe OpenEEW sensor has already shown itself to be [as good as seismometers](https://openeew.com/blog/sensor-benchmark) for the purpose of earthquake early-warnings (EEW). EEWs are concerned only with strong shaking and so expensive broadband seismometers are not necessary for detection.\n\nThis hardware design has been created to drastically reduce the cost of a seismometer through the usage of off-the-shelf parts. The key component is the ADXL355 MEMS accelerometer, which has far lower noise than other accelerometers on the market. This low noise allows it to detect earthquakes at further distances.\n\nThe sensor can be bought from [PCBWay](https://www.pcbway.com/project/gifts_detail/OpenEEW_Node.html), [Grillo](https://grillo.io/product/openeew-node/), or made following these instructions.\n\n## Hardware\nThe PCB features the following:\n- A high performance MEMS accelerometer\n- ESP-32-WROOM-32 which features a dual-core processor, 240MHz frequency, 4MB Flahs, and 8MB PSRAM\n- Ethernet connector and controller to provide more stable internet connections that need to last for years in some installations\n- 3 Neopixel RBG LEDs and a buzzer to provide alarm functions when an alert message is received on the device\n- A USB-C interface for powering and flashing the device\n\n![PCB](https://user-images.githubusercontent.com/6279965/118044476-4dd2c380-b33c-11eb-8baa-c089b383fa31.PNG)\n[Schematic here](/pcb/openeew-schematic.pdf)\n\nPlease note the following pins:\n- ADXL355 > SPI (HSPI) > CS GPIO 15\n- Neopixel data pin> GPIO 16\n- Buzzer > GPIO 32\n\nThe board operates at 3.3V with a minimum current of 0.5A. The accelerometer is accessed via the ESP32's HSPI interface.\n\nThe ethernet uses the LAN8720A transceiver. We have not included PoE in this variant to reduce complexity and cost.\n\nGPS can optionally be added via the UART header, and I2C devices can be added via the I2C header. However we have opted to use NTP as a default for timekeeping, and we use the OpenEEW app to record latitude and longitude when provisioning the device.\n\n### Information\nYou can find the schematics, PCB, and BOM files in [here](/pcb). The board was generated using [Kicad](https://kicad.org/).\n\n<img src=\"/images/openeew-node-withlid.jpg\" width=\"300\">\n<img src=\"/images/openeew-node-blue.jpg\" width=\"300\">\n<img src=\"/images/animated-box.gif\" width=\"300\">\n\n___\n\nEnjoy! Give us [feedback](https://github.com/openeew/openeew-sensor/issues) if you have suggestions on how to improve this information.\n\n## Contributing and Developer information\n\nThe community welcomes your involvement and contributions to this project. Please read the OpenEEW [contributing](https://github.com/openeew/openeew/blob/master/CONTRIBUTING.md) document for details on our code of conduct, and the process for submitting pull requests to the community.\n\n## License\n\nThe OpenEEW sensor is licensed under the Apache Software License, Version 2. Separate third party code objects invoked within this code pattern are licensed by their respective providers pursuant to their own separate licenses. Contributions are subject to the [Developer Certificate of Origin, Version 1.1 (DCO)](https://developercertificate.org/) and the [Apache Software License, Version 2](http://www.apache.org/licenses/LICENSE-2.0.txt).\n",
        "createdAt": "2020-08-02T02:10:38.000Z",
        "updatedAt": "2025-08-21T17:58:09.000Z",
        "language": null,
        "homepage": "https://openeew.com/docs/build-sensor",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/openeew/openeew-sensor/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ippocratiss/Stellar-modelling-and-data-analysis",
        "url": "https://github.com/ippocratiss/Stellar-modelling-and-data-analysis",
        "description": "Reproduction package for the simulations and data analysis of the project \"Dark Matter-Induced Stellar Oscillations\". Online publication is here: https://arxiv.org/abs/2305.03085",
        "stars": 0,
        "forks": 0,
        "readme": "# Reproduction Package for the Paper \"Dark Matter-Induced Stellar Oscillations\"\n\n## Authors\nJeremy Sakstein (<sakstein@hawaii.edu>)  \nIppocratis Saltas (<saltas@fzu.cz>)  \n\n## Software\n\nMESA version 15140 (<http://mesa.sourceforge.net>)   \nMESASDK version 20200325 (<http://www.astro.wisc.edu/~townsend/static.php?ref=mesasdk>)   \nGFORTRAN GCC version 9.2.0   \nGYRE version 7.0   \nPython 3.7.3   \nJupyter Notebook version 6.4.12   \n\n\n## MESA Files\n\n**MESA\\_inlist:** MESA inlist used to produce the equilibrium solar model. Should be placed in $MESA\\_DIR/star/test\\_suit/simplex\\_solar\\_calibration.\n\n**late\\_pre\\_zams\\_1.0M.mod:** Pre-main-sequence model loaded by MESA\\_inlist. Should be placed in $MESA\\_DIR/star/test\\_suit/simplex\\_solar\\_calibration.\n\n**solar\\_ref\\_mesa.mesa:** Pulse data for the equilibrium solar model.\n\n## GYRE Files\n\n**GYRE\\_inlist.in:** GYRE inlist used to produce the eigenfrequencies of the equilibrium solar model.\n\n**summary\\_mesa.txt:** GYRE summary file.\n\n**detail.l1.n+N.h5:** Tabulated eigenfunctions for the mode with n=1 n=N. \n\n## Python Files\n\n**V\\_rms\\_computation.ipynb:** Jupyter notebook that computes the RMS velocity.\n\n**GS98\\_OPAL\\_CALIBR.txt:** Equilibrium solar model. See V\\_rms\\_computation.ipynb.\n\n## Citation Policy\n\nIf you use any part of this reproduction package for independent work we recommend you cite this paper.\n",
        "createdAt": "2023-11-29T19:04:02.000Z",
        "updatedAt": "2024-03-30T14:24:06.000Z",
        "language": "Jupyter Notebook",
        "homepage": "https://zenodo.org/records/7894030",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ippocratiss/Stellar-modelling-and-data-analysis/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ds-modules/EPS-130-SP21",
        "url": "https://github.com/ds-modules/EPS-130-SP21",
        "description": "UC Berkeley EPS 130 (Strong Seismology) Spring 2021",
        "stars": 1,
        "forks": 0,
        "readme": "# EPS-130-SP21\n\n\nEPS 130 - Strong Motion Seismology - Doug Dreger - Spring 2021\n\n [![Datahub](https://img.shields.io/badge/Launch-UCB%20Datahub-blue.svg)](https://datahub.berkeley.edu/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fds-modules%2FEPS-130-SP21&urlpath=lab%2Ftree%2FEPS-130-SP21%2F)\n\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/ds-modules/EPS-130-SP21/main)\n\n",
        "createdAt": "2021-01-06T23:26:42.000Z",
        "updatedAt": "2025-05-30T18:34:07.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ds-modules/EPS-130-SP21/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "pyrocko/fomosto-qssp2017",
        "url": "https://github.com/pyrocko/fomosto-qssp2017",
        "description": "An official read-only mirror of https://git.pyrocko.org/pyrocko/fomosto-qssp2017. QSSP with dynamic memory allocation and many improvements",
        "stars": 6,
        "forks": 3,
        "readme": "# QSSP2017 (packaged as Fomosto backend)\n\nCode for calculating complete synthetic seismograms of a spherical earth using\nnormal mode theory.\n\nQSSP2017 has been written by Rongjiang Wang.\n\nThis is the newest version of the legacy code of QSSP.\nNew features are:\n- dynamic memory allocation\n- todo ...\n\nPackaging has been done by Hannes Vasyura-Bathke.\n\n## References\n\n- Gilbert, F. and Backus, G.: Elastic-gravitational vibrations of a radially\n  stratified sphere, in: Dynamics of Stratified Solids, edited by: Herrmann,\n  G., American Society of Mechanical Engineers, New York, 82–95, 1968.\n  Takeuchi, H., and M. Saito (1972). Seismic surface waves, in Methods in\n  Computational Physics, vol. 11, Bolt, B. A. , Editor Academic Press, New\n  York, 217- 295. \n\n- Wang, R., (1997), Tidal response of the solid earth, in Tidal Phenomena,\n  edited by H. Wilhelm, W. Zürn and H.G. Wenzel, Lecture Notes in Earth\n  Sciences, Vol. 66, pp. 27-57, Springer-Verlag, Berlin/Heidelberg, Germany,\n  1997.\n\n- Wang, R., Heimann, S., Zhang, Y., Wang, H., & Dahm, T., 2017. Complete\n  synthetic seismograms based on a spherical self-gravitating Earth model with\n  an atmosphere ocean mantle core structure, Geophys, J. Int., 210,\n  1739-1764.\n\n## Compile and install\n\n```\nautoreconf -i   # only if 'configure' script is missing\n./configure\nmake\nsudo make install\n```\n",
        "createdAt": "2018-05-24T13:49:30.000Z",
        "updatedAt": "2025-02-08T17:54:35.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/pyrocko/fomosto-qssp2017/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "seismo-savi/Cy-seismo",
        "url": "https://github.com/seismo-savi/Cy-seismo",
        "description": "Scripts to perform seismological processing on the Cyprus Broadband Seismic Network (CQ)",
        "stars": 0,
        "forks": 0,
        "readme": "# Cy-seismo\n Scripts to perform seismological processing on the Cyprus Broadband Seismic Network (CQ)\n",
        "createdAt": "2022-11-20T02:03:57.000Z",
        "updatedAt": "2025-04-01T21:37:26.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/seismo-savi/Cy-seismo/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Virgulon06/Data-driven-Fault-Network-Reconstruction",
        "url": "https://github.com/Virgulon06/Data-driven-Fault-Network-Reconstruction",
        "description": "Internship at Swiss Seismological Service, ETH",
        "stars": 0,
        "forks": 0,
        "readme": "# Fault-Network-Reconstruction\nInternship at Swiss Seismological Service, ETH\n",
        "createdAt": "2024-09-05T17:21:18.000Z",
        "updatedAt": "2025-09-15T11:56:28.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Virgulon06/Data-driven-Fault-Network-Reconstruction/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "sbonaime/seedlink_plotter",
        "url": "https://github.com/sbonaime/seedlink_plotter",
        "description": "Seedlink_plotter A python script to plot real time seismic data from a seedlink server",
        "stars": 40,
        "forks": 19,
        "readme": "## Seedlink-Plotter\n\nA python script to plot real time seismic data from a seedlink server in drum style or line style\n\nThis version works with at least stable ObsPy version 1.3.0\n\nOn some linux box, the time zone must be set to UTC and not GMT\n\n### Installation\n\n    pip install https://github.com/bonaime/seedlink_plotter/archive/master.zip\n\n### Upgrade\n\n    pip install --upgrade https://github.com/bonaime/seedlink_plotter/archive/master.zip\n\n### Usage examples\n\nDrum plots (with longer time range) and with events greater than 5.5:\n\n    seedlink-plotter -s \"G_FDFM:00BHZ\" --x_position 200 --y_position 50 --x_size 800 --y_size 600 -b 24h --scale 20000 --seedlink_server \"rtserver.ipgp.fr:18000\" --x_scale 60m --events 5.5\n\n![Singlechannel](/img/Singlechannel.png)\n\n\nLine plot with single station (with shorter time range):\n\n    seedlink-plotter -s \"G_IVI:00BHZ\" -b 10m --seedlink_server \"rtserver.ipgp.fr:18000\"  --line_plot\n\n![Plot_line](/img/plot_line.png)\n\nLine plots with multiple stations (with shorter time range):\n\n    seedlink-plotter -s \"G_FDFM:00BHZ,G_SSB:00BHZ\" --x_position 200 --y_position 50 --x_size 800 --y_size 600 -b 30m --seedlink_server \"rtserver.ipgp.fr:18000\" --update_time 2s\n    seedlink-plotter -s \"G_FDFM:00BHZ 00BHN 00BHE\" --x_position 200 --y_position 50 --x_size 800 --y_size 600 -b 30m --seedlink_server \"rtserver.ipgp.fr:18000\" --update_time 2s\n\n![Multichannel](/img/Multichannel.png)\n\n### Keyboard Controls\n\nKeyboard controls only work without option `--without-decoration`!\n\n - `f`: toggle fullscreen\n - `<Escape>` or `q`: close window\n\n### Dependencies\n - Python 3.6\n - ObsPy 1.3.0\n - matplolib (>= 1.3.0)\n - scipy\n - numpy\n",
        "createdAt": "2013-03-19T17:24:49.000Z",
        "updatedAt": "2025-11-28T15:33:11.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/sbonaime/seedlink_plotter/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "hyzhak/nasa_space_apps_challenge_2024_seismic_detection",
        "url": "https://github.com/hyzhak/nasa_space_apps_challenge_2024_seismic_detection",
        "description": "NASA Space Apps Challenge 2024 -- Seismic Detection Across the Solar System",
        "stars": 0,
        "forks": 0,
        "readme": "# NASA Space Apps Challenge 2024 (seismic_detection) -- Velyki Mali Shum\n\n<a target=\"_blank\" href=\"https://cookiecutter-data-science.drivendata.org/\">\n    <img src=\"https://img.shields.io/badge/CCDS-Project%20template-328F97?logo=cookiecutter\" />\n</a>\n\nNASA Space Apps Challenge 2024 -- Seismic Detection Across the Solar System (https://www.spaceappschallenge.org/nasa-space-apps-2024/challenges/seismic-detection-across-the-solar-system/)\n\nTeam's page: https://www.spaceappschallenge.org/nasa-space-apps-2024/find-a-team/velyki-mali-shum/\n",
        "createdAt": "2024-10-06T20:20:57.000Z",
        "updatedAt": "2024-10-07T07:45:25.000Z",
        "language": "Jupyter Notebook",
        "homepage": "https://www.spaceappschallenge.org/nasa-space-apps-2024/find-a-team/velyki-mali-shum/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/hyzhak/nasa_space_apps_challenge_2024_seismic_detection/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "tso1257771/Attention-Recurrent-Residual-U-Net-for-earthquake-detection",
        "url": "https://github.com/tso1257771/Attention-Recurrent-Residual-U-Net-for-earthquake-detection",
        "description": "ARRU Phase Picker: Attention Recurrent‐Residual U‐Net for Picking Seismic P‐ and S‐Phase Arrivals",
        "stars": 19,
        "forks": 1,
        "readme": "# ARRU phase picker: Attention-Recurrent-Residual-U-Net-for-earthquake-detection\nWe're working on a more stable model on processing continuous seismograms as well as an useful repository! <br/>\nHere are just the simple scripts for model training and prediction using STandford Earthquake Dataset (STEAD) dataset. \n\nhttps://user-images.githubusercontent.com/30610646/120765835-327a6300-c54c-11eb-99b2-6ea4bf6b1c94.mp4\n\n\n# Script piplines \nBelow describes the workflow from data generation, model training, making predictions, to model evaluation. \n\n1. **Prepare the seismic recordings from STEAD data** : **`P01_make_stream_STEAD.py`**<br/>\nThis script simply generates sac files as well as TFRecord in length of 20 seconds. This would require the [STEAD](https://github.com/smousavi05/STEAD) dataset , please download and place the file 'merge.hdf5' (you could retreive this entire [STEAD](https://mega.nz/folder/HNwm0SLY#h70tuXK2tpiQJAaPq72FFQ) dataset here in the directory '`./data`'. <br/>\nYou can change the variable '`csv_type`' in line 22 with [`train`, `test`, `val`] to generate dataset we used in our study according to the files stored in '`./data/partition_csv`'. Noted that you have to make '`./data/partition_csv/train_STEAD.csv`' by yourselves according to uploaded lists of STEAD dataset for data partition, if needed. <br/><br/>\nOutput directory: **(1) './data/sac_data_STEAD_20s' (2) './data/input_TFRecord_STEAD_20s'**\n\n2. **Model training**: <br/>\nIn this repository we provide two pretrained models,  <br/>**`./pretrained_model/paper_model_ARRU_20s`** and **`./pretrained_model/multitask_ARRU_20s`** <br/> <br/>\nThe former works as seismic phase picker described in our paper, and the latter one provides an additional mask associating the P and S arrivals, which could also be treated as the earthquake event detector. Both of these models were trained with local earthquake events that the maximum separation between P and S arrival is 12 seconds. <br/> <br/>\n**`./P02_train_codes/P01_Unet_train_gpus_STEAD.py`**<br/>\n**`./P02_train_codes/P01_Unet_train_detect_gpus_STEAD.py`**<br/>\n\n3. **Making predictions on STEAD dataset**<br/>\n\n4. **Model evaluation** <br/>\n\n5. Quick example of making predictions using pretrained model <br/>\n```$ python quick_ex.py```\n\n# Binder link\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/tso1257771/Attention-Recurrent-Residual-U-Net-for-earthquake-detection/HEAD)\n# Reference\nWu‐Yu Liao, En‐Jui Lee, Dawei Mu, Po Chen, Ruey‐Juin Rau; ARRU Phase Picker: Attention Recurrent‐Residual U‐Net for Picking Seismic P‐ and S‐Phase Arrivals. Seismological Research Letters 2021; doi: https://doi.org/10.1785/0220200382\n",
        "createdAt": "2021-02-05T02:00:53.000Z",
        "updatedAt": "2025-10-01T18:34:04.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/tso1257771/Attention-Recurrent-Residual-U-Net-for-earthquake-detection/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Almonacid98/seismology",
        "url": "https://github.com/Almonacid98/seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2020-03-31T01:30:30.000Z",
        "updatedAt": "2020-06-04T23:48:19.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "njlindsey/_CPS3.30",
        "url": "https://github.com/njlindsey/_CPS3.30",
        "description": "Computer Programs in Seismology 3.30 Course (Herrmann)",
        "stars": 0,
        "forks": 0,
        "readme": "# _CPS3.30\nComputer Programs in Seismology 3.30 Course (Herrmann)\n",
        "createdAt": "2016-05-16T23:35:02.000Z",
        "updatedAt": "2016-05-16T23:35:02.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/njlindsey/_CPS3.30/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "YijianZhou/Manual-Analysis-Helper",
        "url": "https://github.com/YijianZhou/Manual-Analysis-Helper",
        "description": "Codes for manual (semi-automated) seismological analysis",
        "stars": 8,
        "forks": 2,
        "readme": "# Manual-Analysis-Helper\nCodes for semi-automated seismological analysis.  \nNote that manual efforts are required! Please read the workflow in each directory for the usage.  \n\n### Separated helpers\n- Manual phase picking  \n- Repeater detection  \n- CC clustering  \n- Source spectrum analysis  \n- EGF deconvolution  \n\n### References\n\n- **Zhou, Y.**, H. Yue, L. Fang, S. Zhou, L. Zhao, & A. Ghosh (2021). An Earthquake Detection and Location Architecture for Continuous Seismograms: Phase Picking, Association, Location, and Matched Filter (PALM). *Seismological Research Letters*; 93(1): 413–425. doi: [10.1785/0220210111](https://doi.org/10.1785/0220210111)  \n\n- Lu, W., **Y. Zhou**, Z. Zhao, H. Yue, & S. Zhou (2021). Aftershock sequence of the 2017 Mw 6.5 Jiuzhaigou, China earthquake monitored by an AsA network and its implication to fault structures and strength. *Geophysical Journal International*; 228(3): 1763-1779. doi: [10.1093/gji/ggab443](https://doi.org/10.1093/gji/ggab443)  \n\n- **Zhou, Y.**, H. Yue, S. Zhou, L. Fang, Y. Zhou, L. Xu, Z. Liu, T. Wang, L. Zhao, & A. Ghosh (2022). Microseismicity along Xiaojiang Fault Zone (Southeastern Tibetan Plateau) and the Characterization of Interseismic Fault Behavior. *Tectonophysics*; 833: 229364. doi: [10.1016/j.tecto.2022.229364](https://doi.org/10.1016/j.tecto.2022.229364)  \n\n- **Zhou, Y.**, C. Ren, A. Ghosh, H. Meng, L. Fang, H. Yue, S. Zhou, & Y. Su (2022). Seismological Characterization of the 2021 Yangbi Foreshock-Mainshock Sequence, Yunnan, China: More than a Triggered Cascade. *Journal of Geophysical Research: Solid Earth*; doi: [10.1029/2022JB024534](https://doi.org/10.1029/2022JB024534)  \n",
        "createdAt": "2022-01-09T22:44:25.000Z",
        "updatedAt": "2025-02-08T16:33:35.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/YijianZhou/Manual-Analysis-Helper/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Charleszxx/projectearthquake-seismologyph",
        "url": "https://github.com/Charleszxx/projectearthquake-seismologyph",
        "description": "Backend for Project: Earthquake Seismology PH",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2025-10-12T08:32:51.000Z",
        "updatedAt": "2025-10-25T12:12:03.000Z",
        "language": "JavaScript",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "BenjaminLeblanc/GiantPlanetSeismology",
        "url": "https://github.com/BenjaminLeblanc/GiantPlanetSeismology",
        "description": "Set of python files used to simulate observations of Jupiter oscillations.",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2021-10-05T18:02:18.000Z",
        "updatedAt": "2021-10-05T18:05:41.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "noah-de/NEES",
        "url": "https://github.com/noah-de/NEES",
        "description": "Konno/Omachi Smoothing implemenation for processing in the NEES EEG Data Portal",
        "stars": 0,
        "forks": 0,
        "readme": "# NEES\ntaking the [Konno/Omachi Smoothing implemenation](http://www.eq.db.shibaura-it.ac.jp/papers/Konno&Ohmachi1998.pdf) by https://github.com/arkottke/ and implement it for processing in the [EEG Data Portal](http://nees.ucsb.edu/data-portal)\n\nReferring to [other Konno/Omachi implementations](https://github.com/jsh9/fast-konno-ohmachi) may be helpful.\n\nCheck syntax with **pycodestyle** and debug with either **spyder** or **pudb** (python -m pudb.run utils.py)\n\nThis project was built using [Anaconda distribution of Python](https://www.anaconda.com/download/).\n\nTo set up a similar environment you can start a new 2.7 envrionment:\n - `conda create -n nees python=2.7 ipykernel`\n - `conda install numba pandas`\n\n\nRun [tests](https://nose.readthedocs.io/en/latest/index.html)  with `nosetests` from the command line (after installing dependencies).\n",
        "createdAt": "2018-12-07T06:01:00.000Z",
        "updatedAt": "2019-07-25T03:42:06.000Z",
        "language": "Jupyter Notebook",
        "homepage": "http://nees.ucsb.edu/data-portal",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/noah-de/NEES/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "dsiervo/sc3-autotuner",
        "url": "https://github.com/dsiervo/sc3-autotuner",
        "description": "Bayesian optimization approach for tuning SeisComP3's scautopick and scanloc modules  ",
        "stars": 3,
        "forks": 1,
        "readme": "# sc3-autotuner\nBayesian optimization approach for tuning SeisComP's `scautopick` module.\n\nAutomatic parameter tuning for the SeisComP `scautopick` module using Bayesian optimization.\n\n# Installation\n\n## Requirements\n* SeisComP\n* SeisComP `scanloc` module\n* Ubuntu 18.04 LTS or higher\n\n1. Install the prerequisites for Python 3.8:\n\n    ```bash\n    $ sudo apt update\n    $ sudo apt install software-properties-common\n    ```\n\n2. Install Python 3.8:\n\n    ```bash\n    $ sudo add-apt-repository ppa:deadsnakes/ppa\n    $ sudo apt update\n    $ sudo apt install python3.8 python3.8-dev python3.8-venv python3.8-tk\n    ```\n\n3. Download the sc3-autotuner repository:\n\n    ```bash\n    $ git clone https://github.com/dsiervo/sc3-autotuner\n    $ cd sc3-autotuner\n    ```\n\n4. Create and activate a Python 3.8 virtual environment:\n\n    ```bash\n    $ python3.8 -m venv venv\n    $ source venv/bin/activate\n    ```\n\n5. Update pip:\n\n    ```bash\n    (venv) $ pip install --upgrade pip\n    ```\n\n6. Install dependencies:\n    Install headers and libraries for MySQL or MariaDB:\n    ```bash\n    (venv) $ sudo apt install default-libmysqlclient-dev\n    ```\n    Install Python dependencies:\n    ```bash\n    (venv) $ pip install -r requirements.txt\n    ```\n\n7. Add the S-AIC pick plugin to the `global.cfg` file of your SeisComP installation.\n\n    Open the file **seicomp3/etc/global.cfg** and add *saic* to the end of the plugins list:\n\n    ```bash\n    plugins = hypo71,mlr,saic\n    ```\n\n8. Configure S-AIC as a secondary picker for `scautopick`:\n\n    Open the file **seicomp3/etc/scautopick.cfg** (create it if it doesn’t exist) and add the following line (or replace the existing one):\n\n    ```bash\n    spicker = S-AIC\n    ```\n\n9. Deactivate the Python 3.8 environment and update the SeisComP3 configuration:\n\n    ```bash\n    (venv) $ deactivate\n    $ seiscomp update-config\n    ```\n\n10. Add sc3-autotuner to your PATH in the **~/.bashrc** file:\n\n    ```bash\n    $ echo 'export PATH=<path to sc3-autotuner>:$PATH' >> ~/.bashrc\n    $ source ~/.bashrc\n    ```\n\n11. Change the path to the sc3-autotuner folder in the first line of the **sc3autotuner.py** script.\n\n# Usage\nThe program reads parameters from the `sc3-autotuner.inp` file located in the execution directory.\n\n1. Copy the `sc3-autotuner.inp` file to your working directory.\n2. Modify the `sc3-autotuner.inp` file according to your preferences.\n3. Run the program:\n\n    ```bash\n    $ sc3autotuner.py\n    ```\n\n## sc3-autotuner.inp\nYou can add comments to the `sc3-autotuner.inp` file using the `#` character at the beginning of the line.\n\nThe following parameters are explained below:\n\n### Global Parameters\n**-** `tune_mode`: Controls whether to tune the picker (`picker`) or the associator (`asoc`). Currently, only `picker` is accepted.\n\n**-** `debug`: If `True`, displays debugging information and plots the results.\n\n**-** `deb_url`: IP address of the SQL server for querying picks and events.\n\n**-** `ti`: Start time for querying picks and events. Must be in the `YYYY-MM-DD HH:MM:SS` format.\n\n**-** `tf`: End time for querying picks and events. Must be in the `YYYY-MM-DD HH:MM:SS` format.\n\n**-** `inv_xml`: Path to the XML file containing inventory information. You can use the SeisComP3 [scxmldump](https://docs.gempa.de/seiscomp3/current/apps/scxmldump.html) module to generate the file.\n\n### Picker Mode Options\n**-** `stations`: Comma-separated list of stations in the format `<network>.<station>.<location>.<channel without component>`. For example:\n\n    stations = IU.ANMO.10.BH, CM.BAR2.00.HH, CM.DBB.20.EH\n\n**-** `fdsn_ip`: IP address and port of the FDSN server for downloading waveforms (in SeisComP3, the FDSN server IP is usually the same as the SQL server IP with port 8091).\n\n**-** `max_picks`: Maximum number of manual picks per station to use in tuning.\n\n**-** `n_trials`: Number of attempts the program will make to tune the picker for each phase and station.\n\n## Program Output\n### Important Files\n* Folder `output_station_files`: Contains the tuning results for each station in the appropriate format for incorporation into SeisComP3. To add these results to your system, copy these files to the `seicomp3/etc/key/scautopick` directory and add the following line to the corresponding files in the `seicomp3/etc/key/` path:\n\n      scautopick\n\n  SeisComP3 will now recognize these stations for use by the `scautopick` autopicker and use the parameters specified in the station configuration file in the `seicomp3/etc/key/scautopick` directory.\n\n* Folder `images`: Contains interactive plots of the tuning process for each phase and station. These can be opened with any web browser.\n* Files `results_P.csv` and `results_S.csv`: Compile the best parameters found for each phase and station along with the F1-score value for that iteration.\n\n### Residual Files and Folders\n* `exc_<station>_<phase>.xml`: Contains the picker parameters for the last iteration.\n* `mseed_data`: Folder containing the waveforms used in the tuning process.\n* `picks_xml`: Folder containing XML files in SeisComP3 format with the picks generated in the last iteration.\n",
        "createdAt": "2021-06-22T16:01:37.000Z",
        "updatedAt": "2025-12-04T21:55:32.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/dsiervo/sc3-autotuner/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "aradfarahani/awesome-geophysics",
        "url": "https://github.com/aradfarahani/awesome-geophysics",
        "description": "Awesome Geophysics is a community-curated resource offering essential tools, datasets, and educational materials for geophysical exploration. It’s designed to empower students, researchers, and professionals to analyze data, model Earth processes, and stay connected with the latest industry trends.",
        "stars": 81,
        "forks": 17,
        "readme": "[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)\n<a href=\"https://github.com/aradfarahani/awesome-geophysics\"><img src=\"https://img.shields.io/static/v1?label=%F0%9F%8C%9F&message=If%20Useful&style=flat&color=BC4E99\" alt=\"Star Badge\"/></a>\n<a href=\"https://github.com/aradfarahani/awesome-geophysics/graphs/contributors\"><img alt=\"GitHub contributors\" src=\"https://img.shields.io/github/contributors/aradfarahani/awesome-geophysics?color=2b9348\"></a>\n[![License](https://img.shields.io/github/license/aradfarahani/awesome-geophysics.svg)](https://github.com/aradfarahani/awesome-geophysics/blob/main/LICENSE)\n[![Commits](https://img.shields.io/github/last-commit/aradfarahani/awesome-geophysics.svg?label=last%20contribution)](https://github.com/aradfarahani/awesome-geophysics/commits/main) [![GitHub stars](https://img.shields.io/github/stars/aradfarahani/awesome-geophysics?style=social)](https://github.com/aradfarahani/awesome-geophysics/stargazers) [![GitHub Forks](https://img.shields.io/github/forks/aradfarahani/awesome-geophysics?style=social)](https://github.com/aradfarahani/awesome-geophysics/forks)\n# Awesome Geophysics\n\n# [<img src=\"https://cdn.rawgit.com/aradfarahani/awesome-geophysics/main/cover.png\">](https://github.com/aradfarahani/awesome-geophysics)\n\n<p align=\"justify\">\nWelcome to <strong>Awesome Geophysics</strong> – A community-curated collection of geophysical resources including software, datasets, educational materials, and more. `Test` Whether you're a student just beginning your journey, a researcher pushing the boundaries of the field, or a professional applying cutting-edge methods, this guide is your one-stop destination for software, datasets, educational materials, and much more. Let's explore the Earth's hidden depths and stay connected with the vibrant global geophysics community!\n</p>\n\n---\n\n## Table of Contents\n- [Software and Tools](#software-and-tools)\n- [Datasets and Databases](#datasets-and-databases)\n- [Educational Resources](#educational-resources)\n  - [Textbooks](#textbooks)\n    - [Fundamentals of Seismology and Earth Structure](#fundamentals-of-seismology-and-earth-structure)\n    - [Geodynamics and Earth's Interior](#geodynamics-and-earth's-interior)\n    - [Exploration Geophysics](#exploration-geophysics)\n    - [Mathematical and Computational Geophysics](#mathematical-and-computational-geophysics)\n    - [Specialized Topics in Geophysics](#specialized-topics-in-geophysics)\n  - [Online Courses and Tutorials](#online-courses-and-tutorials)\n  - [Workshops and Webinars](#workshops-and-webinars)\n  - [University Programs and Certificates](#university-programs-and-certificates)\n- [Research Papers and Journals](#research-papers-and-journals)\n- [Tutorials and Cheat Sheets](#tutorials-and-cheat-sheets)\n- [Organizations and Societies](#organizations-and-societies)\n  - [Global Organizations](#global-organizations)\n  - [Regional and National Societies](#regional-and-national-societies)\n  - [Professional Networks and Communities](#professional-networks-and-communities)\n- [Conferences and Events](#conferences-and-events)\n- [Blogs, Podcasts, and Community Forums](#blogs-podcasts-and-community-forums)\n  - [Blogs](#blogs)\n  - [Technical Blogs](#technical-blogs)\n  - [Podcasts](#podcasts)\n  - [Community Forums and Social Media](#community-forums-and-social-media)\n- [Career and Professional Development](#career-and-professional-development)\n- [Industry News and Updates](#industry-news-and-updates)\n  - [Industry Publications](#industry-publications)\n  - [Online News Platforms](#online-news-platforms)\n  - [Market Analysis and Research Reports](#market-analysis-and-research-reports)\n  - [Press Releases and Corporate Blogs](#press-releases-and-corporate-blogs)\n  - [Government and Regulatory Updates](#government-and-regulatory-updates)\n  - [Industry Webinars and Live Updates](#industry-webinars-and-live-updates)\n  - [Geophysical Technology Startups](#geophysical-technology-startups)\n- [Miscellaneous Resources](#miscellaneous-resources)\n  - [Data Visualization Libraries](#data-visualization-libraries)\n  - [Shell Scripting & Automation (Bash)](#shell-scripting--automation-(bash))\n  - [Programming and Scripting Resources](#programming-and-scripting-resources)\n  - [Technical Blogs and Code Tutorials](#technical-blogs-and-code-tutorials)\n  - [Professional Books and eBooks](#professional-books-and-ebooks)\n  - [Software Development Tools](#software-development-tools)\n- [Contributors](#contributors)\n- [How to Contribute](#how-to-contribute)\n- [License](#license)\n\n| ▲ [Top](#awesome-geophysics) |\n| --- |\n---\n\n## Software and Tools\n\nEnhance your geophysical workflows with these essential software solutions:\n\n| **Name** | **Description** | **GitHub Stars** |\n|----------|-----------------| -----------------|\n| **[`Auralib`](https://github.com/whamlyn/auralib)** | Python package to support investigation of geoscience problems including geophysics, rock physics, petrophysics, and data read/write in common formats. | [![GitHub stars](https://img.shields.io/github/stars/whamlyn/auralib?style=social)](https://github.com/whamlyn/auralib/stargazers) |\n| **[`bh_tomo`](https://github.com/groupeLIAMG/bh_tomo)** | Numba-accelerated computation of surface wave dispersion. | [![GitHub stars](https://img.shields.io/github/stars/groupeLIAMG/bh_tomo?style=social)](https://github.com/groupeLIAMG/bh_tomo/stargazers) |\n| **[`Bruges`](https://github.com/agilescientific/bruges/tree/main)** | Various geophysical equations and tools. | [![GitHub stars](https://img.shields.io/github/stars/agilescientific/bruges?style=social)](https://github.com/agilescientific/bruges/stargazers) |\n| **[`celeri`](https://github.com/brendanjmeade/celeri)** | A python-based package designed to image earthquake cycle activity including the spatial and time varying fault coupling across geometrically complex fault systems at large scales. | [![GitHub stars](https://img.shields.io/github/stars/brendanjmeade/celeri?style=social)](https://github.com/brendanjmeade/celeri/stargazers) |\n| **[`deepwave`](https://github.com/ar4/deepwave)** | Deepwave provides wave propagation modules for PyTorch, for applications such as seismic imaging/inversion. You can use it to perform forward modelling and backpropagation. | [![GitHub stars](https://img.shields.io/github/stars/ar4/deepwave?style=social)](https://github.com/ar4/deepwave/stargazers) |\n| **[`disba`](https://github.com/keurfonluu/disba)** | Borehole radar and seismic tomography package. | [![GitHub stars](https://img.shields.io/github/stars/keurfonluu/disba?style=social)](https://github.com/keurfonluu/disba/stargazers) |\n| **[`emsig`](https://emsig.xyz/)** | Controlled-source electromagnetic modellers for layered (`empymod`) and three-dimensional (`emg3d`) anisotropic media. |  |\n| **[`EQcorrscan`](https://github.com/eqcorrscan/EQcorrscan)** | A python package for the detection and analysis of repeating and near-repeating earthquakes. | [![GitHub stars](https://img.shields.io/github/stars/eqcorrscan/EQcorrscan?style=social)](https://github.com/eqcorrscan/EQcorrscan/stargazers) |\n| **[`Fatiando a Terra`](https://www.fatiando.org/)** | A Python toolkit for geophysical modeling, ideal for addressing gravity, magnetics, and seismic challenges. |  |\n| **[`first_break_picking`](https://github.com/geo-stack/first_break_picking)** | A Python package for automatic first break picking in seismic data using deep learning. | [![GitHub stars](https://img.shields.io/github/stars/geo-stack/first_break_picking?style=social)](https://github.com/geo-stack/first_break_picking/stargazers) |\n| **[`Front End`](https://geogiga.com/products/frontend/)** | Preprocess seismic data with various functions, such as resampling, vertical stacking, nodal data assembling, trace swapping, and format conversion. |  |\n| **[`GemPy`](https://github.com/gempy-project/gempy)** | 3-D structural geological modelling software with implicit modelling and support for stochastic modelling. | [![GitHub stars](https://img.shields.io/github/stars/gempy-project/gempy?style=social)](https://github.com/gempy-project/gempy/stargazers) |\n| **[`Geoelectricspy`](https://github.com/aradfarahani/Geoelectricspy)** | An interactive 3D visualization tool for subsurface resistivity data—perfect for understanding geoelectric structures. | [![GitHub stars](https://img.shields.io/github/stars/aradfarahani/Geoelectricspy?style=social)](https://github.com/aradfarahani/Geoelectricspy/stargazers) |\n| **[`GeoPhyInv`](https://github.com/pawbz/GeoPhyInv.jl)** | Julia Toolbox for Geophysical Modeling and Inverse Problems. | [![GitHub stars](https://img.shields.io/github/stars/pawbz/GeoPhyInv.jl?style=social)](https://github.com/pawbz/GeoPhyInv.jl/stargazers) |\n| **[`Geopsy`](http://www.geopsy.org/)** | A tool for processing ambient vibration data, widely used in site characterization and microzonation studies. |  |\n| **[`GMT (Generic Mapping Tools)`](https://www.generic-mapping-tools.org/)** | A robust toolset for creating high-quality maps and plots used across geoscience disciplines. |  |\n| **[`gprMax`](http://www.gprmax.com)** | A comprehensive Python library for seismology, perfect for waveform analysis, data handling, and visualization. |  |\n| **[`GPRPy`](https://github.com/NSGeophysics/GPRPy)** | Multi-format, GUI-based GPR processing and visualization. | [![GitHub stars](https://img.shields.io/github/stars/NSGeophysics/GPRPy?style=social)](https://github.com/NSGeophysics/GPRPy/stargazers) |\n| **[`GSadjust`](https://code.usgs.gov/sgp/gsadjust)** | GSadjust is a cross-platform graphical interface for processing relative gravity surveys. It provides an interface for data selection, drift evaluation and correction, network adjustment, and integrating data from modern relative (Scintrex, ZLS) and absolute (Micro-g Lacoste) gravity meters.  |  |\n| **[`hvsrpy`](https://github.com/jpvantassel/hvsrpy)** | A Python package for horizontal-to-vertical spectral ratio processing. | [![GitHub stars](https://img.shields.io/github/stars/jpvantassel/hvsrpy?style=social)](https://github.com/jpvantassel/hvsrpy/stargazers) |\n| **[`hypopy`](https://github.com/groupeLIAMG/hypopy)** | HYPOcenter location from arrival time data in PYthon. | [![GitHub stars](https://img.shields.io/github/stars/groupeLIAMG/hypopy?style=social)](https://github.com/groupeLIAMG/hypopy/stargazers) |\n| **[`Madagascar`](http://www.ahay.org/wiki/Main_Page)** | An open-source platform designed for reproducible geophysical data processing and analysis. |  |\n| **[`MDIO`](https://mdio.dev)** | Open source chunked and compressed cloud storage for seismic data based on Zarr with fast seismic ingestion and export tools – [Docs](https://mdio-python.readthedocs.io), [Source](https://github.com/TGSAI/mdio-python). |  |\n| **[`modelr.io`](https://github.com/agilescientific/modelr)** | Web app for simple synthetic seismic forward modelling. | [![GitHub stars](https://img.shields.io/github/stars/agilescientific/modelr?style=social)](https://github.com/agilescientific/modelr/stargazers) |\n| **[`ObsPy`](https://github.com/obspy/obspy)** | A comprehensive Python library for seismology, perfect for waveform analysis, data handling, and visualization. | [![GitHub stars](https://img.shields.io/github/stars/obspy/obspy?style=social)](https://github.com/obspy/obspy/stargazers) |\n| **[`OpendTect-Plugins`](https://github.com/waynegm/OpendTect-Plugins)** | Open source plugins for the OpendTect seismic interpretation platform. See [the docs](http://waynegm.github.io/OpendTect-Plugin-Docs) for more information. | [![GitHub stars](https://img.shields.io/github/stars/waynegm/OpendTect-Plugins?style=social)](https://github.com/waynegm/OpendTect-Plugins/stargazers) |\n| **[`OpenSeaSeis`](https://github.com/JohnWStockwellJr/OpenSeaSeis)** | Seismic workflow generator and seismic viewer. | [![GitHub stars](https://img.shields.io/github/stars/JohnWStockwellJr/OpenSeaSeis?style=social)](https://github.com/JohnWStockwellJr/OpenSeaSeis/stargazers) |\n| **[`Pastas`](https://github.com/pastas/pastas)** | Open-source Python framework for the analysis of groundwater time series. | [![GitHub stars](https://img.shields.io/github/stars/pastas/pastas?style=social)](https://github.com/pastas/pastas/stargazers) |\n| **[`pyekfmm`](https://github.com/aaspip/pyekfmm)** | A python package for 3D fast-marching-based traveltime calculation and its applications in seismology. | [![GitHub stars](https://img.shields.io/github/stars/aaspip/pyekfmm?style=social)](https://github.com/aaspip/pyekfmm/stargazers) |\n| **[`PyFWI`](https://pyfwi.readthedocs.io/en/latest)** | It can be used to perform full-waveform inversion (FWI) and time-lapse FWI of seismic data. |  |\n| **[`pyGeoPressure`](https://pygeopressure.readthedocs.io/en/latest)** | Pore pressure prediction using well log data and seismic velocity data. |  |\n| **[`PyGIMLi`](https://www.pygimli.org/)** | A library for geophysical inversion and modeling with an emphasis on ease of use. |  |\n| **[`PyLops`](https://pylops.readthedocs.io/en/latest)** | Linear Operators with some geophysics/seismic modules (e.g., pre- and post-stack AVO inversion, deconvolution, Marchenko redatuming, Radon filtering). |  |\n| **[`Pyrocko`](https://pyrocko.org/)** | A toolkit for seismic waveform analysis and earthquake modeling. |  |\n| **[`PySeis`](https://github.com/stuliveshere/PySeis)** | Pure python seismic data processing | [![GitHub stars](https://img.shields.io/github/stars/stuliveshere/PySeis?style=social)](https://github.com/stuliveshere/PySeis/stargazers) |\n| **[`PySIT`](http://pysit.org)** | A Toolbox for seismic inversion and seismic imaging. |  |\n| **[`pyVDS`](https://github.com/equinor/pyvds)** | Convenience wrapper around Bluware's OpenVDS+ Python bindings which enables reading of VDS files with a syntax familiar to users of segyio. | [![GitHub stars](https://img.shields.io/github/stars/equinor/pyvds?style=social)](https://github.com/equinor/pyvds/stargazers) |\n| **[`pyZGY`](https://github.com/equinor/pyzgy)** | Convenience wrapper around Schlumberger's OpenZGY Python package which enables reading of ZGY files with a syntax familiar to users of segyio. | [![GitHub stars](https://img.shields.io/github/stars/equinor/pyzgy?style=social)](https://github.com/equinor/pyzgy/stargazers) |\n| **[`R2`](http://www.es.lancs.ac.uk/people/amb/Freeware/R2/R2.htm)** | A forward/inverse solution for 3D or 2D current flow in quadrilateral or triangular meshes. |  |\n| **[`RAGU`](https://github.com/btobers/RAGU)** | Radar interpretation GUI compatible with multiple radar datasets. | [![GitHub stars](https://img.shields.io/github/stars/btobers/RAGU?style=social)](https://github.com/btobers/RAGU/stargazers) |\n| **[`readgssi`](https://github.com/iannesbitt/readgssi)** | Fast command line or console-based visualization, filtering, and translation of GSSI radar data. | [![GitHub stars](https://img.shields.io/github/stars/iannesbitt/readgssi?style=social)](https://github.com/iannesbitt/readgssi/stargazers) |\n| **[`RECAST`](https://github.com/keliankaz/recast)** | Flexible and Scalable Earthquake Forecasting. | [![GitHub stars](https://img.shields.io/github/stars/keliankaz/recast?style=social)](https://github.com/keliankaz/recast/stargazers) |\n| **[`RedPy`](https://github.com/ahotovec/REDPy)** | Auto-clustering for seismic events. | [![GitHub stars](https://img.shields.io/github/stars/ahotovec/REDPy?style=social)](https://github.com/ahotovec/REDPy/stargazers) |\n| **[`Refrapy`](https://github.com/AlgoSismos/Refrapy)** | A program for seismic refraction data processing. | [![GitHub stars](https://img.shields.io/github/stars/AlgoSismos/Refrapy?style=social)](https://github.com/AlgoSismos/Refrapy/stargazers) |\n| **[`ResIPy`](https://gitlab.com/hkex/resipy)** | A Python wrapper around the R2 family of codes (for 2D/3D DC/IP inversion). |  |\n| **[`RGPR`](https://github.com/emanuelhuber/RGPR)** | Reads, exports, processes, and plots ground-penetrating radar data. | [![GitHub stars](https://img.shields.io/github/stars/emanuelhuber/RGPR?style=social)](https://github.com/emanuelhuber/RGPR/stargazers) |\n| **[`rsudp`](https://github.com/raspishake/rsudp)** | Continuous ObsPy-based visual display, sudden motion monitoring, and historical replay of Raspberry Shake data. | [![GitHub stars](https://img.shields.io/github/stars/raspishake/rsudp?style=social)](https://github.com/raspishake/rsudp/stargazers) |\n| **[`SAC (Seismic Analysis Code)`](https://ds.iris.edu/ds/nodes/dmc/software/downloads/sac/)** | A go-to tool for seismic waveform analysis, offering powerful data manipulation for seismologists worldwide. |  |\n| **[`Segyio`](https://github.com/equinor/segyio)** | Fast library for seismic SEGY files. | [![GitHub stars](https://img.shields.io/github/stars/equinor/segyio?style=social)](https://github.com/equinor/segyio/stargazers) |\n| **[`SeisComp`](https://github.com/SeisComP/seiscomp)** | Seismic observatory automation toolkit. Autodetection, storage, sharing, processing data and more. | [![GitHub stars](https://img.shields.io/github/stars/SeisComP/seiscomp?style=social)](https://github.com/SeisComP/seiscomp/stargazers) |\n| **[`Seismic Un*x`](https://wiki.seismic-unix.org/start)** | A powerful toolkit for seismic data processing. |  |\n| **[`Seismic_BPMF`](https://github.com/ebeauce/Seismic_BPMF)** | Complete framework for earthquake detection and location: Backprojection and matched-filtering (BPMF), with methods for automatic picking, relocation and efficient waveform stacking. | [![GitHub stars](https://img.shields.io/github/stars/ebeauce/Seismic_BPMF?style=social)](https://github.com/ebeauce/Seismic_BPMF/stargazers) |\n| **[`SeismicZFP`](https://github.com/equinor/seismic-zfp)** | Convert SEG-Y/ZGY files to compressed [SGZ files](https://github.com/equinor/seismic-zfp/blob/master/docs/file-specification.md) & retrieve arbitrary sub-volumes from these, fast. | [![GitHub stars](https://img.shields.io/github/stars/equinor/seismic-zfp?style=social)](https://github.com/equinor/seismic-zfp/stargazers) |\n| **[`SeisUnix`](https://github.com/JohnWStockwellJr/SeisUnix)** | A classic suite for seismic reflection data processing, widely used in both academia and industry. | [![GitHub stars](https://img.shields.io/github/stars/JohnWStockwellJr/SeisUnix?style=social)](https://github.com/JohnWStockwellJr/SeisUnix/stargazers) |\n| **[`SeisWiz`](https://github.com/amustafa9/SeisWiz)** | The ultimate lightweight Matplotlib-based seismic volume viewer with multi-view support and horizon visualization capabilities. | [![GitHub stars](https://img.shields.io/github/stars/amustafa9/SeisWiz?style=social)](https://github.com/amustafa9/SeisWiz/stargazers) |\n| **[`spyro`](https://github.com/NDF-Poli-USP/spyro)** | Seismic parallel inversion and reconstruction optimization framework | [![GitHub stars](https://img.shields.io/github/stars/NDF-Poli-USP/spyro?style=social)](https://github.com/NDF-Poli-USP/spyro/stargazers) |\n| **[`Surfer`](https://www.goldensoftware.com/products/surfer/)** | A contouring and 3D mapping software, great for visualizing subsurface geophysical data. |  |\n| **[`SWIT`](https://github.com/seisfwi/SWIT)** | Seismic Waveform Inversion Toolbox (SWIT-1.0) is a 2-D acoustic Full-waveform Inversion (FWI) package implemented in Fortran and Python. | [![GitHub stars](https://img.shields.io/github/stars/seisfwi/SWIT?style=social)](https://github.com/seisfwi/SWIT/stargazers) |\n| **[`swprocess`](https://github.com/jpvantassel/swprocess)** | A Python package for surface wave processing. | [![GitHub stars](https://img.shields.io/github/stars/jpvantassel/swprocess?style=social)](https://github.com/jpvantassel/swprocess/stargazers) |\n| **[`synthoseis`](https://github.com/sede-open/synthoseis)** | Synthoseis is an open-source, Python-based tool used for generating pseudo-random seismic data. | [![GitHub stars](https://img.shields.io/github/stars/sede-open/synthoseis?style=social)](https://github.com/sede-open/synthoseis/stargazers) |\n| **[`veros`](https://github.com/team-ocean/veros)** | Veros, *the versatile ocean simulator*, aims to be the swiss army knife of ocean modeling. It is a full-fledged primitive equation ocean model that supports anything between idealized toy models and [realistic, high-resolution, global ocean simulations](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021MS002717). | [![GitHub stars](https://img.shields.io/github/stars/team-ocean/veros?style=social)](https://github.com/team-ocean/veros/stargazers) |\n| **[`XTgeo`](https://xtgeo.readthedocs.io/en/latest)** | Python library with C backend to support manipulation of (oil industry) subsurface reservoir modelling. |  |\n\n\n| ▲ [Top](#awesome-geophysics) |\n| --- |\n---\n\n## Datasets and Databases\n\nAccess raw and processed geophysical data from reputable sources:\n\n- **[`GeoMapApp`](https://www.geomapapp.org/data_set_news.html)**\n  GeoMapApp is a map-based application for browsing, visualizing and analyzing a diverse suite of curated global and regional geoscience data sets.\n- **[`Global Seismographic Network (GSN)`](https://www.iris.edu/hq/programs/gsn)**\n  High-quality seismic recordings for deep Earth studies.\n- **[`ICGEM`](http://icgem.gfz-potsdam.de/home)**\n  Hosts gravity field spherical harmonic models and provides a webservice for generating grids of gravity functionals (geoid, gravity anomaly, vertical derivatives, etc).\n- **[`International Seismological Centre (ISC)`](https://www.isc.ac.uk/)**\n  The ultimate catalog of seismic events worldwide, with detailed phase data for researchers.\n- **[`IRIS Data Management Center`](https://ds.iris.edu/ds/nodes/dmc/)**\n  A comprehensive repository of seismic data from global networks, crucial for earthquake and tectonics studies.\n- **[`NGDC (National Geophysical Data Center)`](https://www.ngdc.noaa.gov/)**\n  A vast archive of geophysical data—think marine gravity, terrestrial magnetics, and bathymetry. Now part of NOAA's NCEI.\n- **[`NOAA National Centers for Environmental Information (NCEI)`](https://www.ncei.noaa.gov/)**\n  A goldmine of geomagnetic, gravity, and climate data—ideal for interdisciplinary geophysical studies.\n- **[`Poseidon NW Australia`](https://drive.google.com/drive/folders/0B7brcf-eGK8Cbk9ueHA0QUU4Zjg)**\n  Interpreted 3D seismic (32bit) including reports and well logs.\n- **[`Quantarctica`](https://www.npolar.no/quantarctica/#toggle-id-2-closed)**\n  The Quantarctica data package comprises Antarctic geographic data from data centres worldwide – all wrapped in a project file that works on QGIS.\n- **[`SEG Open Data Catalog`](https://wiki.seg.org/wiki/Open_data)**\n  Catalog of \"geophysical data that is readily available for download from the internet, via mail, or through special request\", maintained by the Society of Exploration Geophysicists.\n- **[`TerraNubis`](https://terranubis.com/datalist/free)**\n  TerraNubis is a cloud-based portal for buying, selling and interpreting seismic data sets and interpretations.\n- **[`USGS Earthquake Hazards Program`](https://earthquake.usgs.gov/)**\n  Provides real-time and historical earthquake data along with hazard maps for seismic research.\n\n| ▲ [Top](#awesome-geophysics) |\n| --- |\n---\n\n## Educational Resources\n\nBuild and refine your geophysics knowledge with these top-tier learning materials:\n\n\n### Textbooks\n\nComprehensive learning materials covering various aspects of geophysics:\n\n\n#### Fundamentals of Seismology and Earth Structure\n\nEssential reading for understanding seismic wave theory and earthquake science:\n\n  - **[`Fundamentals of Geophysics by William Lowrie`](https://www.cambridge.org/highereducation/books/fundamentals-of-geophysics/B6345BD682B6586F75EC40CB84A18EB1#overview)**\n    A well-structured introduction to the principles of geophysics, with clear explanations of gravity, magnetism, and seismology.\n  - **[`Introduction to Seismology by Peter Shearer`](https://www.cambridge.org/highereducation/books/introduction-to-seismology/C1471C1B553C05997E2BC7EB26D4C26D#overview)**\n    A comprehensive guide to seismic wave theory and earthquake science, ideal for students and professionals seeking a solid foundation in seismology.\n  - **[`Modern Global Seismology by Thorne Lay and Terry C. Wallace`](https://www.elsevier.com/books/modern-global-seismology/lay/978-0-12-732870-6)**\n    Explains the principles of seismology at a global scale, including earthquake dynamics and deep Earth structure.\n  - **[`Quantitative Seismology by Keiiti Aki and Paul G. Richards`](https://www.amazon.com/Quantitative-Seismology-Keiiti-Aki/dp/0935702962)**\n    A rigorous and mathematical treatment of seismology, covering wave propagation, earthquake source mechanisms, and data interpretation.\n  - **[`The Seismic Analysis Code: A Primer and User's Guide by George Helffrich, Ian Bastow and James Wookey`](https://www.cambridge.org/core/books/seismic-analysis-code/B6841DCB6C7AE43DFD6D7A338D4141FD)**\n    A practical guide to using the SAC software for seismic data processing, widely used in academic and industry research.\n  - **[`Theoretical Global Seismology by F.A. Dahlen and Jeroen Tromp`](https://press.princeton.edu/books/paperback/9780691001241/theoretical-global-seismology)**\n    A rigorous treatment of global seismology, wave propagation, and normal mode theory.\n\n#### Geodynamics and Earth's Interior\n\nExploration of Earth's tectonic processes and deep structure:\n\n  - **[`Earth Structure: An Introduction to Structural Geology and Tectonics by Ben A. van der Pluijm and Stephen Marshak`](https://www.amazon.com/Earth-Structure-Introduction-Structural-Tectonics/dp/039392467X)**\n    Explores the deformation of Earth's crust and lithosphere, combining geophysics and geology.\n  - **[`Geodynamics by Donald Turcotte and Gerald Schubert`](https://www.cambridge.org/highereducation/books/geodynamics/E0E847DA9FE68BDB90C2E457791F0C98#overview)**\n    An in-depth exploration of Earth's tectonic processes, mantle convection, and planetary evolution.\n  - **[`The Solid Earth by C.M.R. Fowler`](https://www.cambridge.org/highereducation/books/the-solid-earth/0A66AC49BB6105F54AACF52D3C3121A4)**\n    A broad geophysics textbook covering seismology, gravity, geomagnetism, and heat flow.\n\n#### Exploration Geophysics\n\nResources for applied geophysics in exploration and industry:\n\n  - **[`Applied Geophysics by W. M. Telford, L. P. Geldart, and R. E. Sheriff`](https://www.cambridge.org/highereducation/books/applied-geophysics/44E5A30D0C486A109ED2259E9BB6FDFE#overview)**\n    Covers all major geophysical methods, including gravity, magnetics, electrical resistivity, and seismic exploration.\n  - **[`Exploration Seismology by Sheriff and Geldart`](https://www.cambridge.org/core/books/exploration-seismology/CC00727A219943E62F6FF01426DBA9D2)**\n    The bible of seismic exploration—perfect for applied geophysicists in the oil, gas, and mineral industries.\n  - **[`Fundamentals of Geophysical Data Processing by Jon F. Claerbout`](https://www.amazon.com/Fundamentals-Geophysical-Data-Processing-Applications/dp/0935702873)**\n    A classic text on digital signal processing techniques in seismic imaging.\n  - **[`Gravity and Magnetic Exploration: Principles, Practices, and Applications by William J. Hinze, Ralph R. B. von Frese, and Afif H. Saad`](https://www.cambridge.org/highereducation/books/gravity-and-magnetic-exploration/45D1D0D99A67300D5168C5A23AB848E2#overview)**\n    An essential resource for understanding potential field geophysics in mineral and hydrocarbon exploration.\n  - **[`Principles of Geophysical Exploration by D.S. Parasnis`](https://www.geokniga.org/bookfiles/geokniga-principles-applied-geophysics.pdf)**\n    A solid introduction to geophysical exploration methods, with an emphasis on data interpretation.\n\n#### Mathematical and Computational Geophysics\n\nAdvanced mathematical and computational techniques for geophysical analysis:\n\n  - **[`Computational Seismology: A Practical Introduction by Heiner Igel`](https://academic.oup.com/book/26503)**\n    Provides a hands-on approach to numerical methods in seismology, including wave equation solvers and finite difference techniques.\n  - **[`Geophysical Data Analysis: Discrete Inverse Theory by William Menke`](https://www.elsevier.com/books/geophysical-data-analysis/menke/978-0-12-397160-9)**\n    Covers mathematical techniques for solving inverse problems in geophysics.\n  - **[`Global Optimization Methods in Geophysical Inversion by Mrinal K. Sen and Paul L. Stoffa`](https://www.amazon.com/Global-Optimization-Methods-Geophysical-Inversion/dp/1108445845)**\n    Essential for those working on inversion problems in geophysics, with a focus on optimization techniques.\n  - **[`Inverse Problem Theory and Methods for Model Parameter Estimation by Albert Tarantola`](https://www.siam.org/Publications/Books/CS06)**\n    A must-read for geophysicists working with inverse modeling, parameter estimation, and uncertainty analysis.\n  - **[`Numerical Methods of Exploration Seismology by Gary F. Margrave`](https://www.cambridge.org/core/books/numerical-methods-of-exploration-seismology/53A21CAD45D4047D117191E6BF4408E2)**\n    A concise look at applying machine learning to seismic interpretation, geophysical inversion, and subsurface exploration using advanced numerical methods.\n\n#### Specialized Topics in Geophysics\n\nFocused resources on specific geophysical sub-disciplines:\n\n  - **[`An Introduction to Applied and Environmental Geophysics by John M. Reynolds`](https://www.geokniga.org/bookfiles/geokniga-introduction-applied-and-environmental-geophysics_1.pdf)**\n    Focuses on the use of geophysical methods in environmental and engineering applications.\n  - **[`Electromagnetic Methods in Applied Geophysics by Misac N. Nabighian`](https://www.amazon.com/Electromagnetic-Methods-Applied-Geophysics-Investigations/dp/1560800532)**\n    A definitive reference on electromagnetic techniques for subsurface imaging and mineral exploration.\n  - **[`Geomagnetism by Masaru Kono`](https://www.nhbs.com/geomagnetism-treatise-on-geophysics-book)**\n    Covers the physics of the Earth's magnetic field, paleomagnetism, and dynamo theory.\n  - **[`Gravity and Magnetic Exploration, Principles, Practices, and Applications by William J. Hinze, Ralph R. B. von Frese and Afif H. Saad`](https://www.cambridge.org/de/universitypress/subjects/earth-and-environmental-science/solid-earth-geophysics/gravity-and-magnetic-exploration-principles-practices-and-applications?format=HB&isbn=9780521871013)**\n    A succinct overview of gravity and magnetic exploration, covering core principles, practical techniques, and their applications in geophysical studies.\n  - **[`Plate Tectonics: A Very Short Introduction by Peter Molnar`](https://global.oup.com/academic/product/plate-tectonics-a-very-short-introduction-9780198728269)**\n    A concise introduction to the principles of plate tectonics and its geophysical implications.\n  - **[`Seismic Tomography: Theory and Practice by H.M. Iyer and K. Hirahara`](https://www.amazon.com/Seismic-Tomography-Theory-H-M-Iyer/dp/041244230X)**\n    A foundational book on seismic tomography methods used to image Earth's interior.\n\n### Online Courses and Tutorials\n\nExpand your knowledge with these online courses and tutorials:\n\n - **[`ErSE 210 Seismology course`](https://github.com/DIG-Kaust/Seismology)**\n   Teaching material for ErSE 210 Seismology course *GitHub Repo*.\n - **[`Remote Online Sessions for Emerging Seismologists (ROSES)`](https://github.com/roseseismo)**\n   Targeted towards advanced Ph.D. students, who have used Python before and are familiar navigating in Linux/Unix. Lectures cover topics at the intersection of Seismology and Data Science.\n - **[`Theory-of-seismic-waves-II`](https://github.com/daniel-koehn/Theory-of-seismic-waves-II)**\n   Course material for the lecture `Theory of seismic waves II` (SS 2019) at the Institute of Geosciences (Department of Geophysics), Christian-Albrechts-University Kiel\n\n### Workshops and Webinars\n\nInteractive learning opportunities from leading institutions:\n\n - **[`Annual Seismology Skill Building Workshop for Undergraduates`](https://www.earthscope.org/)**\n   Organized by Miami University and EarthScope Consortium.\n\n### University Programs and Certificates\n\nExplore graduate programs, summer schools, and certificate courses in geosciences for deeper academic training.\n\n\n| ▲ [Top](#awesome-geophysics) |\n| --- |\n---\n\n## Research Papers and Journals\n\nKeep abreast of the latest discoveries and methods in geophysics:\n\n- **[`EarthArXiv`](https://eartharxiv.org/)**\n  A preprint server for cutting-edge geophysical research prior to formal publication.\n- **[`Geophysical Research Letters`](https://agupubs.onlinelibrary.wiley.com/journal/19448007)**\n  Rapid publications presenting high-impact research across various geophysical fields.\n- **[`Geophysics`](https://library.seg.org/journal/gpysa7)**\n  The leading journal for applied geophysics and exploration techniques.\n\n| ▲ [Top](#awesome-geophysics) |\n| --- |\n---\n\n## Tutorials and Cheat Sheets\n\n- **[`Geophysics Cheat Sheet`](https://static.squarespace.com/static/549dcda5e4b0a47d0ae1db1e/54a06d6ee4b0d158ed95f696/54a06d70e4b0d158ed9603f5/1350658645407/Cheatsheet_geophysics.pdf)**\n  Cheat Sheet for Geophysics.\n- **[`Petroleum Science Cheat Sheet`](https://static.squarespace.com/static/549dcda5e4b0a47d0ae1db1e/54a06d6ee4b0d158ed95f696/54a06d6fe4b0d158ed96019e/1323808738753/Cheatsheet_petroleum.pdf)**\n  Cheat Sheet for Petroleum Science.\n- **[`Rock Physics Cheat Sheet`](https://static.squarespace.com/static/549dcda5e4b0a47d0ae1db1e/54a06d6ee4b0d158ed95f696/54a06d6fe4b0d158ed960042/1374593568367/Cheatsheet_Rock_Physics.pdf)**\n  Cheat Sheet for Rock Physics.\n\n| ▲ [Top](#awesome-geophysics) |\n| --- |\n---\n\n## Organizations and Societies\n\nNetwork with leading experts, researchers, and industry professionals in geophysics through these key organizations:\n\n\n### Global Organizations\n\nInternational organizations supporting geophysical research and collaboration:\n\n - **[`American Geophysical Union (AGU)`](https://www.agu.org/)**\n   A hub for geoscientists offering resources, events, and professional networking across disciplines.\n - **[`European Association of Geoscientists and Engineers (EAGE)`](https://www.eage.org/)**\n   Offers training, research, and networking opportunities for geoscience professionals.\n - **[`International Association of Geodesy (IAG)`](https://iag-aig.org/)**\n   Covers geophysical research related to Earth's gravity, rotation, and deformation.\n - **[`International Association of Seismology and Physics of the Earth's Interior (IASPEI)`](https://iaspei.org/)**\n   Advances global seismology and deep Earth studies.\n - **[`International Union of Geodesy and Geophysics (IUGG)`](https://www.iugg.org/)**\n   A collective body supporting geophysical research worldwide.\n - **[`Seismological Society of America (SSA)`](https://www.seismosoc.org/)**\n   Specializing in earthquake science and research with valuable educational resources.\n - **[`Society of Exploration Geophysicists (SEG)`](https://seg.org/)**\n   Focused on applied geophysics, providing conferences, training, and publications.\n\n### Regional and National Societies\n\nRegional organizations supporting geophysical research and professionals:\n\n - **[`Australian Society of Exploration Geophysicists (ASEG)`](https://www.aseg.org.au/)**\n   A professional body for geophysicists working in exploration and mining.\n - **[`Brazilian Geophysical Society (SBGf)`](https://sbgf.org.br/)**\n   Organizes geophysics-related events, research, and publications in South America.\n - **[`Canadian Society of Exploration Geophysicists (CSEG)`](https://cseg.ca/)**\n   Provides networking, education, and professional development for geophysicists in Canada.\n - **[`Chinese Geophysical Society (CGS)`](http://www.geophys.cn/)**\n   Promotes geophysical advancements and collaboration in China.\n - **[`Geological Society of London (GSL)`](https://www.geolsoc.org.uk/)**\n   One of the world's oldest geological societies, supporting geophysics research.\n - **[`Indian Geophysical Union (IGU)`](https://iguonline.in/)**\n   Focuses on geophysical research in seismology, hydrology, and geodynamics.\n - **[`National Institute of Geological Sciences (NIGS)`](http://nigs.science.upd.edu.ph/)**\n   Based in the Philippines, dedicated to geophysical and geological research and education.\n - **[`National Iranian Geophysics Society (NIGS)`](https://nigs.ir/en/home-en/about-nigs-en/)**\n   Nonprofit organization for geophysicists and engineers. It is an organization providing a national network of geoscientists and academic professionals.\n - **[`Russian Geophysical Society (RGS)`](https://www.gsras.ru/)**\n   A key organization for seismic and electromagnetic research in Russia.\n\n### Professional Networks and Communities\n\nOnline platforms and communities for geophysics professionals:\n\n - **`Academic Mailing Lists`**\n   Join geophysics-focused lists like IRIS Seismology, SEG’s Technical Sections, and EAGE’s Discussion Groups.\n - **[`LinkedIn Groups`](https://www.linkedin.com/groups/)**\n   Join geophysics-related groups on LinkedIn to network and share knowledge.\n - **[`ResearchGate`](https://www.researchgate.net/)**\n   Connect with researchers, share publications, and collaborate on geophysical projects.\n\n| ▲ [Top](#awesome-geophysics) |\n| --- |\n---\n\n## Conferences and Events\n\nStay connected with the latest advancements through these key events:\n\n- **[`AGU Fall Meeting`](https://www.agu.org/fall-meeting)**\n  The world's largest geoscience conference featuring hundreds of sessions, workshops, and networking opportunities.\n- **[`European Geosciences Union General Assembly`](https://www.egu.eu/meetings/general-assembly/)**\n  A massive European event covering all things Earth and planetary science.\n- **[`SEG Annual Meeting`](https://seg.org/Events/Annual-Meeting)**\n  A premier event showcasing the latest in exploration geophysics, technology innovations, and research.\n- **[`SSA Annual Meeting`](https://www.seismosoc.org/meetings/)**\n  Focused on earthquake science and the latest seismological research.\n\n| ▲ [Top](#awesome-geophysics) |\n| --- |\n---\n\n## Blogs, Podcasts, and Community Forums\n\nEngage with the geophysics community through diverse media channels:\n\n\n### Blogs\n\nRegularly updated blogs covering geophysical topics and research:\n\n - **[`20 Best Geophysics Blogs and Websites`](https://science.feedspot.com/geophysics_blogs/)**\n   A curated list of top geophysics blogs, providing a broad spectrum of perspectives and topics in the field.\n - **[`AGU Blogosphere`](https://blogs.agu.org/)**\n   Hosted by the American Geophysical Union, this platform offers a diverse collection of blogs discussing the latest developments in geoscience and geophysics.\n - **[`Geology & Geophysics Blog – University of Southampton`](https://blog.soton.ac.uk/ggblog/)**\n   Provides insights into recent research, fieldwork, and developments in geology and geophysics.\n - **[`Geophysical Insights Blog`](https://www.geoinsights.com/blog/)**\n   Features articles on machine learning applications in geophysical data analysis and other contemporary topics.\n - **[`Geophysics Blog – University of Texas Institute for Geophysics`](https://ig.utexas.edu/category/news/geophysics-blog/)**\n   Shares updates on research projects, field studies, and scientific findings in geophysics.\n - **[`Geophysics Rocks`](https://geophysicsrocks.com/)**\n   An engaging blog that breaks down complex geophysical concepts in an accessible way.\n\n### Technical Blogs\n\nTechnical resources and advanced geophysical discussions:\n\n - **[`Data-Driven Geophysics: From Dictionary Learning to Deep Learning`](https://arxiv.org/abs/2007.06183)**\n   A scholarly article discussing the evolution of data-driven approaches in geophysics.\n - **[`Deep Learning for Geophysics: Current and Future Trends`](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2021RG000742)**\n   Reviews the application of deep learning techniques in geophysical research.\n - **[`Fundamentals of Geophysical Data Processing - Stanford Exploration Project`](https://sep.sites.stanford.edu/fundamentals-geophysical-data-processing)**\n   Offers comprehensive insights into geophysical data processing techniques.\n - **[`Machine Learning Applications to Geophysical Data Analysis - UBC`](https://www.slim.eos.ubc.ca/content/machine-learning-applications-geophysical-data-analysis)**\n   Explores the integration of machine learning in geophysical data analysis.\n - **[`Subsurface Mapping by Ambient Noise Tomography`](https://en.wikipedia.org/wiki/Subsurface_mapping_by_ambient_noise_tomography)**\n   Provides an overview of ambient noise tomography techniques used in subsurface mapping.\n\n### Podcasts\n\nAudio content featuring geophysical discussions and interviews:\n\n - **[`Oxford University Geophysics Podcasts`](https://www.earth.ox.ac.uk/research/research-groups/seismology/seismology-podcasts/)**\n   The University of Oxford provides a series of podcasts featuring discussions on various geophysics topics, including carbon storage and mathematical aspects of the planet.\n - **[`Sci & Tell`](https://www.agu.org/Share-and-Advocate/Share/Sci-and-Tell)**\n   Another offering from the American Geophysical Union, this podcast features interviews and first-person stories from scientists, providing a personal perspective on scientific endeavors.\n - **[`Seismic Soundoff`](https://seg.org/podcast)**\n   Hosted by the Society of Exploration Geophysicists (SEG), this podcast offers in-depth conversations on applied geophysics, addressing challenges in energy, water, and climate.\n - **[`Third Pod from the Sun`](https://www.agu.org/Share-and-Advocate/Share/Third-Pod-from-the-Sun)**\n   Presented by the American Geophysical Union, this podcast delves into the stories behind the science, offering insights into various geoscience topics.\n\n### Community Forums and Social Media\n\nPlatforms for discussion and networking in geophysics:\n\n - **[`Geophysics Forums`](https://able2know.org/forum/geophysics/)**\n   Dedicated forums like the Geophysics Forum provide spaces to ask questions, share knowledge, and engage in discussions specific to geophysics.\n - **[`LinkedIn Groups`](https://www.linkedin.com/groups/122504/)**\n   Professional groups such as \"Geophysics Forum\" and \"Exploration Geophysics\" on LinkedIn offer opportunities to connect with industry experts, participate in discussions, and stay updated on industry trends.\n - **[`Reddit Communities`](https://www.reddit.com/r/geophysics/)**\n   Subreddits like r/geophysics and r/geology provide platforms for discussions, Q&A, and sharing the latest research and news in the field.\n - **[`Society of Exploration Geophysicists (SEG) Online Communities`](https://seg.org/About-SEG/Communities)**\n   SEG offers various online platforms, including forums and social media groups, where professionals can discuss topics related to exploration geophysics.\n\n| ▲ [Top](#awesome-geophysics) |\n| --- |\n---\n\n## Career and Professional Development\n\nAdvance your geophysical career with these resources:\n\n- **[`AGU Career Center`](https://careers.agu.org/)**\n  Job listings, career advice, and networking opportunities tailored for geoscientists.\n- **[`SEG Career Resources`](https://seg.org/Career-Services)**\n  Tools for professional development including mentoring programs, workshops, and job postings.\n\n| ▲ [Top](#awesome-geophysics) |\n| --- |\n---\n\n## Industry News and Updates\n\nStay informed with the latest trends, breakthroughs, and market news in geophysics:\n\n\n### Industry Publications\n\nPublications covering geophysical industry news and developments:\n\n - **[`EAGE's First Break`](https://eage.org/en/publications/first-break)**\n   Covers applied geoscience topics, case studies, and research developments.\n - **[`Oil & Gas Journal`](https://www.ogj.com/)**\n   Industry-leading coverage on petroleum exploration and energy trends.\n - **[`SEG News`](https://seg.org/News)**\n   Provides updates on geophysical exploration, technology, and society news.\n - **[`The Leading Edge (TLE)`](https://library.seg.org/journal/tle)**\n   Publishes technical advancements in seismic and geophysical methods.\n\n### Online News Platforms\n\nDigital platforms for geophysical and energy industry news:\n\n - **[`GeoExPro`](https://www.geoexpro.com)**\n   Articles on exploration, geophysics, and energy transition.\n - **[`Offshore Engineer`](https://www.oe.digital)**\n   Insights into offshore geophysics and subsea technologies.\n - **[`World Oil`](https://www.worldoil.com)**\n   Reports on upstream exploration and drilling news.\n\n### Market Analysis and Research Reports\n\nResources for industry trends and market intelligence:\n\n - **[`Rystad Energy`](https://www.rystadenergy.com/)**\n   Research on geophysical services and industry spending.\n - **[`S&P Global Energy Research`](https://www.spglobal.com/commodityinsights/en/ci/research-analysis.html)**\n   Analysis on energy sector investments and seismic data markets.\n - **[`Wood Mackenzie`](https://www.woodmac.com/)**\n   Market intelligence for oil, gas, and energy transition.\n\n### Press Releases and Corporate Blogs\n\nOfficial announcements and insights from geophysical companies:\n\n - **[`CGG Newsroom`](https://www.cgg.com/newsroom)**\n   Case studies on multi-client seismic projects and carbon storage monitoring.\n - **[`ION Geophysical`](https://iongeo.com/News/)**\n   Insights into cutting-edge geophysical imaging and seismic data processing.\n - **[`Schlumberger Innovation Blog`](https://www.slb.com/resource-library)**\n   Updates on AI-driven seismic inversion and cloud geoscience.\n - **[`TGS Press Releases`](https://www.tgs.com/news-and-media/press-releases)**\n   Announcements on new seismic data acquisitions and AI exploration tools.\n\n### Government and Regulatory Updates\n\nOfficial resources from government agencies and regulatory bodies:\n\n - **[`British Geological Survey (BGS)`](https://www.bgs.ac.uk/)**\n   Research on subsurface imaging and energy geoscience.\n - **[`Geoscience Australia`](https://www.ga.gov.au/)**\n   Updates on geophysical surveys and mineral exploration projects.\n - **[`United States Geological Survey (USGS)`](https://www.usgs.gov/)**\n   Seismic hazard reports and earthquake risk assessments.\n\n### Industry Webinars and Live Updates\n\nWebinars and live updates from geophysical companies and organizations:\n\n - **[`AAPG Discovery Thinking Series`](https://www.searchanddiscovery.com/specialcollections/discoverythinking.html)**\n   A series of webinars focusing on innovative thinking in geoscience and exploration.\n - **[`EAGE Conferences`](https://eage.org/events/conferences/)**\n   Events focusing on AI in geoscience and renewable energy applications.\n - **[`SEG Webinars`](https://seg.org/events_category/webinar/)**\n   Webinars on various geophysical topics, including seismic data processing, interpretation and  machine learning applications.\n\n### Geophysical Technology Startups\n\nInnovative companies advancing geophysical technologies:\n\n - **[`Fervo Energy`](https://fervoenergy.com)**\n   Utilizes advanced drilling techniques to enhance geothermal well outputs, providing renewable energy solutions for high-demand sectors like AI data centers.\n - **[`Fleet Space Technologies`](https://fleetspace.com)**\n   Combines low Earth orbit nanosatellites with ground-based sensors and AI to revolutionize minerals exploration, creating detailed 3D subsurface maps.\n - **[`Geophysical Technology, Inc. (GTI)`](https://geophysicaltechnology.com)**\n   Innovative geophysical solutions for subsurface exploration, enhancing seismic operational efficiency and earth imaging quality.\n - **[`Kapta Space`](https://kaptaspace.com)**\n   Developing electronically steerable radar-based imaging technology for satellites, enabling faster and more precise scanning of large areas for various applications.\n - **[`Pixxel`](https://pixxel.space)**\n   Developing a constellation of hyperspectral imaging satellites to provide high-resolution Earth observation data across various industries.\n - **[`Quaise Energy`](https://quaise.energy)**\n   Aims to harness geothermal energy by drilling deeper into the Earth's crust using gyrotron technology to vaporize rock, accessing \"superhot\" rock for sustainable energy.\n - **[`SAGA Robotics`](https://sagarobotics.com)**\n   Developing autonomous drones for geophysical surveys.\n - **[`Seisintel`](https://seisintel.com)**\n   AI-based seismic data analytics for offshore exploration.\n - **[`Wheere`](https://wheere.com)**\n   Offers an indoor geolocation system capable of precise positioning even through substantial concrete barriers, utilizing low-frequency wave emission and advanced algorithms.\n - **[`Xcalibur Multiphysics`](https://xcaliburmp.com)**\n   Advances in airborne geophysical exploration.\n\n| ▲ [Top](#awesome-geophysics) |\n| --- |\n---\n\n## Miscellaneous Resources\n\nA few additional resources to enhance your geophysical toolkit:\n\n\n### Data Visualization Libraries\n\nTools for visualizing geophysical data and results:\n\n - **[`GMT (Generic Mapping Tools)`](https://www.generic-mapping-tools.org/)**\n   Widely used for creating high-quality geoscientific maps.\n - **[`Matplotlib`](https://matplotlib.org/)**\n   Essential for 2D plotting in Python, commonly used in geophysics.\n - **[`ParaView`](https://www.paraview.org/)**\n   Open-source tool for 3D visualization of geophysical datasets.\n - **[`Plotly`](https://plotly.com/python/)**\n   Interactive visualizations ideal for geospatial and geophysical data.\n - **[`Seaborn`](https://seaborn.pydata.org/)**\n   Statistical data visualization for enhanced analysis.\n - **[`Tecplot`](https://tecplot.com/)**\n   A premium option for visualizing computational fluid dynamics and geophysical data with stunning, publication-ready graphics.\n - **[`VisIt`](https://visit-dav.github.io/visit-website/)**\n   A high-performance visualization tool for large-scale geophysical simulations—perfect for parallel processing and big data.\n\n### Shell Scripting & Automation (Bash)\n\nShell scripting is crucial for automating repetitive geophysical workflows.\n\n - **`Example Script`**\n   ```bash\n   # Rename all .sgy files to include date\n   for file in *.sgy; do\n       mv \"$file\" \"$(date +%Y%m%d)_$file\"\n   done\n   ```\n - **`Key Commands`**\n   - **Key Shell Commands for Geophysicists:**\n     - **`awk` & `sed`** - Process and clean seismic files.\n     - **`grep` & `cut`** - Extract data from logs.\n     - **`xargs` & `parallel`** - Batch processing.\n     - **`ffmpeg`** - Convert geophysical images.\n     - **`rsync` & `scp`** - Transfer large datasets.\n - **`Resources to Learn Shell Scripting for Geophysics`**\n  - **[`Advanced Bash Scripting Guide`](https://tldp.org/LDP/abs/html/)**\n    Comprehensive reference.\n  - **[`HPC Shell Scripting`](https://www.nersc.gov/users/data-analytics/data-analysis/)**\n    For handling large geophysical datasets.\n  - **[`Linux Command Line for Geoscientists`](https://earthdatascience.org/)**\n    Practical geospatial workflows.\n - **`Shell Scripting Basics`**\n   - **Why Use Shell Scripting in Geophysics?**\n     - Automate seismic data processing workflows.\n     - Manage large datasets efficiently.\n     - Run parallel computations on HPC clusters.\n     - Preprocess and clean geophysical datasets before analysis.\n\n### Programming and Scripting Resources\n\nTools and libraries for geophysical programming and analysis:\n\n - **[`Fatiando a Terra`](https://www.fatiando.org/)**\n   For gravity & magnetics modeling.\n - **[`ObsPy`](https://github.com/obspy/obspy)**\n   Python library for seismology.\n - **[`SimPEG`](https://simpeg.xyz/)**\n   For geophysical inversions.\n\n### Technical Blogs and Code Tutorials\n\nEducational resources for geophysical programming and techniques:\n\n - **[`Geophysics Rocks`](https://geophysicsrocks.com/)**\n   Tutorials on seismic interpretation.\n - **[`Software Underground`](https://softwareunderground.org/)**\n   Community for geoscience programming.\n - **[`Towards Data Science`](https://towardsdatascience.com/)**\n   Covers geospatial data science and ML in geosciences.\n\n### Professional Books and eBooks\n\nSpecialized books for geophysical professionals:\n\n - **[`Seismic Data Analysis - Yilmaz`](https://library.seg.org/doi/book/10.1190/1.9781560801580)**\n   Guide for seismic processing and interpretation.\n - **[`Theoretical & Computational Seismology - Tromp`](https://tromp.princeton.edu/publications)**\n   Advanced computational concepts.\n\n### Software Development Tools\n\nTools for developing geophysical software and workflows:\n\n - **[`Docker`](https://www.docker.com/)**\n   Containerized environments for reproducibility.\n - **[`Git & GitHub`](https://github.com/)**\n   Version control for geophysical code.\n - **[`Google Earth Engine`](https://earthengine.google.com/)**\n   Cloud-based analysis of geospatial and remote sensing data.\n - **[`Jupyter Notebook`](https://jupyter.org/)**\n   Interactive tutorials and visualization.\n - **[`Quantum Geographic Information System (QGIS)`](https://qgis.org/)**\n   A free, open-source software that allows users to create, edit, visualize, analyze, and publish geospatial information.\n\n| ▲ [Top](#awesome-geophysics) |\n| --- |\n---\n\n## Contributors\n\nThanks to our many contributors!\n\n[![Contributors](https://contrib.rocks/image?repo=aradfarahani/awesome-geophysics)](https://github.com/aradfarahani/awesome-geophysics/graphs/contributors)\n\n| ▲ [Top](#awesome-geophysics) |\n| --- |\n\n---\n\n## How to Contribute\n\nThis list is a community effort and grows with your contributions!  \nHave a tool, dataset, blog, or resource to add? Here's how you can help:\n\n1. **Submit a Suggestion:**  \n   Open an issue or pull request on our [GitHub repository](https://github.com/aradfarahani/awesome-geophysics) to add or update resources.\n\n2. **Share Your Expertise:**  \n   Contribute by writing tutorials, guides, or blog posts that explain complex geophysical concepts in an accessible way.\n\nTogether, we can continue to make Awesome Geophysics the definitive resource for the global geophysical community.\n\n> **For more detailed guidelines, please check the [CONTRIBUTING.md](https://github.com/aradfarahani/awesome-geophysics/blob/main/CONTRIBUTING.md) file.**\n\n| ▲ [Top](#awesome-geophysics) |\n| --- |\n\n---\n\n## License\n\n[![CC0](http://mirrors.creativecommons.org/presskit/buttons/88x31/svg/cc-zero.svg)](https://creativecommons.org/publicdomain/zero/1.0)\n\nTo the extent possible under law, all contributors have waived all copyright and\nrelated or neighboring rights to this work. \n\n| ▲ [Top](#awesome-geophysics) |\n| --- |\n\n---\n\n*Whether you're diving into seismic data processing, modeling Earth's subsurface, or simply looking for inspiration, we invite you to explore, share, and contribute. Let's push the boundaries of geophysical exploration and understanding—together!*\n",
        "createdAt": "2025-03-20T20:36:16.000Z",
        "updatedAt": "2025-11-10T05:03:32.000Z",
        "language": "Python",
        "homepage": "https://aradfarahani.com/awesome-geophysics/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/aradfarahani/awesome-geophysics/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "LordKnish/QuakeCord",
        "url": "https://github.com/LordKnish/QuakeCord",
        "description": "A discord bot that retrieves and displays information about recent earthquakes from the European Mediterranean Seismological Centre (EMSC).",
        "stars": 0,
        "forks": 0,
        "readme": "# Quakecord\n\nA discord bot that retrieves and displays information about recent earthquakes from the European Mediterranean Seismological Centre (EMSC). Invite it here: https://discord.com/api/oauth2/authorize?client_id=787970287550464010&permissions=2048&scope=bot\n\n## Table of Contents\n\n- [Program](#program)\n- [Data Source](#data-source)\n- [Libraries Used](#libraries-used)\n- [Instructions](#instructions)\n- [Future Development](#future-development)\n\n## Program\n\nQuakecord is a discord bot that provides information about recent earthquakes to users in a Discord server. It retrieves the information from the European Mediterranean Seismological Centre (EMSC) and displays it in a user-friendly format.\n\n## Data Source\n\nThe data used by Quakecord is sourced from the European Mediterranean Seismological Centre (EMSC). The EMSC is a leading provider of real-time earthquake information and has a vast database of earthquake data.\n\n## Libraries Used\n\nQuakecord uses the following libraries:\n\n- Requests\n- Discord.py\n- Folium\n\n## Instructions\n\n1. Clone this repository to your local machine.\n2. Create a virtual environment and activate it.\n3. Install the required libraries using `pip install -r requirements.txt`.\n4. Create a bot in Discord's Developer Portal and get its token.\n5. Add the bot to your Discord server.\n6. In the `quakecord.py` file, replace `TOKEN` with your bot's token.\n7. Run the `quakecord.py` file to start the bot.\n\n## Future Development\n\n- Ability to notify users of earthquakes that meet certain criteria.\n- More detailed historical information, including graphs and maps.\n- Adding more data sources for earthquake information.\n- Improving the visual representation of the information provided by the bot.\n\n## Contributing\n\nWe welcome contributions from the open-source community. If you would like to contribute to Quakecord, please follow these guidelines:\n\n1. Fork the repository\n2. Create a branch for your changes (e.g. `feature/new-feature` or `bugfix/fix-error`)\n3. Commit your changes with descriptive commit messages\n4. Push your changes to your forked repository\n5. Create a pull request to the main repository with a clear explanation of your changes and the problem they solve.\n\n## License\n\nQuakecord is licensed under the MIT License. See the [LICENSE](LICENSE) file for more information.\n",
        "createdAt": "2023-02-11T20:48:50.000Z",
        "updatedAt": "2023-02-11T20:50:57.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/LordKnish/QuakeCord/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "PrincetonUniversity/SPECFEMPP",
        "url": "https://github.com/PrincetonUniversity/SPECFEMPP",
        "description": "SPECFEM++ is a complete re-write of SPECFEM suite of packages (SPECFEM2D, SPECFEM3D, SPECFEM3D_GLOBE) using C++",
        "stars": 48,
        "forks": 17,
        "readme": "# SPECFEM++\n\n[![Documentation Status](https://readthedocs.org/projects/specfem2d-kokkos/badge/?version=latest)](https://specfem2d-kokkos.readthedocs.io/en/latest/?badge=latest)\n[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](LICENSE)\n\n## About\n\nSPECFEM++ is a complete re-write of SPECFEM suite of packages (SPECFEM2D, SPECFEM3D, SPECFEM3D_GLOBE) using C++. Compared to the earlier version, SPECFEM++ code base provides:\n\n 1. a robust and flexible code structure,\n 2. modularity that allows for easy addition of new features,\n 3. portability that allows the code to run on a variety of architectures (CPU, NVIDIA GPUs, Intel GPUs, AMD GPUs etc.), and\n 4. a user-friendly build infrastructure that allows the code to be easily compiled and run on a variety of platforms.\n\n[specfempp-py](https://github.com/PrincetonUniversity/SPECFEMPP-py) is the official Python package for configuring and running SPECFEM++ with Python.\n\n## Documentation\n\n\nThe online documentation for SPECFEM++ is located\n[here](https://specfem2d-kokkos.readthedocs.io/en/latest/index.html#)\n\n## Getting Started with SPECFEM++\n\nFollow the [Getting Started\nGuide](https://specfem2d-kokkos.readthedocs.io/en/latest/sections/getting_started/index.html)\nto install SPECFEM++ on your system and run the solver.\n\n## Examples\n\nWe recommend starting with the [cookbook\nexamples](https://specfem2d-kokkos.readthedocs.io/en/latest/sections/cookbooks/index.html)\nto learn how to customize the solver for your use case.\n\n## Contributing to SPECFEM++\n\nSPECFEM is a community project that lives by the participation of its members —\ni.e., including you! It is our goal to build an inclusive and participatory\ncommunity so we are happy that you are interested in participating! Please see\n[this\npage](https://specfem2d-kokkos.readthedocs.io/en/latest/sections/developer_documentation/contributing.html)\nfor developer documentation.\n\nIn particular you should follow the git development workflow and pre-commit\nstyle checks when contributing to SPECFEM++.\n\n## License\n\nSPECFEM++ is distributed under the [GPL v3 license](LICENSE)\n",
        "createdAt": "2022-06-21T22:19:31.000Z",
        "updatedAt": "2025-11-25T16:04:05.000Z",
        "language": "Fortran",
        "homepage": "https://specfem2d-kokkos.readthedocs.io/en/latest/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/PrincetonUniversity/SPECFEMPP/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "pequegnc/resif-doi",
        "url": "https://github.com/pequegnc/resif-doi",
        "description": "datacite metadata for seismological and other RESIF newtrok",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2018-03-29T08:02:38.000Z",
        "updatedAt": "2018-03-29T08:02:38.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "un-dirac/Seismology",
        "url": "https://github.com/un-dirac/Seismology",
        "description": "How to install SAC, SEISAN, ObsPy",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2023-12-12T00:20:06.000Z",
        "updatedAt": "2023-12-12T00:20:06.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ExtraNick/NASA-Seismology-Challenge",
        "url": "https://github.com/ExtraNick/NASA-Seismology-Challenge",
        "description": "2024 NASA Space Apps Challenge Seismic Detection Across the Solar System",
        "stars": 0,
        "forks": 0,
        "readme": "# NASA-Seismology-Challenge\n 2024 NASA Space Apps Challenge Seismic Detection Across the Solar System\n# How it works \nI use pandas for the .csv files data manipulation as DataFrames, using Jupyter Notebook for easing the tasks.\n\nFirst, pandas is imported <br>\nThe .csv file is loaded by pandas and designed to the variable \"df\"<br>\nThe dataframe consists of: Absolute Time, Relative Time and Velocity columns<br>\nI then remove Relative time because i didn't find it necessary for the task at hand<br>\nI proceed to split Absolute time in 3 collumns:<br>\n-Hours<br>\n-Minutes<br>\n-Seconds <br>\nThe reason for this is to make the dataframe more readable for myself<br>\nAt this point i also snapshot the Absolute Time under the variable Total_time, which can be used to recovering the Absolute Time column if necessary\n\nThe Data frame is then reorganized as follows:<br>\nHour -> Minute -> Seconds -> Velocity\n\nat this point the Dataframe could be plotted into a graph for viewing, however i ran into many issues in importing modules in both my Windows 11 and Linux machine, so i skipped this step.\n\nThe next step is getting rid of irrelevant data:<br>\nI convert velocity into float for manipulation<br>\nI take the average of the Velocity collumn<br>\nI take the highest (max) value of the Velocity column<br>\nI take the lowest (min) value of the Velocity column<br>\nI subtract the lowest value from the highest.<br>\n\nThis accomplishes a couple things:<br>\nAt this point i could tell by the average that there wasn't that much variation between the quantity of High and Low values<br>\nImagine a graph: The  Low value hangs just as far from the Y=0 axis than the High value hangs from that same point in the graph.<br>\nBy subtracting both values, i can infer a value in which general noise will fluctuate.<br>\n\nKnwoing this, i can proceed to the next step: removing noise from the sample<br>\nThe variable \"median\" represents the value of noise one can expect from the readins.<br>\nI then remove every single row that contains a velocity value that is Positive and lower than the median noise variable<br>\nI then remove every single row that contains a velocity value that is Negative and higher than the median noise variable inverted (negative)<br>\nFor each step described aboved, i create a snashotted dataframe derived from the Original dataframe that stores only values that are deemed relevant<br>\n\nWith this, i now have 3 dataframes:<br>\n-Original dataframe containing ALL values<br>\n-High dataframe containing Values that are higher than the median variable (noise)<br>\n-Low dataframe containing Values that are lower than the inverted median variable (noise)<br>\n\nThe next step is to bring these together into a single dataframe that will be exported.<br>\nI perform an outer join of the High and Low Dataframe, preserving their respective indexes that were defined in the Original dataframe<br>\nI fill the N/A (empty) cells with respective datam from the corresponding Indexes of both High and Low Dataframes on the Original dataframe<br>\n\nAnd after some cleaning up, i export everything into a single file that contains only the filtered and relevant data.\n\n# Results\nA 22,419,039 characters file was filtered into a 424,898 characters<br>\nThe graph of the data as viewed in a 1920x1080 screen ![graph](https://i.imgur.com/OlPQoKR.png)\n<br>\nThe graph of the data as viewed in a 1920x1080 screen ![graph](https://i.imgur.com/ZONkOqW.png)\n\n\n\nY axis -> Velocity <br>\nX axis -> The Index of each reading <br>\nSince the indexes update with  each reading (going off once every 4 seconds on average), i did not include the time values in the graph\n\nData was tested with the Mars training data<br>\nWhen testing was attempted with Moon training data, the code did not work \n\n## TO DO \nThese are things i would work on if i had more time: <br>\nFind a way to get the program to work with moon data <br>\nFix plotly and matplot import <br>\nGett proper graphs <br>\nTest data with more samples\n",
        "createdAt": "2024-10-07T00:02:17.000Z",
        "updatedAt": "2024-10-07T02:55:43.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ExtraNick/NASA-Seismology-Challenge/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "QuakeMigrate/QuakeMigrate",
        "url": "https://github.com/QuakeMigrate/QuakeMigrate",
        "description": "A Python package for automatic earthquake detection and location using waveform migration and stacking.",
        "stars": 157,
        "forks": 38,
        "readme": "<p align=\"center\">\n  <!-- DOI -->\n  <a href=\"https://doi.org/10.5281/zenodo.4442748\">\n    <img src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.4442748.svg\" alt=\"DOI\" />\n  </a>\n  <!-- ReadTheDocs -->\n  <a href=\"https://quakemigrate.readthedocs.io/en/latest\">\n    <img src=\"https://readthedocs.org/projects/quakemigrate/badge/?version=latest\" />\n  </a>\n  <!-- Build Action -->\n  <a href=\"https://github.com/QuakeMigrate/QuakeMigrate/actions\">\n    <img src=\"https://github.com/QuakeMigrate/QuakeMigrate/actions/workflows/build_wheels.yml/badge.svg\" />\n  </a>\n  <!-- PyPI -->\n  <a href=\"https://pypi.org/project/quakemigrate/\">\n    <img src=\"https://img.shields.io/pypi/v/quakemigrate\" />\n  </a>\n  <!-- Coverage -->\n  <a href=\"https://codecov.io/gh/QuakeMigrate/QuakeMigrate\">\n    <img src=\"https://codecov.io/gh/QuakeMigrate/QuakeMigrate/branch/master/graph/badge.svg\">\n  </a>\n  <!-- Python version-->\n  <a href=\"https://www.python.org/downloads/release/python-390/\">\n    <img src=\"https://img.shields.io/badge/python-3.9+-blue.svg\" />\n  </a>\n  <!-- License -->\n  <a href=\"https://www.gnu.org/licenses/gpl-3.0\">\n    <img src=\"https://img.shields.io/badge/License-GPLv3-blue.svg\" />\n  </a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://quakemigrate.readthedocs.io/en/latest\">QuakeMigrate</a> is a Python package for automatic earthquake detection and location using waveform migration and stacking.</a>\n</p>\n\n<p align=\"center\">\n<img src=\"https://github.com/QuakeMigrate/QuakeMigrate/raw/master/docs/img/QMlogoBig.png\", width=\"80%\">\n</p>\n\nKey Features\n------------\nQuakeMigrate uses a waveform migration and stacking algorithm to search for coherent seismic phase arrivals across a network of instruments. It produces—from raw data—catalogues of earthquakes with locations, origin times, phase arrival picks, and local magnitude estimates, as well as rigorous estimates of the associated uncertainties.\n\nThe package has been built with a modular architecture, providing the potential for extension and adaptation at numerous entry points. This includes, but is not limited to:\n* the calculation or import of traveltime grids\n* the choice of algorithm used to identify phase arrivals (for example by kurtosis, cross-covariance analysis between multiple components, machine learning techniques and more)\n* the stacking function used to combine onset functions\n* the algorithm used to perform phase picking\n\nDocumentation\n-------------\nDocumentation for QuakeMigrate is hosted [here](https://quakemigrate.readthedocs.io/en/latest/index.html).\n\nInstallation\n------------\nDetailed installation instructions can be found [here](https://quakemigrate.readthedocs.io/en/latest/installation.html).\n\nIf you're comfortable with virtual environments and just want to get started, QuakeMigrate is available via the Python Package Index, and can be installed via pip:\n\n```console\npip install quakemigrate\n```\n\nUsage\n-----\nWe are working on tutorials covering how each individual aspect of the package works, as well as example use cases where we provide substantive reasoning for the parameter choices used. These examples include applications to cryoseismicity and volcano seismology.\n\nThis is a work in progress - [see our documentation for full details](https://quakemigrate.readthedocs.io/en/latest/tutorials.html).\n\nFor a demonstration of the options available, and a starting point to write scripts for your own use-case, see the [template scripts](examples/template_scripts).\n\nCitation\n--------\nIf you use this package in your work, please cite the following conference presentation:\n\n**Winder, T., Bacon, C.A., Smith, J.D., Hudson, T., Greenfield, T. and White, R.S., 2020.** QuakeMigrate: a Modular, Open-Source Python Package for Automatic Earthquake Detection and Location. *In AGU Fall Meeting 2020. AGU*.\n\n\n```console\nWinder, T., Bacon, C.A., Smith, J.D., Hudson, T., Greenfield, T. and White, R.S., 2020. QuakeMigrate: a Modular, Open-Source Python Package for Automatic Earthquake Detection and Location. In AGU Fall Meeting 2020. AGU.\n```\n\nas well as the relevant version of the source code on [Zenodo](https://doi.org/10.5281/zenodo.4442748).\n\nWe hope to have a publication coming out soon:\n\n**Winder, T., Bacon, C.A., Smith, J.D., Hudson, T.S., and White, R.S.** QuakeMigrate: a Python Package for Automatic Earthquake Detection and Location Using Waveform Migration and Stacking. (*to be submitted to Seismica*).\n\n```console\nWinder, T., Bacon, C.A., Smith, J.D., Hudson, T.S., and White, R.S. QuakeMigrate: a Python Package for Automatic Earthquake Detection and Location Using Waveform Migration and Stacking. (to be submitted to Seismica).\n```\n\nContributing to QuakeMigrate\n----------------------------\nContributions to QuakeMigrate are welcomed. Whether you have identified a bug or would like to request a new feature, your first stop should be to reach out, either directly or—preferably—via the GitHub Issues panel, to discuss the proposed changes. Once we have had a chance to scope out the proposed changes you can proceed with making your contribution following the instructions in our [contribution guidelines](https://github.com/QuakeMigrate/QuakeMigrate/blob/master/CONTRIBUTING.md).\n\nBug reports, suggestions for new features and enhancements, and even links to projects that have made use of QuakeMigrate are most welcome.\n\nContact\n-------\nYou can contact us directly at: quakemigrate.developers@gmail.com\n\nAny additional comments/questions can be directed to:\n* **Tom Winder** - tom.winder@esc.cam.ac.uk\n* **Conor Bacon** - conor.bacon@cantab.net\n\nLicense\n-------\nThis package is written and maintained by the QuakeMigrate developers, Copyright QuakeMigrate developers 2020–2025. It is distributed under the GPLv3 License. Please see the [LICENSE](LICENSE) file for a complete description of the rights and freedoms that this provides the user.\n",
        "createdAt": "2019-04-14T07:05:37.000Z",
        "updatedAt": "2025-11-17T02:50:31.000Z",
        "language": "Python",
        "homepage": "https://quakemigrate.readthedocs.io/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.4442748",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.4442748",
            "dataCite": "10.5281/zenodo.4442748",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/QuakeMigrate/QuakeMigrate/master/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.4442748",
            "title": "QuakeMigrate/QuakeMigrate: QuakeMigrate v1.2.1",
            "journal": "Zenodo",
            "dateReleased": "2025-07-24T00:00:00.000Z",
            "abstract": "This bugfix release addresses a deprecated pandas function, removes use of the deprecated pkg_resources and makes minor tweaks related to figure plotting for the upcoming manuscript describing the software, along with other minor improvements and bugfixes.\n\n\n\nExamples:\n\n\ninclude get_data in run_test_examples.py for the Icequake_Iceland example. 452ba15\n\n\n\nquakemigrate.signal.trigger:\n\n\nAdded new kwarg write_event_time_windows to provide the option to output the MinTime and MaxTime columns in the triggered event file, enabling the trigger summmary plot to be reproduced after the fact. 9e117b1\n\n\n\n\nContributors\n\n@TomWinder & @hemmelig.\n\nWhat's Changed (auto-generated)\n\n\n\n:wrench: Remove use of pkg_resources by @TomWinder in https://github.com/QuakeMigrate/QuakeMigrate/pull/207\n\n:bug: Remove use of removed pandas.DataFrame.iteritems() by @TomWinder in https://github.com/QuakeMigrate/QuakeMigrate/pull/209\n\n:test_tube: Add get_data to run_test_examples.py by @TomWinder in https://github.com/QuakeMigrate/QuakeMigrate/pull/208\n\n:bug: Add checkout as workaround to codecov issue by @TomWinder in https://github.com/QuakeMigrate/QuakeMigrate/pull/212\n\nFEAT: Paper tweaks by @TomWinder in https://github.com/QuakeMigrate/QuakeMigrate/pull/213\n\n:wrench: Bump version to 1.2.1 by @TomWinder in https://github.com/QuakeMigrate/QuakeMigrate/pull/214\n\n\nFull Changelog: https://github.com/QuakeMigrate/QuakeMigrate/compare/v1.2.0...v1.2.1",
            "citationsArray": []
        },
        "publications": [
            {
                "doi": "10.1016/j.epsl.2021.117268",
                "name": "From slab to surface: Earthquake evidence for fluid migration at Uturuncu volcano, Bolivia",
                "source": "Zenodo",
                "authorNames": [
                    "Hudson, Thomas S.",
                    "Kendall, J-Michael",
                    "Pritchard, Matthew E.",
                    "Blundy, Jonathan D.",
                    "Gottsmann, Joachim H."
                ],
                "url": [
                    "http://ui.adsabs.harvard.edu/#abs/2022E&PSL.57717268H",
                    null,
                    "http://doi.org/10.1016/j.epsl.2021.117268"
                ]
            },
            {
                "doi": "10.1029/2020JB021493",
                "name": "Distributed Acoustic Sensing (DAS) for Natural Microseismicity Studies: A Case Study From Antarctica",
                "source": "Zenodo",
                "authorNames": [
                    "Hudson, T. S.",
                    "Baird, A. F.",
                    "Kendall, J. M.",
                    "Kufner, S. K.",
                    "Brisbourne, A. M.",
                    "Smith, A. M.",
                    "Butcher, A.",
                    "Chalari, A.",
                    "Clarke, A."
                ],
                "url": [
                    "http://doi.org/10.1029/2020JB021493",
                    "http://ui.adsabs.harvard.edu/#abs/2021JGRB..12621493H"
                ]
            }
        ]
    },
    {
        "source": "GitHub",
        "name": "Nono140503/Seismic-Activity-Prediction",
        "url": "https://github.com/Nono140503/Seismic-Activity-Prediction",
        "description": "Machine learning powered system that analyzes historical California earthquake data to predict seismic activity, magnitude, and risk levels. Includes visual dashboards, heatmaps, and forecasting models using time series analysis, clustering, and regression to support disaster preparedness and research.",
        "stars": 0,
        "forks": 0,
        "readme": "# Seismic-Activity-Prediction\nMachine learning–powered system that analyzes historical California earthquake data to predict seismic activity, magnitude, and risk levels. Includes visual dashboards, heatmaps, and forecasting models using time series analysis, clustering, and regression to support disaster preparedness and research.\n",
        "createdAt": "2025-11-23T19:32:38.000Z",
        "updatedAt": "2025-11-23T19:33:27.000Z",
        "language": null,
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Nono140503/Seismic-Activity-Prediction/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "FranciscoLlinin/sismo_app",
        "url": "https://github.com/FranciscoLlinin/sismo_app",
        "description": "Application in Ruby on Rails that obtains and delivers information related to seismological data in the United States",
        "stars": 0,
        "forks": 0,
        "readme": "# Seismic Data Project\n\n## General Description\n\nThis project consists of the development of an application in Ruby on Rails that obtains and delivers information related to seismological data in the United States.  It includes:\n\n1. **Data Retrieval and Persistence**: A task to retrieve seismological data from the USGS feed (earthquake.usgs.gov) and persist it in a database, complying with certain validations.\n\n2. **REST API Endpoints**:\n   - **Endpoint 1**: Gets a list of seismic events (features) and allows filtering by `mag_type`, `page` and `per_page`.\n   - **Endpoint 2**: Creates a comment associated to a specific seismic event (feature).\n\n## Requirements\n\n* Ruby version 3.1.0 or  higher\n* Rails version 6.1.4 or higher\n* Database like sqlite3 or PostgreSQL\n\n## Project Configuration\n\n1. **Clone the Repository**:\n   ```\n   git clone https://github.com/FranciscoLlinin/sismo_app.git\n   cd sismo_app\n   ```\n\n2. **Install Dependencies**:\n   ```\n   bundle install\n   ```\n3. **Run the Data Retrieval and Persistence Task in the rails console**:\n   ```\n   EarthquakeDataSyncTask.new.perform\n   ```\n   This task is responsible for fetching seismic data from the USGS feed and persisting it to the database.\n\n4. **Configure the Database**:\n   ```\n   rails db:migrate\n   ```\n\n## Running the Server\nTo bring up the server, run the following command:\n```\nrails server\n```\n\nThe server will start at `http://localhost:3000`.\n\n\n## REST API Endpoints\n\n### Endpoint 1: Get List of Features\n**Method**: `GET`\n**URL**: `http://localhost:3000/api/features`\n\n**Response**:\n\n![alt text](image.png)\n\n**Query Parameters**:\n- `filters[mag_type]`: Filter by one or more magnitude types (md, ml, ms, mw, me, mi, mi, mb, mlg).\n- `page`: Result page number (default: 1)\n- `per_page`: Number of results per page (max.: 1000, default: 20)\n\n**Response**:\n\n![alt text](image-1.png)\n\n### Endpoint 2: Create a Comment\n**POST method\n**URL `http://localhost:3000/api/features/{feature_id}/comments`\n\n**Upload**:\n```json\n{\n    \"comment\":{\n         \"body\": \"This is a comment\"\n    }\n  \n}\n```\n\n**Response**:\n![alt text](image-2.png)\n\n## Frontend with REACT\nTo bring up frontend  in React you need to install NodeJS and npm then run the following commands from your terminal:\n\n1. **Access to frontend folder**:\n   ```\n   cd frontend\n   ```\n2.  **Installation**: Install the dependencies using npm\n\n3. Run the server in development mode using the following command: \n   ```\n   npm run start\n   ```\nThe server will start at `http://localhost:3000`, but but since the rails server is on that port it will open at `http://localhost:3001`.   \n\n4.  Open your browser and the app will be ready to use\n\n### App\n\n![alt text](image-3.png)\n\n### Filters\n\n![alt text](image-4.png)\n\n### Comments\n\n![alt text](image-5.png)",
        "createdAt": "2024-04-07T03:06:27.000Z",
        "updatedAt": "2024-04-09T19:32:36.000Z",
        "language": "Ruby",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/FranciscoLlinin/sismo_app/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "FDSN/OBS-standards",
        "url": "https://github.com/FDSN/OBS-standards",
        "description": "A repository for the discussion and development of Marine Seismology standards",
        "stars": 9,
        "forks": 1,
        "readme": "# OBS-standards\n\nDefinition of marine seismology data and metadata standards, and possibly creation of documents for users\n\nThe goals are to:\n- define standards (and propose new ones if needed)\n- Help data providers to create standardized data and metadata\n- Help users to understand and use marine seismology data\n- include standards in new “Guidelines for specific datasets” within the StationXML and miniSEED documentation\n\nWe want to have a document or documents ready for validation/vote before the next FDSN meeting (September 2025 Lisbon)\n\nThe contents of this repository are:\n- ``standards.md``: proposed standards\n- ``software.md``L useful, publically available software tools\n- The 2024 AGU poster about the Action Group\n- ``References/``: old standards documents\n- ``Meetings/``: notes from Action Group meetings\n- ``other/``: miscellaneous files/directories\n\nThe first two documents were started from the information in the \"References\" directory\n",
        "createdAt": "2024-01-18T15:36:35.000Z",
        "updatedAt": "2025-11-06T18:02:29.000Z",
        "language": null,
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/FDSN/OBS-standards/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "PyGLImER/PyGLImER",
        "url": "https://github.com/PyGLImER/PyGLImER",
        "description": "An automatic seismology toolset for global P-to-S and S-to-P receiver function imaging",
        "stars": 45,
        "forks": 12,
        "readme": "<img src=\"https://github.com/PyGLImER/PyGLImER/raw/master/docs/chapters/figures/logo_horizontal_colour.png\" alt=\"PyGLImER logo\" width=\"600\"/>\n\n[![Build Status](https://github.com/PyGLImER/PyGLImER/actions/workflows/test_on_push.yml/badge.svg?branch=master)](https://github.com/PyGLImER/PyGLImER/actions/workflows/test_on_push.yml) [![Documentation Status](https://github.com/PyGLImER/PyGLImER/actions/workflows/deploy_gh_pages.yml/badge.svg)](https://github.com/PyGLImER/PyGLImER/actions/workflows/deploy_gh_pages.yml) [![License: EUPL v1.2](https://img.shields.io/badge/license-EUPL--1.2-blue)](https://joinup.ec.europa.eu/collection/eupl/introduction-eupl-licence) [![codecov](https://codecov.io/gh/PyGLImER/PyGLImER/branch/master/graph/badge.svg?token=9WK7ZKIZ6N)](https://codecov.io/gh/PyGLImER/PyGLImER)\n\n---\n\n## A workflow to create a global database for Ps and Sp receiver function imaging of crustal and upper mantle discontinuties \n\nPyGLImER **automates receiver function (RF) processing from download of raw waveform data to common conversion point (CCP) imaging with a minimum amount of user interference.**\n\nThe implementation includes:\n\n+ Functions to download raw waveform data from FDSN providers\n+ Functions to feed in local waveform data\n+ An adaptable preprocessing scheme, including various rotational algorithms\n+ A variety of deconvolution algorithms (user-defined algorithms possible)\n+ An implementation of the iasp91 and GyPSum velocity models for depth migration (user-defined models are accepted)\n+ A new, particularly efficient Common Conversion Point Stacking algorithm\n+ A variety of plotting tools to explore datasets and to create prublication ready figures\n+ Efficient and fast processing and data management, support multi-processing, MPI, and HDF5\n\nAs developers, we are particularly concerned to create an **automated, adaptable, efficient, and, yet, easy-to-use** toolkit.\n\nThe project is largely based on the [ObsPy](https://github.com/obspy/obspy) project and can be seen as a more powerful and user-friendly\nsuccessor of the [GLImER](http://stephanerondenay.com/glimer-web.html) project.\n\n## Installation of this package\n\n### Installation from PyPi\nPyGLImER is now deployed on PyPi and can simply be installed using:\n\n```bash\npip install pyglimer\n```\n\n### Installation from source code\nTo obtain the latest updates, you can install PyGLImER from the source code, available on GitHub.\n\n⚠️ **Developers should download the ``dev`` branch**\n\n```bash\n# Download via wget or web-browser\nwget https://github.com/PyGLImER/PyGLImER/archive/refs/heads/master.zip\n\n# For developers\nwget https://github.com/PyGLImER/PyGLImER/archive/refs/heads/dev.zip\n\n# unzip the package\nunzip master.zip  # or dev.zip, depending on branch\n\n# Change directory to the same directory that this repo is in (i.e., same directory as setup.py)\ncd PyGLImER-master  # That's the standard name the folder should have\n\n# Create the conda environment and install dependencies\nconda env create -f environment.yml\n\n# Activate the conda environment\nconda activate pyglimer\n\n# Install your package\npip install -e .\n```\n\nOptionally, you can test the package by running\n\n```bash\npytest -p no:logging tests\n```\n\n## Getting started\nAccess PyGLImER's documentation [here](https://pyglimer.github.io/PyGLImER/).\n\nPyGLImER comes with a few tutorials (Jupyter notebooks). You can find those in the `examples/` directory.\n\n## What it looks like\nWith PyGLImER, we facilitate processing extremely large amounts of teleseismic data. This enables us to create large scale CCP sections as shown for P-to-S and S-to-P receiver function data in the plot below:\n\n| <img src=\"https://github.com/PyGLImER/PyGLImER/raw/master/docs/chapters/figures/map_w_ccp_sections.png\" alt=\"Map With CCP sections\" width=\"600\"/> |\n|:--:| \n| *FIG: Seismic broadband stations with available receiver functions are plotted as downward-pointing red triangles. The locations of the shown cross-sections are demarked as bold black lines. Cross-sections A, B, and D are created from S receiver functions stacked by common conversion point, whereas cross-section C shows a slice through a P receiver function common conversion point stack. Data begin to fade to grey if the respective gridpoint is hit by fewer than 25 rays. Note that the vertical exaggeration varies from panel to panel.* |\n\nPyGLImER also comes with a toolset to create publication ready figures:\n\n| <img src=\"https://github.com/PyGLImER/PyGLImER/raw/master/docs/chapters/figures/combined.jpg\" alt=\"Combined Stack and Section\" width=\"400\"/> |\n|:--:|\n| *FIG: Single station stack and receiver functions sorted by epicentral distance from P receiver function for station GE.DAG.* |\n\n| <img src=\"https://github.com/PyGLImER/PyGLImER/raw/master/docs/chapters/figures/distr.jpg\" alt=\"Distribution of back-azimuth and rayparameters\" width=\"600\"/> |\n|:--:|\n| *FIG: Distribution of back-azimuth and rayparameter for the P receiver functions from GE.DAG as shown above.* |\n\n## Reporting Bugs / Contact the developers\nThis version is an early release. If you encounter any issues or unexpected behaviour, please [open an issue](https://github.com/PyGLImER/PyGLImER/issues/new/choose) here on GitHub.\n\n## Questions?\nIf you have any questions that do not require any changes in the source code, please use the [discussions feature](https://github.com/PyGLImER/PyGLImER/discussions)\n\n## Contributing\nThank you for contributing to PyGLImER! Have a look at our [guidelines for contributors](https://github.com/PyGLImER/PyGLImER/blob/master/CONTRIBUTING.md)\n\n## Citing PyGLImER\nIf you use PyGLImER to produce content for your publication, please consider citing us. For the time being, please cite our [AGU abstract](https://doi.org/10.1002/essoar.10506417.1).\n\n## Latest\nWe are happy to announced that PyGLImER has been awarded an [ORFEUS](http://orfeus-eu.org/) software development grant and are looking forward to further develop this project.\n",
        "createdAt": "2019-03-16T14:25:35.000Z",
        "updatedAt": "2025-11-28T00:43:09.000Z",
        "language": "Python",
        "homepage": "https://pyglimer.github.io/PyGLImER/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/PyGLImER/PyGLImER/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "hifayuna1982/seismology",
        "url": "https://github.com/hifayuna1982/seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# seismology",
        "createdAt": "2017-01-25T04:07:59.000Z",
        "updatedAt": "2017-01-25T04:07:59.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/hifayuna1982/seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "hemmelig/FMTomo-tools",
        "url": "https://github.com/hemmelig/FMTomo-tools",
        "description": "A collection of complementary scripts and notebooks for FMTomo ",
        "stars": 6,
        "forks": 1,
        "readme": "# FMTomo-tools\nA collection of complementary scripts and notebooks for [FMTomo](http://rses.anu.edu.au/~nick/fmtomo.html):\n\n*Rawlinson, N. and Sambridge, M., 2005. The fast marching method: an effective tool for tomographic imaging and tracking multiple phases in complex layered media. Exploration Geophysics, 36(4), pp.341-350.*\n\n*Rawlinson, N. and Sambridge, M., 2004. Wave front evolution in strongly heterogeneous layered media using the fast marching method. Geophysical Journal International, 156(3), pp.631-647.*\n\nLicense\n-------\nThis collection is shared here \"as is\", with no license. Do with it what you will.\n\nRequirements\n------------\nA full list of required libraries can be found in the [requirements.txt](requirements.txt)\n\nContact\n-------\nAny additional comments/questions can be directed to:\n* **Conor Bacon** - cbacon [ at ] ldeo.columbia.edu\n",
        "createdAt": "2020-10-17T14:15:33.000Z",
        "updatedAt": "2025-02-09T23:39:20.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/hemmelig/FMTomo-tools/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "lsimps204/seismology-app",
        "url": "https://github.com/lsimps204/seismology-app",
        "description": "Android application for displaying UK earthquake data",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2019-02-04T15:29:14.000Z",
        "updatedAt": "2019-04-12T11:31:02.000Z",
        "language": "Java",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "core-man/SOD.recipes",
        "url": "https://github.com/core-man/SOD.recipes",
        "description": "SOD recipes to download seismic data",
        "stars": 3,
        "forks": 0,
        "readme": "\nThe fold contains all the SOD recipes I used to download seismic data from the FDSN web services (FDSNWS) of some Data Centers, e.g., [IRIS-DMC](https://ds.iris.edu/ds/nodes/dmc/). You can directly run them in your computer, e.g.,\n```bash\n$ sod -f recipe-csvEvent-fixedNet.xml\n```\n\nDue to IRIS-DMC has updated its StationXML, you need to use the latest SOD, i.e., [SOD 3.2.10](http://www.seis.sc.edu/sod/).\n\nYou may check all the [FDSNWS supporting Data Centers](https://www.fdsn.org/webservices/). Sometimes you can directly use their FDSNWS via [wget](https://www.gnu.org/software/wget/) or [curl](https://curl.haxx.se/). Some of them may also have their own web services, which could be useful, e.g.,, [IRIS-DMC web services](https://service.iris.edu/).\n\n**Content:**\n\n- [eventArm](#eventarm)\n- [networkArm](#networkarm)\n- [waveformArm](#waveformarm)\n- [subsetter notes](#subsetter-notes)\n- [tests](#tests)\n- [references](#references)\n\n\n## eventArm\n\nThose recipes only contain the eventArm, which can be used to download the catalog called `events.csv`. You may revise, comment, or add some subsetters in the recipes according to your purposes.\n\n- [recipe-origin-mag-dep-boxarea.xml](eventArm/recipe-origin-mag-dep-boxarea.xml) : Query catalog from [USGS FDSN Event web service](https://earthquake.usgs.gov/fdsnws/event/1/), within a box area.\n- [recipe-origin-mag-dep-boxarea-more.xml](eventArm/recipe-origin-mag-dep-boxarea-more.xml) : Query catalog with more conditions.\n- [recipe-origin-mag-dep-pointdist.xml](eventArm/recipe-origin-mag-dep-pointdist.xml) : Query catalog within given distance range of the given lat & lon.\n- [recipe-origin-mag-dep-boxarea-NC.xml](eventArm/recipe-origin-mag-dep-boxarea-NC.xml) : Query catalog from [Northern California Earthquake Data Center (NCEDC) FDSNWS](http://service.ncedc.org/).\n\n\n## networkArm\n\nThose recipes only contain the networkArm, which can be used to search stations, including locations (`station.dat`) and instrument responses (`poles and zeros` and/or `responses`). You may revise, comment, or add some subsetters in the recipes according to your purposes.\n\n- [recipe-fixed-net-sta.xml](networkArm/recipe-fixed-net-sta.xml) : Query seismic stations from [IRIS-DMC FDSNWS](http://service.iris.edu/fdsnws/), with specic networks and stations.\n- [recipe-boxarea.xml](networkArm/recipe-boxarea.xml) : Query seismic stations within a box area.\n- [recipe-pointdist.xml](networkArm/recipe-pointdist.xml) : Query seismic stations within given distance range of the given lat & lon.\n- [recipe-boxarea-NC.xml](networkArm/recipe-boxarea-NC.xml) : Query seismic stations from [NCEDC FDSNWS](http://service.ncedc.org/).\n\n\n## waveformArm\n\nThose recipes contain all the three Arms, which are used to download seismic waveforms, including instrument responses (`poles and zeros` and/or `responses`), raw data (`seismograms-raw`), and seismic data with response remove (`seismograms`). You may revise, comment, or add some subsetters in the recipes according to your purposes. The recipes in the above `eventArm` and `networkArm` may be a starting reference.\n\n- [recipe-csvEvent-fixedNet.xml](waveformArm/recipe-csvEvent-fixedNet.xml) : Query seismic waveforms from [IRIS-DMC FDSNWS](http://service.iris.edu/fdsnws/). The catalog (i.e., `events.csv`) is already downloaded using other mehtod, and the networks and stations are set explicitly. You can comment the `stationOR` subsetter so that all the stations in the networks will be downloaded instead.\n- [recipe-csvEvent-fixedNet-phase.xml](waveformArm/recipe-csvEvent-fixedNet-phase.xml) : same as [recipe-csvEvent-fixedNet.xml](waveformArm/) except that a reference phase is used to limit the time window.\n- [recipe-continous.xml](waveformArm/recipe-continous.xml) : Query continous seismic waveforms from [IRIS-DMC FDSNWS](http://service.iris.edu/fdsnws/). I set the output format to be miniseed. Please see the discussion in [SOD email list](https://groups.google.com/a/seis.sc.edu/forum/#!searchin/sod/fake$20event%7Csort:date/sod/lEz3WpG1XNk/hby7SSzGVGcJ).\n- [recipe-IRISPH5.xml](waveformArm/recipe-IRISPH5.xml) : Query seismic waveforms from [IRIS-DMC's PH5 FDSNWS](http://service.iris.edu/ph5ws/). You need at least SOD 3.2.11 to use this recipe due to a bug in old version. A prerelease can be found [here](http://www.seis.sc.edu/downloads/sod/prerelease/3.2.11-SNAPSHOT/). It seems that it **DOES NOT** work now.\n- [recipe-SC.xml](waveformArm/recipe-SC.xml)      : Query seismic waveforms from [Southern California Earthquake Data Center (SCEDC) FDSNWS](https://service.scedc.caltech.edu/)\n- [recipe-NC.xml](waveformArm/recipe-NC.xml)      : Query seismic waveforms from [NCEDC FDSNWS](http://service.ncedc.org/)\n- [recipe-GEOFON.xml](waveformArm/recipe-GEOFON.xml)  : Query seismic waveforms from [GEOFON FDSNWS](http://geofon.gfz-potsdam.de/fdsnws/).\n- [recipe-ORFEUS.xml](waveformArm/recipe-ORFEUS.xml)  : Query seismic waveforms from [ORFEUS FDSNWS](http://www.orfeus-eu.org/fdsnws/)\n\n\n## Subsetter Notes\n\n### Data Servers\n\n- The default of [fdsnEvent](http://www.seis.sc.edu/sod/ingredients/fdsnEvent.html) is to query [USGS FDSN Event web service](https://earthquake.usgs.gov/fdsnws/event/1/), i.e., [ANSS Comprehensive Earthquake Catalog (ComCat)](https://earthquake.usgs.gov/earthquakes/search/). You can also query other Data Center's Event web service (e.g., [IRIS-DMC Event web service](http://service.iris.edu/fdsnws/event/1)).\n- The default of [fdsnStation](http://www.seis.sc.edu/sod/ingredients/fdsnStation.html) is to query [IRIS-DMC FDSN Station web service](http://service.iris.edu/fdsnws/station/1/). By default this does not get restricted channels unless there is a corresponding [fdsnDataSelect](http://www.seis.sc.edu/sod/ingredients/fdsnDataSelect.html) that has a `user` and `password` specified.\n- The default of [fdsnDataSelect](http://www.seis.sc.edu/sod/ingredients/fdsnDataSelect.html) is to query [IRIS-DMC FDSN Dataselect web service](http://service.iris.edu/fdsnws/dataselect/1/). To query restricted data, set `user` and `password` in [fdsnDataSelect](http://www.seis.sc.edu/sod/ingredients/fdsnDataSelect.html). Please see the discussion in [SOD email list](https://groups.google.com/a/seis.sc.edu/forum/#!topic/sod/Rfi_LRr8dwE)\n- To query at non-default Data Center, please refer to [FDSN web service](https://www.fdsn.org/webservices/) to see the supporting Data Centers and their hosts. Below are the host names for some Data Centers. Please see the discussion in [SOD email list](https://groups.google.com/a/seis.sc.edu/g/sod/c/7B0tWrEFeGQ).\n    - [IRIS-DMC](http://service.iris.edu/fdsnws/) : `service.iris.edu`\n    - [IRIS-DMC's PH5 web services](http://service.iris.edu/ph5ws/) : `service.iris.edu`\n    - [SCEDC](https://service.scedc.caltech.edu/) : `service.scedc.caltech.edu`\n    - [NCEDC](http://service.ncedc.org/) : `service.ncedc.org`\n    - [GEOFON](http://geofon.gfz-potsdam.de/fdsnws/): `geofon.gfz-potsdam.de`\n    - [ORFEUS](http://www.orfeus-eu.org/fdsnws/) : `www.orfeus-eu.org`\n- If we'd like to use [IRIS-DMC's PH5 web services](http://service.iris.edu/ph5ws/), we have to set the path of fdsnws (i.e., `fdsnwsPath`) to be `ph5ws` in the [fdsnEvent](http://www.seis.sc.edu/sod/ingredients/fdsnEvent.html), [fdsnStation](http://www.seis.sc.edu/sod/ingredients/fdsnStation.html), and [fdsnDataSelect](http://www.seis.sc.edu/sod/ingredients/fdsnDataSelect.html) subsetters, although the name is `fdsnPath` in the `fdsnEvent` and `fdsnDataSelect` in the online SOD manual. Please check the discussion about this issue in the [SOD email list](https://groups.google.com/a/seis.sc.edu/forum/#!topic/sod/j-rxZxYj1jQ). However, I am not sure if the name is actually `fdsnPath` or `fdsnwsPath` in the `fdsnEvent` subsetter. Maybe you can try both, and report it in the [SOD email list](http://www.seis.sc.edu/sod/) if there exists a bug.\n\n### eventChannel Subsetters\n\n- I use the subsetter [bestChannelAtStation](http://www.seis.sc.edu/sod/ingredients/bestChannelAtStation.html) in [eventChannel](http://www.seis.sc.edu/sod/ingredients/eventChannel.html), but it is unfortunately fragile because stations often have unusual characteristics that keep it from working. We may miss some channels that actually have data. We may not use the subsetter and choose to download all the channels, while we can also use other subsetters within the [eventChannel](http://www.seis.sc.edu/sod/ingredients/eventChannel.html). In the later case, you have to know the channels you want. Please see the discussion about this issue in the [SOD email list](https://groups.google.com/a/seis.sc.edu/forum/#!topic/sod/pWgzAkaggw0).\n\n### availableData Subsetters\n\n- Before SOD asks the data center to send it data, we can use subsetter in [availableData](http://www.seis.sc.edu/sod/ingredients/availableData.html) to ask Data Center if it has data for the time range generated by the request generator. These tactics allow decisions to be made based on the server's response. The default is [fullCoverage](http://www.seis.sc.edu/sod/ingredients/fullCoverage.html). I usually use [someCoverage](http://www.seis.sc.edu/sod/ingredients/someCoverage.html) to accept data if it spans some of the generated request.\n\n### seismogramProcess Subsetters\n\n- [someDataCoverage](http://www.seis.sc.edu/sod/ingredients/someDataCoverage.html) checks the data returned from Data Center against the request generated by the request generator. If at least one piece of data exists during the time specified by the request generator, this processor passes the data onto the next step.\n- [merge](http://www.seis.sc.edu/sod/ingredients/merge.html), [collapseOverlaps](http://www.seis.sc.edu/sod/ingredients/collapseOverlaps.html) and [gapFill](http://www.seis.sc.edu/sod/ingredients/gapFill.html) need to be carefully used. I usually use them in the above order. Be aware that `gapFill` and `merge` might shift later waveforms by up to 1/2 a sample interval in order to get the time basis aligned. Please check the disscussion in [SOD email list](https://groups.google.com/a/seis.sc.edu/g/sod/c/xbCzRkdk-_A).\n    - `merge`: If the seismogram server is returning multiple continuous seismograms for a request, this will join them into a single seismogram. Continuous means that the begin time of the second seismogram is approximately one sample period after the end time of the first seismogram. This processor **DOES NOT** merge overlapping seismograms or seismograms where there is a gap larger than one sample period. For those cases see `collapseOverlaps` and `gapFill` respectively.\n    - `collapseOverlaps`: it checks for overlapping seismograms. If one seismogram is completely contained in another seismogram, it is removed. If two seismograms overlap partially, then the longer of the two is kept in its entirety and the short is cut to no longer overlap This processor should ideally not be needed, as the server should refrain from returning overlapping data.\n    - `gapFill`: If the seismogram server is returning multiple continuous seismograms for a request, this will join them into a single seismogram filling any gaps. This **IMPLICITYLY DOES** a `collapseOverlaps` and a `merge` before filling gaps.\n- [noDataGaps](http://www.seis.sc.edu/sod/ingredients/noDataGaps.html) checks the data returned from Data Center for gaps larger than the sample period of the instrument. If from the begin time of the earliest piece to the end time of the latest piece there are no gaps in time larger than the sample period, this process passes the data onto the next item in the line.\n- If you use SOD to do the [transferResponse](http://www.seis.sc.edu/sod/ingredients/transferResponse.html) process, it is necessary to further multiply the waveform by `1.0e9` to convert from meters to nanometers. Please refer to SAC's `transfer` reference to check the reason (see `POLEZERO OPTION`). You may also refer to some Chinese tutorials about this issue: [Chinese SAC manual](https://seisman.github.io/SAC_Docs_zh/commands/transfer/) and [Difference when doing transfer using RESP and PZ](https://blog.seisman.info/resp-sacpz-difference/). In addition, choose right arguments for `transferResponse`. For example, f4 should be smaller than Nyquist frequency (if sampling rate is 0.01 s, then Nyquist frequency is 50 Hz).\n```bash\n$ SAC\n# see `POLEZERO OPTION`\n$ help transfer\n```\n\n\n## Tests\n\nSome tests I used to examine SOD subsetters and bugs.\n\n\n## References\n\n- If you'd like to know more about SOD, please refer to [SOD documentation](http://www.seis.sc.edu/sod/documentation/index.html) with [some tutorials of the major SOD aspects](http://www.seis.sc.edu/sod/documentation/tutorials/index.html). [A Chinese introduction](https://blog.seisman.info/sod-notes/) is also useful. You can also sign up [SOD email list](http://www.seis.sc.edu/sod/) for annoucements, discussions and questions about using SOD.\n- If you'd like to use different subsetters in eventArm, networkArm, waveformArm (I have indicated the locations of some unused subsetters in those recipes), please refer to the following websites:\n    - [SOD ingredients](http://www.seis.sc.edu/sod/ingredients/index.html)\n    - [Seisman's SOD recipes](https://github.com/seisman/SODrecipes)\n\n",
        "createdAt": "2020-07-28T13:47:32.000Z",
        "updatedAt": "2025-05-29T11:50:11.000Z",
        "language": "Shell",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/core-man/SOD.recipes/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "PALab/pyjamaseis",
        "url": "https://github.com/PALab/pyjamaseis",
        "description": "python-based helicorder software for school seismology ",
        "stars": 3,
        "forks": 3,
        "readme": "# pyjamaseis\npython-based helicorder software for school seismology \n",
        "createdAt": "2016-01-15T00:34:04.000Z",
        "updatedAt": "2021-03-05T00:59:53.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/PALab/pyjamaseis/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "NIA/seismoreg",
        "url": "https://github.com/NIA/seismoreg",
        "description": "Seismo recorder data processing program",
        "stars": 2,
        "forks": 0,
        "readme": "",
        "createdAt": "2013-02-11T18:18:08.000Z",
        "updatedAt": "2020-12-16T04:26:12.000Z",
        "language": "C++",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "sreyadhar/Publications",
        "url": "https://github.com/sreyadhar/Publications",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# Publications\n\nAll the publications from PhD has been uploaded in this repository\n",
        "createdAt": "2021-08-10T01:32:20.000Z",
        "updatedAt": "2021-11-23T04:15:15.000Z",
        "language": null,
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/sreyadhar/Publications/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "kumarDeepak-su/Seismology",
        "url": "https://github.com/kumarDeepak-su/Seismology",
        "description": "Seismology-MCS data reading. processing",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2025-09-10T10:16:36.000Z",
        "updatedAt": "2025-09-10T10:16:37.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "resfahani/HRRT",
        "url": "https://github.com/resfahani/HRRT",
        "description": "Linear Radon Transform (Slant stack) and its high-resolution version for dispersion curve estimation",
        "stars": 18,
        "forks": 5,
        "readme": "# HRRT\nHigh Resolution Linear Radon Transform (Slant stack)\n<br />\n\nThe package contains adjoint, Least squares, and iterative reweighted least squares Radon transform. \n\n<br />\n\nRadon Transform\n![alt text](https://github.com/resfahani/HRRT/blob/master/fig/adj.png)\n\n<br />\n\nLS Radon Transform\n\n![alt text](https://github.com/resfahani/HRRT/blob/master/fig/LS.png)\n\n<br />\n\nIRLS Radon Transform\n\n![alt text](https://github.com/resfahani/HRRT/blob/master/fig/IRLS.png)\n\n\nunder construction <br />\n",
        "createdAt": "2021-10-07T20:54:51.000Z",
        "updatedAt": "2025-08-21T11:19:34.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/resfahani/HRRT/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "pawbz/Interactive-Seismology.jl",
        "url": "https://github.com/pawbz/Interactive-Seismology.jl",
        "description": "Computational lab to provide the students with interactive simulations that help them to comprehend the concepts of seismology.",
        "stars": 9,
        "forks": 3,
        "readme": "# Interactive Seismology Notebooks\n\nA [collection](https://pawbz.github.io/Interactive-Seismology.jl/) of *reactive notebooks* using the [Julia](https://julialang.org/) notebook system, Pluto. These notebooks are designed to illustrate fundamental concepts in seismology in an interactive manner. They are primarily intended to support our courses on Seismology (ES218) and Inverse Problems (ES219), and are accessible to students with minimal programming experience. The notebooks contain all the essential mathematical expressions, which are easily manipulated using [Symbolics.jl](https://symbolics.juliasymbolics.org/dev/), a modern Computer Algebra System (CAS). By using these standalone notebooks, students can intuitively explore and gain a better understanding of wave phenomena in seismology.",
        "createdAt": "2022-09-20T02:17:48.000Z",
        "updatedAt": "2025-10-19T13:22:30.000Z",
        "language": "Julia",
        "homepage": "https://pawbz.github.io/Interactive-Seismology.jl/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/pawbz/Interactive-Seismology.jl/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "d-chambers/advanced_seismology",
        "url": "https://github.com/d-chambers/advanced_seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2023-03-22T23:58:14.000Z",
        "updatedAt": "2023-03-22T23:58:14.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "codepanda64/seistools",
        "url": "https://github.com/codepanda64/seistools",
        "description": "about   seismology tools",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2016-05-16T06:08:41.000Z",
        "updatedAt": "2016-05-16T06:08:41.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "akinremisa/ReceiverFunctionsNL",
        "url": "https://github.com/akinremisa/ReceiverFunctionsNL",
        "description": "Receiver function results from seismological networks in the Netherlands",
        "stars": 0,
        "forks": 0,
        "readme": "# ReceivedFunctionsNL\n\nReceiver function results from seismological networks in the Netherlands\n",
        "createdAt": "2023-09-16T10:36:16.000Z",
        "updatedAt": "2023-09-18T20:24:12.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/akinremisa/ReceiverFunctionsNL/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "hirokiu/csn_www",
        "url": "https://github.com/hirokiu/csn_www",
        "description": "website files for Citizen Seismology Network.",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2015-04-06T10:55:27.000Z",
        "updatedAt": "2022-10-16T14:13:47.000Z",
        "language": "JavaScript",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "roscibely/seismic-inversion",
        "url": "https://github.com/roscibely/seismic-inversion",
        "description": "Seismic inversion",
        "stars": 21,
        "forks": 5,
        "readme": "# Seismic inversion\n\n![Figure](https://github.com/roscibely/seismic-inversion/blob/main/abstract.png)\n\n## Package required:\n[CREWES](https://www.crewes.org/ResearchLinks/FreeSoftware/)\n\n[SeismicLAb](https://www.codebus.net/d-1R0W.html)\n\n[SeisLab for Matlab](https://www.mathworks.com/matlabcentral/fileexchange/53109-seislab-3-02)\n\n## File\n   @MastersThesis{rego2015,\n        title={Inversão Sísmica para Impedância Acústica: Estudo e Aplicação do Método em Dados Sintéticos},\n        author={Rego, R. C. B},\n        school={Federal Rural University of Semi-Arid Region},\n        type={Bachelor's Thesis},\n        year={2015}\n    }\n \n[Bachelor's Thesis file](https://github.com/roscibely/seismic-inversion/blob/main/TCC.pdf)\n\n## Publications related to this project\n\nRego, Rosana CB, and F. Ernandes M. Costa. [\"INVERSAO SÍSMICA EM MEIOS GEOLOGICOS COMPLEXOS.\"](https://www.researchgate.net/profile/R-C-B-Rego/publication/351662466_INVERSAO_SISMICA_EM_MEIOS_GEOLOGICOS_COMPLEXOS/links/60a3ec86299bf1569527a472/INVERSAO-SISMICA-EM-MEIOS-GEOLOGICOS-COMPLEXOS.pdf) (2015).\n",
        "createdAt": "2021-07-24T16:10:03.000Z",
        "updatedAt": "2025-08-18T20:08:15.000Z",
        "language": "MATLAB",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/roscibely/seismic-inversion/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "DomagojHrvojevic/Seismo_Codes",
        "url": "https://github.com/DomagojHrvojevic/Seismo_Codes",
        "description": "Python and Bash codes used in seismological data quality control, DAS measured data, ispaq data metrics, mseed files analysis, stations database, formatting mseed files and seismological data processing.",
        "stars": 1,
        "forks": 0,
        "readme": "# Seismo_Codes\nPython and Bash codes used in seismological data quality control, DAS measured data, ispaq data metrics, mseed files analysis, stations database, formatting mseed files and seismological data processing.\n\n## DATA_QUALITY_CONTROL\n  Set of codes used to develop quality control report of all active/chosen seismological stations.  \n  \n  <ins>**Scripts:**</ins>\n  * `DATA_QC.sh:` Bash script that incorporates _grab_weekly_stations_data.py_, _run_ispaq.py_ from **ISPAQ** [^1] library, _plot_ISPAQ_availability.py_, _data_qc_obspy.py_, _plot_ISPAQ.py_ and _data_quality_control_analysis.py_ to create a quality control report for all ACTIVE seismological stations.\n  * `data_qc_obspy.py:` Python code for creating plots of data availability by using information collected by **ObsPy** [^2] python library.\n  * `data_quality_control_analysis.py:` Python code for creating a .pdf file of quality control analysis of active seismological stations based on collected data.\n  * `grab_weekly_stations_data.py:` Python code for extracting weekly stations data and making them available locally.\n  * `plot_ISPAQ.py:` Python code for plotting useful information collected by **ISPAQ** python library.\n  * `plot_ISPAQ_availability.py:` Python code for creating plots of data availability by using information collected by **ISPAQ** python library.\n  * `smartsolo_qc_obspy.py:` Python code that generates a .pdf quality control analysis report of collected SmartSolo seismological stations data.\n  * `stanice_backup_qc.py:` Python code for creating a .pdf file of quality control analysis of seismological stations backup data.\n  * `stanice_backup_qc_obspy.py:` Python code for generating PSD/PDF graphs with **ObsPy** for stations that haven't got one created from **ISPAQ**.\n<kbd>\n  <p align=\"center\">\n    <img src=\"https://github.com/user-attachments/assets/40bf37cb-4ceb-4056-acdd-afd699d5b4d6\" width=\"20%\">\n    <img src=\"https://github.com/user-attachments/assets/fb54590e-7b1e-4966-8cd0-02e6796a1d50\" width=\"20%\">\n    <img src=\"https://github.com/user-attachments/assets/a1681e60-9037-49a1-b38a-252d6f167197\" width=\"20%\">\n    <img src=\"https://github.com/user-attachments/assets/34f8db1d-f26f-41ea-805f-da4a737485d4\" width=\"20%\">\n    <img src=\"https://github.com/user-attachments/assets/a1f8350c-e5eb-4438-be28-bba07b01f0dd\" width=\"20%\">\n    <img src=\"https://github.com/user-attachments/assets/c8e55808-a028-4dd0-8072-79cb99528326\" width=\"20%\">\n    <img src=\"https://github.com/user-attachments/assets/62316ef2-1eda-4b2d-85fb-64c9adfc62a6\" width=\"20%\">\n    <img src=\"https://github.com/user-attachments/assets/465c1858-9eea-42e0-aec0-23e0eea5677e\" width=\"20%\">\n  </p>\n</kbd>\n  <p align=\"center\">\n    <strong>Figure 1.</strong> An example of weekly quality control report (<a href=\"WEEKLY_REPORT_17_3_2025__23_3_2025.pdf\">PDF</a>)\n  </p>\n\n## DATA_SETUP_CODES\n  Codes used to organize data files (renaming, selecting, converting).\n\n<ins>**Scripts:**</ins>\n  * `backup_DATA_check.py:` Short python code for comparing local files with existing files on server.\n  * `rename_0.sh:` Short bash script for making all names of files in folder uppercase.\n  * `rename_plit_udbi.sh:` Short bash script for making all names of .msd files in directory uppercase and removing .msd extension.\n  * `SANDI_needed_data.py:` Short python code for removing all unnecessary hourly data files are not important for determining earthquakes on PLIT station.\n  * `transfer_needed_sandi_station_data.py:` Similar code to _SANDI_needed_data.py_, including checks for missing data and logging the stations with absent data.\n  * `uppercase.sh:` Similar code to _rename_0.sh_ with removing _.mseed_ extension.\n  * `XML_to_RESP.py:` Python test code for converting station _.XML_ files into _.RESP_ files. Real _.RESP_ files are made from _.XML_ files, that are written in FDSN format, by using command **xml2resp** [^3] from evalresp toolbox.\n\n## FEBUS_DAS_CODES\n  Programming Codes used to analyze data collected by DAS (Distributed Acoustic Sensing) instrument.\n\n<ins>**Scripts:**</ins>\n  * `das_data_test.py:` Python code for testing DAS (distributed acoustic sensing) data files. Using **DASPy** [^4] for data plotting, spike removal, downsampling, and detrending, presented as both waveform and spectrogram, as shown in Figure 2.\n  * `HDF5_data_analysis.py:` Python code for analysis of _HDF5_ data files recorded by FEBUS DAS instrument. Using **h5py** [^5] library for plotting StrainRate data and making them in gif with **imageio** [^6] library (figure 3).\n\n<kbd>\n  <p align=\"center\">\n    <img src=\"https://github.com/user-attachments/assets/5e6ce2ce-0512-4c21-ba0b-2075f28037bc\" width=\"20%\">\n    <img src=\"https://github.com/user-attachments/assets/a5f18c68-b938-4167-846f-51cf4b051fae\" width=\"20%\">\n    <img src=\"https://github.com/user-attachments/assets/3df9871a-226d-41a8-b4c7-9722a6c27022\" width=\"20%\">\n    <img src=\"https://github.com/user-attachments/assets/4b643d60-6304-43aa-9c05-86084f99c64d\" width=\"20%\">\n  </p>\n</kbd>\n  <p align=\"center\">\n    <strong>Figure 2.</strong> An example showing the original DAS data, spike removal, downsampling, detrending, and bandpass filtering. The gauge length is set to 50 m.\n  </p>\n\n<kbd>\n  <p align=\"center\">\n    <img src=\"https://github.com/user-attachments/assets/9e30cb89-c9a9-411a-bca3-1a7ed2ce5a1d\" width=\"100%\">\n  </p>\n</kbd>\n  <p align=\"center\">\n    <strong>Figure 3.</strong> Strain rate data recorded by the FEBUS DAS instrument, shown in GIF format. The gauge length is set to 20 m.\n  </p>\n\n## ISPAQ_CODES\n  Codes used to evaluate status of active seismological stations by using **ISPAQ** python library.\n\n<ins>**Scripts:**</ins>\n  * `ISPAQ_Status.py:` Python code that reads the status .txt files simpleMetrics and PSDMetrics, and outputs errors for seismograph stations. A short error report is sent via email by running the Bash script _mail.sh_.\n  * `ISPAQ_Status_test:` Similar to _ISPAQ_Status.py_ but for testing seismo-instruments.\n  * `mail.sh:` Bash script to send a report of all station problems as a .txt file via email by using **mailx** [^7].\n  * `PDF_run.sh:` Bash script for generating files and graphs of the power spectral density (PSD) and probability density function (PDF) for each seismic station using the **ISPAQ** library.\n  * `PDF_run_test.sh:` Same as _PDF_run.sh_, but for testing seismo-instruments.\n  * `run.sh:` Bash script for calculating customStats .csv files (data quality information). Then, the _ISPAQ_Status.py_ script condenses all SimpleMetrics information into a short error report, which is subsequently sent via email using the _mail.sh_ script.\n  * `run_test.sh:` Similar to _run.sh_, but for testing seismo-instruments.\n  * `run_v2.sh:` Similar to _run.sh_, but uses a different start-time variable.\n\n## MSEED_FILES_ANALYSIS\n  Codes for plotting, reading, naming, merging, formatting MiniSEED data files.\n\n<ins>**Scripts:**</ins>\n  * `backup_data_check_stats.py:` Python code for checking MiniSEED backup files stats.\n  * `check_mseed_stats:` Python code for checking MiniSEED files stats.\n  * `data_modules.py:` Python functions for converting recorded Scream data into hourly (suitable for SANDI) and daily mseed files.\n  * `DF05_analysis.py:` Python code for the analysis of the DF05 station. Uses _data_modules.py_ The script performs the following tasks:\n    * Transfers daily data from the server to the local machine\n    * Converts daily data into hourly segments\n    * Plots daily and hourly seismograms\n    * Calculates daily and hourly Power Spectral Density (PSD) and Probability Density Function (PDF)\n    * generates daily and hourly summary reports\n  * `format_smartsolo_to_sandi.py:` Formatting seismological data gathered by Smartsolo portable seismograph for SANDI software usage (hourly segments).\n  * `MSEED_files_assimilation.py:` Python code for assimilating two different data files: one from seiscomp (network transfer) and other from datalogger (data recorder). Taking into account file naming, MiniSEED status, data availability.\n  * `MSEED_files_assimilation_origin_not_important.py:` Similar to _MSEED_files_assimilation.py_, but this script is intended for merging two MiniSEED files of unknown origin.\n  * `mseed_metadata_standard_naming.py:` Python code for correctly setting MiniSEED station metadata (network, station, location, channel).\n  * `reading_seed_files.py:` Reading MiniSEED files with **ObsPy** library. Just a practice and for testing purposes.\n  * `soh_plot_data.py:` Reading SOH files with **ObsPy** library. Just a practice and for testing purposes.\n\n## STATIONS_DATABASE\n  Codes for creating database of seismological stations metadata.\n  \n<ins>**Scripts:**</ins>\n  * `stations_database.py:` Python code for making database of seismological stations, data and metadata. Using **sqlite3** [^8] python library. Contains functions for:\n    * creating database from **pandas** [^9] dataframe\n    * inserting/storing _.png_ picture in **SQLite** database (not practical)\n\n## STATIONS_VISUALIZATION\n  Codes for visualization of seismological stations on geospatial maps.\n\n<ins>**Scripts:**</ins>\n  * `PLIT_station_visualization_with_folium.py:` Plotting stations on base map with **Folium** [^10] python library.\n  * `Stations_visualization.py:` Python code for creating a _.gif_ file that visualizes the locations of all seismic stations from the stations.gpkg file, with each station displaying its metadata. Python libraries **geopandas** [^11] and **contextily** [^12] are used.\n  * `Stations_visualization_PLIT_basemap.py:` Python script for visualizing all seismo-stations locations for seismo-project. Using **geopandas** and **mpl_toolkits.basemap** libraries.\n  * `Stations_visualization_PLIT_contextily.py:` Similar to _Stations_visualization_PLIT_basemap.py_, but this time using **geopandas** and **contextily**.\n\n<p align=\"center\">\n  <img src=\"https://github.com/user-attachments/assets/f4603cf5-5f38-4eb6-a29d-9ebc466452c5\" height=\"200\">\n  <img src=\"https://github.com/user-attachments/assets/3ddd5e64-c0d7-458e-9998-3e7e7692f7e4\" height=\"200\">\n  <img src=\"https://github.com/user-attachments/assets/783933b6-06a7-4524-9505-13150ea75bb6\" height=\"200\">\n</p>\n<p align=\"center\">\n  <strong>Figure 4.</strong> Stations plotted from left to right using <strong>Basemap</strong>, <strong>Folium</strong>, and <strong>Contextily</strong>.\n</p>\n\n### References\n[^1]: [ISPAQ](https://github.com/EarthScope/ispaq) – Python command-line script that uses R packages to calculate seismology data quality metrics  \n[^2]: [ObsPy](https://github.com/obspy/obspy) – Python toolbox for seismology and seismological observatories  \n[^3]: [xml2resp](https://github.com/EarthScope/evalresp/blob/main/doc/xml2resp.1) – Converts StationXML input into RESP format  \n[^4]: [DASPy](https://daspy-tutorial.readthedocs.io/en/latest/index.html) – Open-source package for Distributed Acoustic Sensing (DAS) data processing  \n[^5]: [h5py](https://docs.h5py.org/en/latest/quick.html) – Container format for datasets and groups  \n[^6]: [Imageio](https://pypi.org/project/imageio/) – Python library for reading and writing image data and scientific formats  \n[^7]: [mailx](https://manpages.ubuntu.com/manpages/xenial/man1/bsd-mailx.1.html) – Intelligent mail processing system  \n[^8]: [sqlite3](https://docs.python.org/3/library/sqlite3.html) – Lightweight disk-based SQL database  \n[^9]: [pandas](https://pypi.org/project/pandas/) – Fast, flexible data structures for labeled/relational data  \n[^10]: [Folium](https://python-visualization.github.io/folium/latest/) – Builds interactive maps using Leaflet.js  \n[^11]: [GeoPandas](https://geopandas.org/en/stable/) – Easier handling of geospatial data in Python  \n[^12]: [contextily](https://contextily.readthedocs.io/en/latest/) – Python package for adding basemaps to plots\n",
        "createdAt": "2025-06-04T15:02:16.000Z",
        "updatedAt": "2025-07-28T12:26:30.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/DomagojHrvojevic/Seismo_Codes/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "resfahani/SparseTFR",
        "url": "https://github.com/resfahani/SparseTFR",
        "description": "Sparse Time-Frequency representation (Sparse STFT, and Sparse Stockwell transfrom)",
        "stars": 14,
        "forks": 6,
        "readme": "# SparseSTFT\nSparse Short Time Fourier Transform (SparseSTFT)\n\nThe algorithm is based on the constant window function and optimization in the time-domain with two examples.\n\n# Example \nThe figure shows the result of **Demo2.py** code.<br />\nThe top subplot shows the ground motion data recorded by Kik-Net stations at Japan. <br />\nThe middle subplot is the **Sparse TF Representation** of seismogram.<br />\nThe bottom subplot is the reisudal between observed and reconstructed data.<br />\n\n![alt text](https://github.com/resfahani/SparseSTFT/blob/master/Figures/Demo2.png)\n\n<br />\n\n**Take a look at second example ([link](https://github.com/resfahani/SparseSTFT/blob/master/Figures/Demo1.png))**\n\n\n# Citation\n\nD. D. Esfahani, Reza & Askari, Radwin & Gholami, Ali. (2018). Sparsity promoting method to estimate the dispersion curve of surface wave group velocity. GEOPHYSICS. 84. 1-40. 10.1190/geo2018-0138.1. \n\nGholami, Ali. (2013). Sparse Time-Frequency Decomposition and Some Applications. IEEE Transactions on Geoscience and Remote Sensing. 51. 3598-3604. 10.1109/TGRS.2012.2220144. \n\n",
        "createdAt": "2021-04-08T17:59:05.000Z",
        "updatedAt": "2025-04-10T00:07:25.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/resfahani/SparseTFR/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "estebanarivasv/P-I-2020-SeismologyFlask",
        "url": "https://github.com/estebanarivasv/P-I-2020-SeismologyFlask",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# :earth_americas: Seismology Proyect\n\nSubject: \"Programación I\"\n<br><br>\nIn 2020, I made a project for a subject called \"Programación I\" (Programming) at Universidad de Mendoza. \n\nWe worked with the Flask framework (including extensions) and we also learned about how do RESTFUL APIs work with WEB clients.\n<br><br>\n\n# :clipboard: Table of Contents\n- [:pencil: Description](#-pencil--description)\n      - [User-case diagram](#user-case-diagram)\n      - [UML Classes diagram](#uml-classes-diagram)\n- [:computer: Developing stages](#-computer--developing-stages)\n      - [(API) Phase 1: Client–server model](#-api--phase-1--client-server-model)\n      - [(API) Phase 2: Data storage](#-api--phase-2--data-storage)\n      - [(API) Phase 3: Authentication with JWT and email sending](#-api--phase-3--authentication-with-jwt-and-email-sending)\n      - [(WEB) Phase 4: Routes and templates](#-web--phase-4--routes-and-templates)\n      - [(WEB) Phase 5: Forms](#-web--phase-5--forms)\n      - [(WEB) Phase 6: Sessions and routes permissions](#-web--phase-6--sessions-and-routes-permissions)\n- [:information_source: Installation and usage for both API and Web client](#-information-source--installation-and-usage-for-both-api-and-web-client)\n      - [1 - Define the environment variables in the .env file](#1---define-the-environment-variables-in-the-env-file)\n      - [2 - Install dependencies](#2---install-dependencies)\n      - [3 - Launch Flask application](#3---launch-flask-application)\n      - [4 - Import requests file for the api in Insomnia or simply launch the web client](#4---import-requests-file-for-the-api-in-insomnia-or-simply-launch-the-web-client)\n\n\n# :pencil: Description\nThe aim of this project is simulating a Seismology Institute center where the main actors of the system are seisms, seismologists and the sensors.\n<br><br>\n@andrea.navarro, my teacher, came up with this idea. She learnt us how do the main system structure works and our job was adapt the project to the requirements.\n<br><br>\nThe project incluides the following items:\n- Make requests to sensos.\n- Save/modify seisms data (basic CRUD methods) with HTTP requests\n- Send emails to administrators of sensors that are not working\n- Web and api integration\n\n#### User-case diagram\nHere in this scheme, I described the general behaviour of the system. \nBasically we've desplayed the system and three different user types: the administrators, the seismologists and the analists or all the rest of the organization.\n\nEvery one of them has specific tasks:<br>\n**Administrators** are able to:\n- Assign sensors to seismologists.\n- Have access to existent sensors: modify, activate or deactivate.\n- Register new users.\n\n**Seismologists** can:\n- List left seisms to validate.\n- List assigned sensors.\n- Modify unverified seisms data. \n  - If it is an excessive amount of data to verify, they can download it. \n  - If there are not mistakes left, the users can validate the seism.\n\n**Analists** (or any other institute member) are be able to:\n- Access verified seisms.\n- Filter and download the sensors data in a CSV or ZIP file.\n  \nThe **system** must:\n- Send notifications to administrators whenever any active sensor stops working.\n\n<img src=\"https://i.ibb.co/VLqc45n/usecase-diag.png\"  width=\"800\">\n\n#### UML Classes diagram\nHere there are the system classes depicted. We have three main tables: users, seisms and sensors\n\n<img src=\"https://i.ibb.co/PrvMvqY/uml.png\"  width=\"800\">\n\n# :computer: Developing stages\n\nFlask framework natively works with routing, debugguing and WSGI (Web Server Gateway Interface). To make other things work, like for example: auth, you will need to include some extensions.\n<br><br>\nBasic components layout:\n<img src=\"https://docs.microsoft.com/es-es/dotnet/architecture/microservices/architect-microservice-container-applications/media/direct-client-to-microservice-communication-versus-the-api-gateway-pattern/custom-service-api-gateway.png\"  width=\"800\">\n\n#### (API) Phase 1: Client–server model\nFlask and API Rest introduction\n#### (API) Phase 2: Data storage\nSQLAlchemy (ORM: Object-Relational Mapping), modelos\n#### (API) Phase 3: Authentication with JWT and email sending\nFlask Mail (MTA: Mail Transport Agent), Flask JWT\n#### (WEB) Phase 4: Routes and templates\nBlueprints, Jinja2, Bootstrap library, macros, stylesheets\n#### (WEB) Phase 5: Forms\nFlask-WTF, form models\n#### (WEB) Phase 6: Sessions and routes permissions\nFlask-Login\n\n# :information_source: Installation and usage for both API and Web client\nSteps to follow in order to get the Flask app up and running\n\n#### 1 - Define the environment variables in the .env file\nYou can rename the .env-example file to .env\n\n:exclamation: Remember you need to declare all the variables including the database path. You can know where you are standing and declare them as the database path with these sentences:\n\n#### 2 - Install dependencies\nTo begin the instalation of libraries and the frameworks needed: `./install.sh`\n\n#### 3 - Launch Flask application\nTo get the app running: `./boot.sh`\n\n#### 4 - Import requests file for the api in Insomnia or simply launch the web client",
        "createdAt": "2020-11-24T20:12:24.000Z",
        "updatedAt": "2023-01-28T05:01:34.000Z",
        "language": "HTML",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/estebanarivasv/P-I-2020-SeismologyFlask/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "bclswl0827/heligo",
        "url": "https://github.com/bclswl0827/heligo",
        "description": "A seismic helicorder plotting library implemented in pure Go.",
        "stars": 2,
        "forks": 0,
        "readme": "# heligo\n\nA seismic helicorder plotting library implemented in pure Go. \n\n## Preview\n\n![Helicorder Plot](https://raw.githubusercontent.com/bclswl0827/heligo/master/preview/helicorder.svg)\n\n## Quick Start\n\nSee [example](https://github.com/bclswl0827/heligo/tree/master/example) for more details.\n",
        "createdAt": "2024-10-08T18:38:36.000Z",
        "updatedAt": "2025-09-08T06:24:58.000Z",
        "language": "Go",
        "homepage": "https://pkg.go.dev/github.com/bclswl0827/heligo",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/bclswl0827/heligo/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "adiboi123/deciphering-seismology",
        "url": "https://github.com/adiboi123/deciphering-seismology",
        "description": "Mini-projects that utilises Python from end-to-end to solve seismological problem statements.",
        "stars": 1,
        "forks": 0,
        "readme": "### <p style=\"text-align: center;\">Deciphering-Seismology: A repository of seismology mini-projects</p>\n<br>\n<br>\n\nThis project is a repository of modularised python source codes to solve specific seismological problems. Each `mini-project` contains a `README` file that summarises the approach for solving the problem statement, an `Example.ipynb` file that provides an executable to demonstrate the solution for resolving the problem statement and the `*.py` file which contains a library of the source files for the solution.\n<br>\n\nCheck out all the mini-projects uploaded in this repo.\n<br>\n<br>\n<u>*LinkedIn*</u>: https://www.linkedin.com/in/aditya-chowdhury-3b0646213/\n",
        "createdAt": "2024-03-25T07:26:43.000Z",
        "updatedAt": "2024-03-28T16:54:40.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/adiboi123/deciphering-seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "hsu000001/fluvial-seismology",
        "url": "https://github.com/hsu000001/fluvial-seismology",
        "description": "MATLAB scripts associated with Hsu, L.; Finnegan, N. J. & Brodsky, E. E. (2011), A seismic signature of river bedload transport during storm events, Geophys. Res. Lett. 38(13), L13407",
        "stars": 3,
        "forks": 4,
        "readme": "fluvial-seismology\n==================\n\nMATLAB scripts associated with Hsu, L.; Finnegan, N. J. &amp; Brodsky, E. E. (2011), \nA seismic signature of river bedload transport during storm events, Geophys. Res. Lett. 38(13), L13407\n\nScripts to analyze data from broadband seismometers to get hourly seismic wave amplitude.\n\nSee http://www.mathworks.com/help/signal/ref/hilbert.html for description of Hilbert function, used in env.m.\n",
        "createdAt": "2014-11-19T04:09:54.000Z",
        "updatedAt": "2024-03-08T00:53:23.000Z",
        "language": "Matlab",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/hsu000001/fluvial-seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "k-weng/earthquake",
        "url": "https://github.com/k-weng/earthquake",
        "description": "Seismological events analysis scripts for a seismology lab at Northwestern University",
        "stars": 0,
        "forks": 0,
        "readme": "# earthquake\nSeismological events analysis scripts for a seismology lab at Northwestern University\n",
        "createdAt": "2016-10-16T04:49:07.000Z",
        "updatedAt": "2016-10-16T05:01:44.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/k-weng/earthquake/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "deysandipan3-droid/cuddly-happiness",
        "url": "https://github.com/deysandipan3-droid/cuddly-happiness",
        "description": "It's all about earthquake seismology.",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2025-11-28T15:06:02.000Z",
        "updatedAt": "2025-11-28T15:06:02.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Mfarouk40/Sgraph",
        "url": "https://github.com/Mfarouk40/Sgraph",
        "description": "Seismo-GRAPHer: integrated tools for seismological data processing",
        "stars": 0,
        "forks": 0,
        "readme": "# Sgraph\nSeismo-GRAPHer: integrated tools for seismological data processing\n",
        "createdAt": "2020-03-08T12:47:01.000Z",
        "updatedAt": "2020-03-08T12:52:41.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Mfarouk40/Sgraph/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ArchRobison/SeismicDuck",
        "url": "https://github.com/ArchRobison/SeismicDuck",
        "description": "Reflection seismology game",
        "stars": 13,
        "forks": 0,
        "readme": "Seismic Duck\n============\n\n_Seismic Duck_ is a reflection seismology game.  See\nhttp://www.blonzonics.us/games/seismic-duck for an introduction to it.\n\nPre-Built Version for Windows\n=============================\n\nInstallers for Windows and MacOS _Seismic Duck_ can be dowloaded from\n[http://www.blonzonics.us/games/seismic-duck/]\n(http://www.blonzonics.us/games/seismic-duck/).\n\nBuilding from Source\n====================\n\nPrerequisites\n-------------\nFor the SDL2 version, you will need the following packages:\n* [Intel(R) Threading Building Blocks (Intel(R) TBB)](https://www.threadingbuildingblocks.org/download)\n* [SDL 2 development library](http://www.libsdl.org/download-2.0.php)\n* [SDL_image 2.0 development library](http://www.libsdl.org/projects/SDL_image)\n\nWindows\n-------\n\n_Seismic Duck_ for Windows can be built using SDL2 or DirectX 9.0c.  In the long term,\nI would like to move to SDL2 exclusively.  However, I am currently maintaining\nboth ports because the DirectX 9.0c version is capable of almost twice the\nframe rate of the SDL2 version.\n\nYou will need Visual Studio 2015, or rely on up-conversion to a newer version.\nThe Community edition suffices.\n\n### SDL2 Version\n\nThe solution file is in `Platform\\SDL-2.0\\VS2015` and\nassumes that prerequisites are in the following locations:\n* TBB is in `$(TBB40_INSTALL_DIR)`.\n* SDL2 headers are in `C:\\lib\\SDL2-2.0.5\\include` and the libraries\n  are in `C:\\lib\\SDL2-2.0.5\\lib\\x86`.\n* SDL_image headers are in `C:\\lib\\SDL2_image-2.0.1\\include` and the libraries are in `C:\\lib\\SDL2_image-2.0.1\\lib\\x86`.\n\n### DirectX 9.0c Version\n\nThe solution file is in `SeismicDuck\\Platform\\DirectX9\\VS2013\\SeismicDuck-DX9.sln`\nand assumes that TBB is in `$(TBB40_INSTALL_DIR)`\n\nMacOS\n=====\n\n_Seismic Duck_ for MacOS uses SDL2 and TBB.\n\nThe build process is Unix style with a Makefile.  \nAssuming bash is your shell, the steps are:\n\n1.  `source /opt/intel/tbb/bin/tbbvars.sh`.  This step sets environment variables `LIBRARY_PATH` and `CPATH` tell the compiler and linker where to find TBB headers and libraries.\n2.  `cd Platform\\SDL-2.0\\MacOS`\n3.  Run `make`, which should build a bunch of `.o` files and link them to an executable `seismic-duck-2.0`.\n4.  Run `seismic-duck-2.0`.\n\nLinux\n=====\n\nThere are no ports yet to Linux.  In principle the SDL2 version should\nbe straightforward to port to other platforms.  Please file an issue if you run\ninto problems doing the port.  If you get a port working, please consider\ncontributing your changes.\n",
        "createdAt": "2014-02-08T23:01:29.000Z",
        "updatedAt": "2025-08-09T15:26:58.000Z",
        "language": "C++",
        "homepage": "http://www.blonzonics.us/games/seismic-duck",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ArchRobison/SeismicDuck/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "fabriziobernardi/wavesdownloader",
        "url": "https://github.com/fabriziobernardi/wavesdownloader",
        "description": "Seismological data discovering, downloading, pre-processing and plotting",
        "stars": 6,
        "forks": 1,
        "readme": "\nWAVESDOWNLOADER\n---------------\nPython based tool for seismological data discovery, \ndownloading, pre-processing and plotting\n\n\n==  Version 1.2.03  - 2013.08.08  ==\n\nDownload:\n---------\nhttp://webservices.rm.ingv.it/wavesdownloader/\n\n\nINSTALL: \n-------- \n\n1. Please install theses packages first:\n - scipy [http://www.scipy.org/]\n - numpy [http://numpy.scipy.org/]\n - obspy [http://obspy.org/]\n\n - For Mac Os a dmg file for easy install, \n   including the three packages, is avaliable:\n\n   http://dl.dropbox.com/u/3866312/ObsPy.dmg \n\n \n2. Instal suds module: \n   - linux: https://fedorahosted.org/suds\n   - mac:   sudo easy_install -z suds \n\n3. Instal rdseed and add the PATH into your .profile/.bash/....\n   http://www.iris.edu/forms/rdseed_request.htm\n\n4. chmod +x wavesDownloader.py\n\nEIDA REGISTRATION:\n------------------\n\nPlease fill form at http://webservices.rm.ingv.it/ingv_ws_registration.php for automatic registration to access EIDA services\n\n\nUSAGE:\n------\nusage: wavesdownloader.py [-h] [--beg BEG] [--end END] [--len LEN] [--usr USR]\n                          [--pas PAS] [--sta STA] [--net NET] [--loc LOC]\n                          [--cha CHA] [--rot ROT] [--res RES] [--mode MODE]\n                          [--center CENTER] [--radius RADIUS]\n                          [--supCor SUPCOR] [--infCor INFCOR]\n                          [--format FORMAT] [--outdir OUTDIR]\n                          [--server SERVER] [--fsfile FSFILE]\n                          [--rmgaps RMGAPS] [--mingap MINGAP]\n                          [--maxgap MAXGAP] [--reject REJECT] [--cfreq CFREQ]\n                          [--demean DEMEAN] [--bandpass BANDPASS]\n                          [--highpass HIGHPASS] [--lowpass LOWPASS]\n                          [--wfiltr WFILTR] [--deci DECI] [--deco DECO]\n                          [--flim FLIM] [--slta SLTA] [--wcf WCF] [--pgm PGM]\n                          [--pgmfile PGMFILE] [--shake SHAKE] [--sa SA]\n                          [--pltmode PLTMODE] [--pltchan PLTCHAN]\n                          [--pltNERT PLTNERT] [--pltazi PLTAZI]\n                          [--summary SUMMARY]\n\nDownload seeds from archives, pre-process and process  data for use\n\noptional arguments:\n-------------------\n  -h, --help           show this help message and exit\n  --beg BEG            Begin Time. !! No defaults!! Format YYYY-MM-DDThh:mm:ss\n                       (UTC Time) (e.g.: 2011-07-25T12:30:00)\n  --end END            End Time. !! No defaults!! F\\ormat YYYY-MM-DDThh:mm:ss\n                       (UTC Time) (e.g.: 2011-07-25T12:35:00)\n  --len LEN            Length of signal in seconds. !! No defaults!! Mandatory\n                       option if --end not specified\n  --usr USR            User name for eida !! MANDATORY OPTION, No defaults!!\n  --pas PAS            Passwd for eida \\!! MANDATORY OPTION, No defaults!!\n  --sta STA            Station list. default=*\n  --net NET            Network list. default=*\n  --loc LOC            Station information location ID. default=*\n  --cha CHA            Channel list. default=BHZ\n  --rot ROT            Rotate horizontal components from North-East -> Radial-\n                       Trasversal. Only with --mode “center”. Default\n                       [Y]/N.\n  --res RES            Instrument response extraction: 0=none; 1=RESP; 2=PAZ\n                       (default); 3=RESP&PAZ\n  --mode MODE          Area selection mode: [circular|rectangular]\n                       default=circular\n  --center CENTER      Lat Lon inner position for circular request. Default\n                       \"41.9 12.5\"\n  --radius RADIUS      Radius in DEGREE from innerPos to outerPos for --mode\n                       circular. Default=\"0 10\"\n  --supCor SUPCOR      Max latitude and longitude for --mode rectangular.\n                       Default=\"60 60\"\n  --infCor INFCOR      Min latitude and longitude for --mode rectangular.\n                       Default=\"10 10\"\n  --format FORMAT      file format extraction storage\n                       [SAC,SACXY,GSE1,GSE2,SH_ASC,WAV]. If format=SAC no data\n                       extracted. Default=None\n  --outdir OUTDIR      directory for data extraction. Default=data\n  --server SERVER      servers [EIDA,IRIS,LOCAL,WEBDC]. LOCAL look for fseed\n                       files stored on your local machine. Only fseed files\n                       are allowed. --server \"LOCAL\" needs --fsfile to be\n                       specified. LOCAL and external server are allowed within\n                       the same request. Default=\"EIDA WEBDC IRIS\"\n  --fsfile FSFILE      fseed file name inclusive of path if path different\n                       than \\. Default None\n  --rmgaps RMGAPS      Remove traces with gaps. default=Y\n  --mingap MINGAP      Minimum gap allowed in seconds. default=0\n  --maxgap MAXGAP      Maximum gap allowed in seconds. default=0\n  --reject REJECT      Minimum length in percent for trace rejection.\n                       default=100\n  --cfreq CFREQ        Get dominant period [N]/Y.\n  --demean DEMEAN      Remove mean and trend. Default=Y\n  --bandpass BANDPASS  Bandpass filter \"corners fimn fmax\". No Defaults. E.g.:\n                       \"2 0.01 0.1\"\n  --highpass HIGHPASS  Highpass filter \"corners freq\". No Defaults. E.g.: \"2\n                       0.01\"\n  --lowpass LOWPASS    Lowpass filter \"corners freq\". No Defaults. E.g.: \"2\n                       0.1\"\n  --wfiltr WFILTR      Write new filtered files into --wfiltr path. Default=N\n  --deci DECI          Decimation factor for sampling rate. Only integer\n                       decimation factor allowed. Default=None\n  --deco DECO          Deconvolution from instrument response [N]/Y. Requires\n                       --res=[2|3]. Default=N\n  --flim FLIM          Corner frequency for deconvolution filtering. Defaults\n                       0.002 0.005 0.5 1\n  --slta SLTA          Make Short-term/long-term average and trigs picks:\n                       --slta \"STA LTA ON OFF\" (--sta \"0.5 5 7 1.5\").\n                       Default=”None” STA: Short-term average LTA: Long-\n                       term average ON: level trigger on OFF: level trigger\n                       off\n  --wcf WCF            Write characteristic function from slta into sac file\n                       format [N]/Y. Only binary sac file format allowed. No\n                       need to use --format option\n  --pgm PGM            Peaks Ground Motion parameters. max_displacement [m],\n                       max_velocity[m/s], max_acceleration[m/s^2]. Output into\n                       summary3.log file. Automatically enable --sa option\n                       (Spectral acceleration response). Default [N]/Y\n  --pgmfile PGMFILE    Name file for shakemap. User file-name MUST end with\n                       \"_dat.xml\" in order to be accepted by ShakeMap.\n                       Default=shakeList_dat.xml\n  --shake SHAKE        Write PGMs for ShakeMap. [N]/Y\n  --sa SA              Modify defaults Spectral Acceleration Response\n                       parameters (only if –pgm=”Y”): damping and corner\n                       frequencies in Hz. Example: --sa “0.1 1 10”;\n                       damping factor (0.1) and corner frequencies in Hz.\n                       Spectral Acceleration Response in [m/s^2]. Defaults:\n                       sa=”0.05 3.33 1.00 3.33”.\n  --pltmode PLTMODE    plot traces. 0=No plot; 1=y_axe regular; 2=y_axe\n                       distance from epicenter. Plots saved in pdf format, see\n                       Notes for details. (Default=0). Plot names:\n                       plotWavesZ.pdf for vertical; plotWavesNS.pdf and\n                       plotWavesEW.pdf with --pltNERT \"NE\"; plotWavesT.pdf and\n                       plotWavesR.pdf with --pltNERT \"RT\"\n  --pltchan PLTCHAN    channel to plot. Default=None\n  --pltNERT PLTNERT    horizontal components [NE|RT]. Default=None !! ONLY\n                       with --rot=Y\n  --pltazi PLTAZI      Plot traces within azimuth. Default=\"0 360\"\n  --summary SUMMARY    Print data request summary on screen. [N]/Y.\n\n  SUMMARY-LOG FILES:\n  ------------------\n \nSummary.log files are automatically saved into the --outdir directory:\n  \n1. summary1.log  :\n\nIncludes a summary of all option used to run wavesdownloader\n\n2. summary2.log  : \nList downloaded stations and metadata.\n\tFormat: One line for each station.channel data. Metadata for each line: \n     \t\tStationCode  \n\t\tNetwork \n\t\tChannel \t\n\t\tLocationCode \n\t\tBegTime \n\t\tEndTime \n\t\tSamp_rate \t\n\t\tdT \n\t\tStation_lat \n\t\tStation_lon \n\t\tEvent_lat \n\t\tEvent_lon \n\t\tEpicentral_distance (degree) \n\t\tAzimut \n\t\tBack_azimuth \t\n\t\tEpicentral_distance (km)\n\n\tExample:\n     \tASQU  IV HHE -- 2012-01-27T14:53:00.00Z   2012-01-27T14:55:00.00Z   100.00   0.0100       12001 43.797  11.789  44.480  10.033  159.811 1.437   117.768 298.987 \n\tASQU  IV HHN -- 2012-01-27T14:53:00.00Z   2012-01-27T14:55:00.00Z   100.00   0.0100       12001 43.797  11.789  44.480  10.033  159.811 1.437   117.768 298.987 \n\tASQU  IV HHZ -- 2012-01-27T14:53:00.00Z   2012-01-27T14:55:00.00Z   100.00   0.0100       12001 43.797  11.789  44.480  10.033  159.811 1.437   117.768 298.987 \n\tBDI      IV HHE -- 2012-01-27T14:53:00.00Z   2012-01-27T14:55:00.00Z   100.00   0.0100       12001 44.062  10.597  44.480  10.033  64.692  0.582   135.664 316.056 \n\tBDI      IV HHN -- 2012-01-27T14:53:00.00Z   2012-01-27T14:55:00.00Z   100.00   0.0100       12001 44.062  10.597  44.480  10.033  64.692  0.582   135.664 316.056 \n\tBDI      IV HHZ -- 2012-01-27T14:53:00.00Z   2012-01-27T14:55:00.00Z   100.00   0.0100       12001 44.062  10.597  44.480  10.033  64.692  0.582   135.664 316.056 \n\n3. summary3.log  : \nList downloaded stations, metadata, PGMs, central frequency and picker values. 2 Lines of format explaining header.\n\tFormat: One line for each station.channel data. Metadata and values for each line: \n     \t\tStationCode  \n\t\tNetwork \n\t\tChannel \t\n\t\tLocationCode \n\t\tStation_lat \n\t\tStation_lon \n\t\tEvent_lat \n\t\tEvent_lon \n\t\tEpicentral_distance (km) \n\t\tAzimut\n\t\tPeakGroundDisplacement [m]\n\t\tPeakGroundVelocity [m/s]\n\t\tPeakGroundAcceleration [m/s^2]\n\t\tList of Spectral Acceleration Response at different corner frequencies [m/s^2] (may be empty)\n\t\tCentral Frequencies [Hz] (may be empty)\n\t\tShort-term average\n\t\tLong-term average\n\t\tPicker trigger on value\n\t\tPicker trigger off value\n\t\tPicking list (Sample after begin trace)\n\n\tExamples:\n     \tASQU  IV    HHE   --      43.797  11.789  44.480  10.033     159.8 117.768         8.530e-05         2.395e-04         4.507e-03                    1.230e-02 3.142e-03 2.264e-04      1.528e-01   -1.00   -1.00   -1.00   -1.00  -1\n\t\n\n      \tBOB   IV    HHZ   --      44.768   9.448  44.480  10.033      56.4 304.766         2.873e-04         1.800e-03         3.392e-02                    6.413e-02 1.634e-02 1.560e-03      1.032e+00    1.00    7.00    5.00    1.00  [2688 3251][3657 4070]\n\t\n\tNote: -1.00 values means that shrt/long term average and trigger are off\n\n  !!! FAILURES:\n  --------------\n  --server        Servers busy or not responding may return into script failure.\n                  run the script later or remove the server which causes error.\n\n  NOTES:\n  ------\n  --cha           Wildcard allowed only for one channel. e.g.: \"BH*\" or \"HH*\"\n                  but not \"BH* HH*\". Use \"BHZ BHN BHE HHZ HHN HHE\" instead.\n  --reject 100    means only traces with full required length are hold\n  --reject 10     means all traces that are only a 10% or shorter with\n                  respect to the total required length are removed\n  --rot           When --rot==Y and --reject < 100,              \n                  rotation of the horizontal component may fail.\n                  Rotation requires identical record length of the\n                  horizontal components. Thus  --rot==Y force --reject 100.\n                  It may occurs that --rot=\"Y\" and --reject 100 do not result true\n                  because of files synchronization.\n  --server \"IRIS EIDA\"\n                  Generally EIDA provides real-time data while IRIS not\n  --pltmode       When enabled, this option also save the plots into pdf file format into \n                  --outdir path. Plot names: plotWavesZ.pdf for vertical; plotWavesNS.pdf\n                  and plotWavesEW.pdf with --pltNERT \"NE\"; plotWavesT.pdf and plotWavesR.pdf\n                  with --pltNERT \"RT\" \n                  When --slta != None and pltmode activated, the picks are plotted automatically\n  --pgm --slta    Applied after deconvolution and filtering  \n \n\n  EXAMPLES:\n  ---------\n\n  1. Circular download.\n  ---------------------\n  ./wavesdownloader.py --usr user@network.net --pas myPassword --beg 2011-01-29T17:41:00 --end 2011-01-29T17:45:00 --center \"47.56 18.34\" --radius \"0 6\" \n\n  2. Circular Download with server EIDA selected and plot (y-axe = Distance [km]).\n  --------------------------------------------------------------------------------\n  ./wavesdownloader.py --usr user@network.net --pas myPassword --beg 2011-05-26T11:35:00 --len 300 --center \"42.94 11.05\" --radius \"0 1.8\"  --server \"EIDA\" --res 3 --pltmode 2 --pltcha B\n\n  3. Rectangular Download with network, channel selectd and plot.\n  --------------------------------------------------------------- \n  ./wavesdownloader.py --usr user@network.net --pas myPassword --beg 2011-01-29T06:55:00 --end 2011-01-29T07:15:00 --mode rectangular --infCor \"40 -10\" --supCor \"85 30\" --net \"MN II\" --cha \"BH*\" --pltmode 1 --pltcha B\n\n  4. Rectangular Download with bandpass filter from IRIS, no user and password required. \n  --------------------------------------------------------------------------------------\n  ./wavesdownloader.py --beg 2011-04-01T13:29:00 --end 2011-04-01T13:49:00 --mode rectangular --infCor \"20 10\" --supCor \"60 60\" --server \"IRIS\" --net \"MN GE II\" --cha \"BH*\" --pltmode 1 --bandpass \"2 0.01 0.02\" --pltcha B\n\n  5. Circulare Download with rotation to GCP and azimuthal selected plot. Lopass filtered data ar stored into \"filterd\" subdirectory. End timewindow 600 second after begin Time.\n  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  ./wavesdownloader.py --usr user@network.net --pas myPassword --beg 2011-07-17T18:30:00 --len 600 --center \"45.01 11.41\" --radius \"0 2\" --cha \"BH*\" --pltmode 2 --rot \"Y\" --pltNERT \"RT\" --pltazi \"0 180\" --lowpass \"4 0.5\" --wfiltr \"filtered\" --format SAC --pltcha B\n\n  6. Circulare Download with multiple channel list selection.\n  -----------------------------------------------------------\n  ./wavesdownloader.py --usr user@network.net --pas myPassword --beg 2011-07-17T18:30:00 --len 600 --center \"45.01 11.41\" --radius \"0 2\" --server \"EIDA\" --cha \"BHZ BHN BHE HHZ HHN HHE\" \n\n  7. Circular Download with slta picking and lowpass filter.\n  ----------------------------------------------------------\n  ./wavesdownloader.py --usr user@network.net --pas myPassword --beg 2011-07-17T18:29:00 --end 2011-07-17T18:34:00 --center \"45.01 11.41\" --radius \"0 1\" --server \"EIDA\" --cha \"BHZ\" --pltmode 2  --pltcha B --cfreq \"Y\" --slta \"1 5 4 1\" --lowpass \"4 1\" --wfiltr filtered --format SAC\n\n  8. Circular Download from IRIS server including local fseed file as a data source. Instrument response deconvolution and user specified corner frequencies for deconvolution. No user and Password required. Default --res 2.\n  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  ./wavesdownloader.py --beg 2011-07-17T18:29:00 --end 2011-07-17T18:34:00 --center \"45.01 11.41\" --radius \"0 20\" --server \"IRIS LOCAL\" --fsfile \"mypath/myfile.fseed\" --net \"II MN IU\" --cha \"BHZ\" --pltmode 2  --cfreq \"Y\" --slta \"1 5 4 1\" --lowpass \"4 1\"  --deco Y --flim \"0.01 0.05 0.1 1\" --pltcha B\n\n  9. ShakeMap configuartion. Corner frequencies limits for deconvolution should be adapted for high frequencies. Pgm values stored into user specified file \"ShakePga_dat.xml\".\n  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n   ./wavesdownloader.py --usr user@network.net --pas myPassword --beg 2012-05-29T10:54:00 --len 200 --center \"44.89 11.01\" --radius \"0 4\" --server EIDA --cha \"HH*\" --res 3 --deco Y --pgm Y --shake Y --pgmfile ShakePga_dat.xml --flim \"0.05 0.1 20 40\"\n",
        "createdAt": "2013-08-08T14:26:25.000Z",
        "updatedAt": "2023-11-03T04:41:24.000Z",
        "language": "Python",
        "homepage": "http://webservices.rm.ingv.it/wavesdownloader",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/fabriziobernardi/wavesdownloader/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "jsh9/fast-konno-ohmachi",
        "url": "https://github.com/jsh9/fast-konno-ohmachi",
        "description": "A Python library that performs Konno-Ohmachi filtering very fast",
        "stars": 6,
        "forks": 0,
        "readme": "# _Fast_ Konno-Ohmachi\n\nA Python library that performs Konno-Ohmachi spectral smoothing very fast (2x speedup compared to the \"vanilla\" Konno-Ohmachi smoothing algorithm).\n\n## Background\nKonno-Ohmachi is a smoothing algorithm proposed by Konno & Ohmachi (1998) [[abstract](http://bssa.geoscienceworld.org/content/88/1/228.short), [PDF](http://www.eq.db.shibaura-it.ac.jp/papers/Konno&Ohmachi1998.pdf)], which achieves a \"uniform-span\" smoothing to frequency spectra in the logarithmic scale.\n\nFor lower frequencies, the Konno-Ohmachi smoothing window is narrower (i.e., less smoothing), and for higher frequencies, the window is wider (i.e., more smoothing).\n\nThis makes the Konno-Ohmachi filter particularly appealing to seismologists, who often try to avoid over-smoothing lower frequencies (< 10 Hz) of seismic wave signals.\n\nThe plot below shows the result of Konno-Ohmachi filter versus a regular [median value filter](https://en.wikipedia.org/wiki/Median_filter). The two filters yield similar results for frequency > 5 Hz, but for lower frequencies, the median filter over-smoothes the original signal, which is undesirable.\n\n![](demo.png)\n#### (The raw signal used in this example is the Fourier amplitude spectrum of a ground acceleration waveform recorded during the [Magnitude-9.0 Tohoku-Oki Earthquake on March 11, 2011](https://en.wikipedia.org/wiki/2011_T%C5%8Dhoku_earthquake_and_tsunami).)\n\n## Computation speed\n\nConventionally, Konno-Ohmachi filtering is time-consuming because its smoothing windows are different at each frequency and need to be calculated one by one.\n\nThis library achieves a 2x speedup by pre-calculating smoothing windows (i.e., trading memory space for speed).\n\nIt can speed up calculation even further by performing parallel computing (the `faster_konno_ohmachi()` function).\n\nThe only minor compromises in order to achieve the 2x speedup are:\n- Only even integer smoothing strengths from 2 to 100 are supported. This is not an issue in reality because people rarely need non-integer smoothing strengths.\n- The smoothing results from `fast_konno_ohmachi()` and `faster_konno_ohmachi()` are not entirely identical to the smoothing result from `slow_konno_ohmachi()` (the \"vanilla\" algorithm). However, the differences are too minor to have any practical implications.\n\n## Installation\n\n```bash\npip install fast-konno-ohmachi\n```\n\n## Usage\n\n```python\nimport numpy as np\nimport fast_konno_ohmachi as fko\n\nfreq = np.arange(0, 2, 0.1)  # just an arbitrary example\nsignal = np.sin(freq)\n\nsmoothed = fko.fast_konno_ohmachi(signal, freq, smooth_coeff=40, progress_bar=True)\n```\n\nor (calculate in parallel)\n\n```python\nsmoothed = fko.faster_konno_ohmachi(signal, freq, smooth_coeff=40, n_cores=4)\n```\n\nor (if you'd like to see how slow the \"vanilla\" implementation can be)\n\n```python\nsmoothed = fko.slow_konno_ohmachi(signal, freq, smooth_coeff=40, progress_bar=True)\n```\n\nYou can also try to run `demo/Demo_konno_ohmachi_smooth.py` to smooth a real-world signal. (You'd need `scipy` and `matplotlib` to run the demo script.)\n\n## Additional notes on parallel computing\n\n1. When using `faster_konno_ohmachi()`, the user should to protect the main script with `if __name__ == '__main__'` (see the demo script). This is **mandatory** for Windows, and **highly recommended** for Mac/Linux.\n2. The `faster_konno_ohmachi()` function uses multiple CPU cores, but it is not necessarily faster than `fast_konno_ohmachi()`, because the data I/O between the CPU cores takes extra time (\"computation overhead\"). Below is a benchmarking of the running time for input signals with different length:\n\n| Length of  signal (x1000) | 1    | 3    | 5    | 7    | 9    | 11   | 13   | 15   | 17   | 19   | 21   | 23   | 25   | 27   | 29   | 31   |\n| ------------------------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n| Time of \"fast\" (sec)      | 0.1  | 0.4  | 0.9  | 1.5  | 2.2  | 3.3  | 4.4  | 5.4  | 6.8  | 8.1  | 9.6  | 11.2 | 13.1 | 14.9 | 16.7 | 18.8 |\n| Time of \"faster\" (sec)    | 2.4  | 2.4  | 2.8  | 3.0  | 3.3  | 3.9  | 4.1  | 4.6  | 4.9  | 5.5  | 5.8  | 6.7  | 7.5  | 8.2  | 9.0  | 9.8  |\n\n\nOr as shown in this figure:\n\n![](./benchmark.png)\n",
        "createdAt": "2021-04-05T00:56:14.000Z",
        "updatedAt": "2025-06-21T15:17:42.000Z",
        "language": "Python",
        "homepage": "https://pypi.org/project/fast-konno-ohmachi/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/jsh9/fast-konno-ohmachi/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "foobarbecue/ultrahelicorder",
        "url": "https://github.com/foobarbecue/ultrahelicorder",
        "description": "Attempt to create a better live seismic data display",
        "stars": 1,
        "forks": 0,
        "readme": "# ultrahelicorder\nExperimenting with live seismic displays based on holoviews\n\nDemo sometimes online at http://sei.sm:8121\n\nTo run on your local machine:\n 1. `git clone` this repository\n 2. `cd` into cloned repository\n 3. `pipenv update`\n 3. `pipenv run python serve.py`",
        "createdAt": "2018-05-07T03:48:43.000Z",
        "updatedAt": "2022-01-05T13:37:46.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/foobarbecue/ultrahelicorder/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "AlbertSeismo/Seismology_Course",
        "url": "https://github.com/AlbertSeismo/Seismology_Course",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "![Seismology](https://github.com/DIG-Kaust/Seismology/blob/main/logo.png)\n\nTeaching material for ErSE 210 Seismology course to be held at KAUST during the Fall semester.\n\n## Material\n\nThe repository is organized as follows:\n\n- **Slides**: deck of slides summarizing the key concept introduced in each class. Some of the figures in these slides are taken from the reference textbook (Shearer, P., Introduction to Seismology). \n- **Data**: input data used in the practical sessions:\n- All of the other folders in this repository contains Python codes and Jupyter Notebooks used in the practical sessions:\n\n   - [**PlaneWave**](https://github.com/DIG-Kaust/Seismology/blob/main/PlaneWave/PlaneWave.ipynb): create and display plane waves in time-space and wavenumber domain.\n   - [**GassmannFluidSub**](https://github.com/DIG-Kaust/Seismology/blob/main/GassmannFluidSub/Gassmann.ipynb): implement basic rock physics equations and Gassmann substitution and apply it to the Smehaia well log.\n   - [**SeismicModelling**](https://github.com/DIG-Kaust/Seismology/blob/main/SeismicModelling/SeismicModellingInversion.ipynb): perform convolutional and AVO modelling, and apply pre-stack inversion\n   - [**RayTrace**](https://github.com/DIG-Kaust/Seismology/blob/main/RayTrace/RayTrace.ipynb): implement 2D raytracing by solving the associated ODE\n   - [**SeismicTomography**](https://github.com/DIG-Kaust/Seismology/blob/main/SeismicTomography/SeismicTomography.ipynb): create the 2D tomographic matrix and solve the associated inverse problem\n   - [**Obspy**](https://github.com/DIG-Kaust/Seismology/blob/main/Obspy/ObspyIntro.ipynb): a short introduction to Obspy and its usage for epicenter localization of earthquakes\n   - [**ReflectionSeismic**](https://github.com/DIG-Kaust/Seismology/blob/main/ReflectionSeismic): implement basic NMO processing and learn how to work with SEGY files using *segyio* and the Volve dataset.\n\n## Environment\n\nTo ensure reproducibility of the results, we have provided an `environment.yml` file. Ensure to have installed Anaconda or Miniconda on your computer. If you are not familiar with it, we suggest using the \n[KAUST Miniconda Install recipe](https://github.com/kaust-rccl/ibex-miniconda-install). This has been tested both on macOS and Unix operative systems.\n\nAfter that simply run:\n```\n./install_env.sh\n```\nIt will take some time, if at the end you see the work `Done!` on your terminal you are ready to go!\n\n## Binder\n\nAlternatively, you can work directly on Binder. Simply click this button and access\nthe material from your web browser without the need for any local installation\n\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/DIG-Kaust/Seismology/HEAD)",
        "createdAt": "2022-06-30T16:37:53.000Z",
        "updatedAt": "2022-06-30T16:38:27.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/AlbertSeismo/Seismology_Course/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "javiquinte/sc3microapi",
        "url": "https://github.com/javiquinte/sc3microapi",
        "description": "Micro web service that exposes basic information from a SeisComP3 system with the aim of integrate it with other systems.",
        "stars": 1,
        "forks": 0,
        "readme": "# sc3microapi\n\n![](https://img.shields.io/pypi/v/sc3microapi.svg) ![](https://img.shields.io/pypi/pyversions/sc3microapi.svg) ![](https://img.shields.io/pypi/format/sc3microapi.svg) ![](https://img.shields.io/pypi/status/sc3microapi.svg)\n\nMicro web service for SeisComP3 systems\n\nOverview\n--------\n\nIn order to integrate systems running SeisComP3 with other systems or tools\nis important to expose at least some basic information related to the system\nin operation. For instance, which networks and stations are present in the\ninventory with some attributes that are not visible through the FDSN Station-WS.\n\nSome other internal data, like access control lists, are important for other\nsystems providing data. Thus, the permissions can be managed in SC3 and used in\nmany other places, offering the operator a single management point.\n\nAPI Specification\n-----------------\n\nFull OpenAPI specification can be seen [here](https://generator.swagger.io/?url=https://raw.githubusercontent.com/javiquinte/sc3microapi/master/swagger.yaml).\n",
        "createdAt": "2017-11-15T08:39:24.000Z",
        "updatedAt": "2025-01-29T09:43:27.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/javiquinte/sc3microapi/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Cuda-Chen/SeisPDF.jl",
        "url": "https://github.com/Cuda-Chen/SeisPDF.jl",
        "description": "Power Spectral Density Probability Density Functions Calculation in Julia.",
        "stars": 4,
        "forks": 0,
        "readme": "\n| **Build Status** | **Documentation** |\n| :---: | :---: |\n| [![][action-img]][action-url] | [![][docs-latest-img]][docs-latest-url] |\n\n# SeisPDF.jl\nPower Spectral Density Probability Density Functions Calculation\ndescribed by [McNamara 2004](https://pubs.usgs.gov/of/2005/1438/).\n\n## TO-DO\n- [x] release workable version\n    - [x] plot (using GMT.jl)\n    - [x] integrate with [SeisIO](https://github.com/jpjones76/SeisIO.jl) data structure\n    - [x] slide on actual timestamp range\n- [ ] GPU support\n    - [ ] CUDA\n    - [ ] AMD (optional)\n- [ ] enhancements\n    - [x] docs\n    - [ ] swappable FFT libraries (e.g., kissFFT)\n    - [ ] min, max, mode PDF\n    - [ ] read high and low noise model into memory when importing\n\n<!-- URLS -->\n[action-img]: https://github.com/Cuda-Chen/SeisPDF.jl/workflows/CI/badge.svg\n[action-url]: https://github.com/Cuda-Chen/SeisPDF.jl/actions/workflows/ci.yml\n[docs-latest-img]: https://github.com/Cuda-Chen/SeisPDF.jl/workflows/CI/badge.svg\n[docs-latest-url]: https://cuda-chen.github.io/SeisPDF.jl/\n",
        "createdAt": "2021-08-26T14:47:47.000Z",
        "updatedAt": "2023-06-11T14:14:08.000Z",
        "language": "Julia",
        "homepage": "https://cuda-chen.github.io/SeisPDF.jl/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Cuda-Chen/SeisPDF.jl/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "archive-GPGN-268-SP23/FP06-volcano-seismology",
        "url": "https://github.com/archive-GPGN-268-SP23/FP06-volcano-seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# FP06-volcano-seismology\n\n## **Names:** Kieran Yanaway (@KieranTheYanaway) Zachary Mathias (@zmathias16)\n\n\n### Summary\nThis project aims to analyze seismic data originating from volcanic eruptions in order to determine whether the onset of an eruption has any distinguishable seismic patterns that can be observed.\n\n### How to use this Repository\nThis repository contains multiple Jupyter Notebook files. The WaveformsLocating.ipynb file contains code for opening and graphing waveforms of various events from the 2018 Kilauea eruption. The FP06-volcano-seismology.ipynb file contains the final product of the figures and analysis of the figures. The Map.ipynb file contains code for constructing the map of the island of Hawai'i with marked locations of stations that we looked at. The dev folder contains code used in the development of the final notebooks contained in the notebooks folder. \n\n### Background Information\nVolcanic eruptions cause seismic activity, and we are looking to analyze the relationship between the eruptions and seismic activity. The files in this repository use data from the Hawaii network, acquired through the IRIS web service using the ObsPy library. The seismic data demonstrate various aspects of the 2018 Kilauea eruption, giving some insight into eruption and seismic mechanics.\n\n\n### Problem Statement\n#### Natural disasters can pose great dangers towards human civilization and infrastructure. The capacity to predict the onset of natural disasters, therefore, provides an opportunity to afford greater protection to those elements of the modern world. Therefore, our research goal is the following: Through analysis of volcanologically originating seismic data, are there discernible, significant, and common patterns of ground-movement present at the onset of volcanic eruptions? If so, how can systems be implemented to be able to detect these patterns and thus determine when an eruption has taken, or is about to take, place?\n\n\n### Datasets\n[Hawaii Network](https://www.fdsn.org/networks/detail/HV/) </br>\n[USGS 2018 Kīlauea](https://www.sciencebase.gov/catalog/item/61a8fa27d34eb622f699a6a6) </br>\n\n\n### Tools/Packages\n- Python\n  - Python will be the main tool for data visualization and processing. Some of the Python libraries will also be used\n- MatPlotLib\n  - This Python library will be utilized for graphing of data and creation of figures\n- Numpy\n  - This Python library will be utilized for processing of datasets and easier manipulation of data\n- ObsPy\n  - This Python library will provide seismic data from the Hawaii seismic network to allow for a wide array of data analysis options.\n\n\n### Planned Methodology\n- Start by researching the datasets, and think about how we can analyze these datasets.\n- Form a plan for what we want to find.\n  - What visualizations should we make?\n  - How many different datasets should we analyze?\n  - What constitutes significant similarities?\n- Analyze the datasets with python.\n  - Perform any operations on datasets needed.\n  - Select correct ranges of data.\n- Interpret and write up a report describing our results.\n\n### Expected Outcomes\nIt is our expectation that we will be able to formulate a comprehensive analysis of eruption seismic data and generate a deeper and useful interpretation of the data such that our problem statement goals are met.\n\n### Anticipated Challenges\nWe anticipate there may be some challenges procuring a sufficient quantity of data for a full, comprehensive analysis of our problem statement. Additionally, determining an appropriate standard for analysis of the data may be challenging. Finally, the process of analysis of the data will likely prove to be generally challenging, including processing the data, comparing datasets, and drawing conclusions about the data.\n",
        "createdAt": "2023-02-23T17:02:17.000Z",
        "updatedAt": "2024-11-14T21:32:27.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/archive-GPGN-268-SP23/FP06-volcano-seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "uafgeoteach/GEOS604_seismo",
        "url": "https://github.com/uafgeoteach/GEOS604_seismo",
        "description": "GEOS 604 Seismology (U. Alaska Fairbanks)",
        "stars": 1,
        "forks": 1,
        "readme": "# GEOS 604 &mdash; Seismology   \n\n**Course:** GEOS604   \n**University:** University of Alaska Fairbanks    \n**Instructor:** Bryant Chow  \n**Website:** https://bryantchow.com/teaching/geos604\n\n**Setup:** See `homework/HW0_GEOS604_setup.md` for environment setup instructions\n",
        "createdAt": "2024-08-27T18:57:54.000Z",
        "updatedAt": "2025-04-15T20:56:49.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/uafgeoteach/GEOS604_seismo/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "keurfonluu/evodcinv",
        "url": "https://github.com/keurfonluu/evodcinv",
        "description": "Inversion of dispersion curves using Evolutionary Algorithms",
        "stars": 113,
        "forks": 36,
        "readme": "evodcinv\n========\n\n|License| |Stars| |Pyversions| |Version| |Downloads| |Code style: black| |Codacy Badge| |Codecov| |Build| |Docs| |DOI|\n\n**evodcinv** is a Python library to invert surface wave dispersion data (e.g., phase velocity dispersion curves) for an isotropic layered velocity model using Evolutionary Algorithms. It relies on `stochopy <https://github.com/keurfonluu/stochopy>`__ for the evolutionary optimizers while forward modeling is heavy-lifted by `disba <https://github.com/keurfonluu/disba>`__.\n\n.. figure:: https://raw.githubusercontent.com/keurfonluu/evodcinv/master/.github/sample.png\n   :alt: sample\n   :width: 100%\n   :align: center\n\n   Inversion of phase velocity dispersion curve (fundamental mode).\n\nFeatures\n--------\n\nInvertible data curves:\n\n-  Love-wave phase and/or group velocity dispersion curves,\n-  Rayleigh-wave phase and/or group velocity dispersion curves,\n-  Rayleigh-wave ellipticity (experimental).\n\nInstallation\n------------\n\nThe recommended way to install **evodcinv** and all its dependencies is through the Python Package Index:\n\n.. code:: bash\n\n   pip install evodcinv --user\n\nOtherwise, clone and extract the package, then run from the package location:\n\n.. code:: bash\n\n   pip install . --user\n\nTo test the integrity of the installed package, check out this repository and run:\n\n.. code:: bash\n\n   pytest\n\nDocumentation\n-------------\n\nRefer to the online `documentation <https://keurfonluu.github.io/evodcinv/>`__ for detailed description of the API and examples.\n\nAlternatively, the documentation can be built using `Sphinx <https://www.sphinx-doc.org/en/master/>`__:\n\n.. code:: bash\n\n   pip install -r doc/requirements.txt\n   sphinx-build -b html doc/source doc/build\n\nUsage\n-----\n\nThe following example inverts a Rayleigh-wave phase velocity dispersion curve (fundamental mode).\n\n.. code:: python\n\n    from evodcinv import EarthModel, Layer, Curve\n\n    # Initialize model\n    model = EarthModel()\n\n    # Build model search boundaries from top to bottom\n    # First argument is the bounds of layer's thickness [km]\n    # Second argument is the bounds of layer's S-wave velocity [km/s]\n    model.add(Layer([0.001, 0.1], [0.1, 3.0]))\n    model.add(Layer([0.001, 0.1], [0.1, 3.0]))\n\n    # Configure model\n    model.configure(\n        optimizer=\"cpso\",  # Evolutionary algorithm\n        misfit=\"rmse\",  # Misfit function type\n        optimizer_args={\n            \"popsize\": 10,  # Population size\n            \"maxiter\": 100,  # Number of iterations\n            \"workers\": -1,  # Number of cores\n            \"seed\": 0,\n        },\n    )\n\n    # Define the dispersion curves to invert\n    # period and velocity are assumed to be data arrays\n    curves = [Curve(period, velocity, 0, \"rayleigh\", \"phase\")]\n\n    # Run inversion\n    res = model.invert(curves)\n    print(res)\n\nExpected output:\n\n.. code-block::\n\n    --------------------------------------------------------------------------------\n    Best model out of 501 models (1 run)\n  \n    Velocity model                                    Model parameters\n    ----------------------------------------          ------------------------------\n             d        vp        vs       rho                   d        vs        nu\n          [km]    [km/s]    [km/s]   [g/cm3]                [km]    [km/s]       [-]\n    ----------------------------------------          ------------------------------\n        0.0298    0.5033    0.2055    2.0000              0.0298    0.2055    0.4000\n        1.0000    2.0586    0.9935    2.0000                   -    0.9935    0.3482\n    ----------------------------------------          ------------------------------\n  \n    Number of layers: 2\n    Number of parameters: 5\n    Best model misfit: 0.0038\n    --------------------------------------------------------------------------------\n\nContributing\n------------\n\nPlease refer to the `Contributing\nGuidelines <https://github.com/keurfonluu/evodcinv/blob/master/CONTRIBUTING.rst>`__ to see how you can help. This project is released with a `Code of Conduct <https://github.com/keurfonluu/evodcinv/blob/master/CODE_OF_CONDUCT.rst>`__ which you agree to abide by when contributing.\n\n.. |License| image:: https://img.shields.io/github/license/keurfonluu/evodcinv\n   :target: https://github.com/keurfonluu/evodcinv/blob/master/LICENSE\n\n.. |Stars| image:: https://img.shields.io/github/stars/keurfonluu/evodcinv?logo=github\n   :target: https://github.com/keurfonluu/evodcinv\n\n.. |Pyversions| image:: https://img.shields.io/pypi/pyversions/evodcinv.svg?style=flat\n   :target: https://pypi.org/pypi/evodcinv/\n\n.. |Version| image:: https://img.shields.io/pypi/v/evodcinv.svg?style=flat\n   :target: https://pypi.org/project/evodcinv\n\n.. |Downloads| image:: https://pepy.tech/badge/evodcinv\n   :target: https://pepy.tech/project/evodcinv\n\n.. |Code style: black| image:: https://img.shields.io/badge/code%20style-black-000000.svg?style=flat\n   :target: https://github.com/psf/black\n\n.. |Codacy Badge| image:: https://img.shields.io/codacy/grade/bd53f27ac85d419d996c434353f08760.svg?style=flat\n   :target: https://www.codacy.com/gh/keurfonluu/evodcinv/dashboard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=keurfonluu/evodcinv&amp;utm_campaign=Badge_Grade\n\n.. |Codecov| image:: https://img.shields.io/codecov/c/github/keurfonluu/evodcinv.svg?style=flat\n   :target: https://codecov.io/gh/keurfonluu/evodcinv\n\n.. |DOI| image:: https://zenodo.org/badge/DOI/10.5281/zenodo.5775193.svg?style=flat\n   :target: https://doi.org/10.5281/zenodo.5775193\n\n.. |Build| image:: https://img.shields.io/github/actions/workflow/status/keurfonluu/evodcinv/ci.yml\n   :target: https://github.com/keurfonluu/evodcinv\n\n.. |Docs| image:: https://img.shields.io/github/actions/workflow/status/keurfonluu/evodcinv/doc.yml?label=docs\n   :target: https://keurfonluu.github.io/evodcinv/\n",
        "createdAt": "2018-03-17T21:48:31.000Z",
        "updatedAt": "2025-11-13T07:18:02.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.5775193",
            "openAlex": "10.5281/zenodo.5775193",
            "openCitations": "10.5281/zenodo.5775193",
            "dataCite": "10.5281/zenodo.5775193",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/keurfonluu/evodcinv/master/README.rst",
        "mainPaper": {
            "doi": "10.5281/zenodo.5775193",
            "title": "evodcinv: Inversion of dispersion curves using Evolutionary Algorithms",
            "journal": "Zenodo",
            "dateReleased": "2023-11-11T00:00:00.000Z",
            "abstract": "",
            "citationsArray": []
        },
        "repoDoi": "10.5281/zenodo.5775193",
        "publications": [
            {
                "doi": "10.5281/zenodo.5775193",
                "name": "evodcinv: Inversion of dispersion curves using Evolutionary Algorithms",
                "source": "",
                "authorNames": [],
                "abstract": "",
                "publicationDate": "2023-12-05T12:16:04.943Z"
            }
        ]
    },
    {
        "source": "GitHub",
        "name": "fgallovic/SlipGen",
        "url": "https://github.com/fgallovic/SlipGen",
        "description": "k-2 hybrid slip generator",
        "stars": 10,
        "forks": 4,
        "readme": "#SlipGen\n-----------\n\nHybrid *k*<sup>-2</sup> hybrid slip generator\n\nThe repository contains the main Fortran code and an example input file. I hope they both are self-explanatory, \nbut do not hesitate to contact me if you have any questions or meet any problems with\nrunning the code.\n\n###Brief description of the I/O files\n\n####Input:\n - `slipgen.in`: fault model definition\n\n####Output:\n - `slipgen.txt`: generated hybrid slip distribution\n - `slipx.txt` and `slipy.txt`: slices of the slip distribution\n - `specx.txt` and `specy.txt`: slices of the slip Fourier spectrum\n\n###Note on compiling:\n\nSome compilers may produce error when compiling. Most likely it will be due to the use an old Fortran trick,\nwhere complex arrays are passed to a subroutine that expects a real array.\nThis can be solved by using an appropriate switch. In Intel Fortran compiler in Microsoft Visual Studio do the following.\nRight-click on your project in the right-hand-side window (\"Solution explorer\") and select\nProperties. Select Fortra/Diagnostics in the dialog and change the option \"Check routine interfaces\" to No.\n\n###References\n - Gallovi�, F., and J. Broke�ov� (2004). On strong ground motion synthesis with k^-2 slip distributions, J. Seismology, 8, 211-224.\n - Gallovi�, F., and J. Broke�ov� (2004). The k^-2 rupture model parametric study: example of the 1999 Athens earthquake, Studia geoph. et geod., 48, 589-613.\n - See also http://geo.mff.cuni.cz/~gallovic/.\n",
        "createdAt": "2016-03-30T10:57:36.000Z",
        "updatedAt": "2024-11-25T12:39:03.000Z",
        "language": "Fortran",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/fgallovic/SlipGen/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "JorgeAGR/seismology",
        "url": "https://github.com/JorgeAGR/seismology",
        "description": "Implementation of ConvNets for Seismology. Graduate research @ NMSU.",
        "stars": 0,
        "forks": 0,
        "readme": "* This set of python scripts will allow to:\r\na) Arrange a set of seismograms into a set of arrays sliced at a 40 second interval \r\n   containing the theoretical arrival of a chosen wave. These are then converted into\r\n   binary objects for ease of use and light size.\r\nb) Using the above binary arrays, they are loaded onto a new script that will initialize\r\n   and train a random number of CNN models (set to 10 by default) in order to find the\r\n   best converging system to predict the arrival for desired seismograms.\r\n\r\nInstructions:\r\n\r\n - Open the 'cnn_config.txt' file and input the desired directories in the appropriate variable.\r\n   Seperate each directory with ',', eg: ../sample/dir0/,../sample/dir1/.\r\n   Input then the desired wave to predict by inputing it's appropriate SAC variable that holds the\r\n   theoretically calculated value.\r\n   \r\n   *!* Always end directories with a forwardslash '/'\r\n   *!* The scripts are ideally executed wtih the 'pred_exe' bash file in the main folder (where this\r\n       README) is, so that will be the working directory for all other directory references.\r\n\r\n - Run the 'pred_exe' bash file. This will look for SAC files in the indicated directories, make them into arrays\r\n   and then predict the arrival times of each seismogram with the established model.\r\n\r\n - The name of the file and its corresponding prediction will then be saved in a CSV file in the 'results/' folder.",
        "createdAt": "2019-02-08T08:47:43.000Z",
        "updatedAt": "2020-11-02T09:11:10.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/JorgeAGR/seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "JUNZHU-SEIS/Seispy",
        "url": "https://github.com/JUNZHU-SEIS/Seispy",
        "description": "A python package for seismological applications",
        "stars": 4,
        "forks": 0,
        "readme": "<img alt=\"Seispy: A Python Toolkit for seismological applications.\" class=\"right\" style=\"width: 20%\" src=\"https://upload.wikimedia.org/wikipedia/en/3/3c/Logo_of_University_of_Science_and_Technology_of_China.svg\"/>\n\n<!---\n\n[![NumFOCUS affiliated project](https://numfocus.org/wp-content/uploads/2018/01/optNumFocus_LRG.png)](https://numfocus.org/sponsored-projects/affiliated-projects)\n\n[![Github Action Status](https://github.com/obspy/obspy/workflows/master_default_tests/badge.svg)](https://github.com/obspy/obspy/actions)\n\n[![Coverage Status](https://codecov.io/gh/obspy/obspy/branch/master/graph/badge.svg)](https://codecov.io/gh/obspy/obspy)\n\n[![Supported Python versions](https://img.shields.io/pypi/pyversions/obspy.svg)](https://pypi.python.org/pypi/obspy/)\n\n[![License](https://img.shields.io/pypi/l/obspy.svg)](https://pypi.python.org/pypi/obspy/)\n[![LGPLv3](https://www.gnu.org/graphics/lgplv3-88x31.png)](https://www.gnu.org/licenses/lgpl.html)\n\n[![PyPI Version](https://img.shields.io/pypi/v/obspy.svg)](https://pypi.python.org/pypi/obspy)\n[![DOI](https://zenodo.org/badge/doi/10.5281/zenodo.3921997.svg)](http://dx.doi.org/10.5281/zenodo.3921997)\n[![Conda](https://img.shields.io/conda/dn/conda-forge/obspy?label=conda%20downloads)](https://anaconda.org/conda-forge/obspy)\n\n[![Discourse status](https://img.shields.io/discourse/status?server=https%3A%2F%2Fdiscourse.obspy.org)](https://discourse.obspy.org)\n[![Gitter](https://badges.gitter.im/JoinChat.svg)](https://gitter.im/obspy/obspy?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n[![Announcements Mailing List](https://img.shields.io/badge/mailing%20list-announcements-blue)](https://mail.python.org/mailman3/lists/obspy.python.org/)\n[![Twitter Follow](https://img.shields.io/twitter/follow/obspy?style=social)](https://twitter.com/obspy/)\n[![Liberapay patrons](https://img.shields.io/liberapay/patrons/obspy?style=social)](https://liberapay.com/Seispy/)\n\n-->\n\n# Seispy\n\nSeispy is an open-source project dedicated to provide a **python package for seismological applications**. It provides seismological tools like seismic event detection, ambient noise imaging, fault zone imaging, etc.\n<!---\n\n(see [Beyreuther et al. 2010](http://www.seismosoc.org/publications/SRL/SRL_81/srl_81-3_es/), [Megies et al. 2011](http://www.annalsofgeophysics.eu/index.php/annals/article/view/4838), [Krischer et al. 2015](http://iopscience.iop.org/article/10.1088/1749-4699/8/1/014003)).\n\n-->\n\nThe goal of the Seispy project is to \n- **integrate the existing seismological tools into a python platform**\n- **develop new methods for seismological research**\n\nSeispy is licensed under the MIT License.\n\n## TODO\n\n### Dependencies\n\n### Installation\n\n### Examples\n",
        "createdAt": "2021-02-25T12:56:22.000Z",
        "updatedAt": "2022-06-17T09:22:26.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.3921997",
            "dataCite": "10.5281/zenodo.3921997",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/JUNZHU-SEIS/Seispy/master/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.3921997",
            "title": "ObsPy 1.2.2",
            "journal": "Zenodo",
            "dateReleased": "2020-06-29T00:00:00.000Z",
            "abstract": "ObsPy: A Python Toolbox for seismology/seismological observatories. ObsPy is an open-source project dedicated to provide a <strong>Python framework for processing seismological</strong> data. It provides parsers for common file formats, clients to access data centers and seismological signal processing routines which allow the manipulation of seismological time series (see Beyreuther et al. 2010, doi: 10.1785/gssrl.81.3.530 ; Megies et al. 2011, doi: 10.1785/gssrl.81.3.530; Krischer et al. 2015, doi: 10.1088/1749-4699/8/1/014003). The goal of the ObsPy project is to facilitate <strong>rapid application development for seismology</strong>.",
            "citationsArray": []
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "milanakarimova/milanakarimova.github.io",
        "url": "https://github.com/milanakarimova/milanakarimova.github.io",
        "description": "📡 Wave Physics | Seismology | Renewable Energy 🧠 Curious about how AI & ML can unlock hidden patterns in geoscience data",
        "stars": 0,
        "forks": 0,
        "readme": "# milanakarimova.github.io\n📡 Wave Physics | Seismology | Renewable Energy 🧠 Curious about how AI &amp; ML can unlock hidden patterns in geoscience data\n",
        "createdAt": "2025-06-30T13:07:53.000Z",
        "updatedAt": "2025-06-30T13:17:19.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/milanakarimova/milanakarimova.github.io/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "GeophysicsResearch/QuantitativeSeismology",
        "url": "https://github.com/GeophysicsResearch/QuantitativeSeismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# QuantitativeSeismology\n\n项目之目的在于翻译以及重新整理AKI前辈的定量地震学\n\n\n| **`基础部分`** | **`理论部分`** | **`实践部分`** | \n|-----------------|---------------------|------------------|\n| **`Qiu Zeng`** | **`Minhan Sheng`** | **`Ziye Yu`** |\n\n\n\n##**基础部分**\n* 第一章\n* 第二章\n* 第三章\n\n##**理论部分**\n* 第一章\n* 第二章\n* 第三章\n\n##**实践部分**\n* 第一章\n* 第二章\n* 第三章\n",
        "createdAt": "2017-02-14T12:59:38.000Z",
        "updatedAt": "2017-02-14T12:59:38.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/GeophysicsResearch/QuantitativeSeismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "dp01101100/anseicca",
        "url": "https://github.com/dp01101100/anseicca",
        "description": "A seismological code for ambient noise source directivity estimation by waveform inversion",
        "stars": 0,
        "forks": 0,
        "readme": "# anseicca\nANSEICCA: Ambient Noise Source Estimation by Inversion of Cross-Correlation Amplitudes. This is a seismological waveform inversion code (written in Python 2.7) for ambient noise source directivity estimation. It implements the technique described by:\n\nDatta, A., Hanasoge, S., & Goudswaard, J. (2019). Finite‐frequency inversion of cross‐correlation amplitudes for ambient noise source directivity estimation. Journal of Geophysical Research: Solid Earth, 124, 6653– 6665. https://doi.org/10.1029/2019JB017602\n\n**********************************************************************************************\nA. PACKAGE CONTENTS AND OVERVIEW\n\n1. There are two versions of this code - serial and parallel - each with their own wrappers and core modules. Other modules are common to the two versions. The parallel code uses MPI for Python (mpi4py) and should be significantly faster than the serial version when solving a large inverse problem (large number of receivers/stations).\n2. The serial code (wrapper) is \"anseicca\\_wrapper\\_serial.py\" and it uses the core module \"hans2013\\_serial.py\".\n3. The parallel code (wrapper) is \"anseicca\\_wrapper\\_parallel.py\" and it uses the core module \"hans2013\\_parallel.py\".\n4. The common modules are \"anseicca\\_utils1.py\" and \"anseicca\\_utils2.py\".\n5. The last script in the package is \"view\\_result\\_anseicca.py\" which is used to visualize (plot) the results produced by the code. Final as well as intermediate results (for the iterative inverse solution) may be accessed and visualized.\n6. EXAMPLES directory - contains input file(s) required by the code, making this repository self-contained. No external input is required to get a demo of the working code.\n\n**********************************************************************************************\nB. HOW TO RUN THE CODE\n\nSimple command line usage:\n\n1. To run the serial code: \"python anseicca\\_wrapper\\_serial.py\"\n2. To run the parallel code: \"mpirun -np {n} python anseicca\\_wrapper\\_parallel.py\"; {n} is the number of processors to use, should be equal to the number of receivers/stations in the problem. NOTE: if running on an HPC cluster, this command can be put into a script to be run with a job scheduler such as PBS.\n3. To visualize the results: \"python view\\_result\\_anseicca.py {arg1} {arg2 (OPTIONAL)}\", where\n\targ1 = pickle file/directory of files containing results produced by code;\n\targ2 = coordinates file for stations or receivers\n\nIf you simply clone this repository and run the code on your system following the above instructions, it should run OK using input files from the EXAMPLES directory which have been hardwired into the two (serial and parallel) wrappers. To be able to use the code for your own purposes, you will need to read the description below.\n\n**********************************************************************************************\nC. CODE DESCRIPTION (USER SETTINGS)\n\nThe key tasks performed (sequentially) by the code, along with associated variables/parameters in the wrapper, are as follows:\n\n1. Read in receiver/station location information (e.g. Easting/Northing, UTM coordinates) from a user-specified coordinates file which is pre-selected and hardwired into the code (variable \"coordfile\").\n2. Place all the receivers on a uniform 2-D cartesian grid whose size and density are defined by the user (\"hlbox\\_outer\" and \"ngp\\_inner\").\n3. Select a subset of receivers to work with, based on a relocation error threshold criterion (\"glerr\\_thresh\"). The relocation is from actual coordinates to uniform grid coordinates.\n4. Read the data (variable \"infile\") and prepare it for use by the code. This step is perfromed IF AND ONLY IF working with real data, as indicated by parameter \"use_reald\". The data is read and processed in the \"anseicca\\_utils2\" module. I have worked with a specific format of input data used by Datta et al. (2019), however you can write your own class to suit your data and add it to \"anseicca\\_utils2\".  \n5. Run the central engine of the code, by calling the core \"h13\" module. This module is an extension of the work by Hanasoge (2013), and its usage is governed by the following key user settings:  \n\t(i) Whether working with synthetic or real data. If working with synthetic data, this is generated internally within the h13 module by forward modelling. \n\t(ii) The geometry of the problem (parameter \"ratio_boxes\"). In case of synthetic tests, the \"true\" sources may lie within or outside the inverse modelling domain, as shown by Figures 3 and B1 respectively, of Datta et al. (2019).  \n\t(iii) The geometry of model parameterization for the inverse problem (parameters \"rad\\_ring\" and \"w\\_ring\").  \n\t(iv) Whether to perform the inversion or not (\"do_inv\").  \n\t(v) Whether to force-stop the inversion after just one iteration or not (\"iter_only1\"). This option can be useful for testing, damping (L-curve) analysis etc.  \n\n6. Store the results in Python binary archive (.npz) format, compatible with visualization script \"view\\_result\\_anseicca.py\".\n\nDetails of different parameters and how to set them are mentioned as comments in the serial code. To familiarize yourself with the code structure, I recommend doing synthetic tests (\"use_reald\"=False) and going through the above steps progressively:\n\n1. Run the code without using the h13 module at all (comment out the line in the wrapper that calls \"h13.inv_cc_amp\"). You will see a map of the receiver network before and after gridding, along with an indication of the selected subset of receivers (parameter \"map_plots\" must be =True in order to see the plots).\n2. Run the code with call to h13 included (undo action above) but with \"do_inv\" = False. You will see the setup of the inverse problem, i.e. the True and Starting source models.\n3. Set \"do\\_inv\"=True. First run with \"iter\\_only1\"=True, then with \"iter\\_only1\"=False. You will get the inversion result after only 1 iteration, and after natural convergence (unknown number of iterations), respectively.\n\n**********************************************************************************************\nD. VISUALIZATION OF RESULTS\n\nThe \"view\\_result\\_anseicca.py\" script has options for plotting:\n\n1. The models (True/Starting/Inversion result)\n2. The misfit kernels (computed in the starting model)\n3. The inversion progress and summary (e.g. Figure 5 of Datta et al., 2019)\n\nYou can choose to plot any or all of the above by turning on or off the corresponding function calls at the bottom of the script.\n\n**********************************************************************************************\nREFERENCES\n\nDatta, A., Hanasoge, S., & Goudswaard, J. (2019). Finite‐frequency inversion of cross‐correlation amplitudes for ambient noise source directivity estimation. Journal of Geophysical Research: Solid Earth, 124, 6653– 6665. https://doi.org/10.1029/2019JB017602.  \nHanasoge, S. M. (2013). The influence of noise sources on cross-correlation amplitudes. Geophysical Journal International, 192(1), 295–309. https://doi.org/10.1093/gji/ggs015.\n",
        "createdAt": "2020-11-25T09:39:37.000Z",
        "updatedAt": "2022-02-07T07:52:40.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/dp01101100/anseicca/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Oboue/MATamf",
        "url": "https://github.com/Oboue/MATamf",
        "description": "MATamf is a Matlab package for  the advanced median filter (AMF) for improving the signal-to-noise ratio of seismological datasets.",
        "stars": 5,
        "forks": 1,
        "readme": "# MATamf\nMATamf is a Matlab package for the advanced median filter (AMF) for improving the signal-to-noise ratio of seismological datasets. The MaATamf package has a variety of applications in exploration and earthquake seismology.\n\n# Description of the testing scripts\n\nThe demos folder contains all the testing scripts of MATamf on a variety of seismological datasets, reproducing all figures presented in the paper. The description of each testing\nscript can be found below:\n\ntest_amf_synlcre.m This script demonstrates the denoising performance on 2D synthetic seismic data containing linear and curved events corrupted by random and strong erratic noise.\n\ntest_amf_rs.m This script demonstrates the denoising performance on pre-stack and post-stack 2D real reflection seismic data contaminated by erratic and random noise.\n\ntest_amf_das1.m This script demonstrates the denoising performance on 2D raw DAS seismic data corrupted by a mixture of strong noise.\n\ntest_amf_das2.m This script demonstrates the adaptability of the proposed method for weak and strong DAS seismic signal denoising.\n\ntest_amf_rf.m This script shows the application of the AMF method to improving receiver function imaging.\n\ntest_amf_ssp.m This script shows the application of the AMF method to enhance the SS precursor signals arising from the earth’s mantle transition zone discontinuities.\n\ntest_amf_sosvmf_somf_mf.m This script is used to conduct a comparison of the denoising performance between the MF, SOMF, SOSVMF, and the proposed AMF methods.\n\ntest_amf_bp_sosvmf_ct_drr_syn.m This script demonstrates the denoising performance of different methods on 2D synthetic containing linear and curved events, corrupted by random and strong erratic noise.\n\ntest_amf_bp_sosvmf_ct_drr_rs.m This script demonstrates the denoising performance of different methods on 2D raw reflection seismic data.\n\ntest_amf_bp_sosvmf_ct_drr_rf.m This script demonstrates the denoising performance of different methods on 2D receiver function data.\n\ntest_amf_bp_sosvmf_ct_drr_ssp.m This script demonstrates the denoising performance of different methods on 2D SS precursor data.\n\ntest_amf_bp_sosvmf_fk_ct_drr_das.m This script demonstrates the denoising performance of different methods on DAS data.\n\n\n# References\n\nOboué, Y.A.S.I, Y. Chen, S. Fomel, and Y. Chen, 2023, An advanced median filter for improving the signal-to-noise ratio of seismological datasets Computer & Geosciences, under review.\n\nWang, H., Chen, Y., Saad, O.M., Chen, W., Oboué, Y.A.S.I., Yang, L., Fomel, S. and Chen, Y., 2022. A Matlab code package for 2D/3D local slope estimation and structural filtering. Geophysics, 87(3), pp.F1–F14.\n\nHuang, G., M. Bai, Q. Zhao, W. Chen, and Y. Chen, 2021, Erratic noise suppression using iterative structure-oriented space-varying median filtering with sparsity constraint, Geophysical Prospecting, 69, 101-121.\n\nChen, Y., S. Zu, Y. Wang, and X. Chen, 2020, Deblending of simultaneous-source data using a structure-oriented space varying median filter, Geophysical Journal International, 222, 1805-1723.\n\nZhao, Q., Q. Du, X. Gong, and Y. Chen, 2018, Signal-preserving erratic noise attenuation via iterative robust sparsity-promoting filter, IEEE Transactions on Geoscience and Remote Sensing, 56, 1558-0644.\n\n# License\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published\nby the Free Software Foundation, either version 3 of the License, or\nany later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details: http://www.gnu.org/licenses/\n\nFor any questions regarding the package, please contact Yangkang Chen (chenyk2016@gmail.com) or Innocent Oboue (obouesonofgod1@gmail.com).\n",
        "createdAt": "2022-10-28T05:44:41.000Z",
        "updatedAt": "2024-08-25T14:59:08.000Z",
        "language": "MATLAB",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Oboue/MATamf/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Huadangfan/EastAsia-Seismology-Model",
        "url": "https://github.com/Huadangfan/EastAsia-Seismology-Model",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "East Asia seismology model\r\n\r\ncontact me: zhangxzh5@stu.pku.edu.cn",
        "createdAt": "2025-08-17T08:41:35.000Z",
        "updatedAt": "2025-08-17T08:50:33.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Huadangfan/EastAsia-Seismology-Model/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "hugotulio/MITcorrelations",
        "url": "https://github.com/hugotulio/MITcorrelations",
        "description": "Code repository for IRIS seismology intern summer 2014",
        "stars": 1,
        "forks": 0,
        "readme": "MITcorrelations\n===============\n\nCode repository for IRIS seismology intern summer 2014\n\nThis repository is setup to host a few MATLAB codes for the IRIS summer intern.\n",
        "createdAt": "2014-10-03T18:42:21.000Z",
        "updatedAt": "2015-12-01T19:44:18.000Z",
        "language": null,
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/hugotulio/MITcorrelations/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "PeterMakus/SeisMIC",
        "url": "https://github.com/PeterMakus/SeisMIC",
        "description": "SeisMIC is a python software suite to monitor phase velocity changes in time and space in seismology using ambient noise.",
        "stars": 73,
        "forks": 18,
        "readme": "<img src=\"https://github.com/PeterMakus/SeisMIC/raw/main/docs/source/figures/seismic_logo_small.png\" alt=\"SeisMIC logo\" width=\"600\"/>\n\n[![Build Status](https://github.com/PeterMakus/SeisMIC/actions/workflows/pytest.yaml/badge.svg)](https://github.com/PeterMakus/SeisMIC/actions/workflows/pytest.yaml?branch=main)\n[![Documentation Status](https://github.com/PeterMakus/SeisMIC/actions/workflows/deploy_gh_pages.yml/badge.svg)](https://github.com/PeterMakus/SeisMIC/actions/workflows/deploy_gh_pages.yml)\n[![License: EUPL v1.2](https://img.shields.io/badge/license-EUPL--1.2-blue)](https://joinup.ec.europa.eu/collection/eupl/introduction-eupl-licence)\n[![codecov](https://codecov.io/gh/PeterMakus/SeisMIC/branch/main/graph/badge.svg?token=DYVHODB6LN)](https://codecov.io/gh/PeterMakus/SeisMIC)\n[![DOI](https://img.shields.io/badge/DOI-10.26443/seismica.v3i1.1099-blue)](https://doi.org/10.26443/seismica.v3i1.1099)\n[![PyPI](https://img.shields.io/pypi/v/seismic)](https://pypi.org/project/seismic/)\n[![Python 3.11](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/release/python-3110/)\n\n## Monitoring Velocity Changes using Ambient Seismic Noise\nSeisMIC (**Seismological Monitoring using Interferometric Concepts**) is a python software that emerged from the miic library. **SeisMIC** provides functionality to apply some concepts of seismic interferometry to different data of elastic waves. Its main use case is the monitoring of temporal changes in a mediums Green's Function (i.e., monitoring of temporal velocity changes).\n\n<img src=\"https://github.com/PeterMakus/SeisMIC/raw/main/docs/source/figures/zhupanov_dv.png\" alt=\"A velocity change time series\" width=\"800\"/>\n\n**SeisMIC** will handle the whole workflow to create velocity-change time-series including:\n+ Downloading raw data\n+ Adaptable preprocessing of the waveform data\n+ Computating cross- and/or autocorrelations\n+ Plotting tools for correlations\n+ Database management of ambient seismic noise correlations\n+ Adaptable postprocessing of correlations\n+ Computation of velocity change (dv/v) time series\n+ Postprocessing of dv/v time series\n+ Plotting of dv/v time-series\n+ Inverting dv/v onto a spatial grid\n\n**SeisMIC** handles correlations and data in an [ObsPy](https://github.com/obspy/obspy)-like manner.\n\n## Installation of this package\n\n### Installation from PyPi (pip install)\n**SeisMIC** is  now deployed on PyPi and can simply be installed using:\n\n```bash\n# We recommend installing mpi4py from the conda-forge channel instead of PyPi\nconda install -c conda-forge mpi4py\n\npip install seismic\n\n# If you want to execute the tutorials, you will have to install jupyter as well\npip install jupyter\n```\n### Installation from Source Code\nTo obtain the lates features, you can install SeisMIC from its source code, available on GitHub.\n\n**Developers should download the ``dev`` branch**\n\nDownload this package via GitHub and install it via bash terminal (the few steps shown below) or using the graphical user interface\n\n```bash\n# Download via wget or web-browser\nwget https://github.com/PeterMakus/SeisMIC/archive/refs/heads/main.zip\n\n# For developers download the dev branch\nwget https://github.com/PeterMakus/SeisMIC/archive/refs/heads/dev.zip\n\n# unzip the package\nunzip main.zip  # or dev.zip\n\n# Change directory to the same directory that this repo is in (i.e., same directory as setup.py)\ncd SeisMIC-main  # That's the standard name the folder should have\n\n# Create the conda environment and install dependencies\nconda install -c conda-forge mpi4py\nconda env create -f environment.yml\n\n# Activate the conda environment\nconda activate seismic\n\n# Install the package in editable mode\npip install -e .\n\n# If you want to execute the tutorials, you will have to install jupyter as well\npip install jupyter\n```\n\n## Getting started\nAccess SeisMIC's documentation [here](https://petermakus.github.io/SeisMIC/index.html).\n\nSeisMIC comes with a few tutorials (Jupyter notebooks). You can find those in the `examples/` directory.\n\n## Acknowledging the Use of SeisMIC in your Work\nIf you should use SeisMIC to create published scientific content please cite the SeisMIC paper: [Makus, P., & Sens-Schönfelder, C. (2024). SeisMIC-an Open Source Python Toolset to Compute Velocity Changes from Ambient Seismic Noise.](https://doi.org/10.26443/seismica.v3i1.1099).\n\n## Reporting Bugs / Contact the developers\nThis version is an early release. If you encounter any issues or unexpected behaviour, please [open an issue](https://github.com/PeterMakus/SeisMIC/issues/new/choose) here on GitHub.\n\n## Questions?\nIf you have any questions that do not require any changes in the source code, please use the [discussions feature](https://github.com/PeterMakus/SeisMIC/discussions)\n\n## Contributing\nThank you for contributing to SeisMIC! Have a look at our [guidelines for contributors](https://github.com/PeterMakus/SeisMIC/blob/main/CONTRIBUTING.md)\n",
        "createdAt": "2021-07-29T12:32:22.000Z",
        "updatedAt": "2025-09-25T06:21:12.000Z",
        "language": "Python",
        "homepage": "https://petermakus.github.io/SeisMIC/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/PeterMakus/SeisMIC/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "SebastianRoa5/Seismology",
        "url": "https://github.com/SebastianRoa5/Seismology",
        "description": "data processing from EQ for descriptions with JN",
        "stars": 0,
        "forks": 1,
        "readme": "# Seismology\ndata processing from EQ for descriptions with JN.\nuse py 37\n",
        "createdAt": "2021-04-02T00:49:21.000Z",
        "updatedAt": "2021-04-02T01:05:43.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/SebastianRoa5/Seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "EIDA/routing",
        "url": "https://github.com/EIDA/routing",
        "description": "Server side application to provide the Routing Service used in EIDA",
        "stars": 6,
        "forks": 4,
        "readme": "Routing Service v1.2\n--------------------\n\nWhy a Routing Service?\n======================\n\nOne of the aims of the\n`European Integrated Data Archive <http://www.orfeus-eu.org/eida/eida.html>`_\n(EIDA) is to provide transparent access and services to high quality, seismic\ndata across different data archives in Europe. In the context of the design\nof the `EIDA New Generation` (EIDA-NG) software we envision a future in which\nmany different data centers offer data products using compatible types of\nservices, but pertaining to different seismic objects, such as waveforms,\ninventory, or event data. EIDA provides one example, in which data centers\n(the EIDA “nodes”) have long offered Arclink and Seedlink services, and now\noffer FDSN web services, for accessing their holdings. In keeping with the\ndistributed nature of EIDA, these services could run at different nodes.\nDepending on the type of service, these may only provide information about a\nreduced subset of all the available waveforms.\n\nTo assist users to locate data, we have designed a Routing Service, which\ncould run at EIDA nodes or elsewhere, including on a user's personal computer.\nThis (meta)service is supposed to be queried by clients (or other services) in\norder to localize the address(es) where the desired information is provided.\n\nThe Routing Service must serve this information in order to help the\ndevelopment of smart clients and/or services of higher level, which can offer\nthe user an integrated view of the whole EIDA, hiding the complexity of its\ninternal structure. However, the Routing Service need not be aware of the\nextent of the content offered by each service, avoiding the need for a large\nsynchronised database at any place.\n\nThe service is intended to be open and able to be queried by anyone without\nthe need of credentials or authentication.\n\nLicense\n=======\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\nany later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nInstallation\n============\n\nThe installation instructions are included in the package, but need first to be\ngenerated. Follow the instructions in the next section to do it.\n\nDocumentation\n=============\n\nTo get the documentation of the current version of the Routing Service you\nplease follow these steps:\n\n1. Go to the \"doc\" subdirectory located where the package was decompressed.\nLet's suppose it is \"/var/www/eidaws/routing/1\". ::\n\n  $ cd /var/www/eidaws/routing/1/doc\n\n2. Build the\ndocumentation. ::\n\n  $ make latexpdf\n\n3. Open the generated PDF file with an appropriate application (e.g. acroread,\nevince, etc). The file will be located under the .build/latex directory. ::\n\n  $ acroread .build/latex/Routing-WS.pdf\n\nCopy this file to the location most suitable to you for future use.\n",
        "createdAt": "2015-09-24T19:41:49.000Z",
        "updatedAt": "2025-01-03T00:00:09.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/EIDA/routing/master/README.rst",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "RiandryDevelop/SismoUI",
        "url": "https://github.com/RiandryDevelop/SismoUI",
        "description": "This is the official frontend of the SismoAPP, An application that collects and delivers seismological data around the world.",
        "stars": 0,
        "forks": 0,
        "readme": "# sismo_ui\n\n## Description\n\nThis project is the official frontend of the SismoAPP, an application that collects and delivers seismological data from around the world.\n\n## Installation\n\nTo get started with this project, follow these steps:\n\n1. Clone the repository:\n   ```bash\n   git clone https://github.com/RiandryDevelop/SismoUI.git\n   ```\n\n2. Navigate into the project directory:\n   ```bash\n   cd sismo_ui\n   ```\n\n3. Install dependencies:\n   ```bash\n   npm install\n   ```\n\n## Usage\n\n### Development\n\nTo start the development server, run:\n```bash\nnpm run dev\n```\nThis command will start a development server using Vite.\n\n### Building for Production\n\nTo build the project for production, run:\n```bash\nnpm run build\n```\nThis command will generate a production-ready build of the project.\n\n### Previewing Production Build\n\nTo preview the production build locally, run:\n```bash\nnpm run preview\n```\nThis command will serve the production build locally.\n\n## Testing\n\nTo run tests using Playwright, execute the following command:\n```bash\nnpm run test\n```\n\nThis command will execute the Playwright tests defined in the project.\n\n## Project Structure\n\nThe project structure is as follows:\n- `src/`: Contains the source code of the application.\n- `public/`: Contains static assets and HTML template.\n\n## Dependencies\n\nThis project relies on the following dependencies:\n- React: ^18.2.0\n- React-DOM: ^18.2.0\n\n## Development Dependencies\n\nThe project utilizes several development dependencies for building, linting, and more:\n- @types/react: ^18.2.66\n- @types/react-dom: ^18.2.22\n- @vitejs/plugin-react-swc: ^3.5.0\n- autoprefixer: ^10.4.19\n- eslint: ^8.57.0\n- eslint-plugin-react: ^7.34.1\n- eslint-plugin-react-hooks: ^4.6.0\n- eslint-plugin-react-refresh: ^0.4.6\n- postcss: ^8.4.38\n- tailwindcss: ^3.4.3\n- vite: ^5.2.0\n\n## Versioning\n\nThe current version of this project is 0.0.0.\n\n## How to use the endpoints and consume the application\n\n## Endpoints\n\n- **Get all features**\n  - Endpoint: `GET /api/features`\n  - Description: Retrieves a list of all features available in the system.\n\n- **Get all comments of a particular feature**\n  - Endpoint: `GET /api/comments/feature/:feature_id`\n  - Description: Retrieves all comments associated with a specific feature identified by `feature_id`.\n\n- **Create a comment**\n  - Endpoint: `POST /api/comments`\n  - Description: Creates a new comment for a feature.\n  - Request Body: JSON object with the following parameters:\n    - `feature_id`: ID of the feature the comment belongs to.\n    - `body`: Text content of the comment.\n\n- **Update a comment**\n  - Endpoint: `PUT /api/comments/:comment_id`\n  - Description: Updates an existing comment identified by `comment_id`.\n  - Request Body: JSON object with the following parameter:\n    - `body`: Updated text content of the comment.\n\n- **Delete a comment**\n  - Endpoint: `DELETE /api/comments/:comment_id`\n  - Description: Deletes a comment identified by `comment_id`.\n\n## Response Format\n\n- Success Response: HTTP status code 200 OK for successful requests.\n- Error Response: HTTP status codes indicating the type of error occurred, along with a relevant error message in the response body.\n\n## Authentication\n\nThis API does not require authentication for accessing the endpoints. However, ensure proper authorization mechanisms are implemented in your application if needed.\n\n\n\n\n\n",
        "createdAt": "2024-04-07T14:33:08.000Z",
        "updatedAt": "2025-09-11T23:34:59.000Z",
        "language": "JavaScript",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/RiandryDevelop/SismoUI/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "br0mabs/planetary-seismology",
        "url": "https://github.com/br0mabs/planetary-seismology",
        "description": null,
        "stars": 0,
        "forks": 1,
        "readme": "# Seismic Detection\nWe developed algorithmic and machine learning models to identify seismic events in real data from the Apollo missions and the Mars InSight mission, minimizing the transmission of unnecessary information. For the Moon, we applied traditional signal processing techniques like Fast Fourier Transform and spectrogram analysis to detect seismic events, achieving reasonable accuracy. For Mars, where the signal-to-noise ratio is lower, we used advanced machine learning, specifically a long short term memory model, to handle the time-series nature of seismic data. This model captures subtle temporal patterns and dependencies, helping to predict the timing of seismic events. Though the accuracy of the machine learning model was lower due to potential noise sensitivity, it offers a scalable solution that can be improved with further refinement. Our models help improve seismic event detection and enhance data efficiency and advance our understanding of planetary activity. \n\n## Additional data\nThis Google Drive contains data that we used for our project. \n[https://docs.google.com/document/d/11yPrLIqcA9FDIywRGnuaOL1iYACTb3Usb5KmRUoA8ms/edit?usp=sharing](https://drive.google.com/drive/folders/1UMmU471yz6-G3Kaj_hpo8AOv1dRT_c5i?usp=sharing)\n\n## Project Details\nPlanetary seismology missions face significant challenges with power limitations, particularly when transmitting continuous seismic data from distant planets back to Earth. Given that only a fraction of the data is scientifically useful, optimizing data transmission is critical. This project aims to address this issue by developing a computer program capable of distinguishing between noise and scientifically valuable seismic signals. By analyzing real data from the Apollo missions and the Mars Interior Exploration using Seismic Investigations, the goal is to identify and extract seismic events, minimizing the need to send back unnecessary information. \n\nIn developing our project, we used Google Colab as the primary development environment and Python as the primary programming language. We decided to use different algorithms for Moon and Mars predictions, because of the differences in signal-to-noise ratio and the amount of data that was available for training. \nWe created a webpage to present our results, and to display our results more efficiently, we created interactive plots for visualization. For creating interactive plots, we used Holoviews, a high-level data visualization library that simplifies the creation of interactive plots, and Panel, a powerful tool for building interactive web applications and dashboards in Python. The interactive plot displays continuous seismic records as curves over time with event markers. By hovering on the curve, users can retrieve information about the relative time and amplitude at that point. The user can also zoom in and out on specific sections of the data using the zoom box, time sliders and amplitude sliders. After doing some visual manipulations, the user can also save the plot as an image.\n\nWe approached this challenge by the following methods:\n### Algorithmic Predictive Models\n  <ul>\n    <li>\n      These models apply an algorithm on the raw data to get the predicted value. These models differ from machine learning due to the fact that there is no need to train or test them, the model cannot gain more “knowledge”. One advantage is that it is simplistic and easy to implement, and there is no need to worry about overfitting. However, the model cannot learn from the data as well as machine learning or artificial intelligence models can.\n    </li>\n    <li>\n      <strong>Fast Fourier Transform on Raw Data:</strong> Fast Fourier transform is a type of wave transform that extracts the amplitudes of the wave signal at each individual frequency (amplitude-frequency). This allows for increased signal to noise ratio as instead of the seismic activity occurring on the graph as a large blob, it instead appears as a single signal corresponding to the frequency of the wave that produces the seismic event. The model looks at the graph generated by the FFT, extracts the frequency with the maximum amplitude, and then maps it back to the time that it happened on the normal frequency-time graph, which gives our prediction. Some of the weaknesses that we found with this model was its inability to distinguish between brief high amplitude noise and a seismic event; it would regularly give a wrong prediction when such an event occurred. Further work on this model includes doing some denoising and filtering out outliers in amplitude to avoid false positives\n    </li>\n    <p align=\"center\"><img src=\"./readme_img/data_pipeline.png\" width=\"600px\"></p>\n    <li>\n      <strong>Spectrogram analysis on high-pass filtered data:</strong> High pass filter denoises the data by picking up the higher frequency signals and removes the lower frequency ones, which constitute most of the unwanted noise. The model then detects the highest power signal on the seismograph and returns the time that it occurred, which corresponds to a seismic event. This method has more success than the FFT model, due to the fact that high amplitude signals do not show up after denoising, which removes false positives. However, this model still has drawbacks due to the fact that only detecting the maximum power may not detect other clusters, which was the case when more than one seismic event occurred in one day. \n    </li>\n    <li>\n      <strong>Multi Prediction spectrogram analysis model:</strong> Same as the single prediction model, but to predict multiple seismic events, we check the spectrogram for power exceeding the 99th percentile, and make note of the times that these occur. We then do interval analysis on these time stamps, to find intervals where high energy readings are clustered, and mark it as a seismic event. This allows us to predict more than one seismic event in a single day. However, this causes false positives as even with denoising, noise can cause the model to incorrectly detect intervals of seismic activity. \n    </li>\n    <li>\n      <strong>Spectrogram cluster analysis:</strong> This method was used in the case of the mars data, due to the fact that high-pass filter denoising was not sufficient for the model to act on clear signals. Instead, we denoise and then find clusters of high power readings, and mark clusters instead on the spectrogram. We then take the highest power cluster and mark that as a prediction. A next step is to extend to generate predictions for multiple seismic events for one day. \n    </li>\n  </ul>\n  \n### Machine Learning Model\n  <ul>\n    <li>\n      An LSTM (Long Short-Term Memory) model is a highly suitable approach for the seismic event detection challenge for several reasons. Firstly, seismic data is a time-series signal, where each data point is dependent on the previous points. LSTM models are specifically designed to handle time-series data as they can maintain information across different time steps. In addition, LSTM models excel at capturing these long-term dependencies, unlike traditional RNNs (Recurrent Neural Networks), which suffer from vanishing gradients and struggle to retain information over longer periods. \n    </li>\n    <p align=\"center\"><img src=\"./readme_img/lstm.png\" width=\"600px\"></p>\n    <li>\n      Observing and testing with the provided training dataset, we noticed that the .mseed file contained a sampling rate and learned that there would be a reasonably large number of samples in a short amount of time, so we split each trace into small segments of 30 seconds. After splitting seismic traces into 30-second segments and extracting features (mean, max, skewness, etc.), we normalized the features and the target earthquake occurrence index. Using PyTorch, the LSTM model, with 64 hidden units, 2 layers, and a 0.2 dropout, was trained to predict the segment containing the earthquake. We used the Adam optimizer with MSELoss, training over 10 epochs with each trace processed independently, while allowing the model to learn the relationships between segments within each trace\n    </li>\n    <p align=\"center\"><img src=\"./readme_img/ml_pipeline.png\" width=\"600px\"></p>\n    <li>\n      This approach allows the model to learn temporal dependencies between consecutive 30-second segments, which is crucial for capturing patterns that may indicate an upcoming earthquake. Also, by using an LSTM, the model can recognize subtle temporal patterns within the seismic data that simple feature-based models might miss. Ultimately, the goal is to improve the accuracy of predicting when an earthquake will occur by identifying the specific segment in which it starts. \n    </li>\n  </ul>\n\n## Developer Team\n- [Luna Nguyen](https://github.com/lunanguyen)\n- [Mathilda Lee](https://github.com/jkmathilda)  \n- [Monica Trinh](https://github.com/monmon0)\n- [Steven Gu](https://github.com/br0mabs)\n\n",
        "createdAt": "2024-10-05T01:08:38.000Z",
        "updatedAt": "2024-10-07T01:18:36.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/br0mabs/planetary-seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "niyiyu/pnwstore",
        "url": "https://github.com/niyiyu/pnwstore",
        "description": "mSEED client for Denolle-Lab member",
        "stars": 1,
        "forks": 3,
        "readme": "# PNWstore: Pacific Northwest Storage\n[![DOI](https://zenodo.org/badge/479659348.svg)](https://zenodo.org/badge/latestdoi/479659348) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nThis is a python-based seismic data query and selection toolbox for [Denolle-lab](https://denolle-lab.github.io) members.\n\n## What data are in pnwstore?\n- mseed data at PNW (1980 - 2023)\n- network metadata (1980 - 2022)\n    - [network list here](./docs/netlist.md)\n- earthquake catalog contributed by\n    - [University of Washington/Pacific Northwest Seismic Network](https://pnsn.org/pnsn-data-products/earthquake-catalogs) (1980-2022)\n- list of known wrong data could be found [here](./docs/wrong_data.md).\n\n## Why use this toolbox?\n1. The waveforms stored in `mseed` can be indexed with [mseedindex](https://github.com/iris-edu/mseedindex), which would dramatically improve the efficieny of data streaming. This is very useful especailly when you are working on a large amount of data.\n2. Although `xml` files contain all information of events and/or seismic networks, extra costs in codes and parsing time may not be ignored especially in reading and parsing complex XML files. It's better to extract key informations and index them in a database system.\n3. The pnwstore client (is trying to) emulate ObsPy FDSN client so that transition from using IRIS to the local data requires very little changes to the codes.\n\n## Usage\n### Query and select stream\n```python\nfrom pnwstore.mseed import WaveformClient\nclient = WaveformClient()\n\ns = client.get_waveforms(network = \"UW\", station = \"SHW\", channel = \"EH?\",\n                         starttime = \"20200101T00:00:00\",\n                         endtime   = \"20200101T01:00:00\")\n```\n\n### Query earthquake catalog\n```python\nfrom obspy.core.utcdatetime import UTCDateTime\nfrom pnwstore.catalog import QuakeClient\n\nclient = QuakeClient(USERNAME, PASSWORD)\n\nclient.query(mintime = UTCDateTime(\"1980-01-01T00:00:00\"),\n             maxtime = UTCDateTime(\"2021-01-01T00:00:00\"),\n             minlatitude = 40,    maxlatitude = 50,\n             minlongitude = -128, maxlongitude = -120,\n             minmagnitude = 5.9)\n\n# A pandas DataFrame is returned.\n#   source_id   origin_timestamp year month day doy hour minute second  microsecond latitude longitude  depth  magnitude contributor number_of_pick\n# 0 uw10313718  748582000.0      1993 9     21  264 3    26     55      630000      42.316    -122.027  8.560  5.9       uw          380\n# 1 uw10313838  748590000.0      1993 9     21  264 5    45     35      230000      42.358    -122.058  8.530  6.0       uw          427\n# 2 uw10530748  983386000.0      2001 2     28  59  18   54     32      830000      47.149    -122.727  51.798 6.8       uw          98\n\n```\n\n### Query phase picks\n```python\nfrom obspy.core.utcdatetime import UTCDateTime\nfrom pnwstore.catalog import PickClient\n\nclient = PickClient(USERNAME, PASSWORD)\n\nclient.query(network = \"UW\", station = \"SHW\", phase = \"P*\",\n             mintime = UTCDateTime(\"2000-01-01\"),\n             maxtime = UTCDateTime(\"2000-01-10\"))\n\n# A pandas DataFrame is returned.\n#   pick_id   source_id network station location channel    timestamp  year month day doy  hour  minute  second  microsecond phase evaluation_mode uncertainty  backazimuth contributor\n# 0 1133412  uw10485733      UW     SHW       --     EHZ  947343000.0  2000   1    8    8    14      57      23       680000     P          manual   0.05         79.0          UW\n# 1 1133508  uw10485213      UW     SHW       --     EHZ  947142000.0  2000   1    6    6     6      52      56       790000     P          manual   0.08         63.6          UW\n# 2 1133612  uw10484688      UW     SHW       --     EHZ  946890000.0  2000   1    3    3     9       0      14       280000     P          manual   0.22        222.1          UW\n```\n\n### Query network meta\n```python\nfrom obspy.core.utcdatetime import UTCDateTime\nfrom pnwstore.station import StationClient\n\nclient = StationClient(USERNAME, PASSWORD)\nclient.query(network = \"UW\", channel = \"EH?\",\n             mintime = UTCDateTime(\"2010-01-15\"),\n             maxtime = UTCDateTime(\"2000-01-16\"))\n\n# A pandas DataFrame is returned.\n#    channel_id network station location channel  latitude  longitude elevation  depth     starttime       endtime sampling_rate azimuth\n# 0       45376      UW     SHW       --     EHZ   46.1935   -122.236   1425.00   0.00   867283200.0  1207008000.0      100.0000    None\n# 1       45377      UW     SHW       --     EHZ   46.1935   -122.236   1425.00   0.00  1207008000.0  1536105600.0      100.0000    None\n```\n\n\n## Database schema\nPNWstore uses mysql to index all seismic data. Below are the schemas for each table.\n### network schema\n```mysql\ncreate table network (                             \\\n    channel_id MEDIUMINT NOT NULL AUTO_INCREMENT,  \\\n    network VARCHAR(3) NOT NULL,                   \\\n    station VARCHAR(5) NOT NULL,                   \\\n    location VARCHAR(3) NOT NULL,                  \\\n    channel CHAR(3) NOT NULL,                      \\\n    latitude FLOAT NOT NULL,                       \\\n    longitude FLOAT NOT NULL,                      \\\n    elevation DECIMAL(6, 2) NOT NULL,              \\\n    depth DECIMAL(6, 2) NOT NULL,                  \\\n    starttime DECIMAL(16, 1) NOT NULL,             \\\n    endtime DECIMAL(16, 1) NOT NULL,               \\\n    sampling_rate     DECIMAL(10, 4) NOT NULL,     \\\n    azimuth DECIMAL(5, 2),                         \\\n    PRIMARY KEY (channel_id)                       \\\n);\n```\n### catalog schema\n```mysql\ncreate table catalog (                             \\\n    source_id VARCHAR(10) NOT NULL,                \\\n    timestamp DECIMAL(16, 6) NOT NULL,             \\\n    year SMALLINT NOT NULL,                        \\\n    month TINYINT NOT NULL,                        \\\n    day TINYINT NOT NULL,                          \\\n    doy SMALLINT NOT NULL,                         \\\n    hour TINYINT NOT NULL,                         \\\n    minute TINYINT NOT NULL,                       \\\n    second TINYINT NOT NULL,                       \\\n    microsecond MEDIUMINT NOT NULL,                \\\n    latitude FLOAT NOT NULL,                       \\\n    longitude FLOAT NOT NULL,                      \\\n    depth FLOAT NOT NULL,                          \\\n    magnitude FLOAT NOT NULL,                      \\\n    magnitude_type VARCHAR(2) NOT NULL,            \\\n    contributor VARCHAR(4) NOT NULL,               \\\n    number_of_pick SMALLINT NOT NULL,              \\\n    PRIMARY KEY (source_id)                        \\\n);\n```\n### mseed schema\nNote that each year relates to an individual table.\n```mysql\ncreate table mseed_YYYY (                         \\\n    mseed_id MEDIUMINT NOT NULL AUTO_INCREMENT,   \\\n    network VARCHAR(3) NOT NULL,                  \\\n    station VARCHAR(5) NOT NULL,                  \\\n    location VARCHAR(3) NOT NULL,                 \\\n    channel CHAR(3) NOT NULL,                     \\\n    quality CHAR(1) NOT NULL,                     \\\n    version VARCHAR(4) NOT NULL,                  \\\n    starttime VARCHAR(26) NOT NULL,               \\\n    endtime VARCHAR(26) NOT NULL,                 \\\n    samplerate FLOAT NOT NULL,                    \\\n    filename VARCHAR(48) NOT NULL,                \\\n    byteoffset INT NOT NULL,                      \\\n    bytes INT NOT NULL,                           \\\n    hash CHAR(32) NOT NULL,                       \\\n    timeindex TEXT NOT NULL,                      \\\n    timespans MEDIUMTEXT NOT NULL,                \\\n    timerates TEXT,                               \\\n    format TEXT,                                  \\\n    filemodtime VARCHAR(26) NOT NULL,             \\\n    updated VARCHAR(26) NOT NULL,                 \\\n    scanned VARCHAR(26) NOT NULL,                 \\\n    PRIMARY KEY (mseed_id)                        \\\n);\n```\n\n### pick schema\nNote that each contributor relates to an individual table.\n```mysql\ncreate table picks_CONTRIBUTOR (                  \\\n    pick_id INT NOT NULL AUTO_INCREMENT,          \\\n    source_id VARCHAR(10) NOT NULL,               \\\n    network VARCHAR(3) NOT NULL,                  \\\n    station VARCHAR(5) NOT NULL,                  \\\n    location VARCHAR(3) NOT NULL,                 \\\n    channel CHAR(3) NOT NULL,                     \\\n    timestamp DECIMAL(16, 6) NOT NULL,            \\\n    year SMALLINT NOT NULL,                       \\\n    month TINYINT NOT NULL,                       \\\n    day TINYINT NOT NULL,                         \\\n    doy SMALLINT NOT NULL,                        \\\n    hour TINYINT NOT NULL,                        \\\n    minute TINYINT NOT NULL,                      \\\n    second TINYINT NOT NULL,                      \\\n    microsecond MEDIUMINT NOT NULL,               \\\n    phase VARCHAR(6) NOT NULL,                    \\\n    evaluation_mode VARCHAR(10) NOT NULL,         \\\n    onset VARCHAR(2),                             \\\n    polarity VARCHAR(2),                          \\\n    uncertainty FLOAT,                            \\\n    backazimuth FLOAT,                            \\\n    contributor VARCHAR(6) NOT NULL,              \\\n    PRIMARY KEY (pick_id)                         \\\n);\"\n```\n\n## Reference\n* https://pnsn.org\n* https://ds.iris.edu/ds/nodes/dmc/\n* https://earthquake.usgs.gov/data/comcat/\n* https://github.com/iris-edu/mseedindex\n",
        "createdAt": "2022-04-09T08:01:14.000Z",
        "updatedAt": "2025-12-03T23:18:11.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/niyiyu/pnwstore/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "lrolland/ISC",
        "url": "https://github.com/lrolland/ISC",
        "description": "Common repository for Ionospheric Seismology Codes",
        "stars": 0,
        "forks": 0,
        "readme": "ISC\n===\n\nCommon repository for Ionospheric Seismology Codes\n",
        "createdAt": "2013-11-14T21:04:55.000Z",
        "updatedAt": "2013-11-14T21:12:52.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/lrolland/ISC/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "rojkhatun9382-spec/Seismology-Earthquake-Event-Analysis",
        "url": "https://github.com/rojkhatun9382-spec/Seismology-Earthquake-Event-Analysis",
        "description": "Event  catalogues,  magnitude,  depth etc; SQL  to store;  Excel  visuals ",
        "stars": 0,
        "forks": 0,
        "readme": "# Seismology-Earthquake-Event-Analysis\nEvent  catalogues,  magnitude,  depth etc; SQL  to store;  Excel  visuals \n\n📊 Purpose:\n\nThe goal of this dashboard is to analyze and visualize earthquake occurrences across different years, months, countries, magnitude categories, and days of the week.\nIt helps to identify trends and patterns in earthquake frequency and intensity.\n\n🧩 Dashboard Components:\n\n🔶 Month-wise Earthquake Trends (Bar Chart):\n\nShows the number of earthquakes per month.\n\nHelps identify which months experience more earthquake activity.\n\n🔷 Year-wise Total Earthquakes (Line Chart):\n\nDisplays yearly earthquake counts over time.\n\nUseful for understanding long-term patterns or increases/decreases in earthquake frequency.\n\n🟠 Magnitude Category-wise Total Earthquakes (Pie Chart):\n\nDivides total earthquakes into categories such as Great, Major, and Minor.\n\nVisually shows the percentage share of each category.\n\n🟣 Day-of-week Earthquake Record (Bar Chart):\n\nRepresents the number of earthquakes on each day of the week.\n\nUseful for checking if any day has relatively more occurrences.\n\n🟢 Country-wise Earthquake Analysis (Bar Chart):\n\nLists countries with their corresponding earthquake counts.\n\nHelps identify which countries experience more frequent earthquakes.\n\n🎛️ Interactive Filters (Slicers):\n\nYou’ve added slicers for:\n\nMagnitude Category\n\nMonth\n\nYear\n\nCountry\n\nThese slicers allow users to filter data dynamically and view insights for specific years, months, or regions.\n\n🎨 Design Highlights:\n\nA dark background for contrast.\n\nBright yellow header with bold red title: “Earthquake Event Analysis Dashboard.”\n\nClean chart layout arranged in rows and columns.\n\nFilters (slicers) placed neatly on the left side for easy interaction.\n",
        "createdAt": "2025-11-12T17:08:27.000Z",
        "updatedAt": "2025-11-12T17:16:17.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/rojkhatun9382-spec/Seismology-Earthquake-Event-Analysis/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "lskk/ecn",
        "url": "https://github.com/lskk/ecn",
        "description": "Earthquake Catcher Network (ECN) Stream Processors",
        "stars": 2,
        "forks": 0,
        "readme": "# Earthquake Catcher Network (ECN) Stream Processors\n\n## Development Setup\n\nRequirements:\n\n* Anaconda / Python 3.6\n* This guide assumes Windows 64bit\n\n1. Create virtual environment:\n\n        python -m venv venv\n\n2. Activate virtual environment\n\n        venv\\Scripts\\activate\n\n3. Install requirements\n\n        pip install -r requirements.txt\n\nEnvironment variables:\n\n    QUEUE_PREFIX=ecn_dev_\n    MONGODB_URI=mongodb://localhost/ecn\n    AMQP_HOST=localhost\n    AMQP_VHOST=/\n    AMQP_USER=guest\n    AMQP_PASSWORD=guest\n\n## Deployment in Production\n\n1. Install Anaconda with Python 3.6.\n   This guide assumes Windows 64bit.\n\n2. Create virtual environment\n\n        python -m venv venv\n\n3. Activate the virtual environment\n\n        venv\\Scripts\\activate\n\n4. Install dependencies\n\n        pip install -r requirements.txt\n\n5. Edit `setenv.cmd` and ensure configuration (get from Dropbox admin)\n6. Run `setenv.cmd`\n7. Run `python stationd.py`\n\n### Autostart Script (Windows Server)\n\n    E:\n    cd \\ecn\n    call setenv\n    venv\\Scripts\\python stationd.py\n\n## Protocol Buffers\n\nThe protobuf file **must** be in sync with the file used by GeoAssistant Android client.\n\nCompile protobuf to Python library:\n\n    E:\\protobuf\\bin\\protoc -I=. --python_out=ecn/ ecn_mobile.proto\n",
        "createdAt": "2018-01-15T08:31:55.000Z",
        "updatedAt": "2024-11-20T11:10:39.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/lskk/ecn/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "dcroman/Tremometer",
        "url": "https://github.com/dcroman/Tremometer",
        "description": "Matlab code to detect and characterize harmonic tremor in continuous seismic data",
        "stars": 8,
        "forks": 5,
        "readme": "README for Tremometer 1.0 - Matlab code for automatic detection and characterization of harmonic tremor \nin continuous seismic data.\n\nLast modified: August 23, 2017\n\nFor details on the analysis method, please read (and cite): Roman, D.C. (2017), Automated detection and characterization of harmonic tremor in continuous seismic data. Geophys. Res. Lett., 44, doi: 10.1002/2017GL073715.\n\nPLEASE NOTE: Requires Matlab R2013b or later including the Matlab Signal Processing and Stats Toolboxes.\n\nReport bugs or suggestions for future releases to droman@carnegiescience.edu. \n\n\nNECESSARY FILES - The working directory must contain three files: \n1. tremometer_control.m\n2. tremometer.m\n3. A single-column ascii file containing one day of seismic data. Note: Tremometer does not provide filtering or instrument response deconvolution, so these operations should be peformed independently before analysis. For this task I recommend using the code described in Haney et al. (2012) Causal Instrument Corrections for Short Period and Broadband Seismometers. Seismological Research Letters 83, 834-845.\n\n\nSETUP\nThe user should set seven parameters in the first section of 'tremometer_control.m': \n\nThe first five parameters describe the seismic data to be analyzed: \n1. x - Assign one day of (instrument-corrected and filtered) data to x\n2. yr - Set start date (four digit year) of input data\n3. mo - Set start date (two digit month) of input data\n4. dy - Set start date (two digit day) of input data\n5. fs - Set sampling frequency (in Hz). \n\nThe next two parameters describe the criteria for detection of harmonic tremor: \n\n6. minfreq - set the minimum allowed frequency of the fundamental (in Hz). Default is 0.5 Hz\n7. minHSI - set the minimum allowed Harmonic Strength Index for the fundamental and first two overtones. Default is 30. \n\nThe final parameter toggles whether a periodogram is displayed for each detected minute of tremor. \n\n8. calmodeflag - Toggle periodogram plots on (1) or off (0). Default is on (1). \n\nOPERATION\nOnce the control parameters have been set, execute the code by running tremometer_control.m at the Matlab command line (from the working directory)\n> tremometer_control\n\n\nEXAMPLE\nAn example of one day of instrument-corrected, bandpass-filtered (0.5-20Hz) seismic data containing multiple episodes of harmonic tremor is provided as 'example.asc'. To run as an example, download this file and unzip it, place it in a directory with tremometer_control.m and tremometer.m, edit tremometer_control.m to set fs=40 and x=example.asc, and run tremometer_control.m at the Matlab command line. You can also adjust parameters 6 and 7 to see the effect on the number of detections. \n\n\nOUTPUT\nBy default the code outputs the following information at the end of each run: \n1. A command window list of each tremor detection (detection time, frequencies of the fundamental and first three harmonics, and HSI for the fundamental and first two harmonics). This information is also contained in the workspace in the 'detections' table. \n\n2. The variable 'final' contains the analysis results for all input data. Each row corresponds to a minute of the analyzed day of data (1440 rows). \n\n3. (Calibration mode) A figure will be displayed for each identified minute of harmonic tremor, showing the periodigram and detected fundamental and harmonics (three '*' symbols). \n\n4. Figure 1: The HSI of the fundamental for the entire day. \n\n5. Figure 2: The frequencies and time(s) for each identified minute of harmonic tremor. \n\n\nTO BE ADDED TO FUTURE VERSIONS\n1. An option to adjust the analysis increment (currently hardwired as one minute). This will help in detection of short episodes of strongly gliding tremor. \n\n2. Advanced options to check a variable number of harmonics, allow different HSI detection thresholds for each harmonic, set the number of consecutive minutes of tremor required for a detection. \n\n3. More/better plotting options\n\n4. Reduction of toolbox dependencies. \n\n5. Port to ObsPy (some day...)\n\nKNOWN BUGS (to be fixed in future versions): \nIt still (rarely) throws 'Index exceeds matrix dimensions' errors. Sometimes because data is not one full day. Add a workaround for this. \n",
        "createdAt": "2017-07-25T01:34:55.000Z",
        "updatedAt": "2024-11-01T15:08:59.000Z",
        "language": "MATLAB",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/dcroman/Tremometer/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "amelia-campos/comp-seismology",
        "url": "https://github.com/amelia-campos/comp-seismology",
        "description": "Exercises from Computational Seismology, a Practical Introduction, Igel (2016).",
        "stars": 0,
        "forks": 0,
        "readme": "Learning about numerical methods for seismic modeling. Using the book [Computational Seismology: A Practical Introduction](https://www.researchgate.net/publication/283566366_Computational_Seismology_A_Practical_Introduction). Igel, Heiner. (2016).\n\nSome highlights:\n\n- [Trapped waves in a fault zone - 2D finite-difference simulation](4.32_FD_2D_5pt_faultzone.ipynb)\n- [Synthetic seismograms of a general layered model in 1D](2.26_Greens_1D_layers.ipynb)\n",
        "createdAt": "2025-06-02T20:28:41.000Z",
        "updatedAt": "2025-09-24T03:28:46.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/amelia-campos/comp-seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "yudhastyawan/quakesee_web",
        "url": "https://github.com/yudhastyawan/quakesee_web",
        "description": "QuakeSee is an application for visualizing earthquake data, including event locations, instrument response, and waveforms from various seismic station networks (web app version).",
        "stars": 0,
        "forks": 0,
        "readme": "# QuakeSee - Seismic Event Viewer\n\nQuakeSee is an application for visualizing earthquake data, including event locations, instrument response, and waveforms from various seismic station networks.\n\n![QuakeSee Web App UI](https://i.imgur.com/1vo0EsW.png)\n\nQuakeSee Web App ver. 0.1.0\n\n\n## Disclaimer:\n\nWe are not responsible for any data processing errors that may occur in this program. \nUsers are encouraged to verify processing results before making decisions based on the \ninformation provided.\n\n## About the Program:\n\nQuakeSee is an application designed to visualize earthquake data with features such as displaying \nearthquake event locations, instrument responses, and waveforms obtained from various seismic station \nnetworks worldwide.\n\n## Developer Information:\n\nThis program was developed by:\nThe QuakeSee Development Team - Yudha Styawan\nLecturer, Geophysics Engineering - Institut Teknologi Sumatera\nContact: yudhastyawan26@gmail.com",
        "createdAt": "2025-03-09T09:15:25.000Z",
        "updatedAt": "2025-03-11T07:54:40.000Z",
        "language": "Python",
        "homepage": "https://pypi.org/project/quakesee-web/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/yudhastyawan/quakesee_web/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "VioletaSeo/earthquake",
        "url": "https://github.com/VioletaSeo/earthquake",
        "description": "Research concerning seismology and earthquake physics.",
        "stars": 1,
        "forks": 0,
        "readme": "# Detection of Gain Problem\n\nProject 1: Detecting potential gain problems in seismic arrays installed in South Korea. The method developed by Park and Ishii(2019) is              applied to perform such detection.\n\n1-1. First, the test is conducted using the teleseismic events recorded by Hi-net stations which are the datasets used by Park and              Ishii(2019) to demonstrate applicability of their method.\n\n1.2. If the method seems to reproduce the result shown by Park and Ishii(2019), the method is applied to seismograms recorded by seismic        arrays of South Korea.\n\nReference: Park, S., & Ishii, M. (2019). Detection of instrument gain problems based on body‐wave polarization: Application to the Hi‐net array. Seismological Research Letters, 90(2A), 692-698.\n",
        "createdAt": "2019-11-17T12:21:16.000Z",
        "updatedAt": "2020-04-03T08:57:33.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/VioletaSeo/earthquake/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "SeisSCOPED/HPS-book",
        "url": "https://github.com/SeisSCOPED/HPS-book",
        "description": "High Performance Seismology JBook for big data and big modeling in seismology",
        "stars": 3,
        "forks": 1,
        "readme": "# HPS Jupyter Book\n\n[![Deploy](https://github.com/SeisSCOPED/HPS-book/actions/workflows/publish.yml/badge.svg)](https://github.com/SeisSCOPED/HPS-book/actions/workflows/publish.yml)\n[![Jupyter Book Badge](https://jupyterbook.org/badge.svg)](https://SeisSCOPED/HPS-book)\n<!-- [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/geo-smart/simple-template/HEAD?labpath=book%2Fchapters) -->\n<!-- [![GeoSMART Use Case](./book/img/use_case_badge.svg)](https://geo-smart.github.io/usecases) -->\n\n\n## Citation\n\nIf you use this book, please cite it as follows:\n\n```{bibliography}\n```\n\n## HPS Book contributors.\n\nWe welcome contribution to this book. Because this is the draft of a textbook, we are still exploring ways to homogeneize the format of the books. We accept the following main threads of research:\n1. **CyberInfrastructure** content (e.g., how to use HPC, how to use the Cloud, open science) that does not replicate content from other sources and that is relevant to seismological research.\n2. **Core Software** content: how to use a core package such as SPECFEM, noisepy, seisbench etc. The tutorials should be described as simple, boiler-plate example on how to use these packages that are the core of a research workflow.\n3. **Research Workflow**: these are fullstack notebooks that leverage CI and core software for specific research question. We will welcome a few to exemplify current research practices.\n\n## How to contribute.\nWe welcome contributions to this book! Follow the steps below to set up your environment, add your content, and ensure it appears correctly in the book.\n\n### Step 1: Set Up Your Environment\n1. **Clone the repository**\n```\ngit clone https://github.com/SeisSCOPED/HPS-book.git\ncd HPS-book\n```\n2. **Create and Activate a Conda Environment**: Ensure you have Conda installed. We recommend miniconda or mamba.\n```\nconda env create -f environment.yml\nconda activate hps-book\n```\n### Step 2: Add Your Content\n1. **Add Rendered Notebooks**: Place your rendered Jupyter notebooks (``.ipynb`` files) or markdown files (``.md`` files) in the appropriate directory within the ``book`` directory. For example, if you are adding a new tutorial, you might place it in ``book/tutorials``.\n\n2. **Modify the Table of Contents (TOC)**: Update the ``_toc.yml`` file to include your new content. This file controls the structure of the book and how the chapters appear in the sidebar.\n\nOpen ``_toc.yml`` and add an entry for your new content. For example:\n```\n- file: intro\n- part: Tutorials\n  chapters:\n    - file: tutorials/your_new_tutorial\n```\nEnsure the path to your file is correct and relative to the ``book`` directory.\n\n\n### Step 3: Format Your Notebooks or Markdown Files\n1. **Notebook Formatting:**\n    * Ensure your notebooks are clean and free of unnecessary output. You can clear the output by going to ``Kernel -> Restart & Clear Output`` in Jupyter Notebook.\n    * Use headings (``#``,``##``, ``###``, etc.) to structure your content.\n    * Add markdown cells to explain your code and results.\n2. **Markdown File Formatting:**\n    * Use proper markdown syntax for headings, lists, links, images, etc.\n    * Ensure your markdown files have a title at the top using a level 1 heading (``#``).\n\n### Step 4: Build the Jupyter Book Locally\n1. **Build the book**\nIn the repository main directory, type \n```\njb build book/\n```\n2. **Preview the book**\nYou can preview the book by opening the ``_build/html/index.html`` file in your web browser.\n\n\n### Step 5: Submit Your Contribution\n1. **Commit Your Changes:**\n```\ngit add .\ngit commit -m \"Add new tutorial on [topic]\"\n```\n2. **Push Your Changes:**\n```\ngit push origin your-branch-name\n```\n3. **Create a Pull Request:** Go to the repository on GitHub and create a pull request. Provide a clear description of your changes and the content you have added.\n\n### Example of a Well-Formatted Notebook\nHere’s an example of how to format a Jupyter notebook to ensure it appears nicely in the book:\n```\n# Title of Your Tutorial\n\n## Introduction\n\nProvide an introduction to your tutorial. Explain what the reader will learn and any prerequisites.\n\n## Step 1: Setup\n\nExplain the setup process. Include any necessary code.\n\n```python\n# Example setup code\nimport numpy as np\nimport matplotlib.pyplot as plt\n```\n\n::notes::\nTo improve pedagogy, we recommend that notebooks are designed to include student-led activities with empty cells, and provide avenues to change parameters in the workflows.\n\n### Step 2: Main Content\nProvide the main content of your tutorial. Use markdown cells to explain each step.\n```\n# Example code\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\nplt.plot(x, y)\nplt.title(\"Sine Wave\")\nplt.show()\n```\n### Conclusion\nSummarize what the reader has learned and provide any additional resources or next steps.\n```\n\n### Example of a Well-Formatted Markdown File\n\nHere’s an example of how to format a markdown file to ensure it appears nicely in the book:\n\n```markdown\n# Title of Your Tutorial\n\n## Introduction\n\nProvide an introduction to your tutorial. Explain what the reader will learn and any prerequisites.\n\n## Step 1: Setup\n\nExplain the setup process. Include any necessary code.\n\n```python\n# Example setup code\nimport numpy as np\nimport matplotlib.pyplot as plt\n```\n\n\n\n## Original Template\nThis repository was made by a GEOSMART template and has the skeleton of a GeoSMART use case book in chapter ML in Seismology. The original authors were Scott Henderson (UW) and other friends at the eScience Institute. Below is how to reuse the template<br>\n\n1. Click \"Use This Template\" and name your repository\n\n2. In your repository edit book/_config.yml\n\n3. Under your repository Settings --> Pages --> Source = GitHub Actions\n\n3. Edit environment.yml, modify notebooks, and your JupyterBook will be published for you! \n",
        "createdAt": "2024-02-26T13:48:29.000Z",
        "updatedAt": "2025-06-11T14:56:12.000Z",
        "language": "Jupyter Notebook",
        "homepage": "http://seisscoped.org/HPS-book/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/SeisSCOPED/HPS-book/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "hoyekan/GEOP591-Theoretical-Seismology",
        "url": "https://github.com/hoyekan/GEOP591-Theoretical-Seismology",
        "description": "This repository contains detailed derivation to some classical solutions in theoretical seismology. ",
        "stars": 0,
        "forks": 0,
        "readme": "# GEOP591 - Theoretical-Seismology\n\nGEOP 591 is a graduate special course (called theoretical seismology) offered at King Fahd University of Petroleum and Minerals (KFUPM). The custodian of this repository took the course in Term 242 (January 12, 2025 - May 28, 2025). The course instructor during this period was Dr. Wei Zhou. So, this repository will contain detailed mathematical derivation to some classical solutions in theoretical seismology. \n\nAlthough some of the solutions in this repository can be found in classic theoretical seismology textbooks, such as Quantitative Seismology by Aki and Richards, I have a few issues with these texts. One major concern is that they often do not include detailed mathematical derivations for problems—something I greatly value. As such, these textbooks can be particularly challenging for beginners in theoretical seismology, as they tend to omit intermediate steps in the derivations. I had the same concern when I first started the course, as the lack of detailed explanations made it difficult to follow the material.\n\nThe tutorials in this repository aim to bridge that gap. As such, a junior or senior undergraduate student with a solid understanding of key concepts in integral and differential calculus should be able to follow the material in this repository without any problem. This material can also used by graduate students or faculties to teach theoretical seismology.\n\nKey References\n\n1. Aki, Keiiti, and Paul G. Richards. Quantitative seismology. 2002.\n",
        "createdAt": "2025-05-28T08:06:55.000Z",
        "updatedAt": "2025-12-01T22:10:32.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/hoyekan/GEOP591-Theoretical-Seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "YoelRoger/seismology",
        "url": "https://github.com/YoelRoger/seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2020-03-27T22:54:37.000Z",
        "updatedAt": "2023-03-07T16:08:18.000Z",
        "language": "HTML",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "NoiseCIEI/RayTomo",
        "url": "https://github.com/NoiseCIEI/RayTomo",
        "description": "Seismic Surface Wave Isotropic/Azimuthally Anisotropic Group/Phase Speed 2-D Tomography",
        "stars": 26,
        "forks": 18,
        "readme": "",
        "createdAt": "2018-11-12T01:03:38.000Z",
        "updatedAt": "2025-07-02T09:47:30.000Z",
        "language": "Roff",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "JanisHe/WindPSD",
        "url": "https://github.com/JanisHe/WindPSD",
        "description": "Computing wind dependent PSD spectra of seismological stations from a local SDS path",
        "stars": 0,
        "forks": 0,
        "readme": "# Wind Dependent PSD spectra\n\n## Installation\nEither download this repository to work in the project or install this project by running\n`pip install git+https://github.com/JanisHe/WindPSD`\n\n## Requirements\n`numpy`, `obspy`, `tqdm`, `meteostat` (https://dev.meteostat.net/python/)\n\n## Setting parameters\nIn `parfiles/parfile.yml` you can find an example for settings. This parameter file is loaded\nin `main`, however, instead of the parameter file the function `main` also takes a dictionary\nas input. When working with a dictionary, the keys are the same keys as in `parfiles/parfile.yml`.\n\n## Compute Spectra\nOnce you have created or modified your `parfile.yml`, you can start the computation of the\nspectra by running the `main` function from `windpsd.main`:\n```python\nfrom windpsd.main import main\n\nparfile = \"./parfiles/parfile.yml\"\nmain(parfile=parfile)\n```\n\n## Possible issues\n- In case you get a ModuleImportError try to add the PythonPath to your project by\n```python\nimport sys\nsys.path.append(\"/path/to/my/windpsd/project\")\n```\n- Others errors are might be raised by the main function. Then please check if your settings are correct.\n\n## Example output\nThe Figure shows two wind dependent PSD spectra in 2012 (a) and 2019 (b). In this time period six\nwind turbines have been installed in the vicinity of that station, leading to the massive\npeaks in the 2019 spectrum.\n![](./examples/psd_bavn.png)\n\n## Citation\n- Heuel, J., & Friederich, W. (2022). Suppression of wind turbine noise from seismological data using nonlinear thresholding and denoising autoencoder. Journal of Seismology, 26(5), 913-934.\n- Stammler, K., & Ceranna, L. (2016). Influence of wind turbines on seismic records of the Gräfenberg array. Seismological Research Letters, 87(5), 1075-1081.\n",
        "createdAt": "2025-10-17T08:15:37.000Z",
        "updatedAt": "2025-10-17T09:30:18.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/JanisHe/WindPSD/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "jamesbcarmichael/Fault-Plane-Automation-Determination-",
        "url": "https://github.com/jamesbcarmichael/Fault-Plane-Automation-Determination-",
        "description": "[A Machine Learning (ML) Based Algorithm]: For Either Seismological/Earthquake or Microseismic Data, “Swarm-Cloud” Investigations",
        "stars": 2,
        "forks": 0,
        "readme": "",
        "createdAt": "2021-12-01T15:00:05.000Z",
        "updatedAt": "2024-04-26T08:55:39.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Boritech-Solutions/PyEarthworm",
        "url": "https://github.com/Boritech-Solutions/PyEarthworm",
        "description": "A Python interface to the Earthworm Seismic Data System.",
        "stars": 13,
        "forks": 7,
        "readme": "# PyEarthworm\n\nPyEarthworm is a python interface to the Earthworm Messaging Transport system. It seeks to create an easy to use framework to create modern earthworm modules with python. The main class handles the EW module basics such as listening to stop messages and creating heartbeats. You can then use python threads to aquire and insert data from multiple EW Rings. Wave data is already returned as a numpy array so you may use fast c-optimized algorithms with cython. It comes with multiple examples which you may modify in order to speed up development.\n\n## Installation\n\n### Preparation\n\nIn order to install PyEarthworm, you need to have first installed Earthworm itself. This module builds successfully on Linux against earthworm-7.10 compiled with `EWBITS=64` and with \"`-m64 -fPIC`\" added to `GLOBALFLAGS` in `${EW_HOME}/environment/ew_linux.bash`. Your milage may vary for other versions of Earthworm, or on other platforms.\n\nTo compile the PyEarthworm C extensions, we need to find the Earthworm `.h` files. We'll look in the following places:\n\n```\n${EW_HOME}/include\n${EW_HOME}/${EW_VERSION}/include\n```\n\nThus, ensure that the `EW_HOME` and `EW_VERSION` environment variables are set apporpriately.\n\nWe will also need the `CFLAGS` you used to compile earthworm with.  The value of `CFLAGS` looks something like this on a Linux system:\n\n```\nCFLAGS=-fno-stack-protector -fPIC -Dlinux -D_LINUX -D_INTEL -D_USE_SCHED -D_USE_PTHREADS -D_USE_TERMIOS -I/path/to/ew/includes\n```\n\n### Installing via pip\n\nTo install via pip, ensure that you have a recent version of pip installed (>=10.0) and do:\n\n```\npip install git+https://github.com/Boritech-Solutions/PyEarthworm\n```\n\n### Install via conda\n\n```\nconda create --name PyEW\nconda activate PyEW\nconda install numpy\nconda install cython\n```\n\n### Download a release and install manually\n\nRecent binary compiled releases can be found [here](\nhttps://github.com/Boritech-Solutions/PyEarthworm/releases/latest) pre-compiled for Linux and Windows intended for python 3.7. These can be placed in the dynload or DLL directory in your python path. \n\n### Install from source\n\n```\ngit clone https://github.com/Boritech-Solutions/PyEarthworm\ncd PyEarthworm\npip install cython numpy\npython setup.py install\n```\n\n### Build without installing\n\n```\ngit clone https://github.com/Boritech-Solutions/PyEarthworm\ncd PyEarthworm\npip install cython numpy\npython setup.py build_ext -i\n```\n\nA file named `PyEW.cpython-36m-x86_64-linux-gnu.so` should have been created.\n\nWe can test this works by importing into python:\n\n    $ python\n    >>> import PyEW\n\n## Usage:\n\nA Jupyter notebook workshop can be found [here](https://github.com/Fran89/PyEarthworm_Workshop).\nThe main class for communication with earthworm is PyEW.EWModule. It is a class that takes care of most of what a module should do, and it is described as follows:\n\n#### PyEW.EWModule:\n  * **PyEW.EWModule(def_ring, mod_id, inst_id, hb_time, db):**  \n  This will initiate a EW Module object with a default ring: def_ring, module id: mod_id, installation id: inst_id, heartbeat interval: hb_time, and debuging set to FALSE (by default). This module will initiate a heartbeat thread that will start by default, it will initate a listener by default to stop if EW emitts a stop message. These listeners and heartbeats will be sent to the default ring! Any message that is sent from this module will also inherit the module id and inst id that was setup here.\n  \n  * **PyEW.EWModule().goodbye():**  \n  This method will begin the mod shutdown if called from somewhere else. It is important to shut down all listener threads before any attempt at shutdown of a program. It will print 'Gracefull Shutdown' when ready to end.\n  \n  * **PyEW.EWModule().mod_sta():**  \n  This method will return a boolean value that reflects if the internal state is ok. Listen to this value to check if there has been a request for the module shutdown.\n  \n  * **PyEW.EWModule().req_syssta()**  \n  This method will request and print the EW System status. Mainly used for testing purposes.\n  \n  * **PyEW.EWModule().add_ring(ring_id):**  \n  This will add a ring to an internal buffer of rings. The ring id: ring_id given will add the listener. For example: Mod.add_ring(1000) will add ring **1000** at location **0**. Should you call this method again (e.g. Mod.add_ring(1005)) will add ring **1005** at location **1** and so on. You can add many rings for multiple inputs and outputs.\n  \n  * **PyEW.EWModule().get_bytes(buf_ring, msg_type)  \n  PyEW.EWModule().get_msg(buf_ring, msg_type):**  \n  These two methods will get either a bytestring (which you would have to decode) or a text string (which has been decoded for you) from the memory buffer at location: buf_ring __(ring must have been added from the add_ring() method in order for this to work)__ and from the message type: msg_type. Be warned get_msg will expect a null terminated string. If nothing is found it will return an empty string, otherwise it will return a python string or python bytestring.\n  \n  * **PyEW.EWModule().put_bytes(buf_ring, msg_type, msg)  \n  PyEW.EWModule().put_msg(buf_ring, msg_type, msg):**  \n  Likewise these two methods will put either a bytestring or a text string into the memory buffer at location: buf_ring __(ring must have been added from the add_ring() method in order for this to work)__ with message type: msg_type. The lenght of the string is determined by the len() method.\n  \n  * **PyEW.EWModule().get_wave(buf_ring):**\n  This method will attempt to retrive a wave message from the memory buffer at location: buf_ring __(ring must have been added from the add_ring() method in order for this to work)__. If it's successfull it will return a python dictionary with the following wave packet information:  \n  \n        {\n          'station': python.string,\n          'network': python.string,\n          'channel': python.string,\n          'location': python.string,\n          'nsamp': python.int,\n          'samprate': python.int,\n          'startt': python.int,\n          'endt': python.int,\n          'datatype': python.string,\n          'data': numpy.array\n        }\n        \n  * **PyEW.EWModule().put_wave(buf_ring, msg):**\n  This method will attempt to insert a wave message into the memory buffer at location: buf_ring __(ring must have been added from the add_ring() method in order for this to work)__. The message must be a python dictionary of the following information:  \n  \n        {\n          'station': python.string, # 4 Sta max \n          'network': python.string, # 2 Net max\n          'channel': python.string, # 3 Cha max\n          'location': python.string,# 2 Cha max\n          'nsamp': python.int,\n          'samprate': python.int,\n          'startt': python.int,\n          'endt': python.int, # This one may be ommited and calculated on the fly.\n          'datatype': python.string, # i2, i4, f4, (\"f8\"?!)\n          'data': numpy.array\n        }\n  It must be stressed that the maximum amount of bytes cannot be more than the one specified in the EW Specification (4096). Additionally, Earthworm does not work well (if at all?) with double precision and extra care must be made when inserting data into EW.\n  \n However PyEarthWorm has various classes that have various degrees of abstractions, these are more low level and should not be used unless absolutely needed. For example:\n\n#### PyEW.ring:\n  * **PyEW.ring(ring_id):**  \n  This will initiate a ring object that can communicate with ring: ring_id.\n  \n  * **PyEW.ring().attach():**  \n  This method will attach the ring.\n  \n  * **PyEW.ring().detach():**  \n  This method will detach the ring.\n  \n  * **PyEW.ring().get_buffer():**  \n  This method will return the pointer to the memory buffer.\n  \n#### PyEW.transport:\n  * **PyEW.transport(ring_id, mod, inst):**  \n  This will initiate a transport object with ring: ring_id, module id: mod, and installation id: inst. This object will be automatically attached to the ring.\n  \n  * **PyEW.transport().detach():**  \n  This method detaches the object from the ring. No way to re-attach.\n  \n  * **PyEW.transport().putmsg(mtype, msg, size):**  \n  This method will insert a message into a ring of type: mtype, bytestring: msg, and size: size.\n  \n  * **PyEW.transport().getmsg_type(mtype):**  \n  This method will attempt to get a message from the ring. Returns a tuple of msg.lengh and msg. The tuple will be 0, 0 if unsuccesfull.\n  \n  * **PyEW.transport().copymsg_type(mtype)**  \n  This method will attempt to copy the messagea from the ring. Returns a tuple of msg.lengh and msg. The tuple will be 0, 0 if unsuccesfull.\n  \n  * **PyEW.transport().reqsta()**  \n  This method will return a tuple with the system status.\n\n### Logging\nAs of May 1, 2019 this module uses python naitive logging levels. In order to see log messages please use:\n\n    import logging\n    logging.getLogger().addHandler(logging.StreamHandler())\n    logging.getLogger().setLevel(logging.INFO)\n\nYou may log to stdout or stderr, or you may add a file handler to be able to see log files from this module.\nInitializing with debug = True will add a much more verbose output of log messages and might create big files,\nuse with caution.\n  \n## Examples:\n  Included with PyEarthworm is a series of examples that may help you in figuring out how this works:\n  * The Ring2Ring module is a reimagined in python Ring2Ring module. It has no way to filter, however it can be added to suit your needs. It may have multiple input and output rings and can be used to collapse multiple ring2ring instances.\n  * The BNC2Ring module is essentially a NMEAString2EW module that will take input from BNC PPP and place it in a EW Ring.\n  * The gsof2Ring module is a modified version of UNAVCO's python script to read gsof and insert them into an EW Ring.\n  * The Ring2Mongo module will take wave information and store it in a mongo database (ew-waves). By default it creates a capped collection of 10 Mb for every station regardless of channel. Unless the (time) version is used it will store 3 minutes of data.\n  * The Mongo2Ring module can create a listener for changes to an Mongo database that is receiving EW Wave JSON objects. It then modifies these objects in order to be able to add them to a EW Ring. A MongoDB has the advantage of being able to push to multiple listeners and sideways scalability making it ideal to connect an EW to a main database. It requires the latest MongoDB and PyMongo, due to the use of watch pointers.\n  * Finally the Ring2Plot is a time limitied Ring2Mongo with a meteor nodejs application (ewrttv) that can be used to plot and display data to a browser (3 components, 1 station). A live version of this can be found [HERE](http://ewrttv.fran89.com). Additionally a single component version is availible in a different branch (singleplot).\n  \n#### To use a module:\n  To start a module you should change directory into the modules main folder and start a python shell:\n  \n    $ cd example/Ring2Ring/\n    $ python\n    >>> import EWMod\n    >>> Module = EWMod.Ring2Ring()\n    >>> Module.start()\n  \n  To stop the module:\n  \n    >>> Module.stop()\n    \n  In order to start and stop it from startstop it must be modified to start from a shell script and then place that shell script in the startstop_\\*.d. Additionally a dummy .desc file must be created in the param directory and placed in statmgr.d so that your startstop doesn't get spammed with:\n  \n    UTC_Thu Jul  5 23:03:35 2018  EW/statmgr msg from unknown module  (statmgr doesn't have a .desc file for this one) inst:141 mod:8 typ:3\n  \nI hope that you can use these examples to build your own modules that are shared with the community.\n  \n### Acknowledgments\n-------------------\n\n  * I would like to thank ISTI and the EW community, without their contributions to EW this software would not be possible.\n  * Chris Malek for making the package installable via PIP.\n  * The development and maintenance of PyEarthworm is funded entirely by software and research contracts with Boritech Solutions.\n  * The python and cython community.\n  \n#### AD:\nBoritech Solutions is a consulting firm that can help you set up and create modules with PyEarthworm. Contact Us today!\nwww.BoritechSolutions.com\n\n~Francisco.\n  \n\n  \n\n",
        "createdAt": "2018-07-05T22:44:46.000Z",
        "updatedAt": "2025-07-16T18:21:16.000Z",
        "language": "C",
        "homepage": "http://www.boritechsolutions.com",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Boritech-Solutions/PyEarthworm/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "shicks-seismo/Seis-Utils",
        "url": "https://github.com/shicks-seismo/Seis-Utils",
        "description": "Various utilities for seismology",
        "stars": 3,
        "forks": 1,
        "readme": "# Seis-Utils\nVarious utilities for seismology\n\n1. station2hypo71.py\nConvert a space-delimited station file to HYPO71 input format. Input format: Network, station, lat, long, elevation (m).\nUsage: station2hypo71.py <infile> <outfile>\n\n2. surface_wave_detect.py\nPython script to download LDEO surface-wave detection events from last week from\nhttps://www.ldeo.columbia.edu/~ekstrom/Research/SWD/current/RADB_SWD_grd.html\nand cross-check with events in USGS-NEIC and ISC catalogues.\nPrints out a list of surface wave detections not found in catalogues and plots map. \nUsage: python surface_wave_detect.py\n",
        "createdAt": "2017-08-14T10:53:41.000Z",
        "updatedAt": "2020-10-07T00:50:20.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/shicks-seismo/Seis-Utils/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "rbherrmann/cpsdocs",
        "url": "https://github.com/rbherrmann/cpsdocs",
        "description": "Documentation for Computer Programs in Seismology",
        "stars": 0,
        "forks": 0,
        "readme": "# cpsdocs\nDocumentation for Computer Programs in Seismology\n",
        "createdAt": "2022-05-17T09:43:47.000Z",
        "updatedAt": "2024-11-10T08:07:00.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/rbherrmann/cpsdocs/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "bgjx/lqt-moment-magnitude",
        "url": "https://github.com/bgjx/lqt-moment-magnitude",
        "description": "A fast, automated program for calculating moment magnitude using full P, SV, and SH energy components with rapid spectral fitting for improved accuracy and efficiency.",
        "stars": 2,
        "forks": 1,
        "readme": "[![Follow me on Twitter](https://img.shields.io/badge/Follow-@zakki_edelo-blue?logo=x&logoColor=white&style=flat)](https://twitter.com/zakki_edelo)\n[![LinkedIn](https://img.shields.io/badge/LinkedIn-arham_zakki-0A66C2?style=flat&logo=linkedin)](https://www.linkedin.com/in/arhamzakki)\n[![CI - Tests](https://github.com/bgjx/lqt-moment-magnitude/actions/workflows/ci-cd.yml/badge.svg)](https://github.com/bgjx/lqt-moment-magnitude/actions/workflows/ci-cd.yml)\n[![GitHub Issues](https://img.shields.io/github/issues/bgjx/lqt-moment-magnitude?style=flat)](https://github.com/bgjx/lqt-moment-magnitude/issues)\n[![GitHub Commits](https://img.shields.io/github/last-commit/bgjx/lqt-moment-magnitude?style=flat)](https://github.com/bgjx/lqt-moment-magnitude/commits/main/)\n[![PyPI](https://img.shields.io/pypi/v/lqtmoment?style=flat$logo=pypi)](https://pypi.org/project/lqtmoment/)\n[![License](https://img.shields.io/badge/License-MIT-green?style=flat)](https://opensource.org/licenses/MIT)\n[![Python](https://img.shields.io/badge/Python-3.9%2B-blue?style=flat&logo=python)](https://www.python.org/)\n\n\n## What is it?\n\n**lqtmoment** is a Python package designed for moment magnitude calculations using pure P, SV, and SH components in the LQT ray coordinate system. It leverages rapid ray tracing to compute incidence angles for component rotation and employs fast spectral fitting to find an optimal seismic moment solution, ensuring high accuracy and efficient automated computation.\n\nBy leveraging vectorized computing and advanced statistical methods, such as in implementing Shooting Snell’s Method for incidence angle estimation and Quasi-Monte Carlo techniques for spectral fitting, lqtmoment excels at calculating moment magnitudes for batches of earthquakes, handling hundreds to thousands of events in a single run.\n\n--------------\n### **Lqtmoment Test with Real Data**\n\nBelow is the `lqtmoment` test using real data, moment magnitudes for `700++ earthquakes` were successfully calculated in `LQT ray coordinate systems`, with an average of ~1.8 seconds per iteration. The seismogram data was recorded at `500 sps` using a 3-component shallow borehole sensor in 15 stations network.\n\n``` python\n\n    # lqtmoment test\n    from lqtmoment import magnitude_estimator\n    from pathlib import Path\n\n    # directory object\n    dirs = {\"wave_dir\": r\"test\\wave\",\n            \"calib_dir\": r\"test\\calibration\",\n            \"catalog_file\": r\"lqt_catalog.csv\",\n            \"config_file\": r\"config.ini\"    \n    }\n\n    merged_lqt_catalog, lqt_moment_result, lqt_fitting_result = magnitude_estimator(    \n                                                                wave_dir= dirs['wave_dir'],\n                                                                cal_dir= dirs['calib_dir'],\n                                                                catalog_file= dirs['catalog_file'],\n                                                                config_file= dirs['config_file'],\n                                                                id_start=2000,\n                                                                id_end=2795,\n                                                                lqt_mode=True,\n                                                                generate_figure=False\n                                                                )\n```\n``` bash\n    Processing earthquakes: 100%|███████| 796/796 [07:31<00:00,  1.76it/s, Failed=0]\n    Finished. Proceed 796 earthquakes successfully,0 failed. Check lqt_runtime.log for details. \n\n```\n\n--------------\n\nThe **lqtmoment** includes modules for building input catalog format, performing moment magnitude calculation, visualizations, and data analysis.\n\nContact the developer: Arham Zakki Edelo (edelo.arham@gmail.com)\n\n* [Installation](#Installation)\n* [Tutorials](#Tutorials)\n* [Scope of Capabilities](#Scope-of-Capabilities)\n* [Examples](#Examples)\n* [References](#References)\n* [Contributing](#Contributing)\n* [Report Bugs](#Report-Bugs)\n* [Support](#Support)\n\n--------------\n### Installation\n**lqtmoment** can be installed and run on multiple platforms, including macOS, Windows, and Linux, using Python versions 3.9 to 3.12. Choose one of the following installation methods:\n\n#### Option 1: Via Anaconda\nIf you have Anaconda installed, create and activate a new environment (for clean installation), then install **lqtmoment** from the `bgjx` channel:\n\n```bash\n    conda create -n lqtmoment python=3.9\n    conda activate lqtmoment\n    conda install -c bgjx lqtmoment\n```\n\n#### Option 2: Via PyPI\nIt'is recommended (but optional) to upgrade `pip` first. Ensure you're in a virtual environment if desired.\n\n```bash\n    python -m pip install --upgrade pip\n    python -m pip install lqtmoment\n\n```\n\n#### Option 3: Build from Source Code\n\nTo build from source, you need `Git` to be installed first in your computer and then you can clone the source from `GitHub` repo:\n\n```bash\n    git clone https://github.com/bgjx/lqt-moment-magnitude.git\n```\n\nNavigate to the package directory, to `lqt-moment-magnitude`\n\n```bash\n    cd lqt-moment-magnitude\n```\n\nAnd install the package\n\n```bash\n    python -m pip install .\n```\n\n--------------\n### Tutorials\n\nA series of tutorials are provided here: \n\n[Tutorials](https://github.com/bgjx/lqt-moment-magnitude/tree/main/lqt-tutorials)\n\nThese tutorials explain how to prepare input data, set initial parameters for your specific case, run the moment magnitude calculation, and use other features. \n\n--------------\n### Scope of Capabilities\nBelow are key details outlining the capabilities and limitations of `lqtmoment` in its current version:\n\n- **1. Earthquake Classification**: \n    **lqtmoment** automatically categorizes earthquake data based on epicentral distance or source depth:\n    - `very_local_earthquake`: Epicentral distance < 30 km or < 2× source depth (whichever is satisfied first).\n    - `local_earthquake`: Epicentral distance > 30 km or > 2× source depth (whichever is satisfied first) and < 100 km.\n    - `regional_earthquake`: Epicentral distance > 100 km and < 1110 km.\n    - `far_regional_earthquake`: Epicentral distance > 1110 km and < 2220 km.\n    - `teleseismic_earthquake`: Epicentral distance > 2220 km.\n\n- **2. Velocity Model Usage**:\n    In this current version, user defined velocity model (in a `.json` file) is applied only for calculating the incidence angle via ray tracing (Shooting Snell's Method) for ZNE to LQT rotation for `very_local_earthquake` and `local_earthquake` classifications. For farther earthquake types,  the model defaults to `TauPyModel` from `obspy.taup` module configurable in `config.ini`\n\n- **3. Velocity Model Limitation**:\n    For this current version, the user-defined velocity model (e.g., `velocity_model.json`, or any name you choose), still and must be a 1-D velocity model for rapid estimation.\n\n- **4. Incidence Angle Calculation**:\n    Incidence angles are computed using ray tracing of refracted P and S waves, enabling ZNE-to-LQT rotation across all earthquake classifications:  \n    - For `very_local_earthquake` and `local_earthquake`, **lqtmoment** uses an internal method: it performs vectorized computation based on Shooting Snell's Method and energy comparison between direct (Pg/Sg) and critically refracted (Pn/Sn) phases, selecting the incidence angle of the stronger phase if their arrival times differ by at least ~2× the dominant period. This ensures robustness, rapidness and precision for local events where refracted waves dominate. \n    - For farther earthquake classifications, incidence angles for P and S phases are retrieved from the TauPyModel (via obspy.taup), supporting LQT rotation without energy-based phase selection.\n    - Reflected waves (e.g., pP, sP) are not currently considered, which may affect accuracy for shallow events (<10 km depth) or teleseismic distances (>2220 km) where reflections contribute significant energy. Future versions may support reflected waves and extended internal ray tracing optionally.\n\n- **4. Testing Status**:\n    Due to limited datasets, **lqtmoment** has been rigorously tested only for `very_local_earthquake` and `local_earthquake` classifications. If you encounter miscalculations in other earthquake categoris,  please report them as issues here: [Report Issues](https://github.com/bgjx/lqt-moment-magnitude/issues), any support will be beneficial for future development.\n\n\n--------------\n### Examples\n\n**lqtmoment** supports two ways to run it: a **Programmatic Approach** for integration with your Python code or data analysis workflows, and a simpler **Command-Line Interface (CLI)** for straightforward usage:\n\n**1. Programmatic Approach**:\n```python\n    from lqtmoment import magnitude_estimator\n    calculated_moment_results, detailed_fitting_results = magnitude_estimator(\n                                                        wave_dir = \"..\\tests\\sample_tests_data\\data\\wave\",\n                                                        cal_dir = \"..\\tests\\sample_tests_data\\data\\calibration\",\n                                                        catalog_file = \"..\\tests\\sample_tests_data\\results\\lqt_catalog\\lqt_catalog.xlsx\",\n                                                        config_file = \"..\\tests\\sample_tests_data\\calculation configuration\\config_test.ini\",\n                                                        fig_dir = \"..\\tests\\sample_tests_data\\figures\",\n                                                        output_dir = \"..\\tests\\sample_tests_data\\results\\calculation\",\n                                                        id_start = 1001,\n                                                        id_end = 1005,\n                                                        lqt_mode = True,\n                                                        generate_figure = True,\n                                                        output_format = 'excel'\n                                                        )\n```\n\nThese programmatic approach will return two pandas `DataFrame` objects:\n    - `calculated_moment_results`: Contains the final moment magnitude results (averaged across all stations) for each successfully calculated earthquake ID.\n    - `detailed_fitting_results`: Provides detailed spectral fitting results per station for each earthquake ID.\n\n**2. CLI Approach**\n```bash\n    $ lqtmoment --wave-dir ..\\tests\\sample_tests_data\\data\\wave --cal-dir ..\\tests\\sample_tests_data\\data\\calibration --catalog-file ..\\tests\\sample_tests_data\\results\\lqt_catalog\\lqt_catalog.xlsx --config-file ..\\tests\\sample_tests_data\\calculation configuration\\config_test.ini --fig-dir ..\\tests\\sample_tests_data\\figures --output-dir ..\\tests\\sample_tests_data\\results\\calculation --id-start 1001 --id-end 1005 --create-figure --output-format excel\n```\n\nFor more details please check out the full tutorials, which include `Tips` , `Notes` and `Cautions` for running the program effectively.\n\n--------------\n### References\nThis program relies on a robust scientific foundation, that refer to following resources:\n\n* Abercrombie, R.E. (1995), \"Earthquake Source Scaling Relationships from −1 to 5  M L  Using Seismograms Recorded at 2.5‐km Depth\", Journal of Geophysical Research: Solid Earth, Vol.100, No.B12, hal. 24015–24036. http://doi.org/10.1029/95JB02397.\n* Aki, K., & Richards, P. G. (2002). Quantitative Seismology (2nd ed.). University Science Books.\n* Boore, D.M. dan Boatwright, J. (1984), \"Average Body-Wave Radiation Coefficients\", Bulletin of the Seismological Society of America, Vol.74, No.5, hal. 1615–1621. http://doi.org/10.1785/BSSA0740051615.\n* Brune, J. N. (1970). Tectonic stress and the spectra of seismic shear waves from earthquakes. Journal of Geophysical Research, 75(26), 4997–5009. https://doi.org/10.1029/JB075i026p04997\n* Červený, V., 2005. Seismic ray theory, First paperback version. ed. Cambridge University Press, Cambridge New York Melbourne Madrid Cape Town Singapore Sa︠o Paulo.\n* Hanks, T.C. dan Kanamori, H. (1979), \"A Moment Magnitude Scale\", Journal of Geophysical Research: Solid Earth, Vol.84, No.B5, hal. 2348–2350. http://doi.org/10.1029/JB084iB05p02348.\n* Havskov, J. dan Alguacil, G. (2016), Instrumentation in Earthquake Seismology, Springer International Publishing, Cham. http://doi.org/10.1007/978-3-319-21314-9.\n* Havskov, J. dan Ottemoller, L. (2010), Routine Data Processing in Earthquake Seismology, Springer Netherlands, Dordrecht. http://doi.org/10.1007/978-90-481-8697-6.\n* Maxwell, S. (2014), Microseismic Imaging of Hydraulic Fracturing: Improved Engineering of Unconventional Shale Reservoirs, Society of Exploration Geophysicists, US.\n\n--------------\n### Contributing\nWe kindly welcome contributions to **lqtmoment!** To get started, follow these steps:\n\n- **Fork this Repository**: Clone the project to your GitHub account by forking it here https://github.com/bgjx/lqt-moment-magnitude.git\n- **Clone Locally**: Download your forked repository to your machine using:\n    ```bash\n        git clone https://github.com/your-username/lqt-moment-magnitude.git \n    ```\n- **Create a Branch**: Work on your changes in a new branch:\n    ```bash\n        git checkout -b your-feature-branch\n    ```\n- **Make Changes**: Implement your improvements or fixes, ensuring they align with the project's goals, or even new revolutionized ideas (you can contact the developer if you will by mail to edelo.arham@gmail.com)\n\n- **Test Your Code**: You can extend the existed tests or add new ones if needed.\n- **Commit & Push**: Save your work and push it to your fork:\n    ```bash\n        git add .\n        git commit -m \"Describe your changes here\"\n        git push origin your-feature-branch \n    ```\n- **Submit and Pull Request**: Open the pull request (PR) on the original repository, detailing your changes and their purpose.\n\n**Who can contributes ?**\n* **Geophysicist/Seismologist**: To validate and improve moment magnitude calculation method for better accuracy.\n* **Python Developer**: To optimize code or expand features.\n* **Data Scientist**: To refine the ray tracing and spectral fitting algorithm to handle larger datasets effectively.\n* **ML/AI Engineer**: To revolutionized the method to Artificial Intelligence approach.\n* **Open-Source Enthusiast**: Anyone passionate about python programming and scientific research.\n\n--------------\n### Report Bugs\nIf you encounter any issues while running **lqtmoment**, please report them here: [Report Bugs](https://github.com/bgjx/lqt-moment-magnitude/issues). To help maintainer address the problem efficiently, include the following details:\n\n- Your operating system and Python Version(e.g., Windows11 Python 3.11)\n- A detailed description of the bug, including steps to reproduce it and any error messages.\n\nYour contributions is crucial for improving the tool!\n\n--------------\n### Support \n\nIf you are willing to support this project, you also can donate using the following cryptocurrency addresses:\n\n- **Bitcoin (BTC)**: `bc1q57uun0dxct4lzk96p3htgcxumm5s97rk2xjdt2`\n    <div align='left'>\n        <img src=\"docs/wallet_address/btc_qr_address.png\" alt=\"Bitcoin QR Code\" width=\"150\">\n    </div>\n\n- **Ethereum (ETH)**: `0x341f9913d0A998bEFbd127823457977d70C0B201`\n    <div align='left'>\n        <img src=\"docs/wallet_address/eth_qr_address.png\" alt=\"Ethereum QR Code\" width=\"150\">\n    </div>\n\n- For fiat donations, please use [GitHub Sponsors](https://github.com/sponsors/bgjx).\n",
        "createdAt": "2025-03-18T23:29:48.000Z",
        "updatedAt": "2025-07-22T07:04:26.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/bgjx/lqt-moment-magnitude/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ashimrijal/Advanced-Computational-Seismology-2016",
        "url": "https://github.com/ashimrijal/Advanced-Computational-Seismology-2016",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# Advanced-Computational-Seismology-2016\n",
        "createdAt": "2017-08-13T13:16:56.000Z",
        "updatedAt": "2017-08-13T13:20:49.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ashimrijal/Advanced-Computational-Seismology-2016/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "serdarkuyuk/python_seismology",
        "url": "https://github.com/serdarkuyuk/python_seismology",
        "description": null,
        "stars": 1,
        "forks": 0,
        "readme": "",
        "createdAt": "2018-08-21T04:57:49.000Z",
        "updatedAt": "2019-12-26T04:27:30.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "jacksonfellows/seismoslide",
        "url": "https://github.com/jacksonfellows/seismoslide",
        "description": "My research on detecting surface events w/ seismology.",
        "stars": 0,
        "forks": 0,
        "readme": "# Seismoslide #\nThis repository contains my research with Prof. Grace Barcheck on the use of machine learning to discriminate between the seismic signals of earthquakes and surface events.\n\n## Conda stuff ##\nUpdate environment after changing definition:\n```\nconda env update -f environment.yml --prune\n```\n",
        "createdAt": "2023-09-03T17:54:40.000Z",
        "updatedAt": "2024-09-21T14:42:41.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/jacksonfellows/seismoslide/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "keurfonluu/bruces",
        "url": "https://github.com/keurfonluu/bruces",
        "description": "A bunch of really useful codes for earthquake stuff",
        "stars": 10,
        "forks": 1,
        "readme": "bruces\n======\n\n|License| |Stars| |Pyversions| |Version| |Downloads| |Code style: black| |Codacy Badge| |Codecov| |Build| |Docs| |DOI|\n\nInspired by `bruges <https://github.com/agilescientific/bruges>`__, **bruces** aims to be a collection of lightweight codes/tools for seismology with an emphasis on computational efficiency.\n\nInstallation\n------------\n\nThe recommended way to install **bruces** and all its dependencies is through the Python Package Index:\n\n.. code:: bash\n\n   pip install bruces[full] --user\n\nOtherwise, clone and extract the package, then run from the package location:\n\n.. code:: bash\n\n   pip install .[full] --user\n\nTo test the integrity of the installed package, check out this repository and run:\n\n.. code:: bash\n\n   pytest\n\nDocumentation\n-------------\n\nRefer to the online `documentation <https://keurfonluu.github.io/bruces/>`__ for detailed description of the API and examples.\n\nAlternatively, the documentation can be built using `Sphinx <https://www.sphinx-doc.org/en/master/>`__:\n\n.. code:: bash\n\n   pip install -r doc/requirements.txt\n   sphinx-build -b html doc/source doc/build\n\nExample\n-------\n\nThe following code snippet will decluster a catalog downloaded with `pycsep <https://github.com/SCECcode/pycsep>`__ using the nearest-neighbor method:\n\n.. code-block:: python\n\n   from datetime import datetime\n\n   import csep\n   import matplotlib.pyplot as plt\n\n   import bruces\n\n   # Download catalog using pycsep\n   catalog = csep.query_comcat(\n      start_time=datetime(2008, 1, 1),\n      end_time=datetime(2018, 1, 1),\n      min_magnitude=3.0,\n      min_latitude=35.0,\n      max_latitude=37.0,\n      min_longitude=-99.5,\n      max_longitude=-96.0,\n   )\n\n   # Decluster pycsep catalog\n   cat = bruces.from_csep(catalog)\n   eta_0 = cat.fit_cutoff_threshold()\n   catd = cat.decluster(method=\"thinning\", eta_0=eta_0)\n\n   # Display declustering result\n   fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n   cat.plot_time_space_distances(eta_0=eta_0, eta_0_diag=eta_0, ax=ax[0])\n   catd.plot_time_space_distances(eta_0=eta_0, eta_0_diag=eta_0, ax=ax[1])\n\n.. figure:: https://raw.githubusercontent.com/keurfonluu/bruces/4272457d2421697833514c5c08ad6b2ccf105748/.github/sample.svg\n   :alt: sample\n   :width: 100%\n   :align: center\n\nContributing\n------------\n\nPlease refer to the `Contributing\nGuidelines <https://github.com/keurfonluu/bruces/blob/master/CONTRIBUTING.rst>`__ to see how you can help. This project is released with a `Code of Conduct <https://github.com/keurfonluu/bruces/blob/master/CODE_OF_CONDUCT.rst>`__ which you agree to abide by when contributing.\n\nNotice\n------\n\nbruces Copyright (c) 2022, The Regents of the University of California, through Lawrence Berkeley National Laboratory (subject to receipt of any required approvals from the U.S. Dept. of Energy). All rights reserved.\nIf you have questions about your rights to use or distribute this software, please contact Berkeley Lab's Intellectual Property Office at `IPO@lbl.gov <mailto:IPO@lbl.gov>`__.\n\nThis Software was developed under funding from the U.S. Department of Energy and the U.S. Government consequently retains certain rights. As such, the U.S. Government has been granted for itself and others acting on its behalf a paid-up, nonexclusive, irrevocable, worldwide license in the Software to reproduce, distribute copies to the public, prepare derivative works, and perform publicly and display publicly, and to permit others to do so.\n\n.. |License| image:: https://img.shields.io/badge/license-BSD--3--Clause-green\n   :target: https://github.com/keurfonluu/bruces/blob/master/LICENSE\n\n.. |Stars| image:: https://img.shields.io/github/stars/keurfonluu/bruces?logo=github\n   :target: https://github.com/keurfonluu/bruces\n\n.. |Pyversions| image:: https://img.shields.io/pypi/pyversions/bruces.svg?style=flat\n   :target: https://pypi.org/pypi/bruces/\n\n.. |Version| image:: https://img.shields.io/pypi/v/bruces.svg?style=flat\n   :target: https://pypi.org/project/bruces\n\n.. |Downloads| image:: https://pepy.tech/badge/bruces\n   :target: https://pepy.tech/project/bruces\n\n.. |Code style: black| image:: https://img.shields.io/badge/code%20style-black-000000.svg?style=flat\n   :target: https://github.com/psf/black\n\n.. |Codacy Badge| image:: https://img.shields.io/codacy/grade/27f1025983384885a3ed0f1089d3775e.svg?style=flat\n   :target: https://www.codacy.com/gh/keurfonluu/bruces/dashboard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=keurfonluu/bruces&amp;utm_campaign=Badge_Grade\n\n.. |Codecov| image:: https://img.shields.io/codecov/c/github/keurfonluu/bruces.svg?style=flat\n   :target: https://codecov.io/gh/keurfonluu/bruces\n\n.. |DOI| image:: https://zenodo.org/badge/DOI/10.5281/zenodo.6422572.svg?style=flat\n   :target: https://doi.org/10.5281/zenodo.6422572\n\n.. |Build| image:: https://img.shields.io/github/workflow/status/keurfonluu/bruces/Python%20package\n   :target: https://github.com/keurfonluu/bruces\n\n.. |Docs| image:: https://img.shields.io/github/workflow/status/keurfonluu/bruces/Build%20documentation?label=docs\n   :target: https://keurfonluu.github.io/bruces/",
        "createdAt": "2020-08-22T03:31:50.000Z",
        "updatedAt": "2025-11-13T02:42:15.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.6422572",
            "openAlex": "10.5281/zenodo.6422572",
            "openCitations": "10.5281/zenodo.6422572",
            "dataCite": "10.5281/zenodo.6422572",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/keurfonluu/bruces/master/README.rst",
        "mainPaper": {
            "doi": "10.5281/zenodo.6422572",
            "title": "bruces: A bunch of really useful codes for earthquake stuff",
            "journal": "Zenodo",
            "dateReleased": "2022-12-14T00:00:00.000Z",
            "abstract": "",
            "citationsArray": []
        },
        "repoDoi": "10.5281/zenodo.6422572",
        "publications": [
            {
                "doi": "10.5281/zenodo.6422572",
                "name": "bruces: A bunch of really useful codes for earthquake stuff",
                "source": "",
                "authorNames": [],
                "abstract": "",
                "publicationDate": "2022-12-05T12:16:18.750Z"
            }
        ]
    },
    {
        "source": "GitHub",
        "name": "avillasenorh/CPSdocs",
        "url": "https://github.com/avillasenorh/CPSdocs",
        "description": "Documentation for Computer Programs in Seismology",
        "stars": 0,
        "forks": 0,
        "readme": "# CPSdocs\nDocumentation for Computer Programs in Seismology\n",
        "createdAt": "2024-11-13T10:25:32.000Z",
        "updatedAt": "2024-11-13T10:58:08.000Z",
        "language": null,
        "homepage": "https://avillasenorh.github.io/CPSdocs/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/avillasenorh/CPSdocs/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "MMesch/vtk_obspy",
        "url": "https://github.com/MMesch/vtk_obspy",
        "description": "python routines for mayavi/vtk visualizations of seismology related things",
        "stars": 8,
        "forks": 0,
        "readme": "VTK OBSPY\n=========\n\n\nPython toolbox for VTK visualizations of seismology related themes. This\npackage is based on obspy.\n\n\nFeatures\n--------\n\n* 3D ray path visualizations\n* 3D moment tensor radiation pattern visualization\n* vtk file export of ray paths and radiation patterns\n\n\nRequirements\n------------\nYou need to have obspy (`>1.1.0`) with geographiclib installed. This can be\ndone with:\n\n```\nconda install -c conda_forge obspy\npip install geographiclib\n```\n\nInstallation\n------------\nThe module just needs to be downloaded, for example with:\n```\ngit clone https://github.com/MMesch/vtkobs.git\n```\nThe plot scripts can be executed directly from the installation directory.\nAnother option is to install the python routines to the system with\n```\npip install .\n```\nbefore using the plot scripts type `./plot_rays.py --help` to get some\nusage information.\n\n### interactive plotting\nFor interactive plotting you need to install `mayavi 4.5.0` and make sure that\nit works. In anaconda this can be done with:\n```\nconda install -c menpo mayavi\n```\nBe careful, you might need to downgrade pyqt to version 4 instead of 5. This\ncan be done with:\n```\nconda install pyqt=4\n```\n\n\n## 3D Ray Path Plots\n### interactive\nthis command:\n\n```\n./plot_rays.py --inv data/IU_stations.txt --phases Pdiff,PKIKP --evlat 0\n               --evlon 20\n\n```\nproduces this plot:\n\n![image](images/example1.png)\n\n### VTK files\nplotting with the ``--vtkfiles`` option produces vtk files that can be\nvisualized with paraview.\n\n```\n./plot_rays.py --inv data/IU_stations.txt --phases Pdiff,PKIKP --evlat 0\n               --evlon 20 --vtkfiles\n```\n\n### plot_rays.py command line options\n```\n$ ./plot_rays.py --help\n```\n\n```\nusage: ./plot_rays.py --cat catalog.xml --inv inventory.xml\n./plot_rays.py --cat catalog.xml --stlat 20 --stlon 30\n\n3D visualization of ray paths. The station and event coordinates have\nto be given by the appropriate arguments.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --cat CAT             path to catalog file\n  --inv INV             path to inventory file\n  --evlat EVLAT         event latitude\n  --evlon EVLON         event longitude\n  --evdep EVDEP         event depth\n  --stlat STLAT         station latitude\n  --stlon STLON         station longitude\n  --phases PHASES       e.g. P,PP\n  --noevlabels          switch off event labels\n  --nostalabels         switch off station labels\n  --vtkfiles            vtk files instead of mayavi\n  --colorscheme COLORSCHEME\n                        dark or bright\n```\n\n\n## 3D Moment tensor plots\n### VTK files\nplotting with the ``--vtkfiles`` option produces vtk files that can be\nvisualized with paraview.\n```\n./plot_mtensor.py --mt 1,0,0,-1,1,0 --vtkfiles\n```\n\n### interactive\nthis command:\n\n```\n./plot_mtensor.py --mt 1,0,0,-1,1,0\n```\nproduces this plot:\n\n![image](images/example2.png)\n\n\n### plot_mtensor.py command line options\n```\n$ ./plot_mtensor.py --help\n```\n\n```\nusage: ./plot_mtensor.py --mt 1,0,0,1,-1,0\n\n3D visualization of moment tensor radiation patterns.\n\noptional arguments:\n  -h, --help  show this help message and exit\n  --mt MT     M11,M22,M33,M12,M13,M23\n  --vtkfiles  vtk files instead of mayavi\n```\n",
        "createdAt": "2017-10-17T11:06:36.000Z",
        "updatedAt": "2024-07-06T06:08:08.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/MMesch/vtk_obspy/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "akuhara/MC3deconv",
        "url": "https://github.com/akuhara/MC3deconv",
        "description": "Bayeisan inversion to recover Green's functions of receiver-side structures from teleseismic waveforms",
        "stars": 19,
        "forks": 7,
        "readme": "# MC3deconv: Multi-Channel deconvolution by reversible-jump Markov-Chain Monte Carlo\n\nOne of the purposes of seismology is to investigate the Earth's subsurface structure using seismic waveforms. The receiver function method extracts useful signals (i.e., P-to-S converted phases) from the teleseismic waveforms by deconvolving P component records from the corresponding SV (or SH) components. Despite the many successful applications, conventional receiver function methods often fail due to numerical instability of the deconvolution and strong multiples on the P components. \n\nThe technique developed here, MC3deconv, nicely overcomes these issues. The method optimizes the equation of multichannel deconvolution, in which two components of the reciver-sided Green's functions are related directly without deconvolution. To regularize the inverse problem, these Green's functions are expressed in the form of successive pulses. The number of pulses, their timing, and amplitudes are inverted using Bayesian techniques, the reversible-jump Markov-chain Monte Carlo and the Parallel Tempering.\n\n(C) 2018 Takeshi Akuhara\n\nEmail: akuhara @ eri.u-tokyo.ac.jp\n\n## Terms of use\n\n* Please cite [Akuhara et al. (2019)](#Reference) when you publish an article or making presentation using this method.\n* Also, make it clear that where readers or audiences can download this program package: you may put the link to the Github repository (https://github.com/akuhara/MC3deconv).\n* Any bug reports are welcome! Looking forward to hearing your experience. \n\n## Limitations so far\n\n* Although this method is designed to retrieve both radial (R) and vertical (Z) components of Green's functions, our experience suggest that the estimated Z-component is not so reliable as the R-component.\n* This method assumes Gaussian noise without temporal correlation. This simplified treatment often leads to overfitting. \n\n## How to install\n\nUse `make` command in the root directory of this package. \n\n* `mpifort` must be linked to the GNU fortran compiler (i.e., `gfortran`).\n* If one wishes to use the Intel compiler (i.e., `ifort`), some modification is necessary in Makefile. \n* An executable file, `mc3deconv`, is created under the `bin` directory.\n\n## How to run\n\n`mpirun -np [N_proc] (path to the root directory of this package)/bin/mc3deconv`, for example. \n * N_proc: Number of processes for parallel computation (must be >= 2, see the note below). \n * A parameter file named \"params.in\" must exist in the current directory. \n\nThe easiest way to test is:\n\n`cd sample1`\n\n`mpirun -np [N_proc] ../bin/mc3deconv`\n\nIn the `sample1` directory, all necessary data and parameter files are already prepared.\n\n### Note on parallel computation\n\nThis program requires parallel computation. One of the processes is used to control the other processes, not performing MCMC sampling at all. Therefore, it is mandatory to use more than two processes.  \n\n## Input files\n\n### Parameter file (params.in)\n\nA parameter file, which sets tuning parameters and input data, etc., must exist in the working directory from which `mc3deconv` is called, with the name \"params.in\". The format of the parameter file is as below, but you can put comment lines that start with \"#\" if necessary.\n\n#### Format\n\n|Line #|1st column|2nd column|\n|:--:|:--:|:--:|\n|1| Number of iterations in burn-in period|-|\n|2| Number of iterations in sampling period|-|\n|3| Number of iterations per generating one sample|-|\n|4| Random number seed|-|\n|5| Number of McMC chains per process (for parallel tempering)|-|\n|6| Number of non-tempered chains|-|\n|7| Maximum temperature|-|\n|8| Input Z component file (in SAC format) | -|\n|9| Input R component file (in SAC format)|-|\n|10| Sampling interval of input data (sec)|-|\n|11| Start time of the analysis window relative to file beginning (s)| End time of the analysis window relative to file beginning (s) |\n|12| Lower prior limit for # of pulses | Upper limit for # of pulses|\n|13| Lower prior limit for Z amplitude | Upper limit for Z amplitude|\n|14| Lower prior limit for R amplitude | Upper limit for R amplitude|\n|15| Lower prior limit for pulse timing relative to direct P arrival (s) | Upper limit for pulse timing relative to direct P arrival (s)|\n|16| Probability of birth proposal (adding a pulse) |\n|17| Probability of death proposal (removing a pulse)|\n|18| Probability of time-shit proposal |\n|19| Probability of amplitude-perturb proposal |\n|20| Standard deviation to perturb Z amplitude | Standard deviation to perturb R amplitude|\n|21| Standard deviation to newly generate Z amplitude | Standard deviation to newly generate R amplitude |\n|22| Standard deviation to shift timing (s) ||\n|23| Total time length of output (s) | -|\n|24| Acausal time length preceding direct P arrival for output (s) | - |\n|25| Factor of Gaussian low-pass filter | - |\n|26| Minimum amplitudes for output| Maximum amplitudes for output|\n|27| Amplitude bin width for output| - |\n\n#### Example \n\nYou can find an example of `params.in` in the `sample1` directory.\n\n### Data file (user's given name)\n\n#### Format\n\n* Input waveform data should be SAC format.\n\n#### Example\n\nExample data files, `true/syn_obs.r` and `true/syn_obs.z`, are installed in the `sample1` directory. \n\n## Output files\n\nFive output files are created after running the program.\n\n### dim.ppd \n\nThe posterior probability distribution of the number of pulses. \n\n#### Format\n|1st column|2nd column|\n|:--:|:--:|\n|# of pulses|probability|\n\n\n### Gr.ppd / Gz.ppd \nThe posterior probability distribution of R and Z component Green's functions.\n\n#### Format\n|1st column|2nd column|3rd colmun|\n|:--:|:--:|:--:|\n|time after P (s)|amplitude|probability|`\n\n### Gr.mean / Gz.mean\n\nThe mean models of R and Z component Green's functions.\n\n#### Format\n\n`Gr.mean` and `Gz.mean` are written in SAC format.\n\n## Sample datasets\n\nThere are two sample datasets, which may be useful for testing the program. These are the same data as used in [Akuhara et al. (2019)](#Reference). \n\n* Sample1: Synthetic data in `sample1`\n* Sample2: Real OBS data in `sample2`\n\n## Acknowledgments\n\nDeveloping this package is supported by JSPS KAKENHI Grant Number JP17H06604. OBS data in the sample2 directory are collected by K. Nakahigashi and T. Yamada, under the program \"Integrated Research Project on Seismic and Tsunami Hazards Around the Sea of Japan\" of the Mistry of Education, Culture, Sports, Science and Technology (MEXT), Japan. This package uses a fortran program, [mt19937.f90](https://gist.github.com/ykonishi/5569005).\n\n## Reference\n\n* T. Akuhara, M. Bostock, A. Plourde, M. Shinohara (2019) Beyond Receiver Functions: Green's Function Estimation by Trans-Dimensional Inversion and Its Application to OBS Data, _Journal of Geophysical Research: Solid Earth_, https://doi.org/10.1029/2018JB016499  \n\n",
        "createdAt": "2018-10-29T03:24:59.000Z",
        "updatedAt": "2024-10-22T07:08:20.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/akuhara/MC3deconv/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "swiss-seismological-service/sed-SeisComP-contributions",
        "url": "https://github.com/swiss-seismological-service/sed-SeisComP-contributions",
        "description": "SED libraries and modules contributed by the Swiss Seismological Service at ETH Zurich",
        "stars": 6,
        "forks": 16,
        "readme": "",
        "createdAt": "2020-03-04T11:21:39.000Z",
        "updatedAt": "2025-12-03T07:24:17.000Z",
        "language": "C++",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "RiandryDevelop/SismoAPI",
        "url": "https://github.com/RiandryDevelop/SismoAPI",
        "description": "This is the official backend of the SismoAPP, An application that collects and delivers seismological data around the world.",
        "stars": 0,
        "forks": 0,
        "readme": "# Sismo App README\n\nWelcome to Sismo App, an application that retrieves and delivers reports on seismological events around the world.\n\nThis README document covers the necessary steps to get the application up and running.\n\n## Ruby version\n\nThis project is developed using Ruby version 3.2.3.\n\n## System dependencies\n\nMake sure you have the following dependencies installed on your system:\n\n- Ruby 3.2.3\n- Rails 7.1.3.2\n- SQLite3\n- Puma web server\n\nAdditionally, ensure you have `rest-client`, `ffi`, and `will_paginate` gems installed.\n\n## Configuration\n\nClone the project repository to your local machine:\n\n```bash\ngit clone <repository_url>\n```\n\nNavigate to the project directory:\n\n```bash\ncd <project_directory>\n```\n\nInstall gem dependencies:\n\n```bash\nbundle install\n```\n\n## Database creation\n\nThe project uses SQLite3 as the database for Active Record. To create the database, run the following command:\n\n```bash\nrails db:create\n```\n\n## Database initialization\n\nOnce the database is created, you can initialize it by running migrations:\n\n```bash\nrails db:migrate\n```\n\n## How to run the test suite\n\nTo run the test suite, execute the following command:\n\n```bash\nrails test\n```\n\n## Services\n\nNo additional services are required to run this application.\n\n## Deployment instructions\n\nTo deploy this application, you can follow standard Rails deployment procedures. Ensure to set up your production environment configuration appropriately.\n\nThat's it! Your project should now be set up and ready to run.\n\n\n## How to use the api service\n\n## Endpoints\n\n- **Get all features**\n  - Endpoint: `GET /api/features`\n  - Description: Retrieves a list of all features available in the system.\n\n- **Get all comments of a particular feature**\n  - Endpoint: `GET /api/comments/feature/:feature_id`\n  - Description: Retrieves all comments associated with a specific feature identified by `feature_id`.\n\n- **Create a comment**\n  - Endpoint: `POST /api/comments`\n  - Description: Creates a new comment for a feature.\n  - Request Body: JSON object with the following parameters:\n    - `feature_id`: ID of the feature the comment belongs to.\n    - `body`: Text content of the comment.\n\n- **Update a comment**\n  - Endpoint: `PUT /api/comments/:comment_id`\n  - Description: Updates an existing comment identified by `comment_id`.\n  - Request Body: JSON object with the following parameter:\n    - `body`: Updated text content of the comment.\n\n- **Delete a comment**\n  - Endpoint: `DELETE /api/comments/:comment_id`\n  - Description: Deletes a comment identified by `comment_id`.\n\n\n## Response Format\n\n- Success Response: HTTP status code 200 OK for successful requests.\n- Error Response: HTTP status codes indicating the type of error occurred, along with a relevant error message in the response body.\n\n## Authentication\n\nThis API does not require authentication for accessing the endpoints. However, ensure proper authorization mechanisms are implemented in your application if needed.\n\n## Rate Limiting\n\nThere are currently no rate limits enforced on this API. However, consider implementing rate limiting on your client-side to prevent abuse.\n\n## Versioning\n\nThis documentation corresponds to version 1 of the API. Any future updates or changes will be documented accordingly.\n\n",
        "createdAt": "2024-04-07T14:34:23.000Z",
        "updatedAt": "2024-04-07T15:32:14.000Z",
        "language": "Ruby",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/RiandryDevelop/SismoAPI/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "northstar-code/SeismicAnalysis",
        "url": "https://github.com/northstar-code/SeismicAnalysis",
        "description": "I have no idea how seismology works, dont use this for anything important",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2023-05-19T23:57:27.000Z",
        "updatedAt": "2023-05-19T23:57:50.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "fabian-kutschera/Computational_Seismology",
        "url": "https://github.com/fabian-kutschera/Computational_Seismology",
        "description": "Computers, Waves, Simulations: A Practical Introduction to Numerical Methods using Python",
        "stars": 1,
        "forks": 0,
        "readme": "# Computational/ Modern Seismology WP2 of the Geophysics Master (2019) at LMU and TUM \n\n(Adjusted) Jupyter notebooks for the COURSERA video course \"Computers, Waves, Simulations\" currently active and accessible here: https://www.coursera.org/learn/computers-waves-simulations\n",
        "createdAt": "2021-04-16T13:57:52.000Z",
        "updatedAt": "2022-01-05T16:02:22.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/fabian-kutschera/Computational_Seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "core-man/blog",
        "url": "https://github.com/core-man/blog",
        "description": "core-man 博客",
        "stars": 0,
        "forks": 1,
        "readme": "\ncore-man 的博客\n\n## 地震学基础\n\n### 地震学入门\n\n- [地震学初步入门材料清单](content/post/2020-03-16-introductory-seismology/index.md)\n\n\n\n## 计算机基础\n\n### 计算机入门\n\n- [文件备份](content/post/2020-03-15-backup/index.md)\n\n\n\n## 学术相关\n\n- [学术阅读和写作的经验](content/post/2020-05-01-reading-writing/index.md)\n- [地学书籍](content/post//2020-05-18-geoscience-books/index.md)\n- [参加国际会议 (以AGU为例)](content/post/2020-05-10-internaltionl-conference/index.md)\n- [AGU 临行清单](content/post/2020-05-15-AGU-checklist/index.md)\n\n\n\n## 新加坡生活和工作\n\n- [留学新加坡](content/post/2020-05-23-oversea-in-singapore/index.md)\n    - [留新准备](content/post/2020-05-21-singapore-preparation/index.md)\n    - [留新校园生活 (NTU)](content/post/2020-05-22-NTU-campus/index.md)\n    - [留新个人经济管理](content/post/2020-05-22-singapore-economy/index.md)\n    - [留新个人生活信息汇总](content/post/2020-05-22-singapore-info-collection/index.md)\n    - [留新个人日常生活](content/post/2020-05-22-singapore-life/index.md)\n    - [留新娱乐活动](content/post/2020-05-22-singapore-recreation/index.md)\n\n\n\n## 其他\n\n- [Hello World](content/post/2020-03-15-hello-world/index.md)\n\n\n",
        "createdAt": "2020-03-15T10:00:05.000Z",
        "updatedAt": "2023-01-28T08:10:33.000Z",
        "language": "Perl",
        "homepage": "https://core-man.github.io/blog",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/core-man/blog/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ofersp/binseis",
        "url": "https://github.com/ofersp/binseis",
        "description": "Binary Stellar Structure and Seismology Toolbox",
        "stars": 0,
        "forks": 0,
        "readme": "Binary Seismology MATLAB Toolbox\n================================\n\nThis MATLAB toolbox accompanies the paper:\n\nOfer M. Springer, Nir J. Shaviv. \"Asteroseismic effects in close binary stars.\"  \nMonthly Notices of the Royal Astronomical Society (2013).\n\nPreprint available here:  \nhttps://arxiv.org/abs/1307.3709\n\nGeneral overview\n----------------\n\nSystem requirements\n-------------------\n\nInstallation\n------------\n\nDirectory structure\n-------------------\n\nExample usage\n-------------\n\n",
        "createdAt": "2012-11-01T08:25:54.000Z",
        "updatedAt": "2017-02-23T13:04:20.000Z",
        "language": "C++",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ofersp/binseis/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "yawarinti/Seismology",
        "url": "https://github.com/yawarinti/Seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "Este es un proyecto de programación de sismologia.\n",
        "createdAt": "2015-10-21T20:29:58.000Z",
        "updatedAt": "2024-05-16T16:45:43.000Z",
        "language": "HTML",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/yawarinti/Seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "tso1257771/ARRU_seismic_backprojection",
        "url": "https://github.com/tso1257771/ARRU_seismic_backprojection",
        "description": "Implementation of `Towards fully autonomous seismic networks: backprojecting deep-learning-based phase time functions for earthquake monitoring on continuous recordings`",
        "stars": 11,
        "forks": 0,
        "readme": "# ARRU_seismic_backprojection\n\nThis repo is the official implementation of \"Towards fully autonomous seismic networks: backprojecting deep-learning-based phase time functions for earthquake monitoring on continuous recordings\".<br/>\n\nIn this repo we provide template codes that backprojects seismic phase-time functions with pre-calculated [travel-time tables](https://drive.google.com/file/d/1OADPD0nwAeX5W843Wt9E6I5K8MiYS7nM/view?usp=sharing). The outputs of following scripts could be retreived [here](https://drive.google.com/file/d/101h8nZopPDV86DnYMxZEwJ7nj293Q1Z-/view?usp=sharing). Download and uncompressed them. <br/>\n```$tar -zvxf out_data.tar.gz``` <br/>\n```$tar -zvxf metadata.tar.gz``` <br/>\n\n**Step 1. Do seismic phase picking on 1-hour-long seismograms using [ARRU phase picker](https://github.com/tso1257771/Attention-Recurrent-Residual-U-Net-for-earthquake-detection)**<br/>\nThis script generates phase-time functions of raw seismograms in SAC format.<br/>\n```$ python P01_continuous_pred.py```<br/>\n\n**Step 2. Convert phase-time functions into binary**<br/>\n```$ python P02_ARRU_sac2bin.py```<br/>\n\n**Step 3. Do seismic backprojection using prepared travel-time tables and phase-time functions**<br/>\n```$ python P03_ARRU_BP_PSwin_MPI.py```<br/>\n\n**Step 4. Find potential earthquake events**<br/>\n```$ python P04_find_potential_events.py```\n\nWhile the postprocess of backprojection results are tedious. in this repo we only provide main scripts of seismic phase picking and backprojection. \nThe full catalog of our work for July, 2019 Ridgecrest earthquake sequence is available at: ```./ARRU_BP_201907_catalog_final.txt```<br/>\n\n# Reference\nWu‐Yu Liao, En‐Jui Lee, Dawei Mu, Po Chen, Ruey‐Juin Rau; ARRU Phase Picker: Attention Recurrent‐Residual U‐Net for Picking Seismic P‐ and S‐Phase Arrivals. Seismological Research Letters 2021; doi: https://doi.org/10.1785/0220200382\n\nWu‐Yu Liao, En‐Jui Lee, Dawei Mu, Po Chen; Toward Fully Autonomous Seismic Networks: Backprojecting Deep Learning‐Based Phase Time Functions for Earthquake Monitoring on Continuous Recordings. Seismological Research Letters 2022; doi: https://doi.org/10.1785/0220210274\n",
        "createdAt": "2022-01-05T02:31:12.000Z",
        "updatedAt": "2024-11-29T02:03:43.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/tso1257771/ARRU_seismic_backprojection/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "HaoGuo-USTC/hguo_seismology.github.io",
        "url": "https://github.com/HaoGuo-USTC/hguo_seismology.github.io",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2025-02-03T03:01:18.000Z",
        "updatedAt": "2025-02-09T05:49:42.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "akuhara/RF_INV",
        "url": "https://github.com/akuhara/RF_INV",
        "description": "Receiver function inversion by reversible-jump Markov-chain Monte Carlo",
        "stars": 30,
        "forks": 12,
        "readme": "# Note \n\n__This software is no longer being updated. \nPlease consider adopting our new software, SEIS_FILO (https://github.com/akuhara/SEIS_FILO), which offers enhanced features like joint inversion with dispersion curves.__\n\n# RF_INV\n\nTransdimensional inversion of receiver function waveforms by reversible-jump Markov-chain Monte Carlo\n\n(c) 2018-2019 Takeshi Akuhara (Email: akuhara @ eri.u-tokyo.ac.jp)\n\nAny bug report and suggestions are welcome!\n<!--\n# IMPORTANT NOTE\nThe code assumes a certain way of normaliztion for input receiver function data, which may be different than ususal.\n-->\n# Features\n \n* Applicable to OBS & borehole station\n    * Model can include the sea water on its top. Also the station may be buried.  \n* Multiple input traces\n    * Can asign different ray parameter and Gaussian-filter for each trace.\n* Can use S receiver functions\n    * Joint inversion of P and S receiver functions is also possible.\n* Parallel tempering\n    * More efficient than conventional MCMC.\n* Invert for velocity perturbation\n    * Non-uniqueness of inversion can be mitigated by constraint from a reference velocity model.\n    \nSee [Wiki](https://github.com/akuhara/RF_INV/wiki) for more details.\n\n# Terms of Use\n* Please clarify the URL of the GitHub repository (https://github.com/akuhara/RF_INV) and developer's name (Takeshi Akuhara) when you make any presentation or publish articles using this program.\n* This program is licensed under the GNU General Public License v3.0.\n\n# Requirements\n* [FFTW library](http://fftw.org/)\n* [LAPACK library](http://www.netlib.org/lapack/)\n* [Open MPI](https://www.open-mpi.org/)\n\n---\n\n# Manual \n\nA manual is available [here](https://github.com/akuhara/RF_INV/wiki).\n",
        "createdAt": "2018-12-17T06:16:58.000Z",
        "updatedAt": "2025-08-14T00:24:45.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/akuhara/RF_INV/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "harrymd/PyLayeredModel",
        "url": "https://github.com/harrymd/PyLayeredModel",
        "description": "Python wrappers for the CPS and Rftn libraries for layered models in seismology",
        "stars": 32,
        "forks": 7,
        "readme": "# PyLayeredModel\n## What it is\n**PyLayeredModel** is a collection of Python wrappers for a small subset of the seismological codes found in the Fortran packages Computer Programs in Seismology ([**CPS**](http://www.eas.slu.edu/eqc/eqccps.html); [Herrmann 2013](https://doi.org/10.1785/0220110096)) and [**Rftn**](http://eqseis.geosc.psu.edu/cammon/HTML/RftnDocs/rftn01.html). This subset of programs performs calculations relating to transversely-isotropic Earth models (consisting of stacks of internally homogeneous layers). The wrappers may allow you to work more quickly with these Fortran-based codes, which are usually manipulated using bash shell scripts. The compiled Fortran routines are called via the Python core module `subprocess`.\n\n## What's included\nCurrently, wrappers are written to calculate the following properties using **CPS**:\n\n* For Rayleigh and Love waves:\n  * Phase and group speed dispersion, using `sdisp96`;\n  * Eigenfunctions and sensitivity kernels, using `sregn96` and `slegn96`;\n  * Sensitivity kernels using `srfker96`.\n* Receiver functions, using `trftn96` ;\n\nAdditionally, one wrapper is provided for **Rftn**, which calculates:\n\n* The Earth-response function, using `respknt`. \n\n## Software requirements\nFor Earth-response functions, you need to compile `respknt` from **Rftn**. For all of the other programs, you need to compile **CPS**. Follow the compilation instructions given in the links above.\n\nTo run this Python code, you need to have [**Python3**](https://www.python.org/) installed with the [**NumPy**](https://numpy.org/) library. To work with Earth-response functions and receiver functions, you will also need the [**ObsPy**](https://github.com/obspy/obspy/wiki) library, which we use to handle the [**SAC**](http://ds.iris.edu/files/sac-manual/) files. For plotting, you will need [**Matplotlib**](https://matplotlib.org/). We recommend using [**Anaconda**](https://www.anaconda.com/) to assemble these Python libraries.\n\n\n## Getting started\n\nCompile Fortran programs and Python modules you need, based on the section above. Clone this project from GitHub, navigate to the base directory, and try\n\n`python3 examples.py 1`\n\nwhere `1` means the first example. You should get something like the figures shown in the next section. Look at the scripts in `examples.py` to see how to build your own models.\n\n## Examples\n\n### 1. Dispersion\n\n![](example_plots/example_1_fig_1.png)\n![](example_plots/example_1_fig_2.png)\n\n### 2. Eigenfunctions and sensitivity kernels\n\n![](example_plots/example_2_fig_1.png)\n![](example_plots/example_2_fig_2.png)\n\n### 3. More sensitivity kernels\n\n![](example_plots/example_3.png)\n\n### 4. Receiver functions\n\n![](example_plots/example_4.png)\n\n### 5. Earth-response functions\n\n![](example_plots/example_5.png)\n\n## Future work\n\nWe only add something if we need it for our research, and no additions are planned at this time. Please feel free to request features, report bugs, or make contributions.\n\n## Similar packages\n\nOther wrappers for **CPS** in Python:\n\n* [**CPSPy**](https://github.com/NoisyLeon/CPSPy) Dispersion, kernels and possibly more.\n* [**CPyS**](https://github.com/kmch/CPyS) A Python and bash wrapper for inversion of receiver functions and surface waves using **CPS**.\n* [**Geopy**](https://github.com/HouseJaay/Geopy) Wrapper for CPS `sacfmt96`, eigenfunction routines, `srfker96`, as well as some format manipulations, plotting, and a variety of other seismological tasks not related to CPS wrappers.\n* [**pysurf96**](https://github.com/miili/pysurf96) Dispersion calculations, wrapped using `f2py` instead of `subprocess`, so it should be faster than **PyLayeredModel**.\n* [**srfpython**](https://github.com/obsmax/srfpython) 'Compute, display and invert 1-D models'.\n\nA Python library calculating dispersion from scratch, including radial anisotropy:\n\n* [**dispersion**](https://github.com/tbmcoding/dispersion)\n\nA different Fortran library offering similar calculations:\n\n* [**SensKernel**](https://github.com/NoiseCIEI/SensKernel).\n\n## Credit\n\nIf you use any of the **CPS** codes in published research, you must cite [Herrmann (2013)](https://doi.org/10.1785/0220110096). If you use `respknt` from **Rftn**, you must cite [Randall (1994)](https://doi.org/10.1111/j.1365-246X.1994.tb04687.x). If you found these Python wrappers helpful, please mention the **PyLayeredModel** [GitHub page](https://github.com/harrymd/PyLayeredModel) in your acknowledgement section.\n",
        "createdAt": "2020-02-10T21:47:10.000Z",
        "updatedAt": "2025-09-19T05:04:55.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/harrymd/PyLayeredModel/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Weixi-tian/EarthquakeNPP-China-public",
        "url": "https://github.com/Weixi-tian/EarthquakeNPP-China-public",
        "description": "This release supports the reproducibility of the study 'Can Seismological Knowledge Improve Neural Spatiotemporal Point Processes for Earthquake Forecasting?'",
        "stars": 0,
        "forks": 0,
        "readme": "# EarthquakeNPP-China-public\nThis release supports the reproducibility of the study 'Can Seismological Knowledge Improve Neural Spatiotemporal Point Processes for Earthquake Forecasting?'\n\nWe forked the EarthquakeNPP from https://github.com/ss15859/EarthquakeNPP and add a new dataset in southwest China.\n\nEarthquakeNPP is an expanding collection of benchmark datasets designed to facilitate testing of Neural Point Processes (NPPs) on earthquake data. The datasets are accompanied by an implementation of the Epidemic-Type Aftershock Sequence (ETAS) model, currently considered the benchmark forecasting model in the seismology community. Derived from publicly available raw data, these datasets undergo processing and configuration to support forecasting experiments relevant to stakeholders in seismology. The datasets cover various regions of California, representing typical forecasting zones and the data commonly available to forecast issuers. Several datasets include much smaller magnitude earthquakes thanks to modern algorithms for detection and dense seismic networks.\n\n\n## Setup \n\n1. Clone the repository and its submodules:\n  ```bash\n  git clone --recurse-submodules https://github.com/Weixi-tian/EarthquakeNPP-China-public\n  ```\n2. Navigate to the cloned directory:\n  ```bash\n  cd EarthquakeNPP-China-public\n  ```\n3. Create the conda environment:\n  ```bash\n  conda env create -f environment.yml\n  ```\n4. Activate the conda environment:\n  ```bash\n  conda activate earthquakeNPP_CHINA\n  ```\n## Dataset\n\n### [CENC Ms3.0](https://github.com/Weixi-tian/EarthquakeNPP-CHINA/blob/main/Datasets/CENC/data/raw/20240509_china3ms.csv)\n### [CENC ML0.0](https://github.com/Weixi-tian/EarthquakeNPP-China-public/blob/main/Datasets/CENC/data/raw/CSES_0.csv)\nThe raw Catalog with Ms>3.0 and ML>0.0 is provided by China Earthquake Network Center.\n\n\n\n## Experiments\n### [ETAS](https://github.com/Weixi-tian/EarthquakeNPP-China-public/tree/main/Experiment/ETAS)\n\n\nEarthquakeNPP facilitates the benchmarking of NPP models against the ETAS model, a spatio-temporal Hawkes process used for operational earthquake forecasting by government agencies in [California](https://pubs.geoscienceworld.org/ssa/srl/article-abstract/91/3/1567/582898/Operational-Earthquake-Forecasting-during-the-2019?redirectedFrom=fulltext), [New-Zealand](https://db.nzsee.org.nz/2017/O3C.4_Christophersen.pdf), [Italy](https://www.earth-prints.org/bitstream/2122/16524/1/Operational%20Earthquake%20Forecasting%20in%20Italy%20validation%20after%2010%20yr%20of%20operativity.pdf), [Japan](https://www.researchgate.net/publication/328643801_Implementation_of_a_Real-Time_System_for_Automatic_Aftershock_Forecasting_in_Japan) and [Switzerland](https://pubs.geoscienceworld.org/ssa/bssa/article/doi/10.1785/0120240007/644286/suiETAS-Developing-and-Testing-ETAS-Based). It is implemented in the [`etas`](https://github.com/lmizrahi/etas) python package.\n\nFor all experiments run,\n  ```bash\n  cd Experiments/ETAS/\n  python invert_etas.py [dataset]\n  python predict_etas.py [dataset]\n  ```\nWhere `[dataset]` is one of `CENC12|CENC14|CENC16|...|CENC40`. These configures can be found at [CSEP_COLLECTION_REGION](https://github.com/Weixi-tian/EarthquakeNPP-China-public/tree/main/Experiment/ETAS/config/CSEP_COLLECTION_REGION).\n\nThe ouput of ETAS can be found directly at [output_data-CENC](https://github.com/Weixi-tian/EarthquakeNPP-China-public/tree/main/Experiment/ETAS/output_data_CENC)\n\n### [Deep-STPP](https://github.com/Weixi-tian/EarthquakeNPP-China-public/tree/main/Experiment/AutoSTPP)\n\nA NPP that constructs a non parametric space-time intensity function governed by a deep latent process ([Zhou et al., 2022](https://arxiv.org/pdf/2112.06351)).\n\nFor all experiments run,\n  ```bash\n  cd Experiments/AutoSTPP_combined_all_model/\n  make run_stpp_earthquakeNPP config = example \n  ```\nWhere all configure files can be found at [CSEP_COLLECTION_REGION](https://github.com/Weixi-tian/EarthquakeNPP-China-public/tree/main/Experiment/AutoSTPP/configs/CSEP_COLLECTION_REGION).\n\nFor the name of configure, for instance, M25_n_mag_n_bg_temporal(exp)_spatial(gaussian)_len20_CENC_deep_stpp_seed_1553_auxiliary represent using magnitude above M2.5, do not use magnitude information and background information, use exponential temporal kernel and 2Dgaussian kernel, sequence length as 20, seed as 1553, using auxiliary period data for traning, respectively.\n\nDue to GitHub storage limits, DeepSTPP output files are hosted at: /Experiment/AutoSTPP/data/data_from_HPC in our [Zenodo repository](https://zenodo.org/records/16745818)\n\n- **[License:](https://github.com/Rose-STL-Lab/AutoSTPP?tab=MIT-1-ov-file)** The MIT License (MIT), Copyright (c) 2022, Zihao Zhou\n- **Credit:** Zhou, Z., Yang, X., Rossi, R., Zhao, H., & Yu, R. (2022, May). Neural point process for learning spatiotemporal event dynamics. In Learning for Dynamics and Control Conference (pp. 777-789). PMLR.\n\n### [Poisson](https://github.com/Weixi-tian/EarthquakeNPP-China-public/tree/main/Experiment/Poisson)\n\nWe use a homogeneous Poisson baseline. The area of  'China_collection_region' and 'ComCat' can be found at the output of ETAS, for example : [area of China_collection_region](https://github.com/Weixi-tian/EarthquakeNPP-China-public/blob/main/Experiment/ETAS/output_data_CENC/CSEP_COLLECTION_REGION/14/parameters_0.json).\n\n## Plotting\n\nAll Figures in the paper can be reproduced via [Plot.ipynb](https://github.com/Weixi-tian/EarthquakeNPP-China-public/blob/main/Plot/Plot.ipynb)\n\n\n",
        "createdAt": "2025-08-08T13:16:57.000Z",
        "updatedAt": "2025-08-20T09:53:28.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Weixi-tian/EarthquakeNPP-China-public/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "dertuncay/ArraySeismology",
        "url": "https://github.com/dertuncay/ArraySeismology",
        "description": "Lecture given by Ozgun Konca from KOERI, BOUN in Department of Geophysics",
        "stars": 0,
        "forks": 0,
        "readme": "![Iberia Network](IB.png?raw=true)\nIB network\n\n![Iberia Network](map.png?raw=true)\nSelected Events\n",
        "createdAt": "2024-10-04T13:41:32.000Z",
        "updatedAt": "2024-10-14T09:47:40.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/dertuncay/ArraySeismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Ra5putin/Seismology",
        "url": "https://github.com/Ra5putin/Seismology",
        "description": "Funding",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2020-01-21T11:19:45.000Z",
        "updatedAt": "2020-01-21T11:19:45.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Prezii/Project",
        "url": "https://github.com/Prezii/Project",
        "description": "About deep learning and time series data in seismology",
        "stars": 0,
        "forks": 0,
        "readme": "# Project\nLos Andes University - Geoscience student\n#\nAbout my degree proyect with python and Deep learning (CNN) about time series in Costa Rica rift.\n#\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1s-g95v61LFbsEbFMJ6nxdHuhWyeklr30)\n#\n## References\nTary, J. B., Mojica Boada, M. J., Vargas, C. A., Montaña Monoga, A. M., Naranjo-Hernandez, D. F., Quiroga, D. E., 2022. Source characteristics of the Mw 6 Mutatá earthquake, Murindo seismic cluster, northwestern Colombia. Journal of the South American Earth Sciences.\n",
        "createdAt": "2022-08-06T18:32:48.000Z",
        "updatedAt": "2022-08-25T16:34:18.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Prezii/Project/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "echambersgeo/plotting_scripts",
        "url": "https://github.com/echambersgeo/plotting_scripts",
        "description": "For all plotting scripts in seismology",
        "stars": 0,
        "forks": 0,
        "readme": "# plotting_scripts\nFor all plotting scripts in seismology\n\nModules needed\nPython 3\nObspy\n\nHow to run scripts:\npython script_name\n\nCurrently contains files to:\nPlot individual seismogram\nPlot individual seismogram but can run for multiple events\n",
        "createdAt": "2021-02-14T12:39:40.000Z",
        "updatedAt": "2021-02-14T12:42:10.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/echambersgeo/plotting_scripts/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "schipp/fast_beamforming",
        "url": "https://github.com/schipp/fast_beamforming",
        "description": "Fast and efficient beamforming in Python - educational notebooks",
        "stars": 50,
        "forks": 11,
        "readme": "# Fast beamforming in Python\n\n[![DOI](https://zenodo.org/badge/684053669.svg)](https://zenodo.org/badge/latestdoi/684053669)\n\n<img align=\"left\" src=\"beampowers.png\" width=\"400px\">\n\nCross-correlation beamforming can be realised in a few lines of matrix operations in Python, making use of `pytorch` linear algebra optimisations for speed. For small problems (=smaller than memory), this is fast, efficient and fully parallel. For large problems, this approach fails, specifically when the matrices containing cross correlations become too large for memory. To solve this, we can employ `dask` to divide the computations automatically into multiple tasks than can run across arbitrary infrastructure while retaining much of the same syntax and logic.\n\nWe demonstrate this with the notebooks in this repository:\n\n* `beamforming_naive.ipynb`: naive beamforming code, **SLOW** \"pure\" Python version for teaching purposes only\n* `beamforming_numpy.ipynb`: same as above, rewritten in `numpy` using broadcasting etc.\n* `beamforming_pytorch.ipynb`: same as above, replacing `numpy` functions with `pytorch` equivalents\n* `beamforming_dask.ipynb`: same as above, moving memory-limited computations to `dask`.\n\nOther variants:\n\n* `beamforming_geo.ipynb`: same as the `pytorch` version, but using geographical coordinates. requires [geokernels](https://github.com/sigmaterra/geokernels) for distance calculations.\n* `beamforming_planewave_data.ipynb`: plane-wave beamforming of seismic field data. requires [obspy](https://docs.obspy.org) for handling seismograms.\n\nIn these notebooks, logic and processing is not abstracted away in a package of functions. Instead, all processing happens within the notebooks for instructional purposes.\n\n## Performance statistics\n\nThese are the runtimes of the cell that performs beamforming (under 3. Beamforming) on a machine with 2x Intel Xeon Gold 6326 (16C/32T), 512 GB RAM for the parameters indicated below.\n\n**Is your code faster? Let me know!**\n\n### `n_sensors = 100`\n\n| notebook version | runtime  | speed-up |\n| ---------------- | -------- | -------- |\n| `naive`          | 43.9 sec | 1x       |\n| `numpy`          | 11.7 sec | 3.75x    |\n| `pytorch`        | 0.9 sec  | 48.8x    |\n| `dask`           | 1.9 sec  | 23.1x    |\n\n### `n_sensors = 1000`\n\n\n| notebook version | runtime    | speed-up |\n| ---------------- | ---------- | -------- |\n| `naive`          | 4861.3 sec | 1x       |\n| `numpy`          | fail       | fail     |\n| `pytorch`        | fail       | fail     |\n| `dask`           | 47.0 sec   | 103.4x   |\n\n`numpy` and `pytorch` versions fail, because `S` would require `2.1TiB` of memory.\n\nOther parameters in both tests: `grid_limit = 100`, `grid_spacing = 5`, `window_length = 100`, `sampling_rate = 10`, `fmin, fmax = 0.1, 1.0`\n\n## Python performance for scientific computing\n\nThis repository is also intended as a case study to teach students and researchers about the potential of a) making best use of the already exisiting computing libraries for significant speed-up compared to naively written Python code and b) how much performance can be gained simply by moving from `numpy` to equivalent `pytorch` code. Note that `pytorch` is significantly faster in the example of cross-correlation beamforming, because large tensors are involved. Further note that all linear equation systems, no matter what physics they express in your specific context, can be coded in matrix formulations, allowing to exploit the linear algebra optimisations developed in the machine learning community for your research.\n\n## Notes on `dask`\n\n`dask` allows to employ the same algorithm and largely the same syntax as the `pytorch` version, which means one doesn't have to worry about developing a different algorithm that is not memory-limited. However, `dask` also introduces a new optimisation problem: The choice of \"good\" chunks sizes for the specific system at hand. This is specific to the compute infrastructure used. On the bright side, this would need to be optimized only once for a given problem-geometry (number of stations, grid points, frequencies). Even without randomly chosen chunksizes (100, 100, 100 in the notebook here), performance is good. Visit the [dask documentation](https://docs.dask.org/en/stable/understanding-performance.html) for more details.\n\n## Methodological background\n\n### What is beamforming?\n\nBeamforming is a phase-matching algorithm commonly used to estimate the origin and local phase velocity of a wavefront propagating across an array of sensors. The most basic beamformer is the delay-and-sum beamformer, where recordings across the sensors are phase-shifted and summed (forming the beam) to test for the best-fitting source origin and medium velocity (Rost and Thomas, 2002).\n\n### Cross-correlation beamforming\n\nThe cross-correlation beamformer (also Bartlett beamformer, conventional beamformer, etc.) applies the same delay-and-sum idea to correlation functions between all sensor pairs (Ruigrok et al. 2017). This has the major advantage that only the coherent part of the wavefield is taken into account. The major disadvantage is that the computation of cross correlations between all station pairs can become expensive fast, scaling with $n^2$.\n\nA few different formulation of this beamformer exist. We write it in frequency domain as\n\n$B = \\sum_\\omega \\sum_j \\sum_{k\\neq j} K_{jk}(\\omega) S_{kj}(\\omega),$\n\nwith $B$ the beampower, $K_{jk}(\\omega) = d_j(\\omega) d^H_k(\\omega)$ the cross-spectral density matrix of recorded signals $d$, $S_{jk}(\\omega) = s_j(\\omega) s^H_k(\\omega)$ the cross-spectral density matrix of synthetic signals $s$, $j$ and $k$ identify sensors, and $H$ the complex conjugate. We exclude auto-correlations $j=k$, because they contain no phase-information. Consequently, negative beampowers indicate anti-correlation.\n\nThe synthetic signals $s$ (often called replica vectors or Green's functions) are the expected wavefield for a given source origin and medium velocity, most often in acoustic homogeneous half-space $s_j = \\exp(-i \\omega t_j)$, where $t_j$ is the traveltime from source to each receiver $j$.\n\n### Plane-wave beamforming\n\nIn seismology, \"beamforming\" is often synonymous with plane-wave beamforming. In plane-wave beamforming $t_j$ is the relative travel time from a reference point (commonly center of array) to the sensor $j$ for a given plane-wave\n\n$t_j = \\boldsymbol{r_j} \\cdot \\boldsymbol{u_h}$,\n\nwith $\\boldsymbol{r_j} = (r_x, r_y)$ the coordinates of sensor $j$ relative to the reference point, and $\\boldsymbol{u_h} = u_h(\\sin(\\theta), \\cos(\\theta))$ the horizontal slowness vector of the plane-wave, with $u_h$ the horizontal slowness and $\\theta$ the direction of arrival. $u_h$ and $\\theta$ are the parameters that are tested for (or equivalently $u_x, u_y$). Because plane waves are assumed, the source origin must be enough far away that the plane-wave assumption becomes adequate. The advantage of this is that the spatial dimension is 1 (direction of arrival), which is cheap to compute.\n\n### Matched field processing\n\nWhen curved wavefronts are allowed, sources may be located within the sensor array and the grid that is tested is defined in space instead of the slowness-domain, adding at least one extra dimension. This is called matched field processing (e.g., Baggeroer et al. 1988). In practice, the difference between plane-wave beamforming and matched field processing lies in the computation of the Green's functions $s_j$, or more precisely the expected traveltimes $t_j$.\n\nIn MFP, the travel time is computed as\n\n$t_j = |\\boldsymbol{r}_j - \\boldsymbol{r}_s| / c$,\n\nwith $|\\boldsymbol{r}_j - \\boldsymbol{r}_s|$ the euclidean distance between sensor and source and $c$ the medium velocity. The parameters tested for in MFP are the source position $\\boldsymbol{r}_s$ (2D, 3D) and, sometimes, the medium velocity $c$. A different name for MFP that is intuitive to seismologists may be curved-wave Beamforming.\n\nThe beamforming in the notebooks here is Matched Field Processing.\n\n### References\n\nRost, S. & Thomas, C., 2002. Array seismology: Methods and applications. *Reviews of Geophysics*, **40**, 2–1–2–27. doi:10.1029/2000RG000100\n\nRuigrok, E., Gibbons, S. & Wapenaar, K., 2017. Cross-correlation beamforming. *J Seismol*, **21**, 495–508. doi:10.1007/s10950-016-9612-6\n\nBaggeroer, A.B., Kuperman, W.A. & Schmidt, H., 1988. Matched field processing: Source localization in correlated noise as an optimum parameter estimation problem. *The Journal of the Acoustical Society of America*, **83**, 571–587. doi:10.1121/1.396151\n\n### Requirements\n\nTo run these notebooks, the following is required\n\n* Python\n* scientific Python stack (numpy, scipy, matplotlib)\n* notebook\n* [torch](https://pytorch.org)\n* [dask](https://www.dask.org)\n* [geokernels](https://github.com/sigmaterra/geokernels) for distances on geographical grids\n\nA functioning installation can be achieved, e.g., via conda by\n\n```bash\n>> conda create -n fast_beamforming python=3.11\n>> conda activate fast_beamforming\n>> conda install pytorch dask scipy matplotlib notebook\n```\n",
        "createdAt": "2023-08-28T11:05:57.000Z",
        "updatedAt": "2025-12-03T22:59:45.000Z",
        "language": "Jupyter Notebook",
        "homepage": "https://doi.org/10.5281/zenodo.16939197",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/schipp/fast_beamforming/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "btab2273/statistical_seismology",
        "url": "https://github.com/btab2273/statistical_seismology",
        "description": "Statistical seismology and hypothesis testing of inter-earthquake time distributions and magnitudes",
        "stars": 2,
        "forks": 0,
        "readme": "# statistical_seismology\nStatistical seismology and hypothesis testing of inter-earthquake time distributions for the Parkland segment of the San Andreas fault line\n\nQuantitative analysis of the magnitude and frequency of earthquakes in Oklahoma before and after gas fracking began in 2010\n",
        "createdAt": "2018-08-07T21:40:38.000Z",
        "updatedAt": "2023-02-09T06:50:21.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/btab2273/statistical_seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "gfreya/OOD-detection-seismology",
        "url": "https://github.com/gfreya/OOD-detection-seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# OOD-detection-seismology\n\nOut-of-distribution detection for time series and signals\n\nThe `src` directory includes source codes which utilized different machine learning models (statistical models to deep learning models) to solve the OOD detections.\n\nThe `data` directory only contains some sample data. The raw data is in .hdf5 format. Link: https://github.com/smousavi05/STEAD \n\n@SciML Research Group https://sites.brown.edu/bergen-lab/\n",
        "createdAt": "2022-09-20T22:49:18.000Z",
        "updatedAt": "2022-09-20T23:19:50.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/gfreya/OOD-detection-seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "yerkodigo/infoSismografiaChile",
        "url": "https://github.com/yerkodigo/infoSismografiaChile",
        "description": "Website for displaying information on seismology in Chile consumed by the Gael Api",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2022-12-31T18:20:11.000Z",
        "updatedAt": "2023-01-06T10:40:16.000Z",
        "language": "JavaScript",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "sergiventosa/FastPCC",
        "url": "https://github.com/sergiventosa/FastPCC",
        "description": "Compute interstation correlations of seismic ambient noise, including fast implementations of the standard, 1-bit, phase and wavelet phase cross-correlations.",
        "stars": 44,
        "forks": 21,
        "readme": "Fast phase cross-correlation\n============================\n\n[![License: LGPL v3](https://img.shields.io/badge/License-LGPL%20v3-blue.svg)](https://www.gnu.org/licenses/lgpl-3.0)\n\nSoftware to compute interstation correlations, including fast implementations of the \nphase cross-correlation [(Ventosa et al., SRL 2019)](https://doi.org/10.1785/0220190022) and wavelet phase cross-correlation\n[(Ventosa & Schimmel, IEEE-TGRS 2023)](https://doi.org/10.1109/TGRS.2023.3294302) with and without using the GPU.\n\nThe software packages of fast phase cross-correlation and [ts-PWS](https://github.com/sergiventosa/ts-PWS) \nstacking are basic building blocks in the design of efficient signal extraction methods \nfrom interstation correlations.\n\nMain features\n-------------\nComputes 4 types of correlations:\n * The Standard (geometrically) normalized cross-correlations (GNCC).\n * The 1-bit amplitude normalization followed by the GNCC (1-bit GNCC).\n * The phase cross-correlation (PCC).\n * The wavelet phase cross-correlation (WPCC).\n\nThe computations of PCC and WPCC are speed-up in several ways:\n * Both are parallelized in the CPU using OpenMP and in the GPU using CUDA (two independent codes).\n * The computational cost of PCC with power of 2 is reduced to about twice the one of 1-bit GNCC.\n\nCompilation\n-----------\nTo compile execute \"make\" in the src directory. Use \"make clean\" to remove \nany previouly compiled code.\n\n * The [Seismic Analysis Code](http://ds.iris.edu/ds/nodes/dmc/software/downloads/sac/) (SAC) is used to read and write sac files.\n * The FFTW double and single precision libraries are used. If you have to compile them \n   for your system, follow the [fftw.org instructions](http://www.fftw.org/fftw3_doc/Installation-and-Customization.html#Installation-and-Customization). \n * The SACHOME enviorment variable should provide the path to the directory where sac is\n   installed. For example, this can be defined in bash as:  \n   export SACHOME=/opt/sac  \n   Change \"/opt/sac\" to your current sac directory if necessary.\n * OpenMP is used to speed up computations. When OpenMP is not available, use \n   make -f makefile_NoOpenMP\".\n\nCompile SAC\n-----------\nThe precompiled sac libraries may not work in some systems/compilers. If you use the gcc \ncompiler on, e.g., Ubuntu you may have errors similar to:\n>  /usr/bin/ld: /opt/sac/lib/sacio.a(getfhv.o): relocation R_X86_64_32 against undefined symbol 'kmlhf' can not be used when making a PIE object; recompile with -fPIC\n\nThis can be solved by compiling the source version of SAC. From the source directory of sac do:\n>  ./configure --enable-optim=2 --prefix=/opt/sac CFLAGS='-march=native -fPIC' \\\n>  make \\\n>  make install\n\nThe key detail here is the -fPIC flag, you can adapt the other options to your needs.\nThe flag --prefix=/opt/sac sets the directory where sac will be installed when doing \n\"make install\". You can change this directory, e.g., to keep using your current \nversion of SAC for other purposes.\n\nWarming up\n----------\n 1. Read ./examples/example.sh\n 2. Execute it, e.g., bash example.sh\n 3. Do PCC_fullpair_1b for the parameters usage.\n   \nOrigin of Phase Cross-Correlation\n---------------------------------\nSchimmel, M., 1999. Phase cross-correlations: Design, comparisons, and applications,\nBulletin of the Seismological Society of America, 89(5), 1366-1378.\n\nSchimmel, M. and Stutzmann, E. & J. Gallart, 2011. Using instantaneous phase coherence \nfor signal extraction from ambient noise data at a local to a global scale, Geophysical \nJournal International, 184(1), 494-506, doi:[10.1111/j.1365-246X.2010.04861.x](https://doi.org/10.1111/j.1365-246X.2010.04861.x)\n   \nPaper to be cited\n-----------------\nVentosa S. & M. Schimmel, 2023. Broadband empirical Green’s function extraction\nwith data adaptive phase correlations, IEEE Transactions on Geoscience and Remote Sensing,\n61:1-17, doi:[10.1109/TGRS.2023.3294302](https://doi.org/10.1109/TGRS.2023.3294302)\n\nVentosa S., Schimmel M. & E. Stutzmann, 2019. Towards the processing of large data \nvolumes with phase cross-correlation, Seismological Research Letters, 90(4):1663-1669, \ndoi:[10.1785/0220190022](https://doi.org/10.1785/0220190022)\n\n2024/12/18 Sergi Ventosa Rahuet (sergiventosa(at)hotmail.com)\n",
        "createdAt": "2019-03-23T16:22:58.000Z",
        "updatedAt": "2025-08-12T23:03:02.000Z",
        "language": "C",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/sergiventosa/FastPCC/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Seismology-AUTh/HTdataless",
        "url": "https://github.com/Seismology-AUTh/HTdataless",
        "description": "Dataless SEED files for the stations of the AUTh Seismological Network plus a few scripts to inspect their contents.",
        "stars": 0,
        "forks": 0,
        "readme": "# HTdataless\nDataless SEED files for the Seismological Network of the Aristotle University of Thessaloniki\nand a few utility scripts to inspect their contents\n\nAll scripts use the ObsPy Framework for Seismology: www.obspy.org\n\n2017-2024 Odysseus Galanis ogalanis@geo.auth.gr\n\nSeismological Network of the Aristotle University of Thessaloniki, Greece\nhttps://seismo.auth.gr/\n",
        "createdAt": "2017-12-21T10:04:23.000Z",
        "updatedAt": "2025-11-09T14:58:31.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Seismology-AUTh/HTdataless/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "UW-geophysics-edu/ESS563-2025",
        "url": "https://github.com/UW-geophysics-edu/ESS563-2025",
        "description": "Advanced Seismology",
        "stars": 0,
        "forks": 1,
        "readme": "# ESS563-2025\nAdvanced Seismology\n\n---\ntitle: \"ESS 563 – Advanced Seismology\"\nlayout: course\npermalink: /teaching/ess563/\n---\n\n\n## Course description\nESS 563 is a graduate course on seismic wave propagation, earthquake source mechanics, and modern computational seismology.  Weekly theoretical lectures draw on *Quantitative Seismology* and *Source Mechanisms of Earthquakes*, while practical sessions use Python/ObsPy, FDSN web services, finite‑difference and spectral‑element modeling (SPECFEM3D/sem3D), and Instaseis to explore real and synthetic seismograms.  Students study continuum mechanics, radiation patterns, layered media, moment tensors, dynamic rupture, and full‑waveform inversion, and apply these concepts to recent large earthquakes and slow‑slip events.\n\n## Learning outcomes\n- Derive and apply the equations of dynamic elasticity to describe seismic wave propagation in homogeneous and layered media.  \n- Analyze point‑source radiation patterns and moment tensors to interpret earthquake mechanisms and rupture kinematics.  \n- Implement numerical methods (finite‑difference and spectral‑element) to generate synthetic seismograms and evaluate their accuracy.  \n- Use Python/ObsPy to retrieve, process, and visualize seismic data from global networks, and compare observations with synthetic predictions.  \n- Interpret finite‑fault and dynamic rupture models of large earthquakes and assess their implications for seismic hazard and earthquake physics.  \n- Explain the role of slow‑slip events and tremor in the earthquake cycle and evaluate emerging tools for earthquake detection and characterization.\n\n## Installation\n\n### Prerequisites\nThis course requires Python 3.9+ with scientific computing packages. We recommend using conda to manage the environment.\n\n### Setting up the environment\n1. **Clone this repository:**\n   ```bash\n   git clone https://github.com/UW-geophysics-edu/ESS563-2025.git\n   cd ESS563-2025\n   ```\n\n2. **Create and activate the conda environment:**\n   ```bash\n   conda env create -f env.yml\n   conda activate ess563-2025\n   pip install -r requirements.txt\n   python -m ipykernel install --user --name ess563-2025 --display-name \"ess563-2025\"\n   ```\n\n3. **Verify installation:**\n   ```bash\n   python -c \"import obspy, matplotlib, pandas, scipy; print('All packages imported successfully!')\"\n   ```\n\n\n### Running Jupyter notebooks\nAfter setting up the environment, start Jupyter to work with the course notebooks:\n```bash\njupyter lab\n# or\njupyter notebook\n```\n\n### Troubleshooting\n- If you encounter issues with ObsPy installation, try installing it separately: `conda install -c conda-forge obspy`\n- For plotting issues, ensure you have a GUI backend: `conda install -c conda-forge pyqt`\n- If you're on macOS with Apple Silicon, make sure to use the `conda-forge` channel for better compatibility\n\n## Resources\n- [Notebook repository](https://github.com/UW-geophysics-edu/ESS563-2025/) – Python notebooks used in class  \n- [10‑week course plan](https://denolle-lab.github.io/teaching/ess563/) – overview of weekly topics, readings, and assignments  \n- Contact the instructor via Canvas for detailed syllabus and assignments.\n\n## Schedule\n\n<table class=\"schedule-table\">\n<thead>\n<tr>\n<th>Week</th>\n<th>Monday – Theory (10:00–11:20 AM)</th>\n<th>Wednesday – Practice (10:00–11:20 AM)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Week 1</strong></td>\n<td><strong>Fundamentals of seismic waves and elasticity</strong> – review continuum mechanics, stress–strain relations, dynamic elasticity and Green's functions.</td>\n<td><strong>Python: stress &amp; strain tensors</strong> – compute stress and strain tensors for simple deformations and verify reciprocity; plot shear stress vs. displacement.</td>\n</tr>\n<tr>\n<td><strong>Week 2</strong></td>\n<td><strong>Point dislocation sources and radiation patterns</strong> – representation theorems, double‑couple sources, body‑force equivalents.</td>\n<td><strong>Python: radiation pattern simulation</strong> – code P‑ and S‑wave radiation patterns for a double‑couple moment tensor; visualize azimuthal variation.</td>\n</tr>\n<tr>\n<td><strong>Week 3</strong></td>\n<td><strong>Plane waves and boundary interactions</strong> – plane waves in homogeneous media, reflection and transmission at interfaces, inhomogeneous/interface waves.</td>\n<td><strong>Python: reflection/transmission coefficients</strong> – calculate and plot reflection and conversion coefficients for P/S waves at contrasting half‑spaces; explore impedance contrasts.</td>\n</tr>\n<tr>\n<td><strong>Week 4</strong></td>\n<td><strong>Spectral‑element methods and full‑waveform modelling</strong> – introduce SEM, mesh construction, accuracy and convergence; discuss SPECFEM3D/sem3D workflows.</td>\n<td><strong>Numerical lab: running SPECFEM/sem3D</strong> – prepare simple 1‑D/3‑D velocity models, generate input files, and run a forward simulation; compare results from a 1‑D finite‑difference solver.</td>\n</tr>\n<tr>\n<td><strong>Week 5</strong></td>\n<td><strong>Wave propagation in layered and depth‑dependent media</strong> – generalized rays, reflectivity methods, ray theory; inversion of travel‑times.</td>\n<td><strong>Python: ray tracing &amp; inversion</strong> – implement multi‑layer ray tracing, compute travel times and invert synthetic data to estimate layer velocities.</td>\n</tr>\n<tr>\n<td><strong>Week 6</strong></td>\n<td><strong>Kinematics of earthquake sources and moment tensors</strong> – far‑ and near‑field signatures, centroid moment tensor theory, source inversion basics.</td>\n<td><strong>Python/ObsPy: moment tensor inversion</strong> – generate synthetic seismograms for specified moment tensors and station geometries; invert synthetic waveforms to recover orientation.</td>\n</tr>\n<tr>\n<td><strong>Week 7</strong></td>\n<td><strong>Fracture mechanics and dynamic rupture</strong> – crack propagation, stress intensity, fracture energy, boundary‑integral methods; introduction to supershear.</td>\n<td><strong>Python: 1‑D spring‑block model</strong> – simulate a Burridge–Knopoff system to explore frictional parameters and observe transitions from subshear to supershear rupture speeds.</td>\n</tr>\n<tr>\n<td><strong>Week 8</strong></td>\n<td><strong>Case studies of recent large earthquakes</strong> – discuss multi‑segment faulting, supershear phases, and slip distributions of recent events (e.g., 2024 Noto Peninsula, 2023 Türkiye).</td>\n<td><strong>Instaseis/Syngine lab</strong> – use finite‑fault source models to generate global synthetic seismograms with Instaseis/Syngine; download and process observed data via FDSN services; align and compare synthetic and observed phases.</td>\n</tr>\n<tr>\n<td><strong>Week 9</strong></td>\n<td><strong>Slow slip events and the earthquake cycle</strong> – classification of slow slip, tremor, repeating earthquakes, and afterslip; implications for moment release.</td>\n<td><strong>Python: detecting slow slip</strong> – analyze GNSS time‑series to identify transient slow slip using cross‑correlation/stacking; compare with tremor catalogues.</td>\n</tr>\n<tr>\n<td><strong>Week 10</strong></td>\n<td><strong>Seismometry and advanced topics</strong> – instrumentation, frequency response, dynamic range; discussion of machine learning, dynamic triggering, and earthquake predictability.</td>\n<td><strong>Final project workshop</strong> – students build a \"synthetic earthquake\" (define fault geometry &amp; slip), generate synthetic waveforms, and invert to recover source parameters. Include optional global synthetics using Instaseis/Syngine.</td>\n</tr>\n</tbody>\n</table> |",
        "createdAt": "2025-09-29T13:02:47.000Z",
        "updatedAt": "2025-11-30T12:42:23.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/UW-geophysics-edu/ESS563-2025/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "grajh/readnordic",
        "url": "https://github.com/grajh/readnordic",
        "description": "Standalone seismological Python tool for reading Nordic data format. ",
        "stars": 0,
        "forks": 0,
        "readme": "## readnordic\nPython tool for reading and manipulating Nordic data format. Useful methods for earthquake and phase filtering, writing output files, calculting parameters and statistics, adding noise and several more are available. The tool creates an internal database of event objects which can then be filtered by the parameters defined in the Nordic format. Picked phase lines can also be filtered.\n\nMore information about the Nordic format can be found [here](https://seismologi.geus.dk/software/seisan/node243.html#5025).\n\nDependencies:\n\n    * NumPy (https://numpy.org/doc/stable/index.html)\n\nLast update: 20. June 2024.\n",
        "createdAt": "2018-03-21T12:59:10.000Z",
        "updatedAt": "2024-06-20T12:45:38.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/grajh/readnordic/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "anyshake/explorer",
        "url": "https://github.com/anyshake/explorer",
        "description": "🌏 Detecting seismic wave using 3 geophones and 3-axis accelerometer, pack & send data to AnyShake Observer over RS232 / RS485 serial.",
        "stars": 68,
        "forks": 14,
        "readme": "<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/anyshake/explorer/master/images/header.png\" width=\"500\"/>\n</p>\n\n## 🚀 **Join the Open Science Movement!** 🚀\n\nAnyShake Explorer is now **live on Crowd Supply**! This open-source, next-generation seismic monitoring system is officially available for crowdfunding.\n\n👉 **[Order now on Crowd Supply](https://www.crowdsupply.com/senseplex/anyshake-explorer)** and be among the first to experience real-time seismic data visualization, analysis, and export with professional-grade performance.\n\n📣 **Help us spread the word and grow the open science community: [www.crowdsupply.com/senseplex/anyshake-explorer](https://www.crowdsupply.com/senseplex/anyshake-explorer)**\n\n---\n\n## Overview\n\n**AnyShake Explorer** is a professional-grade, fully open-source seismic monitoring platform built for **researchers**, **engineers**, **educators**, and **enthusiasts**. It combines **high-sensitivity velocity geophones** with a **32-bit precision ADC** to capture ultra-low-noise seismic data in real time.\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/anyshake/explorer/master/images/product/overall-side-view-with-encolsure.jpg\" width=\"600\"/>\n</p>\n\nDesigned with versatility in mind, AnyShake Explorer transmits data via **RS-232/RS-485 serial interfaces** to its companion software, **AnyShake Observer**, for real-time visualization, storage, and analysis. It supports **simultaneous sampling of 3-axis geophones and accelerometers**, and features advanced capabilities like **GNSS/NTP time synchronization**, **OLED display**, and **digital inclinometer**.\n\nWhether you're monitoring **earthquake activity**, conducting **structural health assessments**, or exploring **educational experiments**, AnyShake Explorer provides a **flexible**, **hackable**, and **field-ready** solution that brings high-resolution seismology within everyone’s reach.\n\n## Features\n\n- 🧠 **32-bit Precision ADC** – ultra-low noise data capture for professional geophones\n- 🎛️ **Simultaneous 3-axis geophone + 3-axis accelerometer sampling** – full 6-channel motion capture\n- 🔌 **Multiple serial interfaces** – 2× RS-232 and 1× RS-485 for flexible deployment\n- ⚙️ **Flexible baud & sample rates** – supports up to 460,800 bps and 250 SPS\n- 📺 **Built-in OLED display** – live status readout without a PC\n- 🧭 **Digital inclinometer** – ensures precise leveling during installation\n- ⏱️ **Time sync via GNSS or NTP** – millisecond-level accuracy\n- 🔒 **Checksum-based data integrity** – reliable transmission over noisy links\n- ⚡ **Low power operation** – only ~0.6W at 9–12V DC, perfect for remote use\n- 🖥️ **Cross-platform companion software** – live streaming, waveform analysis, and export\n- 🛠️ **Fully open-source** – hardware schematics, firmware, and desktop software available\n\n## Use Cases\n\n- **Seismology & Earthquake Research**: High-fidelity data for scientific analysis\n- **Citizen Science**: Deployable anywhere, contributes to global monitoring networks\n- **Structural Health Monitoring**: Track stress on bridges, buildings, etc.\n- **STEM Education**: Real-time data for hands-on earthquake demos\n\n## Technical Spec.\n\n| Category           | Specification                                |\n| ------------------ | -------------------------------------------- |\n| Geophones          | 3x 4.5 Hz velocity sensors                   |\n| Frequency Response | 0.5 – 27 Hz (with proper gain and filtering) |\n| Accelerometer      | 3-axis, 16-bit (±2g)                         |\n| ADC                | 32-bit differential                          |\n| Sampling Modes     | Accel-only, Geo-only, Accel+Geo              |\n| Sample Rate        | 50 / 100 / 200 / 250 SPS                     |\n| Baud Rate          | 57,600 to 460,800 bps                        |\n| Interfaces         | 2x RS-232, 1x RS-485                         |\n| Time Sync          | GNSS & NTP support                           |\n| Display            | 0.96\" OLED                                   |\n| Tilt Sensor        | Digital inclinometer                         |\n| Power              | 9–12V DC @ ~50 mA                            |\n| Software           | AnyShake Observer (open-source)              |\n| Data Export        | MiniSEED, SAC, TXT, WAV                      |\n| Networking         | HTTP Web, SeedLink, TCP raw stream           |\n\n## Comparison\n\n| Feature              | **AnyShake Explorer** | Raspberry Shake 1D | Raspberry Shake 4D | Raspberry Shake 3D |\n| -------------------- | --------------------- | ------------------ | ------------------ | ------------------ |\n| Open-Source Hardware | ✅                    | ❌                 | ❌                 | ❌                 |\n| Open-Source Software | ✅                    | Partial            | Partial            | Partial            |\n| Geophone Channels    | 3                     | 1                  | 1                  | 3                  |\n| Accelerometer        | 3-axis                | ❌                 | 3-axis             | ❌                 |\n| ADC Resolution       | 32-bit                | 24-bit             | 24-bit             | 24-bit             |\n| Sampling Rate        | 50–250 SPS            | 100 SPS            | 100 SPS            | 100 SPS            |\n| Power Usage          | ~0.6W                 | ~2.2W              | ~2.2W              | ~2.2W              |\n| Customizability      | ✅                    | ❌                 | ❌                 | ❌                 |\n\n## Get Started\n\n- [**Hardware**](https://github.com/anyshake/explorer/tree/master/hardware): Schematics, PCB layout, BOM\n- [**Firmware**](https://github.com/anyshake/explorer/tree/master/firmware): MCU code (PlatformIO)\n- [**Software**](https://github.com/anyshake/observer): Cross-platform visualization & analysis\n- [**Docs**](https://anyshake.org/docs/anyshake-explorer/product-overview/): Quick start and configuration guides\n\n## Credits\n\nThis project is maintained by **SensePlex Limited**, a UK-based company dedicated to developing open-source hardware and software.\n\n## License\n\nThis project is dual-licensed:\n\n1. **Open Source License (AGPLv3):**  \n   You may use, modify, and redistribute this project under the terms of the GNU Affero General Public License version 3.0. This license requires that any derivative works also be released under the same license.\n\n2. **Commercial License:**  \n   If you intend to use this project in closed-source, commercial, or proprietary applications, please contact us at [anyshake@senseplex.net](mailto:anyshake@senseplex.net) to obtain a commercial license.\n\n---\n\n![Star History Chart](https://api.star-history.com/svg?repos=anyshake/explorer&type=Date)\n",
        "createdAt": "2023-08-11T17:33:11.000Z",
        "updatedAt": "2025-12-04T06:58:47.000Z",
        "language": "C",
        "homepage": "https://anyshake.org",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/anyshake/explorer/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "YuanYusung/MASW",
        "url": "https://github.com/YuanYusung/MASW",
        "description": "Obtain the array average phase velocity from seismic ambient noise cross-correlations (ANCs) using multichannel analysis of surface waves (MASW)",
        "stars": 1,
        "forks": 0,
        "readme": "# MASW\n\nObtain the array average phase velocity from seismic ambient noise cross-correlations (ANCs) using multichannel analysis of surface waves (MASW; Park et al., 1999).\n\n## Project layout\n- `masw/`: reusable package containing configuration models, preprocessing, MASW computation, plotting, and pipeline helpers.\n- `masw.py`: CLI entry point that forwards to `masw/cli.py`.\n- `Data_CCFs/`: expected location of SAC inputs.\n- `All_sacdata_ZZ.pickle`, `MASW_raw_data.npz`: caches created during preprocessing and dispersion computation.\n\n## Quickstart\nInstall dependencies, ideally in a virtual environment:\n\n```bash\npip install -r requirements.txt\n```\n\nEnsure `Data_CCFs/` contains your SAC files, then run:\n\n```bash\npython masw.py plot\n```\n\nThe command will preprocess inputs, compute dispersion, and save the default figures `Figure1_wavefield.png` and `Figure2_masw_avg_dispersion.png`.\n\n### Example outputs\n\n![Wavefield example](Figure1_wavefield.png)\n\n![Average dispersion example](Figure2_masw_avg_dispersion.png)\n\n## CLI usage\nThree subcommands are available:\n\n- **prepare**: preprocess and cache SAC files.\n  ```bash\n  python masw.py prepare --data-dir Data_CCFs --waveform-cache All_sacdata_ZZ.pickle\n  ```\n- **masw**: compute MASW energy without plotting (useful for batch runs).\n  ```bash\n  python masw.py masw --min-distance 0.8 --zero-padding 6\n  ```\n- **plot**: full pipeline with figures (default).\n  ```bash\n  python masw.py plot --wavefield-figure Figure1_wavefield.png --dispersion-figure Figure2_masw_avg_dispersion.png\n  ```\n\nCommon options:\n- `--bandpass-low` / `--bandpass-high`: bandpass frequencies (Hz).\n- `--taper-vmin` / `--taper-vmax`: velocity bounds (km/s) for time windowing.\n- `--min-distance`: minimum station spacing included in averaging (km).\n- `--zero-padding`: zero padding factor for MASW FFT grid.\n- `--no-cache`: force recomputation instead of reusing cached waveforms.\n\n## Reusing the API\nFor notebook or scripted workflows, import the pipeline helpers:\n\n```python\nfrom masw import (\n    PathConfig,\n    PreprocessConfig,\n    MASWConfig,\n    AveragingConfig,\n    prepare_wavefield,\n    run_pipeline,\n)\n\nwavefield, distances, times, fs = prepare_wavefield(PathConfig(), PreprocessConfig())\nvelocities, freqs, energy = run_pipeline()\n```\n\nAdjust the configuration objects to match new datasets (e.g., data directory, bandpass, velocity grid) without editing the internal code.\n\n## References\n\nPark, C. B., Miller, R. D., & Xia, J. (1999). Multichannel analysis of surface waves. Geophysics, 64(3), 800–808. https://doi.org/10.1190/1.1444590\n",
        "createdAt": "2025-11-22T12:55:04.000Z",
        "updatedAt": "2025-12-04T02:48:26.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/YuanYusung/MASW/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "highfem/tqdne",
        "url": "https://github.com/highfem/tqdne",
        "description": "Generative modeling of seismic waveforms",
        "stars": 18,
        "forks": 4,
        "readme": "# This quake does not exist\n\n[![ci](https://github.com/highfem/tqdne/actions/workflows/ci.yml/badge.svg)](https://github.com/highfem/tqdne/actions/workflows/ci.yml)\n[![arXiv](https://img.shields.io/badge/arXiv-2410.19343-b31b1b.svg)](https://arxiv.org/abs/2410.19343)\n\n> Generative modelling of seismic waveforms using denoising diffusion.\n>\n> ![Generative pipeline](figures/figure1.png)\n\n## About\n\nThis repository implements Generative Waveform Models (GMWs), i.e., generative models that can generate synthetic seismic waveforms.\nIn particular, the repository implements *HighFEM*, the GWM introduced in [High Resolution Seismic Waveform Generation using Denoising Diffusion](https://arxiv.org/abs/2410.19343).\n\nThe repository can be also be used to replicate the results from the manuscript using the experimental code in [experiments](experiments), and for training custom GWMs from scratch.\n\n## Quick start\n\nIf you are only interested in generating synthetic waveforms, install `tqdne` using\n\n```bash\npip install git+https://github.com/highfem/tqdne@<RELEASE>\n```\n\nwhere `RELEASE` should be replaced with the latest release version to be found [here](https://github.com/highfem/tqdne/tags).\nYou should then be able to simulate waveforms by calling\n\n```bash\ngenerate-waveforms\n```\n\nfrom the (Unix) command line.\n\n## Installation\n\nIf you are interested in reproducing the results from the paper, please refer to [`experiments/README.md`](experiments/README.md).\n\nYou can install `tqdne` by following the steps below.\n\n1.\n   a) **Recommended**: Download the latest [release](https://github.com/highfem/tqdne/tags) if you do not require commit history. Releases have been tested and reproduced by us and partners.\n\n   b) Alternatively, clone the repository using:\n\n      ```bash\n      git clone (--depth 1) https://github.com/highfem/tqdne.git\n      ```\n\n2.\n   There are two ways to install the package: you can either install the package with [`uv`](https://github.com/astral-sh/uv) which will use the versions of all dependencies that we are using for development, or you install the package using the `pip` installer which will install the latest versions of all dependencies.\n\n   a) **Recommended**: Install all dependencies and the package with [`uv`](https://github.com/astral-sh/uv), via\n\n      ```bash\n      uv sync\n      ```\n\n   b) Install using `pip` install, for instance, within a `conda` or `virtualenv` environment, via:\n\n      ```bash\n      pip install -e .\n      ```\n\n## Experiments\n\nTo reproduce the experiments from the manuscript, including data preprocessing, training, and evaluation, navigate to the [experiments](./experiments) folder. Refer to the corresponding README files for step-by-step guidance.\n\n## Sampling waveforms\n\nYou can generate your own waveforms by using the scripts in [scripts](./scripts).\nSee the corresponding README files for more information.\n\n## Contributing\n\nContributions in the form of pull requests are more than welcome. In order to contribute:\n\n1) Clone `tqdne` and install `uv` from [here](https://docs.astral.sh/uv/).\n2) Create a new branch locally `git checkout -b feature/my-new-feature` or `git checkout -b issue/fixes-bug`.\n3) Install all dependencies using: `uv sync --all-extras`.\n4) Activate the virtual environment: `source .venv/bin/activate`.\n5) Install `pre-commit` (which we use for auto-formatting and checking code) using: `pre-commit install`.\n6) Implement your contribution.\n7) Run `uv run ruff check .` to check the code for issues.\n8) Push your changes and submit a PR 🙂.\n\n## Acknowledgements\n\nSome Python code has been adapted from the following repositories:\n\n- [EDM](https://github.com/NVlabs/edm)\n- [Consistency Models](https://github.com/openai/consistency_models)\n",
        "createdAt": "2023-10-20T17:52:03.000Z",
        "updatedAt": "2025-12-05T01:08:33.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/highfem/tqdne/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "fatiando/fatiando",
        "url": "https://github.com/fatiando/fatiando",
        "description": "DEPRECATED in favor of our newer libraries (see www.fatiando.org). Python toolkit for modeling and inversion in geophysics. ",
        "stars": 221,
        "forks": 120,
        "readme": ".. image:: https://raw.githubusercontent.com/fatiando/logo/master/fatiando-banner-homepage.png\n    :alt: Fatiando a Terra\n    :target: http://www.fatiando.org\n\n----\n\n`Website <http://www.fatiando.org>`__ | \n`Documentation <http://www.fatiando.org/docs>`__ |\n`Gallery <http://www.fatiando.org/gallery/>`__ |\n`Docs (development version) <http://www.fatiando.org/dev>`__ |\n`Mailing list <https://groups.google.com/d/forum/fatiando>`__ \n\n.. image:: http://img.shields.io/pypi/v/fatiando.svg?style=flat-square\n    :alt: Latest version on PyPI\n    :target: https://pypi.python.org/pypi/fatiando/\n.. image:: http://img.shields.io/travis/fatiando/fatiando/master.svg?style=flat-square&label=linux\n    :alt: Travis CI build status\n    :target: https://travis-ci.org/fatiando/fatiando\n.. image:: https://img.shields.io/appveyor/ci/leouieda/fatiando/master.svg?style=flat-square&label=windows\n    :alt: AppVeyor build status\n    :target: https://ci.appveyor.com/project/leouieda/fatiando\n.. image:: http://img.shields.io/coveralls/fatiando/fatiando/master.svg?style=flat-square\n    :alt: Test coverage status\n    :target: https://coveralls.io/r/fatiando/fatiando?branch=master\n.. image:: https://landscape.io/github/fatiando/fatiando/master/landscape.svg?style=flat-square\n    :target: https://landscape.io/github/fatiando/fatiando/master\n    :alt: Code health report by landscape.io\n.. image:: http://img.shields.io/badge/doi-10.5281/zenodo.157746-blue.svg?style=flat-square\n    :alt: doi:10.5281/zenodo.157746\n    :target: http://dx.doi.org/10.5281/zenodo.157746\n\n\nDisclaimer\n----------\n\nFatiando is under active development and we are still changing the API between\nreleases.\nNames will change and functions will move as we improve our design.\nYou might have to update your scripts and notebooks to get the latest features\nfrom a new release.\n\nPlease bear with us.\n\n\nOverview\n--------\n\nOur goal is provide a comprehensive and extensible framework\nfor geophysical data analysis and the development of new methodologies.\n\n**Research:**\nMake your research more **reproducible** by writing a Python script or Jupyter\nnotebook instead of clicking through complicated menus.\n\n**Development:**\nDon't start from scratch! Build upon the existing tools in Fatiando to develop\nnew methods.\n\n**Teaching:**\nCombine Fatiando with the Jupyter notebook to make rich, interactive documents.\nGreat for teaching fundamental concepts of geophysics!\n\n\nGetting started\n---------------\n\n1. `Install Fatiando <http://www.fatiando.org/install.html>`__ and its\n   dependencies.\n2. Browse the `Gallery <http://www.fatiando.org/gallery/index.html>`__ \n   for examples of what Fatiando can do.\n3. Take a look at the rest of the \n   `Documentation <http://www.fatiando.org/docs.html>`__ for\n   more information about the library.\n4. Get involved in the community and see how you can help\n   in our `Contributor Guide <http://www.fatiando.org/develop.html>`__.\n\n\nContributing and asking for help\n---------------------------------\n\n**Subscribe** to our Google Groups mailing list to stay informed and ask for\nhelp:\n`groups.google.com/d/forum/fatiando <https://groups.google.com/d/forum/fatiando>`__\n\nWe'll post updates to the list about new releases and features, events, and\nfuture plans for the project.\n**Get involved** to help us shape the project and make it even better!\n\nAnother option for reaching out and reporting bugs is to\n`open an issue on Github <https://github.com/fatiando/fatiando/issues>`__.\n\nWe have an **open development** process where everything is discussed through\n`Github issues <https://github.com/fatiando/fatiando/issues>`__. Anyone can\ncomment and give feedback. See our `Roadmap for v1.0\n<https://github.com/fatiando/fatiando/issues/102>`__ to get a feeling for where\nthe project is headed. **Your input is welcome!**\n\n\nSupporting\n----------\n\nIf you use Fatiando in your research, please **cite** it in your publications as:\n\n    Uieda, L., V. C. Oliveira Jr, and V. C. F. Barbosa (2013), Modeling the\n    Earth with Fatiando a Terra, Proceedings of the 12th Python in Science\n    Conference, pp. 91 - 98.\n\nPlease also **cite the method papers** of individual functions/classes.\nReferences are available in the documentation of each module/function/class.\n\nSee the `CITATION.rst\n<https://github.com/fatiando/fatiando/blob/master/CITATION.rst>`__ file or the\n`Citing section <http://www.fatiando.org/cite.html>`__ of the docs for more\ninformation.\n\nYou can also show your support by buying a **sticker** `from Stickermule \n<https://www.stickermule.com/marketplace/16580-fatiando-a-terra>`__.\nWe don't make any money from the sales but it helps spread the word about the \nproject.\n\nLicense\n-------\n\nFatiando a Terra is free software: you can redistribute it and/or modify it\nunder the terms of the **BSD 3-clause License**. A copy of this license is\nprovided in `LICENSE.txt`.\n",
        "createdAt": "2013-01-30T16:57:15.000Z",
        "updatedAt": "2025-12-05T01:06:15.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.157746",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.157746",
            "dataCite": "10.5281/zenodo.157746",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/fatiando/fatiando/master/README.rst",
        "mainPaper": {
            "doi": "10.5281/zenodo.157746",
            "title": "Fatiando a Terra v0.5: Modeling and inversion in geophysics",
            "journal": "Zenodo",
            "dateReleased": "2016-09-28T00:00:00.000Z",
            "abstract": "Fatiando a Terra (Portuguese for Slicing the Earth) is an open-source Python toolkit for modeling and inversion in geophysics. It provides an easy and flexible way to perform and implement geophysical data analysis. Official website and documentation: http://www.fatiando.org Main changes to version 0.5: http://www.fatiando.org/changelog.html#version-0-5",
            "citationsArray": []
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "muh-akhadi/global-seismology-course",
        "url": "https://github.com/muh-akhadi/global-seismology-course",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# global-seismology-course",
        "createdAt": "2025-01-15T05:29:09.000Z",
        "updatedAt": "2025-01-15T05:35:39.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/muh-akhadi/global-seismology-course/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Fran89/Earthworm_Ansible",
        "url": "https://github.com/Fran89/Earthworm_Ansible",
        "description": "Ansible playbook for earthworm",
        "stars": 0,
        "forks": 0,
        "readme": "# Ansible for earthworm\n\nAnsible playbook for deploying an earthworm earthworm. It will leave the system\nin running an earthworm instance in a byobu terminal of user earthworm. The user\npassword is: ewpassword\n\nThis can be configured in the file:\n\n    /group_vars/earthworm\n\n## Edits for accessing the machine:\n\nIn the hosts file add your hosts under the [earthworm] group:\n\n    [earthworm]\n    #ec2-18-207-222-232.compute-1.amazonaws.com\n\nIn the EW\\_FULL\\_SYS.yml change the remote user to a user with root rights:\n\n    remote_user: centos\n\n## Editing your earthworm install:\n\nYou can change your earthworm install variables in the following file:\n\n    /group_vars/earthworm\n\n## Editing your earthworm configuraton.\n\nUpload it to a git repository and change it in:\n\n    /group_vars/earthworm\n\n",
        "createdAt": "2019-06-03T20:15:49.000Z",
        "updatedAt": "2019-06-12T18:29:43.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Fran89/Earthworm_Ansible/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "udiy/seismology",
        "url": "https://github.com/udiy/seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2020-05-01T15:33:01.000Z",
        "updatedAt": "2020-05-04T17:06:38.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Hunter-Github/GitScience",
        "url": "https://github.com/Hunter-Github/GitScience",
        "description": "A curated list of science- and engineering related repositories on GitHub and in neighboring counties",
        "stars": 21,
        "forks": 2,
        "readme": "# GitScience\nA list of science- and engineering related repositories on GitHub and in neighboring counties\n\n## Contents\n\n#### Ocean\n\n * [Acoustics (underwater)](https://github.com/Hunter-Github/GitScience#acoustics-underwater)\n * [Oceanography in general](https://github.com/Hunter-Github/GitScience#oceanography-general)\n\n#### Earth\n\n* [Seismology](https://github.com/Hunter-Github/GitScience#seismology)\n\n#### Air\n\n * [Meteorology](https://github.com/Hunter-Github/GitScience#meteorology)\n\n#### Space\n\n * [Orbital mechanics](https://github.com/Hunter-Github/GitScience#orbital-mechanics)\n * [Astronomy](https://github.com/Hunter-Github/GitScience#astronomy)\n\n#### Technology\n\n * [Radio](https://github.com/Hunter-Github/GitScience#radio)\n * [Optics](https://github.com/Hunter-Github/GitScience#optics)\n * [Nuclear technology](https://github.com/Hunter-Github/GitScience#nuclear-technology)\n * [Chemistry](https://github.com/Hunter-Github/GitScience#chemistry)\n\n### Acoustics (underwater)\n\n* [Acoustic Toolbox](http://oalib.hlsresearch.com/Modes/AcousticsToolbox/) MatLab, Fortran\n    * ... [ACT](http://cmst.curtin.edu.au/products/actoolbox.cfm) - GUI for the Acoustic Toolbox\n* [cTraceo](https://github.com/EyNuel/cTraceo) Does not require MatLab\n\n### Oceanography (general)\n\n * [OCE for R](https://github.com/dankelley/oce) - a smorgasbord of useful commands to process oceanographic datasets\n\n### Radio\n\n* SPLAT!\n    * ... [Signal-Server](https://github.com/Cloud-RF/Signal-Server)\n    * ... [splat](https://github.com/jmcmellen/splat)\n* VOACAP\n    * ... [voacapl](https://github.com/jawatson/voacapl) - a port to Linux\n* ITU-R models\n    * ... [propagation](https://github.com/deepaknadig/propagation)\n* [QRadioPredict](http://qradiopredict.sourceforge.net/)\n\n### Orbital mechanics\n\n* [NASA's GMAT](http://gmat.sourceforge.net/)\n* [SGP4 and SDP4 ported to Javascript](https://github.com/shashwatak/satellite-js) - runs nicely on any system with a browser or with a Node.js engine.\n* [SPICE](http://naif.jpl.nasa.gov/naif/toolkit.html) - many routines to use in space operations and astronomy.\n    * ... [in C](http://naif.jpl.nasa.gov/naif/toolkit_C.html)\n    * ... [in Fortran](http://naif.jpl.nasa.gov/naif/toolkit_FORTRAN.html)\n    * ... [bindings for Python (PySPICE)](https://github.com/rca/PySPICE)\n* [Orekit](http://orekit.org/)\n* [benelsen/spacetrack SpaceTrack API](https://github.com/benelsen/spacetrack) - a Node.js wrapper for [SpaceTrack](https://www.space-track.org) (USSTRATCOM) [TLE](https://en.wikipedia.org/wiki/Two-line_element_set) API.\n* [benelsen/orb](https://github.com/benelsen/orb) - helper JavaScript routines for orbital mechanics problems.\n* [The Primary Repository for code relating to Icarus Interstellar's Project Voyager](https://github.com/zachfejes/ProjectVoyager)\n* [Trajectory optimization tool](https://github.com/Arrowstar/ksptot) - multiple gravity assist planning code.\n* [virtual Apollo Guidance Computer](https://github.com/rburkey2005/virtualagc) - awesome simulation of real-life Apollo Guidance Computer by Ron Burkey.\n* [Basilisk](http://hanspeterschaub.info/bskMain.html) (hat tip to [ChrisR](https://space.stackexchange.com/users/1391/chrisr)) a modular C/C++ astrodynamics simulation framework with Python scripting. Likely the highest fidelity attitude control simulation, algorithms used in ADCS of the upcoming EMM mission.\n* [smd](https://github.com/ChristopherRabotin/smd) - (hat tip to [ChrisR](https://space.stackexchange.com/users/1391/chrisr)) a mission propagator for continuous thrusting via way-point targeting. Can also be used for statistical orbital determination given range and range-rate information. Written in Go.\n* [poliastro](https://github.com/poliastro/poliastro/) (hat tip to [ChrisR](https://space.stackexchange.com/users/1391/chrisr)) a set of Python routines for astrodynamics with an emphasis on interplanetary mission design.\n\n### Seismology\n\n* [SPECFEM-3D](https://github.com/geodynamics/specfem3d)\n* [SPECFEM-3D-Globe](https://github.com/geodynamics/specfem3d_globe)\n* [SPECFEM-2D](https://github.com/geodynamics/specfem2d)\n* [SEM2DPACK](http://sem2d.sourceforge.net)\n\n### Meteorology\n\n* [PyAOSlib](https://github.com/PyAOS/aoslib) - auxiliary routines in Fortran and Python\n\n### Optics\n\n* [poppy](https://github.com/mperrin/poppy)\n* [Py6S](https://github.com/robintw/Py6S) - interface to 6S radiative transfer codes\n\n### Nuclear technology\n\n* [Method of characteristics](https://github.com/mit-crpg/OpenMOC) - nuclear reactor physics calculations from MIT.\n* [Models for nuclear reactor benchmarks](https://github.com/mit-crpg/benchmarks) - another repo from the same MIT group.\n\n### Astronomy\n\n* [NTL Asteroid Data Hunter](https://github.com/nasa/NTL-Asteroid-Data-Hunter) - an asteroid search program with a neat GUI \n* [Binary classification of RR (AB) stars](https://github.com/johnh2o2/rrlyrclassification)\n* [Tkinter program to visualize lightcurves](https://github.com/johnh2o2/pyvislc)\n* [A code to compute the abundances of chemical species in the interstellar medium](https://github.com/smaret/astrochem) - software written by Sébastien Maret.\n* [SunPy](https://github.com/sunpy/sunpy) - Python for solar physics.\n\n### Chemistry\n\n* [GROMACS](https://github.com/gromacs/gromacs) - a molecular simulation toolkit, main site [here](http://www.gromacs.org/).\n\n",
        "createdAt": "2015-02-28T09:16:48.000Z",
        "updatedAt": "2024-07-30T01:33:36.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Hunter-Github/GitScience/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "csgerg/isc-scripts",
        "url": "https://github.com/csgerg/isc-scripts",
        "description": "Seismology related examples written in python.",
        "stars": 1,
        "forks": 0,
        "readme": "# isc-scripts\nSeismology related examples written in python.\n\nUseful links to start:\n\n### Official documentation (basics, good to know):\n\n-   [Python 3.x.x documentation](https://docs.python.org/3/)\n-   [The Python Standard Library](https://docs.python.org/3/library/index.html)\n\n    -   [Built-in Functions](https://docs.python.org/3/library/functions.html)\n    -   [Built-in Constants](https://docs.python.org/3/library/constants.html)\n    -   [Built-in Types](https://docs.python.org/3/library/stdtypes.html), [Data Structures](https://docs.python.org/3/tutorial/datastructures.html)\n    -   [Built-in Exceptions](https://docs.python.org/3/library/exceptions.html)\n-   [More Control Flow Tools](https://docs.python.org/3/tutorial/controlflow.html), [Compound statements](https://docs.python.org/3/reference/compound_stmts.html#while)\n-   [Virtual Environments and Packages](https://docs.python.org/3/tutorial/venv.html)\n-   [Style Guide for Python Code](https://www.python.org/dev/peps/pep-0008/)\n-   [Python Data Analysis Library](https://pandas.pydata.org/)\n-   [Matplotlib](https://matplotlib.org/)\n-   [ObsPy](https://github.com/obspy/obspy/wiki)\n-   [obspyDMT](https://github.com/kasra-hosseini/obspyDMT)\n-   [Full package list](https://pypi.org/)\n\n### Courses, tutorials, examples, apps:\n\n-   Sites:\n    -   [realpython.com - easy to understand, very good examples (start here)](https://realpython.com/python-introduction/)\n    -   <https://learnpythonthehardway.org/book/>\n-   Interactive (online coding+videos):\n    -   [Intro to Computer Science](https://eu.udacity.com/course/intro-to-computer-science--cs101)\n    -   [Programming Foundations with Python](https://eu.udacity.com/course/programming-foundations-with-python--ud036)\n    -   [Intro to Data Analysis](https://eu.udacity.com/course/intro-to-data-analysis--ud170)\n-   Videos:\n    -   [Learn Python - Full Course for Beginners (4 hours fast boot:)](https://youtu.be/rfscVS0vtbw)\n-   Apps:\n    -   [Enki](https://play.google.com/store/apps/details?id=com.enki.insights&hl=en_GB)\n    -   [Mimo](https://play.google.com/store/apps/details?id=com.getmimo&hl=en_GB)\n-   Books:\n    -   [Packt](https://www.packtpub.com//packt/offers/free-learning)\n    -   [Books](https://codeburst.io/15-free-ebooks-to-learn-python-c299943f9f2c)\n    \n### Free Development tools:\n\n-   IDES:\n    -   [PyCharm Community](https://www.jetbrains.com/pycharm/download/#section=linux)  (for beginners)\n    -   [VS Code](https://code.visualstudio.com/)\n\n        -   [Python Extension Pack](https://marketplace.visualstudio.com/items?itemName=donjayamanne.python-extension-pack)\n        -   [Highlight Matching Tag](https://marketplace.visualstudio.com/items?itemName=vincaslt.highlight-matching-tag)\n        -   [Auto Close Tag](https://marketplace.visualstudio.com/items?itemName=formulahendry.auto-close-tag)\n        -   [Auto Rename Tag](https://marketplace.visualstudio.com/items?itemName=formulahendry.auto-rename-tag)\n    -   [Spyder Scientific Python IDE](https://www.spyder-ide.org/)\n    -   [Atom](https://atom.io/)\n    -   [IDEs Versus Text Editors](https://www.datacamp.com/community/tutorials/data-science-python-ide)\n    -   [Top IDE index](http://pypl.github.io/IDE.html)\n    -   [https://www.pylint.org/](http://pylint/)\n    -   [Pycharm Professional Free for open souce projects](https://www.jetbrains.com/buy/opensource/)\n-   SQL Editors, browsers:\n    -   [https://dbeaver.io](https://dbeaver.io/)\n    -   [https://popsql.io](https://popsql.io/)\n    -   [https://teamsql.io](https://teamsql.io/)\n    -   <https://www.oracle.com/technetwork/developer-tools/sql-developer/downloads/index.html> (free registration needed)\n    -   [http://www.squirrelsql.org](http://www.squirrelsql.org/)\n",
        "createdAt": "2019-01-15T15:31:55.000Z",
        "updatedAt": "2020-10-07T01:08:59.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/csgerg/isc-scripts/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "rzamoramx/reader_rss_ssn",
        "url": "https://github.com/rzamoramx/reader_rss_ssn",
        "description": "Reader for RSS of SSN (National Seismological Service) Mexico",
        "stars": 0,
        "forks": 0,
        "readme": "# reader_rss_ssn\nReader for RSS of SSN (National Seismological Service) Mexico. The data is saved in a Sqlite file\n\nThis little project is written in #vlang\n",
        "createdAt": "2020-10-26T03:39:54.000Z",
        "updatedAt": "2021-07-06T16:03:20.000Z",
        "language": "V",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/rzamoramx/reader_rss_ssn/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Luchorigami/mleetas",
        "url": "https://github.com/Luchorigami/mleetas",
        "description": "A python package to use the ETAS model (Seismology)",
        "stars": 6,
        "forks": 0,
        "readme": "![event](logo_mleetas.png)\n# Package MleETAS \n( Maximum Likelyhood Estimation of ETAS and ETASI )\n\nA python package to fit and simulate the Epidemic Type Aftershock Sequence (ETAS) model in seismology (Ogata, 1988). \nAlso include routines for the ETASI model of Hainzl (2021) (i.e.; take into account short term incompletness)\n\nThe parameter space estimation rely on the maximization of a likelyhood function trough scipy's L-BFGS-B optimization routine.\n\n## Overview\nThe mleetas package contain 3 modules:\n\n1. **mleetas.etas**:\n    * A module to fit the classic ETAS model. 5 parameters: (A,c,p,al,mu)\n3. **mleetas.etasi**:\n    * A module to fit the ETASI model. A modified version of the classic ETAS to take into account a rate dependent incompletness effect (Hainzl; 2021). 7 parameters: (A,c,p,al,mu,b,Tb)\n5. **mleetas.simulation**:\n    * A module to generate synthetics ETAS and ETASI catalogs, stationary background catalogs, Gutenberg-Richter magnitude distribution and more.\n\nfor more detail, refer to the code documentation inside functions and the 2 example python file in the dir. \"example/\"\n\n## Package installation\nDownload the repo wherever you like (Under the directory name mleetas/)\n\n    git clone https://github.com/Luchorigami/mleetas.git\n\nInstall the package with pip\n\n    pip install mleetas/\n\n## Example\nYou can find examples to simulate and fit etas with the MleETAS package in the example dir.\n\nNote: The L-BFGS-B Optimization often end with *ABNORMAL_TERMINATION_IN_LNSRCH*. This is probably due to numerical precision errors and likely require a fine tuning of the L-BFGS-B's parameters. Even with this unusual termination, we can confidently interpret the outputed ETAS(I) estimates because such numerical errors seems to happen very close to the convergence criterion. In other words: don't worry but make syntetic tests to be sure that the inversion recover approximatively the same parameters as the one used to generate the synthetic catalogs.  \n\n## References\n\n- Hainzl, S., 2021. ETAS-Approach Accounting for Short-Term Incompleteness of Earthquake Catalogs. Bulletin of the Seismological Society of America. https://doi.org/10.1785/0120210146\n\n- Moutote, L., Itoh, Y., Lengliné, O., Duputel, Z., Socquet, A., 2023. Evidence of a Transient Aseismic Slip Driving the 2017 Valparaiso Earthquake Sequence, From Foreshocks to Aftershocks. Journal of Geophysical Research: Solid Earth 128, e2023JB026603. https://doi.org/10.1029/2023JB026603\n\n- Moutote, L., Marsan, D., Lengliné, O., Duputel, Z., 2021. Rare occurrences of non‐cascading foreshock activity in Southern California. Geophys Res Lett 48, e2020GL091757. https://doi.org/10.1029/2020GL091757\n\n- Zhuang, J., Harte, D., Werner, M.J., Hainzl, S., Zhou, S., 2012. Basic models of seismicity: Temporal models. Community Online Resource for Statistical Seismicity Analysis Theme V.\n\nLuc Moutote\n\n",
        "createdAt": "2023-12-13T09:59:40.000Z",
        "updatedAt": "2025-11-11T11:45:55.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Luchorigami/mleetas/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "shicks-seismo/ATTENpy",
        "url": "https://github.com/shicks-seismo/ATTENpy",
        "description": "Local earthquake attenuation solver (t* inversion) integrated into the ObsPy ecosystem.",
        "stars": 5,
        "forks": 3,
        "readme": "# ATTENpy\n## A Python package for computing body-wave seismic attenuation from local earthquake catalogues.\n\nATTENpy provides an efficient, multi-parallel routine for computing t\\* measurements for local earthquake\nattenuation tomography.\n",
        "createdAt": "2020-07-01T07:01:12.000Z",
        "updatedAt": "2025-05-05T06:59:15.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/shicks-seismo/ATTENpy/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "juliagao3/uq-nppc-seismology",
        "url": "https://github.com/juliagao3/uq-nppc-seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# Uncertainty Quanitification of Seismic Imaging using Neural Posterior Principal Components (NPPC)\n\nThis work explores the application of a novel method NPPC to quantify uncertainties in seismic imaging, which predicts principal components of the posterior distribution in a single forward pass from earthquake measurements.\n\nThe eikonal_models folder contains the main code, while the scripts folder contains the scripts for running the model. Since NPPC uses a trained restoration model, the main code has both a restoration model and NPPC model.\n\n# Interactive Demo\n\nAn interactive demo *travel_time_nppc.ipynb* is added for demonstration. It is self contained notebook with results for the random geometry. It can be altered to use different datasets and geometries. \n\n# Requirements\n## Environment \nA yaml file is provided. Please install the dependencies by:\n```\nconda env create -f uq-nppc-travel-time.yaml\n```\n\n## Data\nPlease download the data via [this link](https://drive.google.com/drive/u/0/folders/1Aie7Nkv8ip1VQESC6K6auG8jJBgzlS9M).\n- **Two velocity datasets**:\n    - **0.3FixedGradGRFSamples100000_28x28_1_1.5.npy**: velocity in km/s, (nx, nz, N) = (28, 28, 100000)\n    - **0.3GradGRFSamples100000_28x28_1_1.5.npy**: velocity in km/s, (nx, nz, N) = (28, 28, 100000)\n    - Pixel spacing: dx, dz = 0.25 km\n      \n- **Folders**: {velocity dataset}_{source-receiver geometry}\n    - **{velocity dataset}_{rand}**\n    ![rand](SouRec_rand.png)\n    - **{velocity dataset}_{transmission}**\n    ![rand](SouRec_tran.png)\n        - SouPos.npy: Source pixel position, (nsrc, 2)\n        - RecPos.npy: Receiver pixel position, (nrec, 2)\n        - TT_0_to_N.npy: Travel time measurements in s, (nsrc, nrec, N) (N=100000, but you can use smaller Ns to start with)\n\nSee **visualize_data.ipynb** for reading and visualizing the data.\n\n# Usage \nTo run the code, use \n\n```bash\n./scripts/run_model_pipeline.sh \n```\nYou will be prompted to choose a specific dataset \"fixed_grad\" or \"grad\" and geometry \"rand\" or \"transmission\". The rest of the arguments are located within the run_model_pipeline.sh that can be updated. \n\n# References\nNPPC is adapted from:\n\nNehme, Elias, Yair, Omer, and Michaeli, Tomer. *Uncertainty Quantification via Neural Posterior Principal Components.* \n**arXiv:2309.15533**, 2023. NeurIPS 2023. \n[https://doi.org/10.48550/arXiv.2309.15533](https://doi.org/10.48550/arXiv.2309.15533)",
        "createdAt": "2025-11-17T07:39:05.000Z",
        "updatedAt": "2025-11-17T07:52:04.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/juliagao3/uq-nppc-seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "rbherrmann/ComputerProgramsSeismology",
        "url": "https://github.com/rbherrmann/ComputerProgramsSeismology",
        "description": null,
        "stars": 8,
        "forks": 3,
        "readme": "Computer Programs in Seismology is a package of programs for making synthetic seismograms and for the inversion of seismic data. these programs were developed over a period of 50 years at Saint  Louis University\n",
        "createdAt": "2024-11-13T17:53:08.000Z",
        "updatedAt": "2025-10-22T16:16:32.000Z",
        "language": "PostScript",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/rbherrmann/ComputerProgramsSeismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "kesmarag/grmot",
        "url": "https://github.com/kesmarag/grmot",
        "description": "Strong Ground Motion Simulation Library - Discrete Wavenumber",
        "stars": 8,
        "forks": 0,
        "readme": "# GRMOT - strong GRound MOTion simulations\n![img](./images/grmot_logo.png)\n\nWarning : the documentation of the library is still under preparation.\n\n## General description\n\nGRMOT generates simulated seismograms, including displacement, velocity, and accelerations, using the 3D discrete wavenumber representation method.\nThe code is designed for parallel execution, ensuring efficiency in large-scale simulations.\nThe library’s core is implemented in Rust for high performance,\nwhile a Python interface provides a user-friendly experience for researchers and engineers.\n\n## Citation\n[![DOI](https://zenodo.org/badge/527664867.svg)](https://doi.org/10.5281/zenodo.14900909)\n\nIf you use this project in your research or work, please cite it as follows:\n\n```bibtex\n@software{Smaragdakis_GrMot_2025,\nauthor = {Smaragdakis, Costas},\ndoi = {https://doi.org/10.5281/zenodo.14900909},\nmonth = feb,\ntitle = {{GrMot}},\nurl = {https://github.com/kesmarag/grmot},\nversion = {v0.9.0},\nyear = {2025}\n}\n```\n\n## Installation\n\nThe GrMot library supports GNU/Linux and requires Python 3.8 or later. To install the appropriate version for your Python environment, run:\n\n```bash\npip install $(python -c \"import sys; version=f'{sys.version_info.major}{sys.version_info.minor}'; print(f'https://github.com/kesmarag/grmot/raw/main/target/wheels/grmot-0.9.0-cp{version}-cp{version}-manylinux_2_17_x86_64.manylinux2014_x86_64.whl')\")\n```\n\n## Using GRMOT\nFirst, we create a reference sub-plane. The following parameters determine this sub-plane\n\n- The top centre point of the fault (north ((x_0) in km), east ((y_0) in km), and depth ((z_0) in km)) with respect to a general reference point.\n- The dip, strike and rake angles.\n\n![img](./images/img1_grmot.jpg)\n\nNext, we divide the fault into multiple subfaults (see rectangle ABCD) and define their properties as follows:\n\n- Center coordinates relative to the reference fault point (in km).\n- Dimensions: Length and width of each subfault (km).\n- Rupture characteristics: Rupture velocity and orientation of the rupture front.\n- Rupture timing: A piecewise linear function defining the rupture onset at each subfault.\n\n![img](./images/rupture.jpg)\n\nThe library provides three environmental setups as it is shown in the following image. In parenteses given the density, velocities of p- and s-waves and thickness for each medium.\n![img](./images/img3_grmot.png)\n\nThese parameters are crucial for simulating seismic wave propagation through different layers of the Earth's crust.\n\nI. A halfspace\n\nCreate a python tupple as follows:\n```python\nmedium = ((rho_1, alpha_1, beta_1, 0),) # with 0 we mean halfspace (infinite thinkness)\n```\n\n\nII. A layer over a halfspace \n\nNote: There are bugs in this setup, and we are currently working on fixing them.\nFor the time being, in the case of two layers, use the third setup (III.) with two identical upper media, each with a thickness of h_1/2.\n\n```python\nmedium = ((rho_1, alpha_1, beta_1, h_1),\n          (rho_2, alpha_2, beta_2, 0),) \n```\n\nIII. Two layers over a halfspace\n\n\n```python\nmedium = ((rho_1, alpha_1, beta_1, h_1),\n          (rho_2, alpha_2, beta_2, h_2),\n          (rho_3, alpha_3, beta_3, 0),) \n```\n\nFollowing a test case that implements the final example (Fig. 6) from the paper **Discrete Wave Number Representation of Elastic Wave Fields in Three Space Dimensions**, Journal of Geophysical Research, Vol. 84, No. B7, by Michael Bouchon.\n\n```python\nfrom grmot import Fault\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx_fault = 5.0\ny_fault = 0.0\nz_fault = 1.0\nx_receiver, y_receiver = 7.0, 1.0\n\nsources = [((3, 10, 0, 0, 2, 270 * np.pi / 180), [(0, 1.0)])]\n\nangles = (90.0 * np.pi / 180.0, 0.0 * np.pi / 180.0, 180.0 * np.pi / 180.0)\nfpars = (1 / 10, 5)\n\nmedium = ((2.4, 2.5, 1.4, 0.5), (2.4, 2.5, 1.4, 0.5), (2.8, 5.0, 2.8, 0))\n\nrvel = 0\nconf = (300, 300, 200.0, 200.0, 1.0)\nloc = (x_fault, y_fault, z_fault)\nfault = Fault(angles, loc, fpars, medium, conf)\n\nreceivers = [(x_receiver, y_receiver)]\n\nnorth, east, vertical, _, _, _, _, _, _ = fault.simulate(sources, receivers, 8192)\n\nt = np.linspace(0, 1 / fpars[0], 8192)\n\n# Vertical Displacement Plot\nplt.figure(figsize=(10, 3))\nplt.plot(t, vertical[0],'black')\nplt.ylabel('Vertical Displacement [m]')\nplt.xlabel('time [s]')\nplt.title(\"Vertical Displacement at Receiver\")\nplt.show()\n\n# North Displacement Plot\nplt.figure(figsize=(10, 3))\nplt.plot(t, north[0],'black')\nplt.ylabel('North Displacement [m]')\nplt.xlabel('time [s]')\nplt.title(\"North Displacement at Receiver\")\nplt.show()\n\n# East Displacement Plot\nplt.figure(figsize=(10, 3))\nplt.plot(t, east[0],'black')\nplt.ylabel('East Displacement [m]')\nplt.xlabel('time [s]')\nplt.title(\"East Displacement at Receiver\")\nplt.show()\n```\n\n![img](./images/vertical1.png)\n\n![img](./images/north1.png)\n\n![img](./images/east1.png)\n\n## Approximation of an Elliptical Crack\n\nThis function `approx_elliptical_crack` approximates an elliptical rupture on a fault by considering a set of rectangular sub-faults. The rupture nucleates at an internal point of an elliptical crack and propagates in a self-similar manner. The instantaneous elliptical rupture front moves toward the crack barrier at a constant velocity.\n\nThis elliptical kinematic rupture model was first introduced by Burridge and Willis. Here, we approximate the rupture using rectangular sub-faults.\n\n\n\n## Function Signature\n```python\n# L, W, dl, radius_xi, radius_eta, xi, eta, coef, delay, nxi, neta, vr, code  \napprox_elliptical_crack(crack_params)\n```\n\n\n## Parameters\n- `L` (float): Length of the fault.\n- `W` (float): Width of the fault.\n- `dl` (float): Grid spacing for discretization.\n- `radius_xi` (float): ξ-axis radius of the elliptical crack.\n- `radius_eta` (float): η-axis radius of the elliptical crack.\n- `xi` (float): ξ-coordinate of the center of the elliptical crack.\n- `eta` (float): η-coordinate of the center of the elliptical crack.\n- `coef` (float): Scaling factor.\n- `delay` (float): Initial time delay of rupture.\n- `nxi` (float): ξ-direction of the nucleation point.\n- `neta` (float): η-direction of the nucleation point.\n- `vr` (float): Rupture velocity (km/s).\n- `code` (str): Unique identifier for the fault model.\n\n## Returns\n- `source_i` (list): List of rupture details.\n- `m0it` (float): Total moment release.\n- `maxslip` (numpy array): Maximum slip distribution.\n- `ruptvel` (numpy array): Rupture velocity distribution.\n- `theta0` (numpy array): Initial rupture angle distribution.\n- `code` (str): Fault model identifier.\n\nThe following image illustrates the key parameters of the approx_elliptical_crack function.\n\n![img](./images/efault.png)\n\nThe nucleation point is denoted by the white dot.\n\n## Test Case: Synthetic Earthquake Simulation Near Samos Island, Greece\nWe aim to simulate a hypothetical earthquake on the same fault that ruptured during the 2020 Samos earthquake, evaluating its potential impact on Karlovasi main square.\n\nBelow is the map of the synthetic rupture, accompanied by the Python code that implements the simulation.\n![img](./images/map_samos.png)\n\n```python\nfrom grmot import approx_elliptical_crack, Fault, latlon_to_km\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nit = [24.0, 60.0, 0.5, 4, 12,  8,  -7,  0.6,  0, -1,  2, 2.0, 'Crack']\n\nsource, m0, m, r, t, code = approx_elliptical_crack(it)\n\nlat_fault, lon_fault, z_fault = 37.822, 26.740, 1.0\nx_fault, y_fault = latlon_to_km(lat_fault, lon_fault)\n\nloc = (x_fault, y_fault, z_fault)\n\nangles = (50*np.pi/180., 276*np.pi/180., -90.*np.pi/180.)\nfpars = (1/40, 3.0) \nmedium = ((2.4, 3.7, 2.25, 0.5),\n          (2.5, 4.6, 2.7, 0.5),\n          (2.6, 5.4, 3.2, 0.0),)\n\nconf = (300, 300, 250, 250, 1.0)\n\nreceivers_db = {\n    'KARLOVASI_SQUARE': (37.7916, 26.7048)\n}\n\ndir_name = './samos_hyp'\nfor receiver_name in receivers_db:\n    receivers=[]\n    x_receiver, y_receiver = latlon_to_km(receivers_db[receiver_name][0], receivers_db[receiver_name][1])\n    receivers.append((x_receiver, y_receiver))\n    dn,de,dv,vn,ve,vv,an,ae,av = fault.simulate(source, receivers, 2048)\n    np.savez(dir_name + '/' + receiver_name + '_' + code + '.npz', \n              dn=dn[0], de=de[0], dv=dv[0], \n              vn=vn[0], ve=ve[0], vv=vv[0], \n              an=an[0], ae=ae[0], av=av[0], \n              m0=m0)\n\n# Load and analyze the simulated data\nkarlovasi = np.load('./samos_hyp/KARLOVASI_SQUARE_' + code + '.npz')\n\n# Calculate moment magnitude\nmw = 2 * np.log10(karlovasi['m0'] * 10**7) / 3 - 10.7\n\nt = np.linspace(0,40,2048)\n\n# Plot results\nplt.figure(figsize=(10, 5))\nplt.subplot(2, 1, 1)\nplt.plot(t, 100*karlovasi['dv'], label='Vertical Displacement (cm)', color='#3548cf')\nplt.legend()\nplt.ylabel('Displacement (cm)')\n\nplt.subplot(2, 1, 2)\nplt.plot(t, 100*karlovasi['vv'], label='Vertical Velocity (cm/s)', color='#3548cf')\nplt.legend()\nplt.xlabel('Time (s)')\nplt.ylabel('Velocity (cm/s)')\n\nplt.suptitle(f'Simulated Earthquake (Mw = {mw:.2f})')\nplt.savefig('simulated.png')\n\n```\n\n\n![img](./images/simulated.png)\n\n## Undocumented Functions \n\nThe library includes several undocumented functions. Please refer to the source code for details on these functions.\n\nhttps://github.com/kesmarag/grmot/blob/main/grmot/utils.py\n\n## License\nGRMOT is distributed as free software (GPL-v3) in the hope that it will useful, but without any warranty.\n\n## Acknowledgements\nI would like to acknowledge Professor Apostolos Papageorgiou for his guidance and valuable advice. \n\n",
        "createdAt": "2022-08-22T17:23:56.000Z",
        "updatedAt": "2025-10-21T05:37:06.000Z",
        "language": "Rust",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.14900909",
            "openAlex": "10.5281/zenodo.14900909",
            "openCitations": "10.5281/zenodo.14900909",
            "dataCite": "10.5281/zenodo.14900909",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/kesmarag/grmot/main/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.14900909",
            "title": "GrMot",
            "journal": "Zenodo",
            "dateReleased": "2025-02-20T00:00:00.000Z",
            "abstract": "",
            "citationsArray": []
        },
        "repoDoi": "10.5281/zenodo.14900909",
        "publications": [
            {
                "doi": "10.5281/zenodo.14900909",
                "name": "GrMot",
                "source": "",
                "authorNames": [],
                "abstract": "",
                "publicationDate": "2025-02-20T00:00:00.000Z"
            }
        ]
    },
    {
        "source": "GitHub",
        "name": "YuanYusung/Single-Station-TeleSeismic-Localization",
        "url": "https://github.com/YuanYusung/Single-Station-TeleSeismic-Localization",
        "description": "Using a single three-component station to determine the back azimuth and incident angle of teleseismic events, and combining with P-wave and S-wave arrival times for inversion to achieve earthquake location determination with a single station.",
        "stars": 0,
        "forks": 0,
        "readme": "# Single-Station-TeleSeismic-Localization\nUsing a single three-component station to determine the back azimuth and incident angle of teleseismic events, and combining with P-wave and S-wave arrival times for inversion to achieve earthquake location determination with a single station.\n",
        "createdAt": "2025-11-05T09:34:30.000Z",
        "updatedAt": "2025-11-27T11:17:51.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/YuanYusung/Single-Station-TeleSeismic-Localization/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Ticny/Seismology",
        "url": "https://github.com/Ticny/Seismology",
        "description": "Seismology package allows to analyze data from seismic catalogs",
        "stars": 0,
        "forks": 0,
        "readme": "Seismology: a demo package\n===============================================\n\nThis is a demo package from R4DS course.\nThis package allows for the manipulation and presentation of seismic catalog data, which can be separated by years and months for greater ease. \n\n- e-mail: seddav2@gmail.com\n\nFunctions\n---------\n- SepYM:\tSeparate the year and month\n- Depsism_Y:\tDotplot with month, depth and magnitude\n- MeanS4YM:\tMean seismic magnitude for year and month\n- MeanSY:\tMean seismic magnitude for year\n\nDescription of variables \n------------------------\nWe work with a catalog with the vatiables: \n\n- time_value: Year, month, day and time of recording of a seismic event \n- depth_value: Depth (Km) of the seismic event\n- magnitude_value_P: P-wave magnitude of the event \n\nInstallation and loading\n------------------------\n\n-   Install the latest version from [GitHub](https://github.com/Ticny/Seismology) as follow:\n\n```r\n# Install\n#> Loading required package: ggplot2, dplyr and magrittr\nif(!require(devtools)) install.packages(\"devtools\")\nif(!require(dplyr)) install.packages(\"dplyr\")\nif(!require(ggplot2)) install.packages(\"ggplot2\")\nif(!require(magrittr)) install.packages(\"magrittr\")\ndevtools::install_github(\"Ticny/Seismology\")\n```\n```r\nlibrary(Seismology)\n```\n\n\n\n",
        "createdAt": "2023-03-31T16:32:53.000Z",
        "updatedAt": "2023-09-13T13:17:41.000Z",
        "language": "R",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Ticny/Seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "afeborgeaud/dsmpy",
        "url": "https://github.com/afeborgeaud/dsmpy",
        "description": "Python package for computation of synthetic waveforms using the Direct Solution Method (DSM)",
        "stars": 12,
        "forks": 3,
        "readme": "# dsmpy\n[![Python 3.7](https://img.shields.io/badge/python-3.7-blue.svg)](https://www.python.org/downloads/release/python-370/)\n[![PyPI version fury.io](https://d25lcipzij17d.cloudfront.net/badge.svg?id=py&type=6&v=1.0a4&x2=0)](https://test.pypi.org/project/dsmpy/)\n\nPython package for computation of synthetic waveforms in spherically homogeneous transversely isotropic (VTI) media using the Direct Solution Method (DSM; [Kawai et al. 2006](https://doi.org/10.1111/j.1365-246X.2005.02829.x)).<br/>\nThe original DSM softwares can be found on the [Github page of UT Global Seismology](https://github.com/UT-GlobalSeismology).<br/><br/>\nThe documentation for dsmpy with several usage examples can be found [here](https://afeborgeaud.github.io/dsmpy/).\n\n# For Linux users\nAt installation, dsmpy needs to compile Fortran and C librairies, and needs openmpi for parallel computing.\nCurrently, dsmpy has been tested with gcc.\n\n# For Windows users\nAt installation, dsmpy needs to compile Fortran and C librairies, and needs openmpi for parallel computing.\nThe installation on Windows machines has been tested on [WSL](https://learn.microsoft.com/en-us/windows/wsl/install)\n\nA quick summary to set up the environment tested:\n\n1) Open a PowerShell (in admin mode)\n\n2) In the PowerShell, type\n```\nwsl --install\n```\nThis should install a ubuntu distribution by default, and launch a Ubuntu bash terminal.\n\n(Optional) to start the ubuntu terminal again after closing the PowerShell, open a new PowerShell and type\n```\nubuntu\n```\n\n3) From the Ubuntu terminal, install python, gcc and openmpi\n```\nsudo apt-get update && apt-get install -y python3 python3-pip\nsudo apt install python-is-python3\nsudo apt-get install gcc\nsudo apt-get install -y openmpi-bin libopenmpi-dev\n```\n\n4) Create a directory for your python project (here we assume you have a ```git``` folder in your home directory), and open it in Visual Studio Code\n```\ncd ~/git\nmkdir my_project\n# open my_project in VS Code\ncode my_project\n```\nYour are ready to proceed to the installation of dsmpy\n\n\n# INSTALLATION\n## Prefererd method: install using pip from github\n1) (Optional) You may want to install dsmpy in the virtual environment [env_name]. If so, from your project in a terminal:\n```\npython3 -m venv .venvs/[env_name]\nsource .venvs/bin/[env_name]/activate\n```\n2)\n```\npip install dsmpy@git+https://github.com/afeborgeaud/dsmpy@v1.0a5\n```\n\n## Build from source using pip\n1) Clone the dsmpy repository\n```\ngit clone https://github.com/afeborgeaud/dsmpy\n```\n2) (Optional) You may want to install dsmpy in a virtual environment. If so, do\n```\npython3 -m venv venv\nsource ./venv/bin/activate\n```\n3) Install requirements\n```\npython3 -m pip install -r requirements.txt\n```\n5) Install [*build*](https://pypi.org/project/build/), a PEP517 package builder\n```\npip install build\n```\n4) To build the dsmpy package, from the root directory ```dsmpy``` run\n```\npython -m build .\n```\n5) This creates ```.whl``` and ```.gz.tar``` dist files in the ```dist``` directory. Now pydsm can be installed with\n```\npip install dist/*.whl\n```\nor\n```\npip install dist/*.tar.gz\n```\n\n# EXAMPLES\n1) Running dsmpy using an input file (run on multiple CPUs).\nA template input file is in ```<path_of_pydsm_folder>/dsmpy/tests/input_files/template.txt```:\n```shell\nsac_files ~/git/dsmpy/tests/sac_files/*T\noutput_folder ~/git/dsmpy/tests/sac_files\n# duration of synthetics (in seconds)\ntlen 3276.8\n# number of points of frequency-domain synthetics\n# minimum period Tmin = tlen / nspc (s)\nnspc 256 \n# sampling frequency for time-domain synthetics\nsampling_hz 20\n# prem, ak135\nseismic_model prem \n# 0: P-SV+SH, 1: P-SV, 2: SH (default: 0)\nmode 0\n# 0: quiet, 1: talkative, 2: debug (default: 0)\nverbose 0\n```\n\nTo run this input file on 2 CPUs:\n1) open a Terminal \n2) change the current directory to the dsmpy directory\n3) paste:\n```shell\nmpiexec -n 2 python dsmpy/main.py tests/input_files/template.txt\n```\n\n2) Running dsmpy from a python script.\nBelow is an example of python script using dsmpy to compute synthetics:\n```python\nfrom dsmpy import dsm, seismicmodel\nfrom dsmpy.event import Event\nfrom dsmpy.station import Station\nfrom dsmpy.utils.cmtcatalog import read_catalog\n# load gcmt catalog\ncatalog = read_catalog()\n# get event from catalog\nevent = Event.event_from_catalog(\n    catalog, '200707211534A')\n# define station FCC\nstations = [\n    Station(\n        name='FCC', network='CN',\n        latitude=58.7592, longitude=-94.0884), \n    ]\n# load (anisotropic) PREM model\nseismic_model = seismicmodel.SeismicModel.prem()\ntlen = 3276.8 # duration of synthetics (s)\nnspc = 256 # number of points in frequency domain\nsampling_hz = 20 # sampling frequency for sythetics\n# create input parameters for pydsm\ninput = dsm.PyDSMInput.input_from_arrays(\n    event, stations, seismic_model, tlen, nspc, sampling_hz)\n# compute synthetics in frequency domain calling DSM Fortran\noutput = dsm.compute(input)\noutput.to_time_domain() # perform inverse FFT\noutput.filter(freq=0.04) # apply a 25 seconds low-pass filter\nus = output.us # synthetics. us.shape = (3,nr,tlen)\nts = output.ts # time points [0, tlen]\n# brackets can be used to access component and station\nu_Z_FCC = output['Z', 'FCC_CN']\n# to plot a three-component record section, use\noutput.plot()\nplt.savefig('output.png')\n# to write synthetics to SAC files, use\noutput.write(root_path='.', format='sac')\n```\n\n3) Running dsmpy using a (Fortran) DSM input file.\nMethods are provided to run dsmpy using an input file for the (Fortran) DSM:\n```python\nfrom pydsm import dsm, rootdsm_sh\nparameter_file = rootdsm_sh + 'AK135_SH.inf'\ninputs = dsm.PyDSMInput.input_from_file(parameter_file, mode=2)\noutputs = dsm.compute(inputs, mode=2)\noutputs.to_time_domain()\nus = outputs.us    # us.shape = (3,nr,tlen)\nts = outputs.ts    # len(ts) = tlen\nstations = outputs.stations        # len(stations) = nr\ncomponents = outputs.components    # len(components) = 3\n```\n",
        "createdAt": "2020-04-30T11:16:40.000Z",
        "updatedAt": "2024-11-12T05:13:41.000Z",
        "language": "HTML",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/afeborgeaud/dsmpy/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ds-modules/EPS-130",
        "url": "https://github.com/ds-modules/EPS-130",
        "description": "UC Berkeley EPS 130 (Strong Motion Seismology) Spring 2019",
        "stars": 0,
        "forks": 0,
        "readme": "# EPS-130\nEPS 130   Strong Motion Seismology\n",
        "createdAt": "2019-01-16T06:19:41.000Z",
        "updatedAt": "2025-07-07T21:27:08.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ds-modules/EPS-130/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "rita-png/solar-seismology",
        "url": "https://github.com/rita-png/solar-seismology",
        "description": "Code developed to study oscillating modes of the sun and infer properties of its interior.",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2022-11-12T16:14:42.000Z",
        "updatedAt": "2022-12-21T23:00:42.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Cuda-Chen/ms2rms",
        "url": "https://github.com/Cuda-Chen/ms2rms",
        "description": "Calculate standard deviation given a time interval of each trace.",
        "stars": 0,
        "forks": 0,
        "readme": "# ms2rms\nCalculate standard deviation given a time interval of each trace.\n\n# How to Compile\n1. Clone this repo by THIS COMMAND: `git clone https://github.com/Cuda-Chen/ms2rms.git`\n2. Type `make` to compile.\n\n## Dependencies\n- [libmseed](https://github.com/iris-edu/libmseed)\n\n# Usage\n```\n$ ./ms2rms [mseedfile] [time window size] [window overlap] [a|r|j]\n```\nWhere:\n- `time window size`: measured in seconds. It should always bigger than `0`.\n- `window overlap`: measured in percentage. It should always smaller than `100`.\n- `a|r|j`: indicate output file format.\n    - r: rms only\n    - j: json only\n    - a: rms and json\n\n# Output Format\n## .rms\n```\n<time stamp of the first window>,<station>,<network>,<channel>,<location>,<CR><LF>\n<time difference between this window to the first window>,<mean>,<SD>,<min>,<max>,<minDemean>,<maxDemean>,<CR><LF>\n<time difference between this window to the first window>,<mean>,<SD>,<min>,<max>,<minDemean>,<maxDemean>,<CR><LF>\n...\n```\n\n# Note\n- This program can ONLY accept single channel record.\nMultiple channels result in incorrect output.\n- The `rms` and `mean` value are rounded to hundrendth place.\n",
        "createdAt": "2020-02-14T01:50:23.000Z",
        "updatedAt": "2024-12-27T18:05:40.000Z",
        "language": "C",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Cuda-Chen/ms2rms/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "pyfinder-dev/paramws-clients",
        "url": "https://github.com/pyfinder-dev/paramws-clients",
        "description": "Python clients for parametric amplitude data web services (RRSM, ESM, EMSC, …) for seismology",
        "stars": 0,
        "forks": 0,
        "readme": "[![Run tests](https://github.com/pyfinder-dev/paramws-clients/actions/workflows/tests.yml/badge.svg)](https://github.com/pyfinder-dev/paramws-clients/actions/workflows/tests.yml)\n\n# paramws-clients\n\n## Table of Contents\n- [Introduction](#paramws-clients)\n- [Features](#features)\n- [Installation](#installation)\n- [Usage Examples](#usage-examples)\n- [Use Cases](#use-cases)\n- [API Reference](#api-reference)\n- [Testing](#testing)\n- [License](#license)\n- [Acknowledgment](#acknowledgment)\n\n**paramws-clients** is a collection of web service clients for the parametric data providers (ESM, EMSC, RRSM). It initially started as a part of the **pyfinder** package, which is a Python wrapper for the [FinDer](https://docs.gempa.de/sed-eew/current/base/intro-finder.html#finder) (Finite Fault Rupture Detector) library.  \n\n## Features\n\nThis package is designed to query the ESM, EMSC and RRSM web services to retrieve the acceleration amplitudes and felt reports. \n\nThis tool is able to query:\n- The ESM ```shakemap``` endpoint, using both ```format=event_dat``` for amplitudes and ```format=event``` option to retrieve the basic event information. See the [ESM Shakemap web service](https://esm-db.eu//esmws/shakemap/1/query-options.html) for more information. \n\n- RRSM ```shakemap``` web service, which also uses the same web service as the ESM. The tool only implements minor changes such as the base service URL and order of options for queries. RRSM queries are slightly different than ESM, and support ```type``` instead of the ```format``` option. The tool is designed to handle these nuances.  \n\n- The same RRSM shakemap data is also retrieved via the ```peak-motions``` service end point. This service returns a json file that includes a list of stations merged with event and amplitude information.  \n\n- The EMSC felt reports, for the basic event information (```includeTestimonies=false```) and intensities (```includeTestimonies=true```).\n\nMore information on the web services implemented in this tool are avaliable on:\n- RRSM: https://www.orfeus-eu.org/rrsm/about/\n- ESM: https://esm-db.eu/#/data_and_services/web_services and https://esm-db.eu//esmws/shakemap/1/\n- EMSC: https://www.emsc.eu/Earthquake_data/Data_queries.php\n\nFor further information on FinDer, see the references below and [Swiss Seismological Service at ETH Zurich](http://www.seismo.ethz.ch/en/knowledge/earthquake-data-and-analysis-tools/EEW/finite-fault-rupture-detector-finder/) web page.\n\n_**References**:_\n\n> Böse, M., Heaton, T. H., & Hauksson, E., 2012. Real‐time Finite Fault Rupture Detector (FinDer) for large earthquakes. Geophysical Journal International, 191(2), 803–812, doi:10.1111/j.1365-246X.2012.05657.x\n>\n> Böse, M., Felizardo, C., & Heaton, T. H., 2015. Finite-Fault Rupture Detector (FinDer): Going Real-Time in Californian ShakeAlertWarning System. Seismological Research Letters, 86(6), 1692–1704, doi:10.1785/0220150154\n>\n> Böse, M., Smith, D., Felizardo, C., Meier, M.-A., Heaton, T. H., & Clinton, J. F., 2018. FinDer v.2: Improved Real-time Ground-Motion Predictions for M2-M9 with Seismic Finite-Source Characterization. Geophysical Journal International, 212(1), 725-742, doi:10.1093/gji/ggx430\n>\n> Cauzzi, C., Behr, Y. D., Clinton, J., Kastli, P., Elia, L., & Zollo, A., 2016. An Open-Source Earthquake Early Warning Display. Seismological Research Letters, 87(3), 737–742, doi:10.1785/0220150284\n\n## Installation\n\n> **Python Version Requirement:**  \n> This package requires **Python 3.9 or higher**.\n\nThe package is not yet installable via PyPI. To install locally from source, follow these steps:\n\n1. **Clone the repository and enter the project directory:**\n   ```bash\n   git clone https://github.com/pyfinder-dev/paramws-clients.git\n   cd paramws-clients\n   ```\n\n2. **(Recommended) Create and activate a virtual environment:**\n   ```bash\n   python -m venv .venv\n   # On Unix/macOS\n   source .venv/bin/activate\n   # On Windows\n   .venv\\Scripts\\activate\n   ```\n\n3. If you just want to use the package:\n    ```bash\n    pip install .\n    ```\n    \n    If you plan to contribute or develop, install the package in **editable mode**:\n    ```bash\n    pip install -e .\n    ```\n\n## Usage Examples\nComing soon\n\n## Use Cases\nComing soon\n\n## API Reference\nComing soon\n\n## Testing\nAutomated tests are executed on GitHub Actions for every push and pull request. For local development, you can also run the tests manually:\n\n```bash\npython run_tests.py\n```\n\nAlternatively, if you prefer using `pytest`, install it first:\n\n```bash\npip install pytest\n```\n\nThen, trigger the test modules:\n\n```bash\nPYTHONPATH=. pytest -v\n```\n\n## License\nLicensed under the MIT License – see [LICENSE](./LICENSE) for details.\n\n## Acknowledgment\n**pyfinder** and this package were initally developed as part of the EU project \"A Digital Twin for Geophysical Extremes\" (DT-GEO; https://dtgeo.eu/) and has received funding from Horizon Europe under Grant Agreement No 101058129 for the Digital Twin Component (DTC) E6 (\"Rapid Source and Shaking Characterization\") which aims to provide rapid information on ground shaking and warnings for significant earthquakes in the Euro-Mediterranean region.\n",
        "createdAt": "2025-09-05T12:51:07.000Z",
        "updatedAt": "2025-09-18T18:07:10.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/pyfinder-dev/paramws-clients/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "aisyahkhns/Sleipner-Seismology-CO2",
        "url": "https://github.com/aisyahkhns/Sleipner-Seismology-CO2",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2025-10-05T06:06:21.000Z",
        "updatedAt": "2025-10-05T06:11:54.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "paudetseis/Telewavesim",
        "url": "https://github.com/paudetseis/Telewavesim",
        "description": "Teleseismic body wave modeling through stacks of (submarine/anisotropic) layers",
        "stars": 80,
        "forks": 19,
        "readme": "![](./telewavesim/examples/picture/tws_logo.png)\n\n## Software for teleseismic body wave modeling through stacks of anisotropic layers\n\nThe structure of the Earth's crust and upper mantle gives useful information on the\ninternal composition and dynamics of our planet. Some of the most widely used techniques\nto infer these properties are based on examining the effect of teleseismic body wave\n(i.e., P and S waves that originate from distant earthquakes and arrive as plane waves)\npropagation (e.g., transmission and scattering) through stratified media. Modeling the\nseismic response from stacks of subsurface layers is therefore an essential tool in\ncharacterizing their effect on observed seismograms.\n\nThis package contains `python` and `fortran` modules to synthesize teleseismic\nbody-wave propagation through stacks of generally anisotropic and strictly horizontal\nlayers using the matrix propagator approach of [Kennett (1983)](#references), as implemented\nin [Thomson (1997)](#references).\nThe software also properly models reverberations from an overlying column of water using the R/T\nmatrix expressions of [Bostock and Trehu (2012)](#references),\neffectively simulating ocean-bottom seismic (OBS) station recordings. The software\nwill be useful in a variety of teleseismic receiver-based studies, such as P or S\nreceiver functions, long-period P-wave polarization, shear-wave splitting from\ncore-refracted shear waves (i.e., SKS, SKKS), etc. It may also be the starting point\nfor stochastic inverse methods (e.g., Monte Carlo sampling). The main part of the\ncode is written in `fortran` with `python` wrappers. Common computational\nworkflows are covered in the Jupyter notebooks bundled with this package.\n\n[![JOSS paper](https://joss.theoj.org/papers/10.21105/joss.01818/status.svg)](https://doi.org/10.21105/joss.01818)\n[![DOI](https://zenodo.org/badge/204565459.svg)](https://zenodo.org/badge/latestdoi/204565459)\n[![PyPI version](https://badge.fury.io/py/telewavesim.svg)](https://badge.fury.io/py/telewavesim)\n<!-- [![tests](https://github.com/paudetseis/Telewavesim/actions/workflows/tests.yml/badge.svg)](https://github.com/paudetseis/Telewavesim/actions/workflows/tests.yml)\n[![codecov](https://codecov.io/gh/paudetseis/telewavesim/branch/master/graph/badge.svg)](https://codecov.io/gh/paudetseis/telewavesim)\n -->\n\nInstallation, Usage, API documentation and Jupyter Notebooks are described at https://paudetseis.github.io/Telewavesim/\n\n#### Citing\n\nIf you use `Telewavesim` in your work, please cite the [Zenodo DOI](https://zenodo.org/badge/latestdoi/204565459)\nand the following paper:\n\n- Audet, P., Thomson, C.J., Bostock, M.G., and Eulenfeld, T. (2019). Telewavesim:\nPython software for teleseismic body wave modeling. Journal of Open Source Software,\n4(44), 1818, https://doi.org/10.21105/joss.01818\n\n#### Contributing\n\nAll constructive contributions are welcome, e.g. bug reports, discussions or suggestions for new features. You can either [open an issue on GitHub](https://github.com/paudetseis/Telewavesim/issues) or make a pull request with your proposed changes. Before making a pull request, check if there is a corresponding issue opened and reference it in the pull request. If there isn't one, it is recommended to open one with your rationale for the change. New functionality or significant changes to the code that alter its behavior should come with corresponding tests and documentation. If you are new to contributing, you can open a work-in-progress pull request and have it iteratively reviewed.\n\nExamples of straightforward contributions include adding more elastic constants or notebooks that describe published examples of teleseismic body-wave modeling. Suggestions for improvements (speed, accuracy, etc.) are also welcome.\n\n#### References\n\n- Bostock, M.G., and Trehu, A.M. (2012). Wave-field decomposition of ocean-bottom seismograms. Bulletin of the Seismological Society of America, 102, 1681-1692. https://doi.org/10.1785/0120110162\n\n- Kennett, B.L.N. (1983). Seismic wave propagation in stratified media. Cambridge University Press, 342pp. https://www.oapen.org/search?identifier=459524\n\n- Thomson, C.J. (1997). Modelling surface waves in anisotropic structures: I. Theory. Physics of the Earth and Planetary interiors, 103, 195-206. https://doi.org/10.1016/S0031-9201(97)00033-2\n",
        "createdAt": "2019-08-26T21:33:34.000Z",
        "updatedAt": "2025-11-17T22:24:22.000Z",
        "language": "Jupyter Notebook",
        "homepage": "https://paudetseis.github.io/Telewavesim/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/paudetseis/Telewavesim/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "mengk95/hello-seismology",
        "url": "https://github.com/mengk95/hello-seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# hello-seismology",
        "createdAt": "2021-08-03T06:06:01.000Z",
        "updatedAt": "2021-08-03T06:06:04.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/mengk95/hello-seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ds-modules/EPS-130-SP20",
        "url": "https://github.com/ds-modules/EPS-130-SP20",
        "description": "UC Berkeley EPS 130 (Strong Motion Seismology) Spring 2020",
        "stars": 0,
        "forks": 1,
        "readme": "# EPS-130-SP20\nEPS 130 - Strong Motion Seismology - Doug Dreger - Spring 2020\n\n [![Datahub](https://img.shields.io/badge/Launch-UCB%20Datahub-blue.svg)](http://datahub.berkeley.edu/user-redirect/interact?account=ds-modules&repo=EPS-130-SP20&branch=master&path=)\n\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/ds-modules/EPS-130-SP20/master)\n\n",
        "createdAt": "2020-01-15T17:11:32.000Z",
        "updatedAt": "2025-06-04T23:35:17.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ds-modules/EPS-130-SP20/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "gthompson/kitchensinkGT",
        "url": "https://github.com/gthompson/kitchensinkGT",
        "description": "Miscellaneous research codes, organized into projects with a common set of libraries. This is just where I actively work, before packing codes up for release when publishing papers, datasets etc.",
        "stars": 2,
        "forks": 0,
        "readme": "# kitchensinkGT\r\n\r\nHost repo for various projects\r\n[Repo website](https://gthompson.github.io/kitchensinkGT/)\r\n\r\n[Launchpad Erosion Project](https://gthompson.github.io/kitchensinkGT/PROJECTS/ROCKETSEIS/launchpad_erosion/html/index.html)\r\n\r\n\r\nUseful _OTHER 3rd party repos:\r\n\r\n1. Codes at https://github.com/geoscience-community-codes\r\n2. Codes at https://github.com/giseislab\r\n3. https://github.com/usgs/swarm\r\n4. https://github.com/usgs/shakemap\r\n5. https://github.com/usgs/libcomcat\r\n6. https://github.com/usgs/earthquake-detection-formats\r\n7. https://github.com/usgs/earthquake-detection-formats\r\n8. https://github.com/usgs/winston\r\n9. https://github.com/usgs/pensive\r\n10. https://github.com/gthompson/seismic-event-matlab-suite (Ketner, for GISMO?)\r\n11. https://github.com/usgs/CatStat (Paul Earle, for GISMO?)\r\n12. https://github.com/usgs/matcomcat (MATLAB code for searching ANSS ComCat catalog)\r\n13. https://github.com/ROBelgium/MSNoise\r\n14. https://github.com/obspy/obspy\r\n15. https://github.com/markcwill/hashpy\r\n16. https://github.com/uafgeotools\r\n17. https://github.com/pyrocko/pyrocko\r\n",
        "createdAt": "2019-09-21T05:31:36.000Z",
        "updatedAt": "2025-03-03T07:23:18.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/gthompson/kitchensinkGT/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "MHasnain200/Seismology",
        "url": "https://github.com/MHasnain200/Seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2020-09-28T18:33:10.000Z",
        "updatedAt": "2020-09-29T04:37:41.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "jpampuero/sbiemlab",
        "url": "https://github.com/jpampuero/sbiemlab",
        "description": "SBIEMLAB - A Spectral Boundary Integral Equation Method for 2D mode III rupture dynamics in Matlab",
        "stars": 8,
        "forks": 1,
        "readme": "# SBIEMLAB\n\n## A Spectral Boundary Integral Equation Method for 2D mode III rupture dynamics in Matlab\n\nVersion 0.1\n\n\nCONTENTS\n---------\n\tI.   Directory contents\n\tII.  Overview\n\tIII. Pre-requisites\n\tIV.  Getting started\n\tV.   Questions and bug reports\n\tVI.  Copyright and license\n\n\nI. DIRECTORY CONTENTS\n----------------------\n\n* README : the current file\n* GNU_GPL : GNU General Public License\n* SBIEM*jpg : example output figures\n* src/SBIEM.m : the SBIEM solver\n* src/friction.m : slip weakening friction laws\n* src/SBIEM_ex*.m : examples (1 to 3)\n* src/speed.m : estimates front speed\n\nThe function SBIEM creates and maintains a directory called 'kernels', \nin which some numerical coefficient tables are stored to avoid \nrecomputations during subsequent calls.\n\n\nII. OVERVIEW\n-------------\n\nSBIEMLAB is a collection of Matlab functions and scripts that solve \nantiplane (mode III) rupture dynamic problems with slip-weakening friction, \non a 1D fault embedded in a 2D homogeneous elastic unbounded medium.\nThis is one of the most elementary models of dynamic earthquake rupture. \n\nThe problem is initially formulated as a Boundary Integral Equation (BIE),\nwith space-time convolutions describing the elastodynamic stress transfer\nalong the fault. In the Spectral version of the BIE (SBIEM),\nthe space convolutions in the stress transfer functionals are expressed \nin the spectral domain associated with the along-fault spatial wavenumber (k).\nThe elastodynamic kernels are obtained analytically.\nThe method is introduced in detail by \n\n  J. W. Morrisey and P. H. Geubelle (1997),\n  \"A numerical scheme for mode III dynamic fracture problems\"\n  Int. J. Num. Meth. Eng., 40 (7), 1181-1196.\n\nand has been improved and extensively used by Alain Cochard, Nadia Lapusta, etc.  \nSBIEMLAB implements the \"velocity\" formulation of the stress transfer functional.\nTime truncation of time-convolutions is implemented in SBIEMLAB as in \n\n  N. Lapusta et al. (2000), \"Elastodynamic analysis for slow \n  tectonic loading with spontaneous rupture episodes on faults \n  with rate- and state-dependent friction\"\n  J. Geoph. Res. 105 (B10), 23765-23789.\n\nThe friction law assumed in SBIEMLAB is slip weakening (linear or not).\nThe method is however largely independent of the nature of the friction law,\nand can be easily adapted to rate-and-state friction (e.g. Lapusta et al, 2000).\n\nSBIEMLAB is intended to introduce new users to the SBIEM or \nto introduce researchers and students to computational earthquake dynamics.\nFor serious simulations you should turn to an optimized Fortran \nimplementation of the SBIEM or of the Spectral Element Method (SEM):\n  + MDSBI, https://pangea.stanford.edu/~edunham/codes/codes.html\n  + SEM2DPACK, https://github.com/jpampuero/sem2dpack\n\n\nIII. PRE-REQUISITES\n--------------------\n\nI assume you have some familiarity with computational earthquake dynamics.\nIf this is not the case yet, the following paper is a good starting point:\n\n  S. M. Day, L. A. Dalguer, N. Lapusta and Y. Liu (2005)\n  Comparison of finite difference and boundary integral solutions \n  to three-dimensional spontaneous rupture \n  J. Geophys. Res. 110 (B12), B12307.\n\nIn particular you should pay attention to the grid spacing required to \nachieve adequate numerical resolution of the rupture process zone, otherwise\nyou will obtain numerical noise, notably in the slip rate histories.\nSBIEMLAB gives limited warning about numerical resolution. You are responsible\nfor setting the grid spacing according to the usual criterion: several grid points\nwithin the process zone size, defined as the zone between the rupture front\nand the end of weakening where slip = Dc (e.g. see reference above).\n\nThe SBIEM implemented here assumes periodicity along the fault, beyond the\nlimits of the modelled fault segment. Stresses propagating at the S wave speed \ncan wrap around the limits and create unphysical or undesired perturbations \nif the fault segment is not set large enough. SBIEMLAB gives limited warning\nabout this situation, and you are responsible for setting the fault size accordingly.\n\nSBIEMLAB assumes you are familiar with Matlab's \"structures\" (variables containing\nnamed fields).\n\n\nIV. GETTING STARTED\n--------------------\n\n1. Under Matlab, type \"help SBIEM\" to get a complete description of the solver's\ninput and output arguments.\n\n2. Type \"help friction\" for a description of the friction law parameters.\n\n3. Explore the commented example script SBIEM_ex1.m,\n   it illustrates the typical usage of SBIEM and visualization of results\n\n4. If needed, explore the remaining examples scripts SBIEM_ex*.m \n\n\nV. QUESTIONS AND BUG REPORTS\n-----------------------------\n\nFor any problem running SBIEMLAB, submit an \"issue\" on github (preferred way) or contact the author:\n\nJean-Paul Ampuero\nampuero@geoazur.unice.fr\n\nhttps://jpampuero.github.io/\n\n\nVI. COPYRIGHT AND LICENSE\n--------------------------\n\nCopyright (C) 2006-2007 Jean-Paul Ampuero\n\nThis software is freely available for scientific research purposes. \nIf you use this software in writing scientific papers or reports\ninclude proper attributions to its author, Jean-Paul Ampuero.\n\nThis program is free software; you can redistribute it and/or\nmodify it under the terms of the GNU General Public License\nas published by the Free Software Foundation; either version 2\nof the License, or (at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program; if not, write to the Free Software\nFoundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n\n",
        "createdAt": "2021-03-29T06:58:20.000Z",
        "updatedAt": "2025-03-30T02:26:33.000Z",
        "language": "MATLAB",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/jpampuero/sbiemlab/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "uchide/Uchide2020GJI",
        "url": "https://github.com/uchide/Uchide2020GJI",
        "description": "This repository to disclose focal mechanisms determined by Uchide (2020).",
        "stars": 1,
        "forks": 0,
        "readme": "# Uchide2020GJI\nHere, the focal mechanisms determined by Uchide (2020) is available. See Uchide2020_mecalist.txt. The file format is as follows: \n* Column  1: Year of the origin time in Japan Standard Time (JST=UTC+9)\n* Column  2: Month\n* Column  3: Day\n* Column  4: Hour\n* Column  5: Minute\n* Column  6: Second\n* Column  7: Latitude of the hypocenter\n* Column  8: Longitude\n* Column  9: Depth (km)\n* Column 10: Magnitude (JMA magnitude)\n* Column 11: Strike (deg.) of one of nodal planes\n* Column 12: Dip (deg.)\n* Column 13: Rake (deg.)\n* Column 14: Rank (quality) of the solution given by the HASH code.\n* Column 15: Number of stations used for the focal mechanism determination.\n\n## References\nUchide, T. (2020). Focal mechanisms of small earthquakes beneath the Japanese islands based on first-motion polarities picked using deep learning. Geophysical Journal International, 223(3), 1658–1671, https://doi.org/10.1093/gji/ggaa401.\n",
        "createdAt": "2020-09-08T23:42:09.000Z",
        "updatedAt": "2020-12-21T04:41:21.000Z",
        "language": null,
        "homepage": "https://doi.org/10.1093/gji/ggaa401",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/uchide/Uchide2020GJI/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ishwarya1505/Seismology_Prediction_",
        "url": "https://github.com/ishwarya1505/Seismology_Prediction_",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# Seismology_Prediction_\n According to BBC statistics, data changes for each earthquake that has occurred thus far. Thousands of people are killed, 50,000 are injured, 1-3 million are dislocated, and a considerable number of people go missing or become homeless. The structural damage is over 100 percent. It also has an impact on the economic loss, which ranges from ten to sixteen million dollars. A magnitude of 5 or higher is considered the worst. The most deadly earthquake to date happened in Indonesia, killing around 3 million people, injuring 1-2 million, and causing 100% structural damage. As a result, the effects of an earthquake are devastating, causing huge changes in everything from the environment and lifestyle to the economy. Each of these factors contributes to earthquake prediction. A precise forecaster is being constructed and developed in the current circumstances , a machine that will predict the future disaster. Using machine learning methods focuses on recognizing early indicators of an earthquake. The system is entitled to the fundamentals of constructing learning systems as well as the data science life cycle. Government sources provide data sets for the Indian subcontinent and the rest of the world. After preprocessing the data, a stacking model is built using RF and SVM algorithms. This mathematical model is created by algorithms using \"training data-set\".  The  models seeks out patterns which lead to disaster & adapts its structure to them in order to make decisions and forecasts without being explicitly adapted to carry out the mission. Keywords: Support vector Machine, Earthquake, Forecast, Machine Learning, Random Forest.\n\n A Stacking Regressor Model :\n\n             Bagging, Boosting, and stacking are just a few of the strategies to ensemble models in machine learning. Stacking is a prominent ensemble machine learning strategy for predicting several nodes and improving performance of the model. It allows to train numerous model to address the similar issues & then create a new model with superior performance based on their combined output.\nA ml model that integrates the predictions of two or more models is known as an ensembled model.\nA method of ensemble learning is stacking regression. As a result of the collaboration of  Using the meta-features generated by independent regression models using an an absolute training set as a basis, a meta-regressor is developed that determines the best fit. \n\nResults:\n \n              For large datasets, the combination of the random forest and support vector machine models works well. In comparison to the bagging and enhancing precision, the accuracy of the The stacking model has the highest score of 83.percent. For each methodology, the response time is the same. The time spent training for stacking is slightly longer.\n",
        "createdAt": "2024-01-31T14:08:39.000Z",
        "updatedAt": "2024-01-31T14:38:15.000Z",
        "language": "MATLAB",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ishwarya1505/Seismology_Prediction_/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "hazardgoat/Shear_Wave_Velocity_Profiles",
        "url": "https://github.com/hazardgoat/Shear_Wave_Velocity_Profiles",
        "description": "A Python script that plots shear wave velocity profiles and calculates VS30 for selected stations from the Yong et al. 2013 ARRA report.",
        "stars": 3,
        "forks": 0,
        "readme": "# Shear Wave Velocity Profiles\nA Python script that plots shear wave velocity profiles and calculates VS30 for selected stations from the Yong et al. 2013 ARRA report.\n\n## Station DRE\n![Vs_Plot_DRE](https://user-images.githubusercontent.com/74040471/141198923-c62cb8df-7dc0-4247-8d8d-a9d45d48e8ad.png)\n\n## Station ERR\n![Vs_Plot_ERR](https://user-images.githubusercontent.com/74040471/141198945-f9d3606d-3532-4c35-89f1-a87cdc74b7df.png)\n\n## Station SWS\n![Vs_Plot_SWS](https://user-images.githubusercontent.com/74040471/141198984-f672888e-689a-4140-b912-c4ff87ff12c2.png)\n",
        "createdAt": "2021-11-10T21:47:24.000Z",
        "updatedAt": "2024-08-05T03:54:04.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/hazardgoat/Shear_Wave_Velocity_Profiles/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "HelioGiroto/Sismos",
        "url": "https://github.com/HelioGiroto/Sismos",
        "description": "Em andamento...",
        "stars": 0,
        "forks": 0,
        "readme": "# Sismos\nEm andamento...\n\n## O propósito desse projeto ##\n\n1. Fazer um alerta sismológico scrapeando constantemente as informações do usgs.gov.\n\nInvestigar a teoria que diz que as explosões solares são gatilhos de sismos no planeta Terra. Sendo assim, \n\n2. Obter os dados K-index do órgão do governo americano: noaa.gov e fazer envio de alertas;\n\n3. Cruzar os dados realizando um gráfico em linguagem R para mostrar de maneira observável se os momentos que ocorrem as explosões solares, quais e como elas são a ponto de poder interferir na atividade sismológica em todo mundo. Tentando encontrar algum padrão.\n\n4. O programa (disponível tanto em linguagem BASH como em Python) envia avisos (ao email e/ou à pulseira Mi-Band ou relógio smart) além de dar a opção de abrir o mapa do local (via browser) onde estão havendo as últimas atividades sísmicas.\n\n5. Se estuda a possibilidade de usando os dados no formato KML se possa desenvolver uma apresentação dinânima no Google Earth (app ou mesmo web google earth), percorrendo locais de registro sísmicos.\n\nDesenvolverei todo esse projeto usando pelo menos 4 linguagens de programação: Python, BASH, AWK, e R. E se pretende rodar todo esse sistema tanto em Windows, como em Linux (do qual se origina), como em Mac, como também na WEB (podendo acessá-lo desde qualquer celular, tablet, etc) via navegador de internet.\n\n\n[Clique aqui para ver o desenrolar do projeto](https://raw.githubusercontent.com/HelioGiroto/Sismos/master/sismos.txt)\n\nDedico esse projeto a meu amado povo mexicano.\n\n\n\nAutor Helio Giroto.\n\n[Sob licença de direitos autorais do MIT](https://raw.githubusercontent.com/HelioGiroto/Sismos/master/LICENSE)\n",
        "createdAt": "2018-01-19T03:29:28.000Z",
        "updatedAt": "2019-01-06T02:51:43.000Z",
        "language": "Shell",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/HelioGiroto/Sismos/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "davide-piccinini/Toolseis",
        "url": "https://github.com/davide-piccinini/Toolseis",
        "description": "Some seismological MatLab tools ",
        "stars": 0,
        "forks": 0,
        "readme": "Some seismological MatLab tools created to interact with INGV & other webservices\n",
        "createdAt": "2024-11-16T17:15:56.000Z",
        "updatedAt": "2025-09-05T13:35:26.000Z",
        "language": "MATLAB",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/davide-piccinini/Toolseis/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "anowacki/seismo-fortran",
        "url": "https://github.com/anowacki/seismo-fortran",
        "description": "Utility modules for dealing with generally-anisotropic elastic constants and other geophysics problems",
        "stars": 15,
        "forks": 6,
        "readme": "",
        "createdAt": "2014-09-09T11:53:20.000Z",
        "updatedAt": "2023-11-05T05:30:33.000Z",
        "language": "Fortran",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "christiano06/HelloSeismology",
        "url": "https://github.com/christiano06/HelloSeismology",
        "description": "The testing repository of Seismology toolkits",
        "stars": 0,
        "forks": 0,
        "readme": "# HelloSeismology\nThe testing repository of Seismology toolkits.\n\nBy Seismology Lab @ University of Minnesota\n",
        "createdAt": "2017-09-22T16:17:40.000Z",
        "updatedAt": "2017-09-22T16:17:40.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/christiano06/HelloSeismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "liamtoney/nodal",
        "url": "https://github.com/liamtoney/nodal",
        "description": "Assessing seismoacoustic coupling on seismic nodes at Mount Saint Helens volcano",
        "stars": 1,
        "forks": 0,
        "readme": "# nodal\n\nThis repository contains the code accompanying the paper \"Examining infrasound\npropagation at high spatial resolution using a nodal seismic array\" by\n[Liam Toney](mailto:ldtoney@alaska.edu), David Fee, Brandon Schmandt, and Jordan W.\nBishop.\n\n## Installing\n\nA conda environment specification file, [`environment.yml`](environment.yml), is\nprovided. You can create a conda environment from this file by executing\n```shell\nconda env create\n```\nfrom the repository root.\n\nYou must define two environment variables to use the code:\n- `NODAL_WORKING_DIR` — the path to this repository\n- `NODAL_FIGURE_DIR` — the directory where figure files should be saved\n\n## Citation\n\nIf you use the tools contained in this repository, please cite our paper:\n\n> Toney, L., Fee, D., Schmandt, B., & Bishop, J. W. (2023). Examining infrasound\n> propagation at high spatial resolution using a nodal seismic array. *Journal of\n> Geophysical Research: Solid Earth*, 128, e2023JB027314.\n> https://doi.org/10.1029/2023JB027314\n\n## Acknowledgements\n\nThis work was supported by the Nuclear Arms Control Technology (NACT) program at the\nDefense Threat Reduction Agency (DTRA). Cleared for release.\n",
        "createdAt": "2021-03-26T20:32:37.000Z",
        "updatedAt": "2025-02-14T05:46:56.000Z",
        "language": "Python",
        "homepage": "https://doi.org/10.1029/2023JB027314",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/liamtoney/nodal/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "maalekmo/Work",
        "url": "https://github.com/maalekmo/Work",
        "description": "seismology",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2019-09-15T08:48:44.000Z",
        "updatedAt": "2020-02-19T14:46:15.000Z",
        "language": "HTML",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "lyricalsoul/seiscomp",
        "url": "https://github.com/lyricalsoul/seiscomp",
        "description": "A simple JS library for interacting with SeisComP servers.",
        "stars": 1,
        "forks": 0,
        "readme": "A library for interacting with SeisComP, the modular system for processing and distributing seismic data. Runs on Node\nand in the browser. Developed with [bun.js](https://github.com/oven-sh/bun) (a Node.js alternative using\nJavaScriptCore - you should try it! it's faster than Node and you don't have to build your TypeScript project to test\nit). Also fully typed.\n\n## Installation\n\n```bash\nnpm install seiscomp\n```\n\n## Documentation\n\n[lyricalsoul.github.io/seiscomp](https://lyricalsoul.github.io/seiscomp/) - proudly generated\nby [typedoc](https://typedoc.org/)!\n\n## Features\n\nCurrently, seiscomp.js only supports a subset of the FDSNWS module. Support for seedlink is planned, using a modular\napproach for maximum compatibility. If you need for a module to be implemented, do not hesitate on contacting by opening\nan issue.\n\n## Notes\n\n- The library is still in development. Bugs may occur.\n\n## Examples\n\n```js\nimport { FDSNWS } from 'seiscomp'\n\n\nconst client = new FDSNWS('https://moho.iag.usp.br/fdsnws/')\n\nawait client.station.queryNetwork('BL')\n\nawait client.station.query()\n  .channel.network('BR')\n  .channel.station('VIL?')\n  .time.startAfter('2013-01-01')\n  .finish()\n  .then(console.log)\n```\n\n",
        "createdAt": "2023-01-13T20:36:14.000Z",
        "updatedAt": "2023-09-30T11:42:15.000Z",
        "language": "TypeScript",
        "homepage": "https://lyricalsoul.github.io/seiscomp/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/lyricalsoul/seiscomp/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "ds-modules/EPS130-SP24",
        "url": "https://github.com/ds-modules/EPS130-SP24",
        "description": "UC Berkeley EPS 130 (Strong Motion Seismology) Spring 2024",
        "stars": 0,
        "forks": 0,
        "readme": "# EPS130-SP24\n## Strong Motion Seismology Spring 2024\n**Instructor: Douglas Dreger**\n\n\n[![Datahub](https://img.shields.io/badge/Launch-UCB%20Datahub-blue.svg)](https://datahub.berkeley.edu/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fds-modules%2FEPS130-SP24&branch=main&urlpath=tree%2FEPS130-SP24%2F)\n [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/ds-modules/EPS-130-SP24/main)\n",
        "createdAt": "2024-01-04T17:01:39.000Z",
        "updatedAt": "2025-07-07T21:21:23.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/ds-modules/EPS130-SP24/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Stich-endeavor/SM_IntroductionToSeismology",
        "url": "https://github.com/Stich-endeavor/SM_IntroductionToSeismology",
        "description": "This is a repository for the solutions of the exercises in the book \"Introduction to Seismology - 2nd Edition\" written by Peter M. Shearer. ",
        "stars": 0,
        "forks": 0,
        "readme": "# SM_IntroductionToSeismology\nThis is a FREE repository for the SOLUTION MANUAL of the book \"Introduction to Seismology - 2nd Edition\" written by Peter M. Shearer. \n\n# 👉 About the solver\nI'm from South America and I have a BS degree in Geoscience. This is my attempt to demonstrate my commitment to both science (i.e. geophysics) and a foreing language (i.e. English). \nI want to devote my life to science so here I'm trying to make knowledge more accessible for anyone with curiosity or some troubles in this subject. \n\nBe aware that I'm not an expert, so do not hesitate in \"Pull requests\" or contact  me to perform some modifications or give some advice.\nI'd like to ask to anyone who read these Python Notebooks (ipynb) to rate my english in order to improve my skills with this language. \n\n\n\n",
        "createdAt": "2021-06-25T00:28:52.000Z",
        "updatedAt": "2021-08-04T04:02:01.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Stich-endeavor/SM_IntroductionToSeismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "geotechno/computational-seismology",
        "url": "https://github.com/geotechno/computational-seismology",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# computational-seismology\n",
        "createdAt": "2018-01-09T21:18:37.000Z",
        "updatedAt": "2018-01-25T12:06:03.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/geotechno/computational-seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "schipp/matched_field_processing",
        "url": "https://github.com/schipp/matched_field_processing",
        "description": "Matched Field Processing using numerical Green's Functions from pre-computed databases, used in Schippkus & Hadziioannou 2022.",
        "stars": 9,
        "forks": 5,
        "readme": "# Matched Field Processing using numerical Green's Functions from pre-computed databases\n\n[![DOI](https://zenodo.org/badge/423388944.svg)](https://zenodo.org/badge/latestdoi/423388944)\n\n<img align=\"left\" src=\"assets/mfp.png\" width=\"400px\">\n\nMatched Field Processing (MFP) is a technique to locate the source of a recorded wave field. It is the generalization of beamforming, allowing for curved wavefronts. In the standard approach to MFP, simple analytical Green's functions are used as synthetic wave fields that the recorded wave fields are matched against. We introduce an advancement of MFP by utilizing Green's functions computed numerically for real Earth structure as synthetic wave fields. This allows in principle to incorporate the full complexity of elastic wave propagation, and through that provide more precise estimates of the recorded wave field's origin. \n\nThis repository acts as the development platform for this approach.\n\nA manuscript describing the method in detail is available as a pre-print at EarthArXiv [doi.org/10.31223/X5492H](https://doi.org/10.31223/X5492H) and submitted to Geophysical Journal International for peer review. A separate repository contains the information (what data was used, settings files, figure scripts) to reproduce the results we present in our manuscript: [seismology-hamburg/schippkus_hadziioannou_2022](https://github.com/seismology-hamburg/schippkus_hadziioannou_2022).\n\n## Instructions\n\n* Install requirements\n* Download or clone the repository\n* Configure `settings.yml` in `./code/`\n* `python logic.py` in `./code/`\n## Requirements\n\n- python 3.8+\n- [obspy](https://github.com/obspy/obspy/wiki/) to read seismic data\n- [cartopy](https://scitools.org.uk/cartopy/docs/latest/index.html) to plot maps\n- [tqdm](https://tqdm.github.io) to get progressbars\n- [pyyaml](https://pypi.org/project/PyYAML/) to parse the `settings.yml`\n- [instaseis](https://instaseis.net) to read the Green's function database\n- local green's function database readable by instaseis, e.g., downloaded from [syngine](http://ds.iris.edu/ds/products/syngine/)\n- [global_land_mask](https://pypi.org/project/global-land-mask/) for fast checks whether cells are on land\n",
        "createdAt": "2021-11-01T08:22:15.000Z",
        "updatedAt": "2024-06-17T03:05:10.000Z",
        "language": "Python",
        "homepage": "https://doi.org/10.1093/gji/ggac240",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.undefined",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/schipp/matched_field_processing/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "velgueta/Seismologyclass",
        "url": "https://github.com/velgueta/Seismologyclass",
        "description": "repo for homeworks",
        "stars": 0,
        "forks": 0,
        "readme": "# Seismologyclass\nrepo for homeworks\n",
        "createdAt": "2023-03-01T06:02:49.000Z",
        "updatedAt": "2023-03-01T06:04:30.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/velgueta/Seismologyclass/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "TopoSeismologist/EQNeMix",
        "url": "https://github.com/TopoSeismologist/EQNeMix",
        "description": "EQNeMix is a mixed architecture that combines: ConvNetQuake (Perol et al., 2018) and EQTransformer (Mousavi et al., 2020), through a Gaussian Mixture Model to perform a Bayesian Inference. The outcome is a probabilistic location obtained with just a single seismic station.",
        "stars": 2,
        "forks": 0,
        "readme": "# EQNeMix\n\nCopyright &copy; 2024 R. Ortega, C. Meneses-Ponce & J.D. Castro Morales\n\n## 1. What will you find in this repository?\nEQNeMix is a mixed architecture that combines two widely-used neural networks in seismology: **ConvNetQuake** (*Perol et al., 2018*) and **EQTransformer** (*Mousavi et al., 2020*). Our algorithm employs a *Gaussian Mixture Model* to perform a *Bayesian Inference* using the outputs generated by both neural networks. The ultimate outcome is a *probabilistic location* pinpointed using just a *single seismic station*.\n\n* An integral facet of its versatile design is the algorithm's adaptability, as it is not confined to a single travel-time algorithm. It accommodates a spectrum of options ranging from simpler to more intricate travel-time methods. Furthermore, various sampling techniques such as variational inference, Hamiltonian sampling, among others, can be seamlessly integrated. \n\n* This algorithm is applicable not only to individual seismic stations but can also be extended to entire seismic networks.\n\n  \n",
        "createdAt": "2023-12-10T23:38:20.000Z",
        "updatedAt": "2025-02-27T06:53:43.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/TopoSeismologist/EQNeMix/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Iron486/Bachelor_Thesis",
        "url": "https://github.com/Iron486/Bachelor_Thesis",
        "description": "Statistical data analysis of the italian seismic catalogue from 1985 to 2020. ",
        "stars": 4,
        "forks": 1,
        "readme": "# Bachelor Thesis\nStatistical data analysis of the Italian seismic catalogue from 1985 to 2020. The results and the methods showed in the notebooks were described in my Thesis.\n\nThe [Thesis](https://github.com/Iron486/Bachelor_Thesis/blob/main/TESI_Diego_E_Farchione.pdf) and the [presentation](https://github.com/Iron486/Bachelor_Thesis/blob/main/Presentazione_Tesi.pptx) are in Italian and they are available in the \"main\" branch or clicking on the words \"Thesis\" and \"presentation\". \n\n",
        "createdAt": "2020-11-21T15:08:04.000Z",
        "updatedAt": "2024-01-02T08:50:19.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Iron486/Bachelor_Thesis/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "OpenSeismology/OpenSeismology.github.io",
        "url": "https://github.com/OpenSeismology/OpenSeismology.github.io",
        "description": null,
        "stars": 0,
        "forks": 0,
        "readme": "# OpenSeismology.github.io",
        "createdAt": "2018-10-19T02:31:55.000Z",
        "updatedAt": "2018-10-19T02:31:58.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/OpenSeismology/OpenSeismology.github.io/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "seisman/SODrecipes",
        "url": "https://github.com/seisman/SODrecipes",
        "description": "Collection of SOD recipes for quick reference",
        "stars": 11,
        "forks": 3,
        "readme": "# SOD Recipes\n\n[SOD](http://www.seis.sc.edu/sod/) (Standing Order for Data), is a program\nthat automates tedious data selection, downloading, and routine processing tasks\nin seismology. Files that configure SOD's operation are called recipes.\n\nIf you're new to SOD, you can refer to the [official tutorial](http://www.seis.sc.edu/sod/documentation/tutorials/index.html)\nor [a Chinese tutorial](https://blog.seisman.info/sod-notes/) written by me.\n\nThis project collects some common-used recipes.\n\n## SOD Arms\n\n### eventArm\n\n- [origin-magnitude-depth-boxarea.xml](eventArm/origin-magnitude-depth-boxarea.xml): Select events based on origin time, location, magnitude and depth\n- [complexEvent.xml](eventArm/complexEvent.xml): Select events based on complex rules\n- [custom-events-from-csv.xml](eventArm/custom-events-from-csv.xml): Read events information from [a CSV file](eventArm/customEvents.csv)\n- [custom-events.xml](eventArm/custom-events.xml): Custom events information\n- [continuous-waveform.xml](eventArm/continuous-waveform.xml): Fake events for continuous waveform data\n\n### networkArm\n\n- [network.xml](networkArm/network.xml): Select one network\n- [networkOR.xml](networkArm/networkOR.xml): Select multiple networks\n- [stationOR.xml](networkArm/stationOR.xml): Select multiple stations\n\n### waveformArm\n\n- [breqfast.xml](waveformArm/breqfast.xml): Generate requests which can be further used in BREQ_FAST\n- [sacWriter.xml](waveformArm/sacWriter.xml): Request data, basic data processing and save data in SAC format\n\n## Ready-to-use Recipes\n",
        "createdAt": "2016-09-04T03:47:59.000Z",
        "updatedAt": "2023-06-06T09:12:39.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/seisman/SODrecipes/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "JCBucio/Seismology-analysis",
        "url": "https://github.com/JCBucio/Seismology-analysis",
        "description": "Repo for analyze seismic data with Python and Jupyter Notebooks",
        "stars": 0,
        "forks": 0,
        "readme": "# Seismology-analysis\n",
        "createdAt": "2021-09-23T01:36:24.000Z",
        "updatedAt": "2021-09-23T01:43:53.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/JCBucio/Seismology-analysis/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "RongjiangWang/SPGRN_2020",
        "url": "https://github.com/RongjiangWang/SPGRN_2020",
        "description": "A variant of FORTRAN code QSSP to generate a Green's function database of synthetic seismograms based on a spherically symmetric earth model",
        "stars": 0,
        "forks": 0,
        "readme": "A variant of FORTRAN code QSSP to generate a Green's function database of synthetic seismograms based on a spherically symmetric earth model.\n\nFor Windows user, the executable file is provided under folder \"WindowsEXE\". Linux user may compile the source codes with \"gfortran\" via a single command like, e.g.,\n\n~>cd .../SourceCode\n\n~>gfortran -o spgrn2020 *.f -O3\n\nto get the excutable code spgrn2020.\n\nAfter start the executable code, the program ask for an input file in the ASCII format. An example input file is provided under folder \"InputFile\". You may change the input data included in this file for your own applications.\n\nReferences\n\nWang, R., S. Heimann, Y. Zhang, H. Wang, and T. Dahm (2017). Complete synthetic seismograms based on a spherical self-gravitating Earth model with an atmos-phere-ocean-mantle-core structure. Geophysical Journal International, doi: 10.1093/gji/ggx259.\n",
        "createdAt": "2025-04-11T07:12:02.000Z",
        "updatedAt": "2025-07-05T11:47:23.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/RongjiangWang/SPGRN_2020/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "IPGP/JSeisCal",
        "url": "https://github.com/IPGP/JSeisCal",
        "description": "JSeisCal for remote control of Metrozet E300 andSTS-1 installation",
        "stars": 0,
        "forks": 0,
        "readme": "# JSeisCal\nEclipse project\nJAVA 1.5\n\n\n# Run JSeisCal #\njava -jar ./jSeisCal.jar\n",
        "createdAt": "2018-02-16T09:19:16.000Z",
        "updatedAt": "2018-03-14T09:00:17.000Z",
        "language": "Java",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/IPGP/JSeisCal/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "AnikoWirp/Seismology",
        "url": "https://github.com/AnikoWirp/Seismology",
        "description": "DR 2d simulation",
        "stars": 0,
        "forks": 0,
        "readme": "# Seismology\nDR 2d simulation\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/AnikoWirp/Seismology.git/master)\n",
        "createdAt": "2020-05-20T14:00:36.000Z",
        "updatedAt": "2020-05-20T14:26:30.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/AnikoWirp/Seismology/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "vshalaeva/ML-Seismology-reading-group",
        "url": "https://github.com/vshalaeva/ML-Seismology-reading-group",
        "description": null,
        "stars": 0,
        "forks": 1,
        "readme": "# ML-Seismology-reading-group\n\nEvery Tuesday at 13h00.\n\n## Papers\n- 15/01/2021\nM.P.A. van den Ende, J.-P. Ampuero, Automated Seismic Source Characterisation Using Deep Graph Neural Networks.\nhttps://hal.archives-ouvertes.fr/hal-02931963\n\n- 22/01/2021\nM. Picozzi, A. Giovanni Iaccarino, Forecasting the Preparatory Phase of Induced Earthquakes by Recurrent Neural Network. https://www.mdpi.com/2571-9394/3/1/2/htm\n\n- 29/01/2021\nAndreas Köhler,  Matthias Ohrnberger,  Frank Scherbaum, Unsupervised pattern recognition in continuous seismic wavefield records using Self-Organizing Maps \nhttps://onlinelibrary.wiley.com/doi/abs/10.1111/j.1365-246X.2010.04709.x\n\n- 05/01/2021\nDeep Clustering to Identify Sources of Urban Seismic Noise in Long Beach, California https://pubs.geoscienceworld.org/ssa/srl/article-abstract/doi/10.1785/0220200164/593124/Deep-Clustering-to-Identify-Sources-of-Urban?redirectedFrom=fulltext\n\n- 19/01/2021\nGravitational physics meets Seismology: for this meeting we are going to talk about how seismology helps to detect gravitational waves. There are already a couple of papers out there and they all seem quite interesting. I would say we should focus on two papers to understand the topic better: \n1. A paper about how an earthquake causes perturbations in the gravitational field: https://authors.library.caltech.edu/94686/1/ggy436.pdf\n2. A paper, which includes some machine learning to cancel out seismic noise to improve detections of graviational waves: https://arxiv.org/pdf/2005.09289.pdf\n\n- 10/03/2021\nMousavi M. et al. Earthquake transformer—an attentive deep-learning model for simultaneous earthquake detection and phase picking. https://www.nature.com/articles/s41467-020-17591-w\n\n- 19/03/2021\nQuakeCast, an Earthquake Forecasting System Using Ionospheric Anomalies and Machine Learning\nhttps://www.essoar.org/pdfjs/10.1002/essoar.10506273.1\n\n- 27/04/2021\nVincent P., Larochelle H., Lajoie I., Bengio Y., Manzagol P.-A., Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion.\n\n- 04/05/2021\nAmato F., Guignar F., Robert S., and Kanevski M. A novel framework for spatio‑temporal predictionof environmental data using deep learning.\nhttps://www.nature.com/articles/s41598-020-79148-7.pdf\n\n## Planned papers\n- Seismic features and automatic discrimination of deep and shallow induced-microearthquakes using neural network and logistic regression\nhttps://academic.oup.com/gji/article/207/1/29/2583533 \n",
        "createdAt": "2020-12-22T13:21:29.000Z",
        "updatedAt": "2021-04-27T11:50:47.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/vshalaeva/ML-Seismology-reading-group/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "albertleonardo/skynet",
        "url": "https://github.com/albertleonardo/skynet",
        "description": "Seismological Knowledge Yardstick Networks",
        "stars": 12,
        "forks": 0,
        "readme": "# SKYNET\n**S**eismological **K**nowledge **Y**ardstick **NET**works\nA series of deep learning models for seismic phase picking at regional distances.\n\nThe manuscript has just been published! check it out here\n[https://seismica.library.mcgill.ca/article/view/1431]\n>Aguilar Suarez, A. L., & Beroza, G. (2025). Picking Regional Seismic Phase Arrival Times with Deep Learning. Seismica, 4(1). https://doi.org/10.26443/seismica.v4i1.1431\n\n\n\n## Quick Installation\nClone the repository\n```bash\ngit clone https://github.com/albertleonardo/skynet.git\ncd skynet\n```\nCreate an enviroment \n```\nconda env create -f env.yaml\nconda activate skynet\n```\n\n## Applying to continuous data\nThe model can be applied to any waveform in the form of an obspy Stream, allowing many different [formats](https://docs.obspy.org/packages/autogen/obspy.core.stream.read.html).\nSee the [tutorial](https://github.com/albertleonardo/skynet/skynet_tutorial.ipynb) for details on loading a model, and applying it to getting picks.\n```python\nimport skynet\nmodel = skynet.load_model('regional_picker')\n```\n\n## Seisbench integration\nModels are now available via [Seisbench](https://github.com/seisbench/seisbench)\n\n\n\n\n## Data\n\nWe trained intial models using the CREW dataset, details here: https://github.com/albertleonardo/CREW\n\nThe paper is here: https://seismica.library.mcgill.ca/article/view/1049\n\nThe dataset is here: https://redivis.com/datasets/1z6w-e1w70hpmt\n\nIt is also integrated into Seisbench, info here: https://seisbench.readthedocs.io/en/stable/pages/benchmark_datasets.html#crew\n\nWe are actively working on bridging the local and regional scales.\n",
        "createdAt": "2023-09-09T17:10:00.000Z",
        "updatedAt": "2025-11-05T19:51:24.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/albertleonardo/skynet/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "sajeebju/EQ_Seismology",
        "url": "https://github.com/sajeebju/EQ_Seismology",
        "description": null,
        "stars": 1,
        "forks": 0,
        "readme": "# EAS5930 Earthquake Seismology \n",
        "createdAt": "2024-02-05T21:01:47.000Z",
        "updatedAt": "2025-09-11T20:33:31.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/sajeebju/EQ_Seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "RongjiangWang/QSEISFK_2011",
        "url": "https://github.com/RongjiangWang/QSEISFK_2011",
        "description": "A variant of QSEIS, for calculating synthetic f-k spectrum (dispersion curves) based on a layered elastic halfspace model",
        "stars": 0,
        "forks": 0,
        "readme": "A variant of QSEIS, for calculating synthetic f-k spectrum (dispersion curves) based on a layered elastic halfspace model.\n\nFor Windows user, the executable file is provided under folder \"WindowsEXE\". Linux user may compile the source codes with \"gfortran\" via a single command like, e.g.,\n\n~>cd .../SourceCode\n\n~>gfortran -o qseisfk *.f -O3\n\nto get the excutable code qseisfk.\n\nAfter start the executable code, the program ask for an input file in the ASCII format. An example input file is provided under folder \"InputFile\". You may change the input data included in this file for your own applications.\n\nReferences\n\nWang, R., (1999), A simple orthonormalization method for stable and efficient computation of Green's functions, Bulletin of the Seismological Society of America, 89(3), 733-741.\n",
        "createdAt": "2025-04-11T06:43:50.000Z",
        "updatedAt": "2025-06-22T06:27:20.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/RongjiangWang/QSEISFK_2011/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Shihao-Yuan/ReMi-DAS",
        "url": "https://github.com/Shihao-Yuan/ReMi-DAS",
        "description": "A Python toolkit for applying Refraction Microtremor analysis to Distributed Acoustic Sensing data",
        "stars": 1,
        "forks": 1,
        "readme": "# **ReMi-DAS**  \n**Refraction Microtremor Processing for Distributed Acoustic Sensing Data**\n\n**ReMi-DAS** is an open-source toolkit for applying Refraction Microtremor (ReMi) analysis to Distributed Acoustic Sensing (DAS) data.  \nIt extends conventional ReMi workflows to accommodate strain/strain-rate measurements from DAS arrays, enabling efficient and scalable shear-wave velocity (Vs) profiling—particularly in urban or infrastructure-constrained environments.\n\nThis package is developed using core functionality from the [**DASCore**](https://github.com/DASDAE/dascore/tree/master) project — a flexible Python library for reading, processing, and visualizing DAS data.\n\n---\n\n### 🔧 Features\n\n- 📓 A Jupyter notebook demonstrating the complete ReMi workflow\n- 🧩 Modular Python scripts for:\n  - Preprocessing DAS data (e.g., tapering, filtering)\n  - Slowness-frequency transformation using Tau-P methods\n  - Dispersion curve picking and visualization\n- 📊 Tools for slowness-frequency image plotting and Rayleigh-wave dispersion analysis\n\n---\n\n### References\n\n- McMechan, G.A. and Yedlin, M.J., 1981. Analysis of dispersive waves by wave field transformation. *Geophysics*, 46(6), pp.869-874.  \n- Louie, J.N., 2001. Faster, better: shear-wave velocity to 100 meters depth from refraction microtremor arrays. *BSSA*, 91(2), pp.347-364. \n- Chambers, D., Jin, G., Tourei, A., Issah, A.H.S., Lellouch, A., Martin, E.R., Zhu, D., Girard, A.J., Yuan, S., Cullison, T. and Snyder, T., 2024. Dascore: A python library for distributed fiber optic sensing. *Seismica*, 3(2), pp.10-26443.\n\n",
        "createdAt": "2025-07-08T16:17:36.000Z",
        "updatedAt": "2025-08-16T11:35:25.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Shihao-Yuan/ReMi-DAS/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "core-man/link",
        "url": "https://github.com/core-man/link",
        "description": "core-man's links",
        "stars": 3,
        "forks": 0,
        "readme": "# core-man's link\n\nI have been cooperated with [seisman](https://github.com/seisman) to maintain [Seismo Links](https://github.com/seismo-learn/links) since Oct. 2020.\nMost links in the website have been merged to [Seismo Links](https://github.com/seismo-learn/links), while the left will not been maintained any more.\n\nThis site collects a lot of useful sites I found during my daily life and researches.\nThe links are available on [GitHub](https://github.com/core-man/link).\n\n- [Journals](content/post/journals/): already merged to [Seismo Links](https://seismo-learn.org/links/journals/)\n- [Codes](content/post/codes/): already merged to [Seismo Links](https://seismo-learn.org/links/codes/)\n- [Database](content/post/database/): already merged to [Seismo Links](https://seismo-learn.org/links/database/)\n- [Tools](content/post/tools/): already merged to [Seismo Links](https://seismo-learn.org/links/tools/)\n- [Jobs](content/post/jobs/): already merged to [Seismo Links](https://seismo-learn.org/links/jobs/)\n- [Scientists](content/post/scientists/): not maintained any more\n- [Institutions](content/post/institutions/): not maintained any more\n- [Learning](content/post/learning/): not maintained any more\n- [NTU](content/post/ntu/): already merged to [MIG_Docs](https://migg-ntu.github.io/MIG_Docs/links)\n",
        "createdAt": "2020-03-16T07:44:39.000Z",
        "updatedAt": "2023-04-03T09:05:15.000Z",
        "language": null,
        "homepage": "https://core-man.github.io/link/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/core-man/link/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "yonapas/seismology_analysis-",
        "url": "https://github.com/yonapas/seismology_analysis-",
        "description": "file manipulate with out file, seismology subject ",
        "stars": 0,
        "forks": 0,
        "readme": "# seismology_analysis-\n",
        "createdAt": "2017-11-03T08:39:06.000Z",
        "updatedAt": "2017-11-20T10:22:47.000Z",
        "language": "JavaScript",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/yonapas/seismology_analysis-/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Mark-Noble/FTeik-Eikonal-Solver",
        "url": "https://github.com/Mark-Noble/FTeik-Eikonal-Solver",
        "description": "FTeik package: 2D and 3D Eikonal solver to compute first arrival traveltimes in a heterogeneous isotropic velocity model, with the possibility to use different grid spacing in all directions.",
        "stars": 27,
        "forks": 3,
        "readme": " \n    FTeik PACKAGE VERSION 1.0\n    Copyright (c) 2019 Mark NOBLE, MINES ParisTech, France\n    Email: mark.noble@mines-paristech.fr\n    VERSION 1.0: 2019-08-01 , First public release \n \n    The FTeik package is free software; you can redistribute it and/or modify it under the terms of\n    the GNU General Public License as published by the Free Software Foundation; either version 2 of\n    the License, or (at your option) any later version.\n    \n    This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n    without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n    \n    See the GNU General Public License for more details. You should have received a copy of\n    the GNU General Public License along with this program; if not, write to: Free Software\n    Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA\n\n# FTeik-Eikonal-Solver\n\nFTeik 2D and 3D Eikonal solver to compute first arrival traveltimes in a heterogeneous isotropic velocity model, with the possibility to use different grid spacing in all directions.\n\n## Reference paper\nDetailed implementation of local operator and global propagation scheme implemented in these subroutines come form the paper of : M. Noble, A. Gesret and N. Belayouni, 2014, Accurate 3-D finite difference computation of traveltimes in strongly heterogeneous media, Geophys.J.Int.,199,(3),1572-158.\n\n**If you use this program for an academic project or a commercial project, citing this paper would be appreciated.**\n\n## Compiling and running the code\n\nThe Eikonal solver subroutine includes 2 files:\n- In 2D fteik2d.f90 and Include_FTeik2d.f\n\nThe subroutines are written in Fortran 90\n\nThe package comes along with a simple example of a main program to show how to call the Eikonal solver\n- In 2D mainFTeik2d.f90\n\nWith the Gnu compiler \"gfortran\", it is recommended to use the options. Also included an example of a Makefile\n  -O3 -ffree-form\n  \n  **Compile and execute code**\n  \n ```console\n make all\n ./mainFTeik2d.exe\n ./mainFTeik3d.exe\n ```\n The program writes on disk the traveltimes in the file called ttmap.\n\n## Arguments required to call the Eikonal Solver subroutine\n\n - In 2D: call fteik2d(slow, tt, nz, nx, zsrc, xsrc, dz, dx, eps, n_sweep)\n \n - In 3D: call fteik3d(slow, tt, nz, nx, ny, zsrc, xsrc, ysrc, dz, dx, dy, eps, n_sweep)\n \n **NOTE 1**: TravelTime field array and slowness field array have the same\n           dimension. Slownesses are defined at center of cell, whereas times\n           are computed on the corners. In practice the last row and last\n           column of slowness field are not used in the computation. This is\n           the same as in Podvin and Lecomte algorithm.\n\n **NOTE 2**: In order to get accurate traveltimes, all real numbers (scalars and arrays)\n           must be decalred in double precision.\n\n- integer*4 - nz,nx,ny : Dimensions of the time field array tt\n                      in 2D tt(nz,nx) or in 3D tt(nz,nx,ny)\n                      No dimension may be lower than 3.\n\n - real*8    - dz,dx,dy : Mesh spacing along the 3 axis\n\n - real*8    - tt       : Travel time field array: tt((nz,nx) or tt(nz,nx,ny)\n\n - real*8    - slow     : Slowness field array: slow(nz,nx) or slow(nz,nx,ny)\n\n - real*8    - zs,xs, : Point source coordinates referred expressed in meters\n                    Licit ranges: [0.0,(nz-1.)*dzin][0.0,(nx-1.)*dxin]\n\n - integer*4 - epsin : radius in number of grid points arround source where then\n                   spherical approximation will be used (for most applications\n                   5 to 10 is enough.\n\n - integer*4 - nsweep : Number of sweeps over model. 1 is in general enough\n \n \n## Calling the eikonal solver from PYTHON\n\n- Specify which Fortran compiler should be used by F2PY. A list of Fortran compilers available to F2PY on the target system can be displayed by executing e.g. ```python -m numpy.f2py -c --help-fcompiler ```. Note that the output from ``` --help-fcompiler``` also displays the default compiler flags.\n\n- First compile the Eikonal subroutines with **f2py** (example for the 2D solver).\n```f2py -c fteik2d.f90 -m eik --fcompiler=gfortran --f90flags=-03 --quiet```\n\n- The syntaxe for calling the routine from python is shown using the python command ```print(eik2d.fteik2d.__doc__)```\n\n- Once he subroutine is compiled, the file **fteik2d-ex1.py** is a simple example that shows how to call the Eikonal solver from python. The velocity model along with the traveltime contours can then be plotted using the file **plot-ex1.py**.\n\n\n \n",
        "createdAt": "2019-08-20T08:12:40.000Z",
        "updatedAt": "2025-10-27T09:59:02.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Mark-Noble/FTeik-Eikonal-Solver/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "wangliang1989/gfmf",
        "url": "https://github.com/wangliang1989/gfmf",
        "description": "GFMF：一个基于理论波形（格林函数）的匹配滤波方法",
        "stars": 5,
        "forks": 0,
        "readme": "# GFMF\n\nGFMF 全称为 Green's Function Matched Filter。\nGFMF 是一种匹配滤波（模板匹配）方法。\n它相对于同类的其它方法的特点是采用的模板波形为依据波动方程合成的理论波形。\nGFMF 被称为“格林函数的”是因为技术上是将理论格林函数和观测波形互相关，以减少互相关的次数，\n节省结算时间。但是，其计算结果与使用理论波形等价。\n\nGFMF 按照 GPL v3 协议发布，即你可以使用、修改和再发布。但修改后也需要开源（包括增量部分）。\n详情请看[GPL v3 协议英文版](LICENSE)\n\n**目前，本程序尚不能称为一个应用软件。教程的撰写和公开尚在进行过程中。\n你若很了解匹配滤波方法，且对 Perl 语言很熟练了，你可以尝试使用本程序。**\n\n## 版本与下载\n\n我平时是向 dev 分支推送更新。dev 分支有问题的可能性比较大，而且有些修改我可能会改回去。\nmaster 分支则只包含我**自认为**无误的修改。\n你在 [release](https://github.com/wangliang1989/gfmf/releases)\n页面下载的则是带版本号的版本。\n\n## 安装\n\n安装 gfmf 并不需要先安装 sac 和 fk。但使用的时候需要 sac、fk 和 Perl 的并行模块\n[Parallel::ForkManager](https://metacpan.org/pod/Parallel::ForkManager)。\n\n### 编译\n可以使用 Gfortran 、 Intel Fortran 或 NAG Fortran 任一进行编译。\n````bash\ncd bin/\n# 以下命令执行其一即可，注意不应有任何报错\nmake -f Makefile_gfortran # 使用 Gfortran 编译\nmake -f Makefile_ifor # 使用 Intel Fortran 编译\nmake -f Makefile_nag # 使用 NAG Fortran 编译\n````\n\n### 修改环境变量\n将以下内容加入环境变量：\n````bash\nexport GFMF=你自己的真实路径\nexport PATH=$GFMF/bin:$PATH\n# 不要忘记 source\n````\n\n## 下一步\n\n1. 使用[GFMF_tiny](https://github.com/wangliang1989/GFMF_tiny)验证安装。\n2. 其它进一步的学习待续\n\n## 文章下载与引用信息\n\n下载论文及其 BibTex 和 Endnote 文件，请直接前往《地球物理学报》官网：\nhttp://www.geophy.cn/CN/abstract/abstract15922.shtml\n\n> 王亮, 梁春涛. 2021. 以虚拟地震的理论格林函数为模板搜寻小地震. 地球物理学报,64(7): 2374-2393, doi: 10.6038/cjg2021O0361\n\n> WANG Liang, LIANG ChunTao. 2021. Detecting small earthquakes using the theoretical Green's function of virtual earthquakes as templates Chinese Journal of Geophysics(in Chinese), 64(7): 2374-2393, doi: 10.6038/cjg2021O0361\n\n## 已引用本方法的论文\n\n如果你在论文中引用了我的上述论文。无论你的文章的主题为何，你都可以把你的已正式刊出的论文发给我。\n我会在此处列出。这样可以让别人知道你的研究工作，潜在地增加你的论文的引用量。\n",
        "createdAt": "2021-08-06T12:05:34.000Z",
        "updatedAt": "2025-08-06T06:55:23.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/wangliang1989/gfmf/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "schipp/higher_order_correlations_c2",
        "url": "https://github.com/schipp/higher_order_correlations_c2",
        "description": "Accompanying repository for \"Source effects in higher-order ambient seismic field correlations\" by Schippkus et al., in review",
        "stars": 0,
        "forks": 0,
        "readme": "# Source effects in higher-order ambient seismic field correlations\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.17778709.svg)](https://doi.org/10.5281/zenodo.17778709)\n\nThis repository contains all data products, metadata, and codes necessary to reproduce all figures of the manuscript \"Source effects in higher-order ambient seismic field correlations\" by Schippkus et al., in review.\n\n> [!NOTE]\n> Some of the notebooks will not run on laptops or personal PCs due to memory limitations. We ran the analysis on a server with 64 CPU threads and 512 GB of RAM.\n\n## Abstract\n\n<img align=\"left\" src=\"assets/snakeeye.webp\" width=\"250px\"> \nSeismic interferometry of the ambient seismic field is widely used for surface wave imaging. It typically requires synchronous station recordings and assumes uniform noise source distributions. Higher-order correlations, such as the re-correlation of direct waves (C2), have been suggested to facilitate imaging with asynchronous data and to improve an incomplete source distribution. Using field data and simulations, we show that C2 surface wavefields are instead highly sensitive to the original source distribution and even amplify the effects associated with the directional incidence. This can lead to systematic errors in the obtained velocity estimates and the downstream subsurface images. Strategies for selecting auxiliary stations in the re-correlation process do not mitigate this bias but can introduce additional wavefield distortions. Local and far-field imaging approaches using higher-order C2 correlation wavefields are affected by significant and systematic velocity estimation errors. Our results show that the re-correlation of direct waves is not an all-purpose correlation wavefield enhancement technique, and highlight the need for a careful consideration of source effects for improved imaging.\n\n## Repository structure\n\n- `data/`: Folder to hold data and simulations. For instructions to download and generate the data, see below.\n- `figures/`: All manuscript figures, generated by the Jupyter notebooks in `notebooks/`.\n- `meta/`: Station metadata and matplotlib style file.\n- `notebooks/`: Jupyter notebooks that implement all processing and generate the manuscript figures.\n\n## Data\n\nThe folder `data/` is empty at the start. All field data correlation functions have to be downloaded and simulations generated. With the same settings as used in the manuscript and notebooks, this will in total require ~70GB of disk space. We do not provide raw field data, but only the correlation functions necessary for reproduction of our results.\n\n### $C_1$ correlation functions \n\n#### Field data\n\n> [!IMPORTANT]\n> Field data $C_1$ correlations are hosted at University of Hamburg research data repository at [![DOI](https://www.fdr.uni-hamburg.de/badge/DOI/10.25592/uhhfdm.18152.svg)](https://doi.org/10.25592/uhhfdm.18152)\n\nDownload the files and save them in the `data/` directory.\n\nThe new files are \n\n- `correlations_for_c1_data.pt`: $C_1$ cross-correlations of all 1990 receiver stations with the master station in the center. Saved as a `torch.tensor` with shape `[1990, 3001]`. Sampling rate 5 Hz, 300 seconds of anti-causal and causal lapse time included. First dimension (the receiver stations) is sorted alphabetically by station name. Required for comparison of $C_1$ and $C_2$ wavefields.\n- `correlations_for_c2_data.pt`: $C_1$ cross-correlations of all 1990 receiver stations, including the master station, with the 304 auxiliary stations surrounding them. Saved as a `torch.tensor` with shape `[1990, 305, 3001]`. Sampling rate 5 Hz, 300 seconds of anti-causal and causal lapse time included. First dimension (the receiver stations) and second dimension (the auxiliary stations) are sorted alphabetically by station name. The basis for computing $C_2$ correlations.\n\nThese correlations are computed as described in the manuscript: ~4 weeks of continuous recordings are cut into 1-hr windows and spectrally whitened. All windows are cross-correlated and linearly stacked. No additional processing.\n\n#### Simulations\n\nRun the notebook `compute_correlations.ipynb` in `notebooks/` with the parameter `synthetic = True` in the second cell to generate both the simulated $C_1$ and $C_2$ correlation functions. \n\nRun it three times for the different `source_mode` settings (`source_mode=\"both\"`, `source_mode=\"boundary\"`, `source_mode=\"isolated\"`) to produce all sets of correlation functions used in the manuscript.\n\n### $C_2$ correlation functions\n\n#### Field data\n\nAfter downloading the data, run the notebook `compute_correlations.ipynb` in `notebooks/` with the parameter `synthetic = False` to compute the $C_2$ correlation functions from the field data $C_1$ correlations.\n\n#### Simulations\n\nSee $C_1$ simulation instructions above.\n\n## Requirements\n\nThe `pyproject.toml` file lists all packages required to run all notebooks. Follow your favourite installation procedure via `uv`, `pip`, or `conda`.",
        "createdAt": "2025-11-14T10:42:17.000Z",
        "updatedAt": "2025-12-02T07:28:56.000Z",
        "language": "Jupyter Notebook",
        "homepage": "https://doi.org/10.5281/zenodo.17778708",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.17778709",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.17778709",
            "dataCite": "10.5281/zenodo.17778709",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/schipp/higher_order_correlations_c2/main/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.17778709",
            "title": "schipp/higher_order_correlations_c2: Pre-print",
            "journal": "Zenodo",
            "dateReleased": "2025-12-01T00:00:00.000Z",
            "abstract": "Accompanying repository for \"Source effects in higher-order ambient seismic field correlations\" by Schippkus et al., in review",
            "citationsArray": []
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "shicks-seismo/simul2000_utilities",
        "url": "https://github.com/shicks-seismo/simul2000_utilities",
        "description": "Various utilities for the simul2000 local earthquake tomography program",
        "stars": 2,
        "forks": 2,
        "readme": "# simul2000_utilities\n",
        "createdAt": "2020-01-08T10:48:11.000Z",
        "updatedAt": "2022-01-05T13:42:14.000Z",
        "language": "Python",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/shicks-seismo/simul2000_utilities/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "seismology-hamburg/master",
        "url": "https://github.com/seismology-hamburg/master",
        "description": "Material for the Master's programme in Geophysics at University of Hamburg - Seismology.",
        "stars": 0,
        "forks": 0,
        "readme": "# M.Sc. Geophysics at University of Hamburg - Seismology\n\n<img align=\"left\" src=\"images/UHH_logo.png\" width=\"100px\">\n\nIn this repository, some material relevant to the master's program at University of Hamburg from the Seismology group is hosted. If you have questions, feel free to contact us.\n\n### programming skills\n\nTo get an impression of the skill level we expect in Python, you can try out the Python crash course on binder here: [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/seismology-hamburg/master/main). Wait for the binder server to load and click on `notebooks > E00_python_crash_course.ipynb` to launch the notebook. You can then directly work inside the jupyter notebook. For more information on jupyter notebooks, please check: [jupyter.org](https://jupyter.org)",
        "createdAt": "2021-05-10T13:36:04.000Z",
        "updatedAt": "2021-05-10T15:56:50.000Z",
        "language": "Jupyter Notebook",
        "homepage": "https://www.geo.uni-hamburg.de/geophysik/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/seismology-hamburg/master/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "RU21Seismology/Recfunk21",
        "url": "https://github.com/RU21Seismology/Recfunk21",
        "description": "The latest distribution of Recfunk codes for computation of receiver functions and harmonic decomposition analysis. ",
        "stars": 15,
        "forks": 3,
        "readme": "# Recfunk21 （Version21 released on 2021:021:21:00:00）\nThis software package **Recfunk21** (including all necessary shell scripts) is precompiled and provided for doing receiver function analysis using multitaper spectral correlation estimates. A step-by-step guide is provided together with this package. If you have any questions, please contact RU seismic group members: Xiaoran Chen (xiaoran.chen@rutgers.edu) or Vadim Levin (vlevin@eps.rutgers.edu).\n\nRelevant background reading: **Park and Levin (2000)** and **Park and Levin (2016a, 2016b)**\n\n**A Practical Guide.pdf** is developed by current and former members of Rutgers Seismology group (Xiaoran Chen, Yiran Li, James Bourke), with contributions from Zhenxin Xie, and encouragement from Prof. Vadim Levin (Rutgers) and code’s author Prof. Jeffrey Park (Yale). It is based on the previous guide to J. Park’s codes by (then) members of Rutgers Seismology group Alex Nikulin and Ben Marshall.\n\n**Instructions** described in this guide include\n  1) downloading and using SAC files fetched by PyWEED,\n  2) performing initial quality control using Seisgram2K,\n  3) running basic receiver function analysis,\n  4) running receiver function analysis with harmonic decomposition\n\n### ESSENTIAL NOTE 1: \nThis guide will explain how to perform data analysis, but not why this analysis is done, that part is a responsibility of the user.\n\n### ESSENTIAL NOTE 2: \nTo get a better idea of what the goals of receiver function analysis are – read the background papers and references therein. To understand what the codes and scripts do to get the analysis done – read them.\n\n### References:\nPark, & Levin. (2016a). Anisotropic shear zones revealed by backazimuthal harmonics of teleseismic receiver functions. Geophysical Journal International, 207(2), 1216-1243. http://dx.doi.org/10.1093/gji/ggw323\n\nPark, & Levin. (2016b). Statistics and frequency-domain moveout for multiple-taper receiver functions. Geophysical Journal International, 207(1), 512-527. https://doi.org/10.1093/gji/ggw29116\n\nPark, & Levin, V. (2000). Receiver Functions from Multiple-Taper Spectral Correlation Estimates. Bulletin of the Seismological Society of America, 90(6), 1507-1520. http://dx.doi.org/10.1785/0119990122\n",
        "createdAt": "2021-01-20T22:01:46.000Z",
        "updatedAt": "2025-11-03T12:01:08.000Z",
        "language": "Fortran",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/RU21Seismology/Recfunk21/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "KowalskiThomas/IPGP-EarlyWarning",
        "url": "https://github.com/KowalskiThomas/IPGP-EarlyWarning",
        "description": "The EarlyWarning (Alarme Précoce) application, used by the Piton de la Fournaise Volcano Observatory (Reunion Island)",
        "stars": 0,
        "forks": 0,
        "readme": "# IPGP / OVPF - EarlyWarning\n\nThis repository has moved and is now hosted on IPGP's GitHub. [Check it out](https://github.com/IPGP/AlarmePrecoce) or [See my commits](https://github.com/IPGP/AlarmePrecoce/commits?author=KowalskiThomas)\n",
        "createdAt": "2018-06-11T10:46:13.000Z",
        "updatedAt": "2019-11-08T12:32:30.000Z",
        "language": null,
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/KowalskiThomas/IPGP-EarlyWarning/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "SuwenJunliu/PyRFDector",
        "url": "https://github.com/SuwenJunliu/PyRFDector",
        "description": "An receiver function selector base on Convolutional Neural Networks and Sequencer in seismology",
        "stars": 1,
        "forks": 0,
        "readme": "# PyRFDector\nAn receiver function selector base on Convolutional Neural Networks and Sequencer in seismology\n",
        "createdAt": "2021-09-27T04:21:57.000Z",
        "updatedAt": "2021-09-28T00:14:33.000Z",
        "language": null,
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/SuwenJunliu/PyRFDector/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "computational-seismology/computational-seismology.github.io",
        "url": "https://github.com/computational-seismology/computational-seismology.github.io",
        "description": "Please read https://github.com/computational-seismology/computational-seismology.github.io/blob/gh-pages/README.md before contributing.",
        "stars": 1,
        "forks": 0,
        "readme": "",
        "createdAt": "2017-10-23T20:15:44.000Z",
        "updatedAt": "2020-10-07T01:01:12.000Z",
        "language": "JavaScript",
        "homepage": "https://computational-seismology.github.io/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "mronacgiannone/DL_Seismoacoustic_Fusion",
        "url": "https://github.com/mronacgiannone/DL_Seismoacoustic_Fusion",
        "description": "Repository associated with \"Deep Multimodal Learning for Seismoacoustic Fusion to Improve Earthquake-Explosion Discrimination within the Korean Peninsula\" (Ronac Giannone et al., 2024)",
        "stars": 1,
        "forks": 1,
        "readme": "# DL_Seismoacoustic_Fusion\nThe information in this repository outlines how to extract and process the seismic and infrasound data used to train both seismic and seismoacoustic neural networks as introduced in the manuscript, \"Deep Multimodal Learning for Seismoacoustic Fusion to Improve Earthquake-Explosion Discrimination within the Korean Peninsula\", (Ronac Giannone et al., 2024, in review). ObsPy and TensorFlow software packages are used for geophysical and deep learning analyses, respectively.  \n## Install\nTo install:\nconda env create -f environment.yml\n## Activate\nTo activate:\nsource activate koreageonet\n## Data\nThe data used in this study can be found at 10.5281/zenodo.10795252.\n## Additional Info\nArray locations can be found in Korea_Array_Locations.zip. Earthquake-Explosion databases as well as spreadsheets containing information on individual array detections can be found in Spreadsheets.zip.\n",
        "createdAt": "2024-03-07T19:52:49.000Z",
        "updatedAt": "2024-10-25T19:59:11.000Z",
        "language": "Jupyter Notebook",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "10.5281/zenodo.10795252",
            "openAlex": "",
            "openCitations": "10.5281/zenodo.10795252",
            "dataCite": "10.5281/zenodo.10795252",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/mronacgiannone/DL_Seismoacoustic_Fusion/main/README.md",
        "mainPaper": {
            "doi": "10.5281/zenodo.10795252",
            "title": "mronacgiannone/DL_Seismoacoustic_Fusion: Zenodo",
            "journal": "Zenodo",
            "dateReleased": "2024-01-01T00:00:00.000Z",
            "abstract": "Earthquake-Explosion seismic and infrasound data. Both types of data are windowed based on databases outlined in https://github.com/mronacgiannone/DL_Seismoacoustic_Fusion.git. Seismic waveforms are filtered 1-10 Hz and infrasound waveforms are filtered 0.5-10 Hz. The Korea Institute of Geoscience and Mineral Resources (KIGAM) provided the explosions database (event origin times and locations). Earthquake events were extracted from the Korean Meteorological Administration (KMA) and Han et al. (2023). ",
            "citationsArray": []
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "MrXiaoXiao/ESPRH",
        "url": "https://github.com/MrXiaoXiao/ESPRH",
        "description": "Automatic earthquake catalog building workflow: EQTransformer + Siamese EQTransformer + PickNet + REAL + HypoInverse",
        "stars": 25,
        "forks": 4,
        "readme": "# ESPRH\n区域地震目录自动构建流程脚本，整合了开源的各个模块，用于从连续波形自动构建区域地震目录。\n\nAutomatic regional-scale earthquake catalog building workflow: EQTransformer + Siamese EQTransformer + PickNet + REAL + HypoInverse.\n\n## 安装说明 Installation\n```Bash\nconda create -n ESPRH\nconda activate ESPRH\nconda install python=3.6 tensorflow-gpu=1.14 keras-gpu=2.3.1 h5py=2.10 matplotlib=3.2 pyyaml cudatoolkit cudnn pandas tqdm pyproj jupyter notebook basemap\nconda install -c conda-forge obspy\npip install keras-rectified-adam\n```\n注：只是推理的话不需要GPU也可执行，则将对应的tensorflow-gpu, keras-gpu换成tensorflow, keras。并且不要安装 cudatoolkit和cudnn。\n\n复制该程序包到你的计算机。\nClone this project to your machine. \n\n```bash\ngit clone https://github.com/MrXiaoXiao/ESPRH\ncd ESPRH\n```\n\nREAL和HypoInverse的安装请参照它们对应的说明.HypoInverse在bin下的文件名请设置为‘hyp1.40'.\n\n\n## 使用说明 Usage\n进入目录，依次执行脚本00-06，在default_pipline_config.yaml修改对应参数。\n\nEnter the directory. Execute scripts 00-06. Customize your configurations in file default_pipline_config.yaml.\n\n## 常见问题说明 FAQ\n(1) 目前代码只老版本的REAL, 请使用REAL_scripts文件夹里面的REAL_old_version.zip。\n    Current codes support only the old version of REAL. Please use 'REAL_old_version' under the 'REAL_scripts' folder;\n    \n(2) 在一些机器上perl可能需要添加执行权限，请为REAL_scripts文件夹里面的runREAL_temp.pl增加执行权限。\n    Some computers may need permission for Perl scripts. Please add permission to execute for 'runREAL_temp.pl' under the 'REAL_scripts' folder.\n\n\n## 相关的工作 Related research\n[1] Wu, Xueshan; Huang, Song; Xiao, Zhuowei; Wang, Yuan (2022): Building Precise Local Submarine Earthquake Catalogs via a Deep-Learning-Empowered Workflow and its Application to the Challenger Deep. Frontiers. Collection. https://doi.org/10.3389/feart.2022.817551 \n\n[2] Shun Yang, Zhuowei Xiao, Yue Zhu, Yumei He, Mingming Jiang, Chit Thet Mon, Guangbing Hou, Myo Thant, Kyaing Sein. (2021, In Preparation). A deep-learning-empowered pipeline for building regional earthquake catalogs and its application to the central Myanmar region.\n\n## 引用 Citation\n如果你使用该脚本，请在文章中引用以下工作：\n\nFor EQTransformer, please cite:\n\nS. Mostafa Mousavi, William L Ellsworth, Weiqiang Zhu, Lindsay Y Chuang, and Gregory C Beroza. (2020). Earthquake transformer—an attentive deep-learning model for simultaneous earthquake detection and phase picking. Nature Communications 11, 3952. https://doi.org/10.1038/s41467-020-17591-w\n\nFor Siamese EQTransformer, please cite:\n\nZhuowei Xiao, Jian Wang*, Chang Liu, Juan Li, Liang Zhao, and Zhenxing Yao. (2021). Siamese Earthquake Transformer: A pair-input deep-learning model for earthquake detection and phase picking on a seismic array. Journal of Geophysics Research: Solid Earth. https://doi.org/10.1029/2020JB021444\n\nPickNet for phase refinement:\n\nWang, J., Xiao, Z., Liu, C., Zhao, D., & Yao, Z. (2019). Deep Learning for Picking Seismic Arrival Times. Journal of Geophysical Research: Solid Earth, 124(7), 6612–6624. https://doi.org/10.1029/2019JB017536\n\nREAL for linking seismic phases:\n\nMiao Zhang, William L Ellsworth, and Gregory C Beroza. (2019). Rapid Earthquake Association and Location. Seismological Research Letters, 90(6), 2276–2284. https://doi.org/10.1785/0220190052\n\nHypoInverse for locating earthquakes:\n\nFred W Klein. (2002). Userʼs Guide to HYPOINVERSE-2000, a Fortran Program to Solve for Earthquake Locations and Magnitudes 4/2002 version. USGS, Open File Report 02-171 Version, 1, 123.\n\n## (拖延症的)近期更新计划\n1. 增加02_run_S-EqT步骤的并行加速\n2. 增加计算里氏震级（好像obspy有内置函数，回头加一下）。有需要的也可以自己加一下（https://docs.obspy.org/packages/autogen/obspy.signal.invsim.estimate_magnitude.html）。\n\n## 问题反馈 Bug report\n如果遇到程序上的问题，请在这个repo开启一个issue（尽量不要邮件联系）。\n\nIf you occur any bugs or questions, you can open a new issue in this repo. \n\n## 邮箱 E-mail\nxiaozhuowei@mail.iggcas.ac.cn\n\n## 致谢 Acknowledgments\nWe would like to thank S. Mostafa Mousavi and his colleagues for developing the EqT model (https://github.com/smousavi05/EQTransformer), which is the base of our S-EqT model.\n\nWe would like to thank Miao Zhang for developing REAL (https://github.com/Dal-mzhang/REAL).\n\nWe would like to thank Fred Klein for developing HypoInverse (https://www.usgs.gov/software/hypoinverse-earthquake-location)\n\nWe would like to thank Yijian Zhou for developing the python interface for HypoInverse (https://github.com/YijianZhou/Hypo-Interface-Py)\n",
        "createdAt": "2021-12-01T13:26:06.000Z",
        "updatedAt": "2025-08-19T12:48:08.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/MrXiaoXiao/ESPRH/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Shankho-Niyogi/b-value_mapping",
        "url": "https://github.com/Shankho-Niyogi/b-value_mapping",
        "description": "The Matlab script takes a catalog of earthquakes and creates a b value distribution map.",
        "stars": 0,
        "forks": 0,
        "readme": "# b-value_mapping\nThe Matlab script takes a catalog of earthquakes and creates a b value distribution map.\n\nThe input excel file should be configured exactly as the one provided in the repository. \nThe required input to the script is magnitude of completeness, intervals of latitude and longitude.\n",
        "createdAt": "2023-03-18T00:21:50.000Z",
        "updatedAt": "2023-03-18T00:59:17.000Z",
        "language": "MATLAB",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Shankho-Niyogi/b-value_mapping/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "arkonique/sac2mt5",
        "url": "https://github.com/arkonique/sac2mt5",
        "description": "sac2mt5 v1.3 - SAC to DSN format converter",
        "stars": 0,
        "forks": 0,
        "readme": "# sac2mt5\n\n\nsac2mt5 v1.3\nSAC to DSN format converter\n\n-------------------------\nsac2mt5 is an abstraction layer for the SACtoDSN.pl perl script written by McCaffrey & Walter available at http://www.geology.cwu.edu/facstaff/walter/mt5/SACtoDSN.pl\n\n-------------------------\n\n\n## Installation\n\nYou need to clone this repository to use sac2mt5 - \n\n```console\ngit clone https://github.com/arkonique/sac2mt5.git\n```\n\n\nBefore installing sac2mt5, please make sure some of the paths used in the package are consistent with your system.\n1. Inside sac2mt5/TauP-0.01/lib/Taup.pm, there are the paths of the Taup binaries and velocity models, relative to the path of your Taup installation. Please change these accordingly.\n    ```perl\n     $self->{'taup_curve'} = $ENV{'TAUP_HOME'}.'/bin/taup_curve';\n     $self->{'taup_time'} = $ENV{'TAUP_HOME'}.'/bin/taup_time';\n    ```\n    ```perl\n     my $tvelFile = $ENV{'TAUP_HOME'}.'/StdModels/'.$model.'.tvel';\n     my $ndFile = $ENV{'TAUP_HOME'}.'/StdModels/'.$model.'.nd';\n    ```\n\n2. Inside sac2mt5/SAC2DSN.pl, change the following paths according to your installation:\n\n\t```perl\n\tuse lib 'usr/local/share/perl/5.26.1/'; # Path to perl libraries\n\t```\n\t```perl\n\tmy $taup_path='/home/arkonique/TauP-2.4.5'; # Path to Taup installation\n\t```\n\nTo install sac2mt5, run:\n\n```bash\nsource install.sh\n```\n\n## Usage\n\n\n### Usage:\n\n```bash \n    sac2mt5 [-d/--directory<data directory>] [-o/--output<output file>] [-t/--date <YYMMDDHHmmss.s = date and time>] [-l/--latlong <LAT/LONG>] [-n/--depth <event depth>] [-h/--help]\n```\n\n### Options:\n\n    -d/--directory   Specify the directory containing all the SAC files. \n                     This directory must also contain a subdirectory called RESP containing all the instrument response files\n\n    -o/--output      Specify the name of the output DSN file. This will be created inside a directory called selected_s2m in\n                     the data directory. Preferrably a 6 digit code. The files will be generated as outputP1.DSN and so on\n\n    -h/--help        Display this help\n\n    -t/--date        Specify the date and time in YY/MM/DD/HH/mm/ss.s format\n\n    -l/--latlong     Specify the latitude and longitude of the event in LAT/LON format\n\n    -n/--depth       Specify the depth of the event in km\n\n\n**NOTE:**\n\n1. Please make the required libraries and make sure all the paths are specified correctly in the perl script SACtoDSN.pl\n2. Before using this script please make sure to install the three libraries provided along with this (No need to do this if you installed using `install.sh`)\n3. Additional paths might need to be changed based on your TauP installation. Check where your \".tvel\" files are within your installation. Put in the required directory name in TauP.pm inside Taup-0.01\n\n--------------------\n\n### Updates:\n\nThe older repository containing v1.0 has been removed so those changes are no longer available. This is a complete list of changes that have been made:\n\n\n1. Added capability to also mark P and S wave arrival times, along with adding the necessary headers and selecting only those seismograms with a P or S wave arrival within the specified window\n\n2. Added capability to create separate DSN files for each 100 stations in the data directory as the maximum limit for MT5 for the number of stations in 100\n\n3. Added capability to separately created different DSN files for P ans S waves for an easier handling of the results when using MT5INT\n\n\n### Bugfixes:\n\n- Changed the input format for dates due to a datetime input error, which caused the script to fail when passing inputs from an event list file through a script when time is in single digits. The change in format also allows for more readable inputs.\n- Fixed station selection algorithm for selecting each 100 stations, which caused stations with similar names to get selected.\n- Fixed creation of S wave DSN files, which are created with the E component of a station, followed by the N component, which were being created haphazardly in the beginning\n- Fixed datetime parsing which caused wrong inputs to be given as event dates\n\n--------------------\n\n### Known Bugs and Problems:\n\n- Stations with clear P and S wave marks are sometimes not used\n- SAC files need to be copied to the present working directory for this to work, making the script much slower\n- Installation script trashes .bashrc sometimes\n\n### Upcoming updates:\n\n- [ ] A settings file to enable selection of required paths to avoid manual adjustments\n- [ ] A possible GUI\n- [ ] A powershell port\n\n\n#### No longer required:\n\n~~The data directory must only contain files which have a valid P and S wave arrival time within the seismogram. If not, the program will exit with an error. So select all good usable waveforms, put them in a directory, along with their response files as specified above and provide the path to that directory to the script.~~\n",
        "createdAt": "2019-01-05T20:12:56.000Z",
        "updatedAt": "2025-10-09T19:01:07.000Z",
        "language": "Perl",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/arkonique/sac2mt5/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "sandialabs/pycheron",
        "url": "https://github.com/sandialabs/pycheron",
        "description": "Pycheron - A python library for quality control of seismic data based on IRIS Mustang.",
        "stars": 26,
        "forks": 7,
        "readme": "# Pycheron v3.0.0<br>\n\nDeveloped by Kale Aur (kaaur@sandia.gov), Jessica Bobeck (jbobeck@sandia.gov), Anthony Alberti (aalber@sandia.gov), \nand Phillip Kay (prkay@sandia.gov)\n\nFor an overview of Pycheron, its features, basic workflow, and an example application using a synthetic QC dataset, please refer to the following paper:\n\n* **Aur, K.A., Bobeck, J., Alberti, A., Kay, P. (2021). Pycheron: A Python-based seismic quality control software package. Seismological Research Letters. https://doi.org/10.1785/0220200418**\n\nPython library for seismic data quality control originally ported from IRIS's IRISMustangMetrics, IRISSeismic, and\nseismicRoll R packages:\n\n*   IRISMustangMetrics R Cran Package\n    (Callahan, J., R. Casey, M. Templeton, and G. Sharer (2020, March 20). CRAN-Package IRISMustangMetrics.\n    The Comprehensive R Archive Network. Retrieved from\n    https://cran.r-project.org/web/packages/IRISMustangMetrics/index.html)\n\n*   IRISSeismic R Cran Package\n    (Callahan, J., R. Casey, G. Sharer, M. Templeton, and C. Trabant (2019, Oct 22). CRAN-Package IRISSeismic.\n    The Comprehensive R Archive Network. Retrieved from https://cran.r-project.org/web/packages/IRISSeismic.index)\n    \n*   seismicRoll R Cran Package\n    (Callahan, J., R. Casey, M. Templeton, and G. Sharer (2020, July 8). CRAN-Package seismicRoll. The Comprehensive R\n    Archive Network. Retrieved from https://cran.r-project.org/web/packages/seismicRoll.index)\n\n*   Many of the baseline function thresholds are based on IRIS's thresholds found in the section labeled \n    `Some Metric Tests for Broadband Data`: \n    IRIS: Tutorials: Seismic Data Quality Assurance Using IRIS MUSTANG Metrics. (2016, April 26). Tutorials: Seismic Data Quality Assurance Using IRIS MUSTANG \n    Metrics. https://ds.iris.edu/ds/nodes/dmc/tutorials/seismic-data-quality-assurance-using-iris-mustang-metrics/\n\n\n## Operating Systems\nPycheron has been tested on Linux/OSX systems, but issues still exist on Windows machines. **Please ensure the OS is either Linux/OSX before using.**\n\n## Setting up a virtual environment before install\nThe tool used for dependency management ([Poetry](https://python-poetry.org/)) requires the use of a virtual environment manager. \nBelow are a few virtual environment management options; however, depending on the system architecture (i.e., Windows, Mac, or Linux), some may work better than others. Given the option, it is `strongly recommended` to use Anaconda:\n\n* [venv](https://docs.python.org/3/library/venv.html) (comes default with Python 3.3+)\n* [PyEnv](https://github.com/pyenv/pyenv) (does not work on Windows)\n* [Anaconda](https://docs.anaconda.com/anaconda/install/)\n* [PipEnv](https://github.com/pypa/pipenv)\n\nAlternatively, if a specific environment manager is not chosen beforehand, Poetry will create one. \n\nNote, a **Python 3.6.9** environment is required and must be *activated before installing packages and using Pycheron. \n\n## Installing\nBefore installing Pycheron, download:\n\n1. [poetry](https://python-poetry.org)\n2. [git-lfs](https://git-lfs.github.com/)\n\nThe links above will have install instructions available.\n\nAfter git-lfs is installed, run the following commands in the Pycheron root directory:\n```bash\ngit lfs install\ngit lfs fetch\ngit lfs pull\n```\n\n\nThe easiest way to install Pycheron is to run:\n\n`./build.sh 3.0.0`\n\nwhere 3.0.0 is the version number of Pycheron. This bash script will uninstall any old versions of Pycheron and install all necessary Python dependencies.\n\nIf contronted with an `EOFerror` raised by Pebble, try installing Pebble version 4.3.10 and re-run the `build.sh` script.\n\n## Build from scratch\n\nTo build the Pycheron version from scratch run the following command from the Pycheron top-level directory:\n\n`poetry build`\n\nand then:\n\n`cd dist`\n\nThen use pip to install the .whl file:\n\n`pip install pycheron-<version>-py3-none-any.whl`\n\n## Installing Oracle dependencies\n`cx_Oracle` requires that Oracle Instant Client be installed. Instructions for installation can be found [here](https://cx-oracle.readthedocs.io/en/latest/user_guide/installation.html#install-oracle-client). Be sure to export the `LD_LIBRARY_PATH` environment variable before using.\n\n## Known Issues\nIf using Mac OSX with an Anaconda environment and receive errors relating to matplotlib, matplotlib will need to be reinstalled using version 2.2.5 with the command `conda install matplotlib=2.2.5\". Afterwards, re-build Pycheron using the build script. \n\n## Fortran\nIt is recommended to use gfortran 6.3 for OSX. Currently Fortran compilations only work on OSX and Linux. If using a Windows operating system, there is an option to turn off Fortran in the functions that use it (e.g., `staltaMetric`, `psdMetric`,`spikesMetric`, `repeatedAmplitudeMetric`)\nby setting the variable `fortran` equal to `False`. Please refer to the 'Operating Systems' section regarding Windows use with Pycheron.\n\n## How to run Pycheron's backend\nData is ingested into Pycheron by directing the package to a local directory \ncontaining data files, a single data file, a CSS3.0 wfdisc table (flatfile), an ObsPy stream, or a database containing \nCSS3.0 formatted tables. Pycheron’s main interface is via a wrapper code, `callPycheronMetric.py`, that reads from a \nconfiguration YAML Ain’t Markup Language (YAML) file, `pycheronConfigTemplate.yaml`. The `pycheronConfigTemplate.yaml` file can be found in the top-level directory of Pycheron.\n<br> \nThe configuration template file specifies: \n\n   1) from where Pycheron reads input data; \n   2) what QC metrics are calculated; \n   3) QC metric parameter and threshold settings; \n   4) plotting information; and \n   5) various other parameterizations\n\n`callPycheronMetric` combines all of the metrics in the `pycheron.metrics` package and executes them. For each QC issue, \nweighting is assigned based on user input and an overall QC summary is created on a per station basis. Currently, all QC\nmetrics are assigned a default weight of 1 (i.e., 100%), meaning they are weighted equally. Users can adjust each QC \nmetrics weights within the UI and the quality summary report and color coding will be adjusted on the fly. Work is in \n\nAll results are stored in a Sqlite3 database and can either be visualized in a user interface (UI) via Plotly’s \nDashboard Web UI, Dash, or output to comma-separated value (CSV) files that are more readily processed through an \nautomated pipeline. \n\n`pycheronConfigTemplate.yaml` is a configuration template containing all configurable input parameters \nand their default values. The `pycheronConfigTemplate.yaml` file can be found in the top-level directory of Pycheron.\n\nWhen updating values which require file paths, ensure the **full path** is used and not the relative path.\n\nPlease refer to the sections below for instructions on how to run Pycheron with the following data types:\n\n* Directory (of MSEED files)\n* Wfdiscs (CSS)\n* Obspy Streams\n* Singular MSEED file \n* Wfdisc database (Oracle)\n\n## Datasets\n\nPycheron can ingest several different data formats. `callPycheronMetric` currently accepts five data types:\n\n- Directory (of MSEED files) (datatype = `dir`)\n- Wfdisc (CSS) (datatype = `wfdisc`)\n- Obspy Stream (datatype = `stream`)\n- Singular MSEED file (datatype = `mseed`)\n- Wfdisc database (Oracle) (datatype = `wfdb`)\n\nAll data formats are converted into ObsPy Stream or Trace objects for use inside Pycheron. \nThus, any format read by ObsPy can also be used within Pycheron. The data type needs to be defined in the YAML\nconfiguration file, as does the data file's directory path. \nDepending on the data ingested there are different parameters that can be set; these will be discussed in the \n`callPycheronMetric Example` section. \n\n### Directory (datatype = `dir`)\n\nIf a directory is utilized, Pycheron assumes that all files are `MSEED` formatted files. The following \nfolder structure is required:\n\n```\nMy Folder\n    BGU_EHE_001.mseed\n    BGU_EHN_001.mseed\n    BGU_EHE_002.mseed\n    BGU_EHN_002.mseed\n    JPU_EHE_001.mseed\n    JPU_EHN_001.mseed\n    JPU_EHE_002.mseed\n    JPU_EHN_002.mseed\n    TCRU_EHE_001.mseed\n    TCRU_EHN_001.mseed\n    TCRU_EHE_002.mseed\n    TCRU_EHN_002.mseed\n```\n\nEach file should be formatted as follows: `<station>_<channel>_<julian day>.mseed`. \nThe data utilized within this tutorial is already formatted correctly. The `<station>_<channel>_<julian day>.mseed` format is required when using the `dir` option from callPycheron.\n\n### Wfdisc (CSS) (datatype = `wfdisc`)\nIf a Wfdisc (CSS) flat file is utilized, the `.wfdisc` file is the only input file needed.  \nHowever, it is important to ensure that the `.w` file paths in the dir column listed in the `.wfdisc` file are \nrelative to the file. This directory structure is required when reading wfdisc data.\n\nFor example, using the following directory structure below:\n\n```\nMy Folder\n    data.wfdisc\n    data_directory\n        OWUT_EHZ_001.w \n        OWUT_EHZ_002.w \n        OWUT_EHZ_003.w \n        .\n        .\n        .\n        waveformN.w\n```\n\nThe dir column in the `data.wfdisc` file should appropriately point to the My Folder/data_directory for each `.w` file \nso that Pycheron is able to properly retrieve the respective file:\n\n```\nOWUT   EHZ       1293840000.00000   1927888      774  2011001  1293860963.29000  2096330  100.000000          1.00000         -1.00000 -      o i4 - /My Folder/data_directory/               OWUT_EHZ_001.w                            0        -1 10-APR-18        \nOWUT   EHZ       1293861020.31000   1927890      774  2011001  1293861041.29000     2099  100.000000          1.00000         -1.00000 -      o i4 - /My Folder/data_directory/               OWUT_EHZ_002.w                      8385320        -1 10-APR-18        \nOWUT   EHZ       1293861041.41000   1927892      774  2011001  1293865831.39000   478999  100.000000          1.00000         -1.00000 -      o i4 - /My Folder/data_directory/               OWUT_EHZ_003.w                      8393716        -1 10-APR-18        \n```\n\nWhen using this datatype, it is required to specify the network to process stations within (set via the `network` \nparameter)\n\nUsers also have the option to:\n \n *  Specify station to process using the `station` parameter; only processes that single station's data \n *  Split data up by days into a stream object using the `byDay` parameter \n *  Specify which station to start processing at using the `stationStartAt` parameter; starts processing data for that \n    station instead of processing data from the beginning of the wfdisc file and continues processing subsequent \n    stations \n\n\n### Mseed (datatype = `mseed`)\n\nThis datatype indicates that the user wishes to read in a single `mseed` formatted file.  \nIf the file contains multiple days worth of data, it is recommended to use the directory datatype and split\nthe data into smaller segments. \n\n### Stream (datatype = `stream`)\n\nThis datatype should only be used when inside a Python console and not while executing via the command line. \nWhen this type is specified, it is assumed that the Stream has either been manually loaded using \n`obspy.read()` or downloaded using the ObsPy client and then calling `callPycheronMetric()` as a function.\n\n### Connecting to a database with Wfdisc tables (datatype = `wfdb`)\n\nPycheron currently supports connection to Oracle databases via SQLAlchemy. This datatype option supports \nreading Wfdisc database tables of the CSS 3.0 format.\n\nBefore accessing the Oracle Wfdisc database, users will need to check their connection settings from the `connect_dict` value in the [configuration file](./pycheronConfigTemplate.yaml) to ensure the correct db values exist and a connection can be made with the database. One option would be with [Oracle SQL Developer](https://www.oracle.com/database/technologies/appdev/sqldeveloper-landing.html).\n\nThe following tables are required in the Wfdisc database for the `wfdb` option to function correctly:\n\n* WFDISC\n* SITE\n* SENSOR\n* INSTRUMENT\n* AFFILIATION\n* SITECHAN\n\nThese may be named differently, but must exist, and are set via the `table_names` parameter. A more detailed description of which fields/types must exist in each of these tables can be found [here](ftp://ftp.pmel.noaa.gov/newport/lau/tphase/data/css_wfdisc.pdf).\n\nWhen retrieving data from a database, there will be instances where results from the tables will include a directory to retrieve files from (i.e., an INSTRUMENT table may have a field that points to a file location of a RESP file). In these instances, it's important that the file is accessible by the user locally. If accessing files from an external drive/directory, ensure the drive/directory is mounted before attempting to access it. These paths must be **full filepaths** (not relative paths) to access the data.\n\nThu user has the option to specify the wfdisc `start_time` and `end_time` using the `wfdb_start` and `wfdb_end` parameters.\n\nWhen modifying the configuration file, the user also has the option to configure the minimum amount of time between `start_time` and `end_time` of the WFDISC table in the database using the `wfdb_mintime`. The available options are:\n\n* \"day\" (with quotes, lowercase)\n* \"hour\" (with quotes, lowercase)\n* None (without quotes, uppercase N)\n\nThere are some functions in Pycheron (when calculating PSDs, for example) that will not return valuse if a given `start_time` or `end_time` are less than a fixed amount of time (e.g., PSDs require at least one hour). This gives the user the option to filter out any results that wouldn't be able to generate metrics otherwise.\n\n\n### `callPycheronMetric` Input Parameters\n\nThe `callPycheronMetric.py` script is installed as a script so that it can be executed directly from the command line. \nThe `callPycheronMetric` has over 50 input parameters specified, with a majority of them related to setting default \nthreshold values for each of the corresponding metrics. These thresholds determine whether a QC issue is flagged. \nEach QC metric has its own set of parameters that may be adjusted.\nThus, it is recommended to thoroughly read through the documentation page within the  `callPycheronMetric` script as \neach parameter is well-documented. \n\n### Running Pycheron via Command Line\n\nThe easiest way to run `callPycheronMetric` is via the command line with the `pycheronConfigTemplate.yaml` file. \nAfter installation and configuration are complete, Pycheron can be run via the terminal \nwith the following command:\n\n`python <path>/<to>/callPycheronMetric.py <path>/<to>/<CONFIG FILE>.yaml`\n\nFor this tutorial, the `pycheronConfigTemplate.yaml` is accessible via the `data` folder.\n\n## Pycheron Output Options\n\n### CSV output \n\nOutput to a CSV file is possible. The output is saved into the following folder structure: \n\n```\n<output_directory>\n    <network_dir>\n        <station_dir>\n            <channel_1_dir>\n                <metric>.csv\n                <metric_plot>.png\n            <channel_2_dir>\n                <metric>.csv\n                <metric_plot>.png\n            <station_level_metrics>.csv\n            <station_level_plot>.png\n         <network_level_metric>.csv\n         <network_level_plot>.png\n```\n\n### Database output \n\n\nIf desired, Pycheron can output results into a local sqllite database (this method of output is required to use \nthe Pycheron UI). The default name for the database is `pycheron.db` but this can be changed within the \n`pycheronConfigTemplate.yaml` file using the `database` parameter. The `session` parameter, which sets the session name\nin the database, can also be utilized to group different processing runs/experiments (default = None). Lastly, \nthe `overwrite` option exists to overwrite entries that have the same snclq and start/end time. \n\nThe database tables that exist within the generated output database will depend on which QC metrics were specified \nto be calculated. If `calcAll = True` was specified, then all available tables will be generated. Pycheron can generate \na table for the following information: \n\n*   A main pycheron table containing basic metadata information \n*   Each QC metric within the metrics directory (SOH metric is broken up into several tables)\n*   Station and network noise models \n*   PsdStatistics function\n*   Plotting function tables (e.g., psdPlot, dailyPDFPlot) \n*   Summary report counts and values table used for the QC Summary Report within the Pycheron UI (see UI section\n    below for more information \n\nUsers can choose to view the database tables via any sqqlite database viewer of their choosing, such as \n[sqlitebrowser](https://sqlitebrowser.org/) (more on this [below](#viewing-results)).\n\n\n##  callPycheronMetric Examples\n\nThis section will walk users through how to use Pycheron with each input data type. \n\nSample data files for this tutorial live within the `tutorials/data` directory within the Pycheron top-level directory. \n\nTo process the data, execute the following steps: \n\n1. To de-compress the data, first cd into the aforementioned directory: \n\n   `cd /tutorials/data` \n\n    then run:\n\n    ```\n    tar -xzvf callPycheronMetric_tutorial_small.tar.gz \n    ```\n\n    This should create a directory called `pycheron_test` that contains six `mseed` data files,\n    a `.wfdisc` file, and a `.w` file nested in a `/data` directory. The `mseed` data spans from 2011/01/01-2011/01/02, \n    for one stations within the University of Utah (UU) seismic network: BGU (HH* channels). The wfdisc table/file \n    contains data for BGU HHZ 2011/01/01.\n    \n    >Alternatively, for a  larger data set, run the following command instead (`NOTE: the larger data set will take significantly longer to run`):\n    >\n    >```\n    >tar -xzvf callPycheronMetric_tutorial_data.tar.gz \n    >```\n    >\n    >This should create a directory called `pycheron_test` that contains several `mseed` data files, a `.wfdisc` file, \n    >and a `.w` file nested in a `/data` directory. The `mseed` data spans from 2011/01/01-2011/01/06, for 5 stations \n    >within the University of Utah (UU) seismic network: BGU, CTU, HVU, MTPU, and ZNPU. The wfdisc table/file contains \n    >data for BGU HHZ 2011/01/01.\n\n2. Create a new directory inside `tutorials/data` called `pycheron_tutorial` by running the following command:\n\n   `mkdir pycheron_tutorial`\n\n3. Open the `callPycheronMetric_tutorial_config.yaml` file in a text editor. \n\n   3.1. Change the `output_dir` in the `callPycheronMetric_tutorial_config.yaml` to the absolute path of the \n        `pycheron_tutorial` directory that was recently created above. One way to obtain the absolute path is\n        to navigate to `pycheron_tutorial` directory in a terminal and execute the `pwd` command and then copy \n        and paste it as the value for the `output_dir` input parameter:\n        \n        `output_dir = \"/Users/username/pycheron/tutorials/data/pycheron_tutorial\"`\n\n   3.2. Next, change the `data` field to the absolute path of the directory which contains the data to be processed by \n   Pycheron. For this tutorial, the `data` field should point to the `pycheron_test` directory:\n        \n        `data = \"/Users/username/pycheron/tutorials/data/pycheron_test\"`\n\n   3.3. Change the `datatype` field to be `\"dir\"`. This setting will read all `mseed` files within the directory:\n   \n        `datatype = \"dir\"`\n\n   3.4. By default, the `calcAll` parameter is set to true, which will calculate all available metrics. If desired, \n        change this parameter to `False` and individually set each metric to either `True` or `False` to delineate which\n        metrics to calculate. \n        \n   3.5. For simplicity within this tutorial, it is recommended to keep all other default thresholds and input parameters.\n        However, if the user would like to experiment with updating other input parameters it is recommended to\n        thoroughly read through the documentation to learn which input parameters map to which metric and what their \n        default settings are. Remember, if on a Windows system, turn off Fortran in the functions that use it. \n\n4. Execute the following command from `tutorials/data/` to run the `callPycheronMetric` script: \n\n    ```\n    python ../../pycheron/callPycheronMetric.py callPycheronMetric_tutorial_config.yaml\n    ```\n\nExample output while Pycheron is processing data: \n```\n-----------------------------------------------\nPlotting UU.BGU Jul Date: 002\n-----------------------------------------------\n/Users/prkay/Workspace/gh-pycheron/tutorials/data/pycheron_tutorial/pycheron.db already exists. Connecting to /Users/prkay/Workspace/gh-pycheron/tutorials/data/pycheron_tutorial/pycheron.db...\nNo results found\nNo results found\nFinished PDFgrid and Line plots: UU.BGU\n/Users/prkay/Workspace/gh-pycheron/tutorials/data/pycheron_tutorial/pycheron.db already exists. Connecting to /Users/prkay/Workspace/gh-pycheron/tutorials/data/pycheron_tutorial/pycheron.db...\nFinished stationNoisePlot: UU.BGU\n/Users/prkay/Workspace/gh-pycheron/tutorials/data/pycheron_tutorial/pycheron.db already exists. Connecting to /Users/prkay/Workspace/gh-pycheron/tutorials/data/pycheron_tutorial/pycheron.db...\nFinished psdPlot: UU.BGU\n/Users/prkay/Workspace/gh-pycheron/tutorials/data/pycheron_tutorial/pycheron.db already exists. Connecting to /Users/prkay/Workspace/gh-pycheron/tutorials/data/pycheron_tutorial/pycheron.db...\nFinished pdfPlot: UU.BGU\n....\n....\n\n```\n\nIf pycheron completed running the data successfully, the terminal should print out something similar to:\n\n```\n-----------------------------------------------\nFinished Metric Calculations for UU.BHUT\n-----------------------------------------------\nTime in minutes: 0.04375616709391276\n```\n\nwithout errors.\n\nTo test this with `wfdisc` data, change the `datatype` field to be `\"wfdisc\"`, and re-run.\n\n## Manually inserting latitude and longitude values\nPycheron has the ability to set latitude and longitude values manually through the [latlon configuration file](./latlon_config.toml). The `manual` setting in the [pycheron config file](./pycheronConfigTemplate.yaml) can be set to `True`. If using this feature, it is required that every network/station be included in the latlon configuration file before running callPycheron.\n\n## Manually inserting Response files and Inventory files\nWhen running the metadataComplianceMetric with data which wasn't obtained from IRIS, users will be required to provide response file locations and inventory file locations for that data. The `iris_compatible` setting in the [pycheron config file](./pycheronConfigTemplate.yaml) should be set to `False` in these instances. Refer to the [inventory config file](./inventoryfile_config.toml) and [response config file](./responsefile_config.toml) for more information.\n\n## Viewing results\nOnce callPycheron has completed successfully, the user may view the results in the sqlite database located in the output directory specified in the configuration file. There are a number of ways this can be done:\n* using the Pycheron UI, which can be run with `python pycheron/UI/createDashUI.py` (more on this [below](#how-to-run-the-pycheron-user-interface-(ui)))\n* programmatically via a Python console and the `sqlite` library\n* through a database browser (one option would be [sqlitebrowser](https://sqlitebrowser.org/))\n\nIf users experience issues viewing the entirety of the data from the UI, it is recommended to browse the results using the sqlitebrowser mentioned above.\n\n## How to run the Pycheron User Interface (UI)\nBefore running the UI, users will need to obtain a [mapbox access token](https://docs.mapbox.com/help/glossary/access-token). \nAfter a mapbox account and a mapbox token have been created, execute the following steps:\n\n1. Create a file named `.env` in the `pycheron/UI` folder\n2. Create a variable named `MAPBOX_ACCESS_TOKEN` in the `.env` file and assign it the value of the mapbox token created \n   above\n\nAn example of what this should look like is provided in `pycheron/UI/.env_example` file.\n\nAfter a database has been created (i.e., a `.db` file exists) and the corresponding network/station/channel directories \nwith plots exist: \n\n1.  Navigate to `pycheron/UI` folder and execute the following command in the terminal: \n\n    `python createDashUI.py` to generate the UI. \n\n2.  Navigate to `localhost:8050` in a browser\n\n3.  Copy/paste the absolute path to the database in the *CONNECT* box, which should be located in the output directory \n    specified in the configuration file, before clicking `connect`. If experiencing errors connecting to the db, or would like to connect another db, refresh the page.\n\n\n4. A map should be visible and populated with the stations, where the quality color being displayed is the average quality score for the network, station, and channel (NOTE: This will not necessarily be a daily average. If wfdisc data or a wfdisc Oracle database is used, that option is configurable via the Pycheron configuration file, but MSEED data may include traces which only account for some set interval of time which could be less than a day). If a map is not displayed, it may be the case that a MAPBOX key was never set (please refer to the setup instructions [here](https://docs.mapbox.com/help/glossary/access-token)).\n\n### Summary Report\nPycheron includes a quality summary report, that provides users with a quick view of quality issues to make decisions \nabout which stations to keep or throw out. Color coding provides an immediate visualization of each channel’s quality, \nwhich is scored according to a good/marginal/bad quality scheme. Within the UI, users can review this high-level report \nof station metrics or descend into in-depth reviews of QC metric information sorted by network, station, and channel.  \nThe quality summary report can be generated by clicking the green button displayed below the map on the overview tab. \nFrom here, the user will be presented with a display that looks like the following \n![picture](./pycheron/UI/sample_images/summary-report.png).\n\nThe summary report will populate with data spanning the entire range of the dates present in the Pycheron sqlite database. The calendar picker at the top of the window can be used to modify the date ranges of the metrics to display.\n\nThe results table presented below the calendar picker displays a color code with the following scheme:\n* Red: BAD Quality\n* Yellow: MEDIUM Quality\n* Green: GOOD Quality\n\n\nThe metrics listed as white boxes (unshaded) on the results table represent counts which contribute to the quality score. Metrics located near the end of the table in light blue (shaded) represent values that may be of interest to the user, but don't factor into the overall quality score.\n\nThe bottom of the window includes an editable table, which can be used to set the weights for quality metrics in the results table above it. Users can increase the weight size (any value greater than 1) to emphasize the importance of a metric, or reduce the weight size (between 0 and 1) to de-emphasize the importance of a metric. Once those weights are set, the user may select the blue \"ADJUST QUALITY WEIGHTS\" button to update the results table above.\n\nThe results table may also be also be exported as a `.xlsx` file by clicking the green \"DOWNLOAD REPORT\" button at the top of the window. This will download the report into Pycheron's root directory as a file named `output.xlsx`, which will contain 3 separate pages:\n* Full_report: displays exactly what would be visible in the results table\n* counts: displays only the count result of metrics (values contributing directly to the quality score) from the results table\n* values: display only the value result of metrics (values which don't contribute to the quality score) from the results table\n\n### Plotting\nThe UI provides the ability to generate plots for the data in the loaded database. In order to correctly generate plots, the Network, Station, and Channel must be selected in order. Each tab will have plots available at the corresponding level (Network tab has plots at the network level). In order to set the Network for a Station plot, it first must be selected in the Network tab, before selecting the station in the Station tab. \n\n1) Click on the `Network` tab. In the upper right-hand corner a drop down menu exists that allows users to \n   select the available networks within the loaded database. Once a network is selected, the types of plots available \n   for that network will be available in the drop down menu under `Plots`. Here is a sample of the \n   ![Network tab](./pycheron/UI/sample_images/network.png)\n\n2) Click on the `Station` tab. In the upper right-hand corner a drop down menu exists that allows users to \n   select the available stations of the Network selected in Step 1. Once a station is selected, the dropdown menu on the \n   left-hand side will be populated with the available plots for the selected station. Here is an example of the \n   ![Station tab](./pycheron/UI/sample_images/station.png)\n\n3) Click on the `Channel` tab. In the upper left-hand corner a drop down menu exists that allows users to select \n   the available channels, given the selected Network and Station from Steps 1 and 2. Two drop-downs exist to select the \n   available plots for the select channel. Here is a sample of the ![Channel tab](./pycheron/UI/sample_images/channel.png)\n\n## Formatting\nFor any changes, code must follow [Black](https://github.com/psf/black) formatting rules. Users can install/run Black \nagainst any changes before pushing with:\n* `pip install black`\n* `black <new-or-changed-py-files>`\n\n## Developing Locally\nFor users who intend to make changes to Pycheron before running, it is suggested to execute the following commands\nso that Pycheron will not have to be rebuilt with each change:  \n\n* From the root directory, run `cd dist` after Pycheron has been built\n* Extract the tarball with `tar -xzvf pycheron-<verion-number-here>.tar.gz`\n* Run `cd pycheron-<version-number-here>`, and copy/paste the `setup.py` into the Pycheron root directory \n* From the root directory, run `pip install -e .`\n\n## Documentation\nRefer to [\"Making the HTML pages\" in the docs folder](./docs/README.md) tutorial to create Pycheron's Sphinx \ndocumentation. \n\n",
        "createdAt": "2020-08-05T21:51:26.000Z",
        "updatedAt": "2025-07-11T07:41:58.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/sandialabs/pycheron/github/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "comp-geoph-itera/lindu-software",
        "url": "https://github.com/comp-geoph-itera/lindu-software",
        "description": "Progress of Lindu Software Codes (for seismological data processing: determining and relocating hypocenter; travel time tomography).",
        "stars": 4,
        "forks": 0,
        "readme": "# The Developer Branch of Lindu Software\n<p align=\"center\">\n\t<img src=\"/lindugui/images/screenshots/lindu-logo.png\" alt=\"Lindu Logo\" width=\"200\"/>\n\t<br>\n\tLindu Software Logo\n\t<br>\n</p>\nProgress of Lindu Software Codes (for seismological data processing: determining and relocating hypocenter; traveltime tomography)\n\nWe are still developing the package version: see more on [package](https://github.com/comp-geoph-itera/lindu-software/tree/package) branch.\n\nThis is the development branch for the future release.\n\nSee the [changelog file](https://github.com/comp-geoph-itera/lindu-software/blob/dev/CHANGELOG.md)\n\n# Developer\nIf you would like to be the collaborator of this software, you could use these several steps for making your own environment in Lindu software repository.\n\n## 1. Python version\nwe use python 3.6.12 version. However, you can create `conda` environment based on this version if you are using Anaconda or Miniconda. After that, you can clone the repository into your local disk. `git clone git@github.com:comp-geoph-itera/lindu-software.git`.\n\n## 2. Create Python environment\nafter you clone this repository, go to `lindu software` directory and then create `.venv` by commanding\n`python -m venv .venv`\nthen activate it\n`.venv/Scripts/activate`\n\n## 3. Microsoft Visual C++ 14.0 or more\nCheck your Windows if it has been installed MSVC or not. You can check this page:\n[https://visualstudio.microsoft.com/visual-cpp-build-tools/](https://visualstudio.microsoft.com/visual-cpp-build-tools/)\n\n## 4. Install the requirement.txt\n*Caution: Didn't try it yet*\n\nThe packages that you need to be installed:\n```\napptools==4.5.0\nbasemap==1.2.2\ncertifi==2020.6.20\nchardet==3.0.4\nclick==7.1.2\nconfigobj==5.0.6\ncx-Freeze==5.0.2\ncycler==0.10.0\ndecorator==4.4.2\nFlask==1.1.2\nfuture==0.18.2\ngeos==0.2.2\nidna==2.10\nitsdangerous==1.1.0\nJinja2==2.11.2\nkiwisolver==1.2.0\nlxml==4.5.2\nMarkupSafe==1.1.1\nmatplotlib==3.3.2\nmayavi==4.5.0+vtk71\nnumpy==1.19.2\nobspy==1.2.2\nPillow==7.2.0\npyface==5.1.0\nPygments==2.7.1\npyparsing==2.4.7\npyproj==2.6.1.post1\nPyQt4==4.11.4\npyshp==2.1.2\npython-dateutil==2.8.1\nrequests==2.24.0\nscipy==1.5.2\nsix==1.15.0\nSQLAlchemy==1.3.19\ntraits==4.6.0\ntraits-stubs==6.1.0\ntraitsui==5.1.0\nurllib3==1.25.10\nVTK==7.1.1\nWerkzeug==1.0.1\n```\nOr you can use this command recursively:\n\n~~pip install -r requirements_all.txt~~\n\n**Update:**\n\nYou need to install manually of these packages:\n\nPyQt4:\n\n[https://www.lfd.uci.edu/~gohlke/pythonlibs/#pyqt4](https://www.lfd.uci.edu/~gohlke/pythonlibs/#pyqt4)\n\nVTK:\n\n[https://www.lfd.uci.edu/~gohlke/pythonlibs/#vtk](https://www.lfd.uci.edu/~gohlke/pythonlibs/#vtk)\n\nMayavi:\n\n[https://www.lfd.uci.edu/~gohlke/pythonlibs/#mayavi](https://www.lfd.uci.edu/~gohlke/pythonlibs/#mayavi)\n\nBasemap:\n\n[https://www.lfd.uci.edu/~gohlke/pythonlibs/#basemap](https://www.lfd.uci.edu/~gohlke/pythonlibs/#basemap)\n\n\nand install to your environment:\n\n`pip install <package-file-name>`\n\n## 6. Check the compatibility of Windows version\nCurrently the program is working on Windows 10.\n\n## 7. Build an .exe program\n`python setup.py build`\n\n# References\nIf you will use this software, please add these references to your research:\n```\n@article{styawan_lindu_2020,\n\ttitle = {Lindu {Software}: {A} {Free} {Seismological} {Data} {Processing} {Software} {For} {Traveltime} {Tomography} {Using} {Python} {Framework}},\n\tvolume = {537},\n\tcopyright = {All rights reserved},\n\tissn = {1755-1315},\n\tshorttitle = {Lindu {Software}},\n\turl = {https://doi.org/10.1088%2F1755-1315%2F537%2F1%2F012017},\n\tdoi = {10.1088/1755-1315/537/1/012017},\n\tabstract = {Earthquake data can be used to infer some physical properties for representing the subsurface condition. The 3-Dimensional (3D) seismic velocity structure as a kind of these important properties contains the information of variation in lithology change and fluid saturation. The most common method for inverting from the travel time of seismic event into 3D seismic velocity structure is travel time tomography which is based on the relation between velocity and travel time of P- and S-wave. Based on this concept, we develop a module of Lindu software to infer this seismic velocity structure from travel time data. This module is a part of seismological data processing sequences that have been integrated into Lindu software. The Lindu software uses Python framework, a kind of high-level programming languages. The pseudo-bending raytracing method is employed to calculate the travel time between the event sources and stations and also to build the kernel matrix. The resolution test that relates density of rays and resulted tomogram uses the synthetic Checkerboard Resolution Test (CRT) by using Damped-Least Squares (DLS) method for the inversion. For validating this module, it has been tested by using both synthetic and real data.},\n\tlanguage = {en},\n\turldate = {2020-09-07},\n\tjournal = {IOP Conference Series: Earth and Environmental Science},\n\tauthor = {Styawan, Yudha and Firdaus, Ruhul and Yudistira, Tedi and Suhendi, Cahli},\n\tmonth = aug,\n\tyear = {2020},\n\tnote = {Publisher: IOP Publishing},\n\tpages = {012017}\n}\n\n@article{styawan_preliminary_2019,\n\ttitle = {The preliminary results of {Lindu} software: a free seismological data processing using python framework},\n\tvolume = {311},\n\tcopyright = {All rights reserved},\n\tissn = {1755-1315},\n\tshorttitle = {The preliminary results of {Lindu} software},\n\turl = {https://doi.org/10.1088%2F1755-1315%2F311%2F1%2F012078},\n\tdoi = {10.1088/1755-1315/311/1/012078},\n\tabstract = {LINDU software is developed to solve integrated earthquake data processing. It is GUI based software that fulfil the needed for user friendly type of software. The Python framework is used for computation and visualization and integrates the common programs for earthquake data processing, such as GAD.exe, JHD.exe, and HypoDD.exe. It is also integrates the common procedure of routine data processing in earthquake seismology and works in local and regional scale. In this paper, we shows the preliminary results of LINDU software for several functions. To identify arrival time of P-wave we employ Akaike Information Criterion (AIC), MER (Modified Energy Ratio) and S/L Kurt�s method. The results of these method will be considered as guided � auto picking. However, the results also can be treated as reference for picking manually with Seisgram2k.jar. Geiger�s method is employed to locate the event location. The events can be relocated and 1D velocity can be updated by employing Joint Hypocenter Determination (JHD). The next method to relocate the event location is Double Difference (DD) algorithm. The precision result of Lindu software has been tested using IRIS and real data available which run seamlessly.},\n\tlanguage = {en},\n\turldate = {2020-09-07},\n\tjournal = {IOP Conference Series: Earth and Environmental Science},\n\tauthor = {Styawan, Yudha and Andika, Putu Pradnya and Suhendi, Cahli and Firdaus, Ruhul and Sudibyo, Maria R. P. and Erlangga, I. F. and Ry, Rexha Verdhora},\n\tmonth = aug,\n\tyear = {2019},\n\tnote = {Publisher: IOP Publishing},\n\tpages = {012078}\n}\n\n@article{andika_lindu_2019,\n\ttitle = {Lindu {Software}: {An} {Open} {Source} {Seismological} {Data} {Processing} {Using} {Python} {Framework} {To} {Relocate} {Hypocenter} ({Preliminary} {Software})},\n\tvolume = {318},\n\tcopyright = {All rights reserved},\n\tissn = {1755-1315},\n\tshorttitle = {Lindu {Software}},\n\turl = {https://doi.org/10.1088%2F1755-1315%2F318%2F1%2F012021},\n\tdoi = {10.1088/1755-1315/318/1/012021},\n\tabstract = {Recorded seismogram of an earthquake data contains the earth structure information. Researchers developed the method to extract the information and derives it into the program codes. However, generally, the program codes developed only for specific function and work on only specific scale. Almost the existing programs have a limitation, for example, they work on command-line based and less user-friendly. Lindu software is developed to solve these problems. In this paper, we show the preliminary results of Lindu software, a GUI � based software which is open source and developed in python platform. This software integrates the common procedure of routine data processing in earthquake seismology and works in local and regional scale. It is designed to read multi-component data on multi-station. To identify events automatically, we employ SL Kurt�s method and use the results as guided auto�picking. However, the picked time also can be changed manually. Furthermore, we employ Joint Hypocenter Determination (JHD) algorithm to locate the hypocenter of earthquake events and update the 1D velocity model simultaneously. Then the events can be relocated by employing the double-difference method. The software was tested on the available data from IRIS and BMKG and shows the acceptable and reliable results.},\n\tlanguage = {en},\n\turldate = {2020-09-07},\n\tjournal = {IOP Conference Series: Earth and Environmental Science},\n\tauthor = {Andika, Putu Pradnya and Styawan, Yudha and Suhendi, Cahli and Firdaus, Ruhul},\n\tmonth = aug,\n\tyear = {2019},\n\tnote = {Publisher: IOP Publishing},\n\tpages = {012021}\n}\n```\n\n# CHANGELOG\nfor creating the similar CHANGELOG, this is the format that is used in PowerShell:\n\n```shell script\nfunction changelog {\n\techo \"# CHANGELOG`n`n\" > CHANGELOG.md\n\tgit log --all --abbrev-commit --decorate --format=format:'- %C(bold cyan)%aD%C(reset) %C(white)%s%C(reset) %C(dim white)([%C(bold blue)%h%C(reset)](https://github.com/comp-geoph-itera/lindu-software/commit/%C(bold blue)%H%C(reset)))' >> CHANGELOG.md\n}\n```\n\n# Galleries\n<p align=\"center\">\n\t<img src=\"/lindugui/images/screenshots/lindu-dev-1.PNG\" alt=\"Lindu Development\" width=\"800\"/>\n\t<br>\n\tLindu Progress (2020-10-05)\n\t<br>\n\t<img src=\"/lindugui/images/screenshots/lindu-dev-2.PNG\" alt=\"Lindu Development\" width=\"800\"/>\n\t<br>\n\tLindu Progress (2020-10-06)\n\t<br>\n\t<img src=\"/lindugui/images/screenshots/lindu-dev-3.PNG\" alt=\"Lindu Development\" width=\"800\"/>\n\t<br>\n\tLindu Progress (2020-10-06)\n\t<br>\n\t<img src=\"/lindugui/images/screenshots/lindu-dev-4.png\" alt=\"Lindu Development\" width=\"800\"/>\n\t<br>\n\tLindu Progress (2020-10-08)\n\t<br>\t\n</p>\n",
        "createdAt": "2020-09-28T07:48:30.000Z",
        "updatedAt": "2024-01-30T19:29:18.000Z",
        "language": "Arc",
        "homepage": "https://yudhastyawan.github.io/repository/",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/comp-geoph-itera/lindu-software/dev/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "turingtest37/SequencerJ.jl",
        "url": "https://github.com/turingtest37/SequencerJ.jl",
        "description": "Julia-language port of the Sequencer algorithm, originally developed in python (https://github.com/dalya/Sequencer). The Sequencer finds trends in 1-dimensional data sets and has been used by its original authors for data analysis in astrophysics, seismology, image processing, etc. Contributions are welcome!",
        "stars": 4,
        "forks": 1,
        "readme": "# SequencerJ\n\n__SequencerJ__ is a pure [Julia](https://julialang.org/) implementation of the [Sequencer Algorithm](https://github.com/dalya/Sequencer/), a data analysis tool to identify and extract the principal trends in a set of 1-d data vectors.\n\nGetting started with SequencerJ is easy. From the Julia REPL:\n```julia\n    julia> using Pkg; Pkg.add(\"SequencerJ\")\n    [...]\n    julia> using SequencerJ\n    [ Info: Precompiling SequencerJ [348581b9-6e84-42e0-ac4e-fe9177c221e6]\n    [...]\n```\nYou may get **WARN**INGs upon compilation. Sorry, I'm working on that! You can safely ignore them for most purposes, but if you are developing SequencerJ locally and use the `Revise` package, note that you may have to restart your Julia environment more often than usual.\n\n```julia\n    julia> A = rand(50,100); #some data to process. \n\n    julia> m = ALL_METRICS\n    (Euclidean(0.0), EMD(nothing), KLDivergence(), Energy(nothing))\n\n    julia> s = (1,2,4)\n    (1, 2, 4)\n\n    julia> seqres = sequence(A; metrics=m, scales=s)\n    ┌ Info: Sequencing data with\n    │     shape: (50, 100)\n    │     metric(s): (Euclidean(0.0), EMD(nothing), KLDivergence(), Energy(nothing))\n    └     scale(s): (1, 2, 4)\n    [...]\n```\n\n`seqres` is a `SequencerResult` type that may be used to retrieve results of the sequencing run. The `order` function returns the best reordering of the data columns that was found.\n\n```julia\n    julia> bestseq = order(seqres)\n    100-element Array{Int64,1}:\n    10\n    15\n    13\n    [...]\n```\n\nThe Sequencer also calculates a fitness coefficient `η` that can be used to compare quality of solutions using various metrics and scales against the same data. Bigger is better. η is returned by the `elong` function.\n```julia\n    julia> eta = elong(seqres)\n    6.2345\n```\n\n\nThe paper that describes the Sequencer algorithm and its applications can be found \non Arxiv: [https://arxiv.org/abs/2006.13948].\n",
        "createdAt": "2020-08-11T18:03:22.000Z",
        "updatedAt": "2022-10-24T20:59:13.000Z",
        "language": "Julia",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/turingtest37/SequencerJ.jl/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "GuoKeKF/Computer-Programs-in-Seismology",
        "url": "https://github.com/GuoKeKF/Computer-Programs-in-Seismology",
        "description": "How to install Computer Programs in Seismology？如何安装CPS？",
        "stars": 1,
        "forks": 1,
        "readme": "# Computer-Programs-in-Seismology-CPS-\nHow to install Computer Programs in Seismology？如何安装CPS？\n\nIf copyright infringement or offense occurs to you, please contact me to remove it. For study purposes only, commercial use is prohibited.如果侵权或冒犯到您，请联系我删除。仅用于学习用途，禁止商用。\n\n1.看我安装centos7的教程后，在centos中安装CPS\n\n2.先看 CPS330安装教程.pdf 后再看 文件说明.png\n\n3. start_cps_backpup已经更名为 cps. 反演时在终端窗口中输入：bash cps再按回车即可\n\n4. 9_1.disp中的数据格式为SURF96 R C X 0 0.082 0.315 0.100 其中6，7行为周期(1/频率)，相速度。是自己从频散普中提取出来的。R表示瑞雷波\n\n5. 9_2.disp中的数据格式SURF96 R C T 0 0.0820 0.3695 0.0000其中6，7行为反演出来的周期(1/频率)，相速度。是CPS反演出来的结果\n\n6. 9_1.mdl中的数据格式 H(KM) VP(KM/S) VS(KM/S) RHO(GM/CC) QP QS ETAP ETAS FREFP FREFS中的VS是我们反演需要的结果。\n\n7. model.txt中的H(KM) 是厚度，DEPTH是深度，厚度=下一层深度-上一层深度。所有厚度总和等于最后一层深度，所以第一层厚度和最后一层厚度不用前面的公式，随便凑到加上其他厚度等于最后一层深度即可。\n\n8. 反演其他面波，请参考SURF96格式说明，在CPS330O文件第19页，60页，63页。\n\n9. 一个反演文件夹example中至少需要放入9_1.disp  CPS  model.txt   sobs.d 这四个文件，在example文件夹中右键单击打开终端(bash)。\n",
        "createdAt": "2024-01-25T11:32:16.000Z",
        "updatedAt": "2024-04-24T03:17:54.000Z",
        "language": "Shell",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/GuoKeKF/Computer-Programs-in-Seismology/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "swiss-seismological-service/scdetect",
        "url": "https://github.com/swiss-seismological-service/scdetect",
        "description": "A computationally efficient earthquake detection module for SeisComP",
        "stars": 19,
        "forks": 7,
        "readme": "# SCDetect - A SeisComP Module for cross-correlation based earthquake detection\n\n[![continuous-integration](https://github.com/swiss-seismological-service/scdetect/actions/workflows/continuous-integration.yml/badge.svg)](https://github.com/swiss-seismological-service/scdetect/actions/workflows/continuous-integration.yml) [![Documentation Status](https://readthedocs.org/projects/scdetect/badge/?version=latest)](https://scdetect.readthedocs.io/) [![License: AGPL v3](https://img.shields.io/badge/License-AGPL_v3-blue.svg)](https://www.gnu.org/licenses/agpl-3.0)\n\n## About\n\nSCDetect is a [SeisComP](https://github.com/SeisComP) package. With the\nextension module `scdetect-cc` it implements both real-time and classical\noffline earthquake detection based on waveform cross-correlation, also called\nmatched filtering or template matching. Again, the underlying cross-correlation\nalgorithm is based on computing\nthe [Pearson Correlation Coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)\n.\n\nThe module allows both single-stream and multi-stream earthquake detection.\n\nIn case the detection parameters exceed the configured thresholds, `scdetect-cc`\ndeclares a new origin.\n\nBesides, magnitudes may be estimated based on multiple magnitude estimation\nmethods (regression, amplitude ratios).\n\n## Documentation\n\nFor user documentation please refer to [scdetect.readthedocs.io](https://scdetect.readthedocs.io/).\n\n## Compiling and Installation\n\n### Clone\n\nGet a copy of\n[SeisComP](https://github.com/SeisComP):\n\n```bash\n# Select a tag for an apropriate SeisComP release (https://github.com/SeisComP/seiscomp/releases)\nTAG='X.Y.Z'\n\n# Get a copy of SeisComP (follow the official documentation https://www.seiscomp.de/doc/base/build.html?highlight=compile)\ngit clone --branch $TAG https://github.com/SeisComP/seiscomp.git \ngit clone --branch $TAG https://github.com/SeisComP/common.git seiscomp/src/base/common\ngit clone --branch $TAG https://github.com/SeisComP/main.git seiscomp/src/base/main\n\n# [... etc ...]\n\n```\n\nNext, clone SCDetect:\n\n```bash\ngit clone https://github.com/swiss-seismological-service/scdetect.git seiscomp/src/extras/scdetect\n```\n\n### Dependencies\n\nBesides of\nthe [SeisComP core dependencies](https://github.com/SeisComP/seiscomp#prerequisites)\nthe following packages must be installed to compile SCDetect:\n\n- `libsqlite3-dev` (Debian, Ubuntu), `sqlite-devel` (RedHat, Fedora, CentOS),\n  `dev-db/sqlite` (Gentoo)\n\nE.g. on Ubuntu simply invoke:\n\n```\nsudo apt-get install libsqlite3-dev\n```\n\n### Compile and Install\n\nFor compiling and installing SeisComP (including SCDetect), please refer to\nhttps://github.com/SeisComP/seiscomp#build.\n\n## Tests\n\n> **NOTE**: executing SCDetect related tests requires SeisComP to be installed,\n> beforehand.\n\nIn order to run all SeisComP tests (including those of `scdetect-cc` and\npossibly additionally installed third party modules), either execute\n\n```bash\nmake test\n```\n\nin the build directory, or use the\n[ctest](https://cmake.org/cmake/help/latest/manual/ctest.1.html) executable from\n[cmake](https://cmake.org/) (also within the build directory). E.g. in order to\nrun only SCDetect related tests, invoke\n\n```bash\nctest -R \"^test_scdetect.*\"\n```\n\nFor additional information, please also refer to\nSeisComP's [unit testing guide](https://docs.gempa.de/seiscomp/current/base/tests.html)\n.\n\n## Issues\n\nPlease report bugs, issues, feature requests, etc on\n[GitHub](https://github.com/swiss-seismological-service/scdetect/issues).\n\n## Contributions\n\nContributions are very welcome. Made with :two_hearts::rainbow:.\n\n## Cite\n\n*Mesimeri M., Armbruster D., Kästli P., Scarabello L., Diehl T., \nClinton J., Wiemer S.* (2024)<br>\nSCDetect: A SeisComP Module for Real‐Time Waveform Cross‐Correlation‐Based Earthquake \nDetection.<br>\n*Seismological Research Letters, 95 (3): 1961–1975*\nDOI: https://doi.org/10.1785/0220230164\n\n\n\n## License\n\nLicensed under the the [AGPLv3](https://www.gnu.org/licenses/agpl-3.0.en.html).\nFor more information see the\n[LICENSE](https://github.com/swiss-seismological-service/scdetect/tree/master/LICENSE) file.\n",
        "createdAt": "2020-08-07T11:42:11.000Z",
        "updatedAt": "2025-12-03T15:16:44.000Z",
        "language": "C++",
        "homepage": "https://scdetect.readthedocs.io",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/swiss-seismological-service/scdetect/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "TobbeTripitaka/geo_to_numpy",
        "url": "https://github.com/TobbeTripitaka/geo_to_numpy",
        "description": "Reading unusual and or annoying file formats to numpy",
        "stars": 0,
        "forks": 0,
        "readme": "# Geo to numpy\n_Reading unusual and or annoying file formats to numpy_\n\n---\n\nHere I collect code to convert some unusual file formats to Python numpy arrays and export to e.g. netCDF. \n\nCode here is not necissary optimised for performance, rather to be easy to follow and modify as needed. \n\nSome methods might be added to agrid later. \n",
        "createdAt": "2020-05-04T22:48:16.000Z",
        "updatedAt": "2020-05-05T11:16:14.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/TobbeTripitaka/geo_to_numpy/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "natastoyan/Registrator",
        "url": "https://github.com/natastoyan/Registrator",
        "description": "Registrating seismological data tool",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2019-06-05T10:52:07.000Z",
        "updatedAt": "2019-06-05T10:52:14.000Z",
        "language": "C#",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "iegorushkin/Central_Kamchatka_EQT",
        "url": "https://github.com/iegorushkin/Central_Kamchatka_EQT",
        "description": "A set of scripts implementing necessary procedures for earthquake seismogram processing for subsequent seismic tomography of the Central Kamchatka region. ",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2025-03-17T14:00:23.000Z",
        "updatedAt": "2025-03-17T14:09:06.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "solldavid/6DOF_processing_tutorial",
        "url": "https://github.com/solldavid/6DOF_processing_tutorial",
        "description": "Tutorial on seismological processing of six-component data",
        "stars": 6,
        "forks": 2,
        "readme": "# READ\n**Please note that this code is no longer supported/maintained. A more efficient and stable version of the code is now included in the TwistPy toolbox. Please refer to https://twistpy.org and specifically, this [example](https://twistpy.org/examples/alaska_wave_parameter_estimation.html#sphx-glr-examples-alaska-wave-parameter-estimation-py \"6DOF processing example\"), which reproduces the example from this tutorial.**\n\n# Seismological processing of six degree-of-freedom ground-motion data\n\nIn this tutorial, you will learn how to process six degree-of-freedom ground-motion data comprised of three components of translational motion and three components of rotational motion. The data used in this tutorial was recorded on the large ring laser gyroscope ROMY in Germany (http://www.romy-erc.eu), after the 2018 M7.9 gulf of Alaska earthquake as described in the accompanying paper:  \n\nSollberger, D., Igel, H., Schmelzbach, C., Edme, P., van Manen, D.-J., Bernauer, F., Yuan, S., Wassermann, J., Schreiber, U., and Robertsson, J. O. A. (2020): **Seismological processing of six degree-of-freedom ground-motion data**, *Sensors*, 20(23), 6904, https://doi.org/10.3390/s20236904.\n\nPlease refer to this paper for further details on the algorithm used in the analysis. Additional information on 6DOF polarization analysis can be found in the following publication:\n\nSollberger, D., Greenhalgh, S. A., Schmelzbach, C., Van Renterghem, C., and Robertsson, J. O. A. (2018): **6-C polarization analysis using point measurements of translational and rotational ground-motion: theory and applications**, *Geophysical Journal International*, 213(1), https://doi.org/10.1093/gji/ggx542.\n\nNote that at the time of publication, the underlying code used in this analysis is still under development and some features might not yet work as intended. If you encounter any bugs or problems, please report them. You can also directly propose changes via a pull request. Any queries about this code should be directed to David Sollberger, Institute of Geophysics, ETH Zurich (david.sollberger@erdw.ethz.ch). \n\n## Prerequisites\nThe code requires the following prerequisites:\n- Obspy (Python framework for processing of seismological data)\n- PyTables\n- tqdm (Progress bar)\n- Jupyter (to run the notebook)\n\nWe recommend to install them using anaconda in a new environment via:\n\n```\nconda create -n 6DOF python=3.7 \n\nconda activate 6DOF\n\nconda config --add channels conda-forge # This adds the conda-forge channel to your Anaconda configuration\n\nconda install obspy pytables tqdm jupyter\n\n```\n\nNow clone this repository into your working directory:\n\n```\ngit clone https://github.com/solldavid/6DOF_processing_tutorial\n\ncd 6DOF_processing_tutorial\n\n```\n\nYou can now start the notebook by:\n\n$ jupyter notebook\n",
        "createdAt": "2020-11-25T13:20:40.000Z",
        "updatedAt": "2024-10-29T08:28:57.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/solldavid/6DOF_processing_tutorial/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "deki325-lang/WickSeismo-MTF",
        "url": "https://github.com/deki325-lang/WickSeismo-MTF",
        "description": "_A Seismic STA/LTA Model for Multi-Timeframe Trend Detection in Financial Markets_ ",
        "stars": 0,
        "forks": 0,
        "readme": "# 🌏 WickSeismo  \n### _A Seismic STA/LTA Model for Multi-Timeframe Trend Detection in Financial Markets_  \nby [HidekiF]  \n\n---\n\n## 🧭 Overview\n\n**WickSeismo** is a *seismic-inspired market analysis model* that detects early-stage trend formation  \nby measuring **short-term vs long-term energy ratio (STA/LTA)** based on candlestick structure  \n— not just price averages.  \n\nIt treats each candlestick’s **wick and body balance** as a form of *market stress*,  \nand observes how this stress accumulates and releases across multiple timeframes.  \n\n👉 In short: **A digital seismograph for market trends.**\n\n---\n\n## ⚙️ Core Concept\n\n| Seismology | Finance Analogy |\n|-------------|----------------|\n| Short-term average (STA) | Short-term candlestick imbalance (P-wave) |\n| Long-term average (LTA) | Market background trend (S-wave) |\n| STA/LTA ratio | Energy ratio of market stress |\n| Wave propagation | Trend resonance between timeframes |\n\nThe **STA/LTA ratio** is computed as:\n\n\\[\nR(t) = \\frac{STA(t)}{LTA(t)} = \\frac{\\sum_{i=t-S}^{t} |Imb_i| / S}{\\sum_{i=t-L}^{t} |Imb_i| / L}\n\\]\n\nwhere  \n`Imb_i` = normalized buy/sell imbalance derived from wick & body analysis.\n\n---\n\n## 📊 Model Architecture\n\n### 1️⃣ Candlestick Stress Extraction\nEach candle generates a *buy/sell imbalance* signal based on its structure:\n\n\\[\nupR = \\frac{High - \\max(Open,Close)}{High - Low}\n\\]\n\\[\nlowR = \\frac{\\min(Open,Close) - Low}{High - Low}\n\\]\n\nThen:\n\n| Structure | Meaning | Signal |\n|------------|----------|--------|\n| Long upper wick | Selling pressure | −1 |\n| Long lower wick | Buying pressure | +1 |\n| Large body | Strong conviction | ±1 (by direction) |\n\n---\n\n### 2️⃣ STA/LTA Energy Ratio\n- **STA** = average imbalance over short window  \n- **LTA** = average imbalance over long window  \n- **Ratio = STA / LTA** → indicates “stress acceleration”\n\n\\[\nSeismoRatio = \\frac{|STA|}{|LTA|} \\times sign(STA)\n\\]\n\n---\n\n### 3️⃣ Multi-Timeframe Resonance\nWhen the **Base timeframe (e.g., M15)** and **Higher timeframe (e.g., H1)**  \nshow STA/LTA ratios of the same sign and above threshold →  \na “resonance” occurs (analogous to a main shock).\n\n| State | Interpretation |\n|--------|----------------|\n| Only base active | Pre-shock (noise) |\n| Both in same direction | Main shock (trend trigger) |\n| Both return to 0 | Aftershock (trend fading) |\n\n---\n\n## 🪄 Indicator Visualization\n\n| Layer | Description |\n|--------|-------------|\n| 🟩 **Green Line** | STA/LTA ratio of Base TF (e.g. M15) |\n| 🔴 **Red Line** | STA/LTA ratio of Higher TF (e.g. H1) |\n| 🟩 Vertical Line | BUY resonance detected |\n| 🔴 Vertical Line | SELL resonance detected |\n\n---\n\n## 🧩 Usage Examples\n\n| Setup | Base TF | HTF | Purpose |\n|--------|----------|-----|----------|\n| Scalp | M5 | M15 | Detects micro shocks |\n| Daytrade | M15 | H1 | Best balance (standard) |\n| Swing | H1 | H4 | Captures mid-term trend waves |\n| Position | H4 | D1 | Long-term structural stress |\n\n---\n\n## ⚡ Practical Strategy\n\n| Phase | Condition | Action |\n|--------|------------|--------|\n| **Pre-shock** | Base STA/LTA spikes, HTF neutral | Observe |\n| **Resonance** | Base & HTF same direction | Enter |\n| **Aftershock** | Both fading to 0 | Exit / Take profit |\n\n---\n\n## 🧠 Comparison with Classic Indicators\n\n| Indicator | Measures | Nature | Difference |\n|------------|-----------|--------|-------------|\n| MACD | Moving average difference | Lagging | Based on prices only |\n| RSI | Overbought/Oversold | Oscillator | Ignores candle structure |\n| BB Width | Volatility | Breakout filter | Measures result, not cause |\n| **WickSeismo** | Wick/Body Imbalance | Seismic Model | Detects *cause* of trend before breakout |\n\n---\n\n## 🔬 Example Chart\n![WickSeismo Example](https://github.com/deki325-lang/WickSeismo-MTF/blob/main/images/WickSeismo_MTF_Example.png)\n\n📜 License: [MIT](LICENSE)\n\n---\n🧩 Created by [@deki325-lang](https://github.com/deki325-lang)  \n💬 Contact: (deki325@gmail.com)\n\n",
        "createdAt": "2025-10-09T09:44:55.000Z",
        "updatedAt": "2025-10-09T13:28:42.000Z",
        "language": "MQL5",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/deki325-lang/WickSeismo-MTF/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "anowacki/QuakeML.jl",
        "url": "https://github.com/anowacki/QuakeML.jl",
        "description": "Read and write QuakeML in Julia",
        "stars": 1,
        "forks": 0,
        "readme": "# QuakeML\n\n## Build status\n\n[![Build Status](https://github.com/anowacki/QuakeML.jl/workflows/CI/badge.svg)](https://travis-ci.org/anowacki/QuakeML.jl)\n[![Coverage Status](https://coveralls.io/repos/github/anowacki/QuakeML.jl/badge.svg?branch=master)](https://coveralls.io/github/anowacki/QuakeML.jl?branch=master)\n\n## Documentation\n[![](https://img.shields.io/badge/docs-stable-blue.svg)](https://anowacki.github.io/QuakeML.jl/stable)\n[![](https://img.shields.io/badge/docs-dev-blue.svg)](https://anowacki.github.io/QuakeML.jl/dev)\n\n## What is QuakeML.jl?\n\nQuakeML.jl is a Julia package to read and write information about\nearthquakes and seismic events in the\n[QuakeML format](https://quake.ethz.ch/quakeml).\n\n## User-facing functions\n- `QuakeML.read`: Read a QuakeML file.  (This function is not exported.\n  and requires the module prefix `QuakeML`.)\n- `QuakeML.readstring`: Read a QuakeML document from a string.  (This \n  function is not exported.)\n- `write`: Write a set of `EventParameters` as a QuakeML XML document.\n- `preferred_focal_mechanism`: Get the preferred focal mechanism for an event\n- `preferred_magnitude`: Get the preferred magnitude for an event\n- `preferred_origin`: Get the preferred origin for an event\n- `has_focal_mechanism`: Check to see if an event contains any\n  focal mechanisms\n- `has_magnitude`: Check to see if an event contains any magnitude\n- `has_origin`: Check to see if an event contains any origins\n- `quakeml`: Create an XML document from a set of events which can\n  be written with `print(io, quakeml(qml))`\n\n## Examples\n\n### Reading\nTo read a QuakeML document on your computer (e.g., one of the ones\nsupplied with QuakeML.jl), do:\n```julia\njulia> using QuakeML\n\njulia> qml_file = joinpath(dirname(dirname(pathof(QuakeML))), \"test\", \"data\", \"nepal_mw7.2.qml\");\n\njulia> qml = QuakeML.read(qml_file)\n```\n\nTo read a set of events from a string:\n```julia\njulia> QuakeML.readstring(String(read(qml_file)))\n```\n\n### Writing\nTo write a set of events to disk:\n```julia\njulia> write(\"file/on/disk.xml\", qml)\n```\n\nFor more control of output, convert your set of `EventParameters`\ninto an XML document, and write that:\n```julia\njulia> xml = quakeml(qml);\n\njulia> println(\"/tmp/quakeml_file.qml\", quakeml(qml))\n```\n\nNote that here `xml` is an\n[`EzXML.XMLDocument`](https://bicycle1885.github.io/EzXML.jl/stable/manual/).\n\nOr convert your XML document into a `String`:\n```julia\njulia> str = string(xml)\n```\n\n## Export of types\n\nBy default, QuakeML does not export the types it uses.  The user should\nusually create sets of `EventParameters`, for example, by calling the\ntype's qualified constructor:\n```julia\njulia> QuakeML.EventParameters()\nQuakeML.EventParameters\n  comment: Array{QuakeML.Comment}((0,))\n  event: Array{QuakeML.Event}((0,))\n  description: Missing missing\n  creation_info: Missing missing\n  public_id: QuakeML.ResourceIdentifier\n```\n\nTo allow less typing, one could create a module alias, such as:\n```julia\njulia> const QML = QuakeML\n```\n\n### `QuakeML.Types` module\nAs an **experimental** feature, the user may use the `QuakeML.Types`\nmodule, which exports all the types which are needed to construct a\nfull set of `EventParameters`.  For example, to specify a catalogue\nwith one event with an unspecified magnitude type with magnitude 1.0:\n\n```julia\njulia> using QuakeML.Types\n\njulia> event = Event(magnitude=[Magnitude(mag=1.0)])\nQuakeML.Event\n  description: Array{QuakeML.EventDescription}((0,))\n  comment: Array{QuakeML.Comment}((0,))\n  focal_mechanism: Array{QuakeML.FocalMechanism}((0,))\n  amplitude: Array{QuakeML.Amplitude}((0,))\n  magnitude: Array{QuakeML.Magnitude}((1,))\n  station_magnitude: Array{QuakeML.StationMagnitude}((0,))\n  origin: Array{QuakeML.Origin}((0,))\n  pick: Array{QuakeML.Pick}((0,))\n  preferred_origin_id: Missing missing\n  preferred_magnitude_id: Missing missing\n  preferred_focal_mechanism_id: Missing missing\n  type: Missing missing\n  type_certainty: Missing missing\n  creation_info: Missing missing\n  public_id: QuakeML.ResourceIdentifier\n```\n\n## Repo status\n\nQuakeML.jl is beta software.  All functionality included is tested\nand works as advertised, but the public API of the package is\nstill to be decided and may break in v0.2, as per\n[SemVer](https://semver.org/).  So long as any packages you have\ncreated declare their compatibility with QuakeML.jl correctly,\nthis will cause no problems.\n\n### Activating debugging messages\nTo turn debugging messages on when running QuakeML, set the\nenvironment variable `JULIA_DEBUG` to `QuakeML` or `\"all\"`, which can\neven be done at run time in the repl like so:\n```julia\njulia> ENV[\"JULIA_DEBUG\"] = QuakeML\n```\n\nUnsetting this value will turn these debugging messages off.\n\nSee the [manual section on environment variables and logging messages](https://docs.julialang.org/en/v1/stdlib/Logging/#Environment-variables-1) for more information on setting the debug level for QuakeML or other modules.\n",
        "createdAt": "2020-03-31T16:57:49.000Z",
        "updatedAt": "2025-11-25T13:49:23.000Z",
        "language": "QML",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/anowacki/QuakeML.jl/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "liamtoney/saul",
        "url": "https://github.com/liamtoney/saul",
        "description": "SeismoAcoustic Utilities Library",
        "stars": 7,
        "forks": 2,
        "readme": "# SAUL\n\n[![API documentation status](https://readthedocs.org/projects/saul/badge/?version=latest)](https://saul.rtfd.io/)\n\n**SAUL** is the **S**eismo**A**coustic **U**tilities **L**ibrary. It's my take on the\ncollection of tools that I imagine exist, in some form, on every seismoacoustican's\ncomputer — utilities for gathering waveform data, plotting waveforms in the time and\nfrequency domain, visualizing key metadata such as station locations, _et cetera._ The\ngoal of SAUL is to make these fundamental data exploration tools as easy-to-use as\npossible. Thus, priority is placed upon straightforward (e.g., easily memorized)\ncommands and time-saving helper functions — while attempting to leverage existing\ndependencies as much as possible to avoid duplicated effort.\n\n> 🚧 **Disclaimer** 🚧  \n> As a workhorse \"everyday tools\" repository, SAUL is currently (perpetually?) under\n> rapid development. Expect to encounter breaking changes after a `git pull` update!\n\n## Installing\n\nBoth of these options assume that you've already\n[installed Miniforge](https://github.com/conda-forge/miniforge?tab=readme-ov-file#install)\n(which provides the commands [`conda`](https://docs.conda.io/en/latest/) and\n[`mamba`](https://mamba.readthedocs.io/en/latest/)), and that you've cloned this\nrepository and have navigated to the root directory.\n\n**Option 1:** Create a new environment named `saul`.\n```\nmamba env create --file environment.yml\n```\n\n**Option 2:** Install SAUL into an existing environment of your choosing.\n```\nmamba env update --name <existing_environment> --file environment.yml\n```\n\nSAUL is primarily developed on macOS, but it ought to work on Linux — and Windows via\n[Windows Subsystem for Linux (WSL)](https://learn.microsoft.com/en-us/windows/wsl/).\n\n## Using\n\nBe sure that the environment you've installed SAUL into is activated. Here's a simple\n[usage example](examples/example_psd.py) which highlights SAUL's object-oriented\ninterface:\n```python\nfrom saul import PSD, Stream\n\nst = Stream.from_earthscope('AK', 'HOM', 'BDF', (2023, 9, 1, 0, 5), (2023, 9, 1, 0, 15))\nst.detrend().taper(0.05).remove_response()  # SAUL Stream objects behave like ObsPy's\nPSD(st, method='multitaper').plot(show_noise_models=True)\n```\n<img src=\"_doc/example_psd.png\" width=550>\n\nFor detailed usage information, see the [API documentation](https://saul.rtfd.io/).\n\n## Developing\n\nTo install the development packages for SAUL, run the following command from the root\ndirectory of this repository, with your environment containing SAUL (see\n[Installing](#installing)) activated.\n```\npip install --requirement requirements.txt\n```\n",
        "createdAt": "2023-08-29T18:26:11.000Z",
        "updatedAt": "2025-08-27T12:58:44.000Z",
        "language": "Python",
        "homepage": "https://saul.rtfd.io",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/liamtoney/saul/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Jollyfant/nodejs-seedlink-latencies-proxy",
        "url": "https://github.com/Jollyfant/nodejs-seedlink-latencies-proxy",
        "description": "NodeJS application that connects to a Seedlink server and caches stream latencies that are then exposed through an HTTP API. ",
        "stars": 1,
        "forks": 1,
        "readme": "# nodejs-seedlink-latencies-proxy\nA proxy written for NodeJS that caches channel latencies available from a number of Seedlink servers at a configurable interval. The cached information is made available through a simple HTTP API. The proxy refreshes the latency information automatically.\n\n## Installation\n\n    npm install\n\n## Configuration\nModify config.json to suit your needs. You may configure multiple Seedlink servers latency sources that will be offered through the proxy.\n\n## Running\n\n    node index.js\n\n## Docker\n\n    docker build -t seedlink-latencies:1.0 .\n    docker run -p 8087:8087 [--rm] [-d] [-e \"SERVICE_PORT=8087\"] [-e \"SERVICE_HOST=0.0.0.0\"] seedlink-latencies:1.0\n\nFour envrionment variables can passed to Docker run to modify settings at runtime. Otherwise information is read from the built configuration file.\n\n  * SERVICE\\_HOST\n  * SERVICE\\_PORT\n\n## API\nThe supported parameters are valid SEED stream identifiers. Multiple stream identifiers may be delimited by a comma.\n\n  * network\n  * station\n  * location\n  * channel\n\n## Example\n\n    $ curl \"127.0.0.1:8087?network=GE&station=MARCO&channel=HHZ\"\n\n    [{\n        \"network\": \"GE\",\n        \"station\": \"MARCO\",\n        \"location\": \"\",\n        \"channel\": \"HHZ\",\n        \"end\": \"2018-07-06T12:44:49.970Z\",\n        \"msLatency\": 3542\n    }]\n",
        "createdAt": "2018-02-05T10:58:16.000Z",
        "updatedAt": "2024-05-29T12:53:27.000Z",
        "language": "JavaScript",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Jollyfant/nodejs-seedlink-latencies-proxy/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "cran/seismicRoll",
        "url": "https://github.com/cran/seismicRoll",
        "description": ":exclamation: This is a read-only mirror of the CRAN R package repository.  seismicRoll — Fast Rolling Functions for Seismology using 'Rcpp'  ",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2015-03-11T18:44:01.000Z",
        "updatedAt": "2023-08-22T09:31:20.000Z",
        "language": "R",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "Iron486/Italian_Earthquake_Catalogue_1985_2020",
        "url": "https://github.com/Iron486/Italian_Earthquake_Catalogue_1985_2020",
        "description": "List of 181777 earthquakes with location, time, magnitude and magnitude type",
        "stars": 0,
        "forks": 0,
        "readme": "# Italian Earthquake Catalogue 1985 2020\n\nThe earthquake catalogue was fetched from the following link: [http://terremoti.ingv.it/en](http://terremoti.ingv.it/en). \nAll the earthquakes with a magnitude equal or greater than 1.5, within 1985 and April 2020 and in a range of latitude and longitude between 36.1°N-47.1°N and 6.3°E-18.4°E respectively were selected.\nA total of 181777 events were obtained.\n\nThe columns `Catalog`,`Contributor`,`ContributorID`,`MagAuthor` were removed from the catalogue since they didn't contain any data.\n\nClick the following file [catalogue_manipulation.ipynb](https://github.com/Iron486/Italian_Earthquake_Catalogue_1985_2020/blob/main/catalogue_manipulation.ipynb) to take a look at the preprocessing steps.\n\nStar the repository if you found the dataset useful and inspiring :).\n\n### **Inspiration** \n\nIt is possible to perform an exploratory data analysis of the catalogue, analyzing and plotting earthquakes in the three dimensions, geolocating them, plotting the earthquakes in time and studying the occurrence of the events in the space-time domain. \n\nFurthermore, it's possible to perform more advanced analysis derived from statistical seismology, such as analyzing the number of events for each bin magnitude in time and space, analyzing multivariate plots or visualizing the type of magnitude over time.\n\nThis notebook [italian-earthquake-catalogue-eda-and-plots.ipynb](https://github.com/Iron486/Italian_Earthquake_Catalogue_1985_2020/blob/main/italian-earthquake-catalogue-eda-and-plots.ipynb) is an example of what can be done. \n\nIn [THIS](https://github.com/Iron486/Bachelor_Thesis) repository there are instead more complex notebooks to perform more accurate analysis.\n\n### **License**\n\n[Attribution 4.0 International (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/)\n",
        "createdAt": "2022-06-30T00:37:55.000Z",
        "updatedAt": "2022-06-30T22:12:22.000Z",
        "language": null,
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/Iron486/Italian_Earthquake_Catalogue_1985_2020/main/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "nestor-ld93/SpectralSOURCE",
        "url": "https://github.com/nestor-ld93/SpectralSOURCE",
        "description": "Programa semi-automático con el objetivo de estimar las características de la fuente sísmica utilizando análisis espectral de las ondas P.",
        "stars": 2,
        "forks": 0,
        "readme": "# SpectralSOURCE\nPrograma semi-automático con interfaz PyQt5 en lenguaje C-Shell, Bash y Matlab con el objetivo de estimar las características de la fuente sísmica (momento sísmico, magnitud momento, dimensiones del área de ruptura, caída de esfuerzos y deslizamiento) utilizando el método del análisis espectral de las ondas P en la componente vertical (BHZ).\n\nLas salidas generadas por **SpectralSOURCE** son las siguientes:\n\n- Resultados del momento sísmico y frecuencia esquina para cada estación: **[SALIDA_ESTACIONES].txt**.\n- Resultados finales del momento sísmico, magnitud momento, dimensiones del área de ruptura, caída de esfuerzos y deslizamiento considerando diversos modelos: **[SALIDA_FINAL].txt**.\n- Gráficas en formato EPS o PNG de los espectros de desplazamiento para cada estación: **Gráficas_FFT_02**.\n\n## IMÁGENES PRINCIPALES (en KDE Plasma 5.19)\n\n![app menu](https://lh3.googleusercontent.com/-eHlAFbOYRu8/X2k_mpOUxzI/AAAAAAAABLs/xOujU6437BsNiurVKM9BHNZBjh03EohwQCLcBGAsYHQ/h809/SpectralSOURCE_PyQt5_01.png \"Interfaz gráfica en PyQt5 de SpectralSOURCE: Spectral\")\n![app menu](https://lh3.googleusercontent.com/-fgcGzpxn1jE/X2k_msP1_8I/AAAAAAAABLw/h2Gz70N1cGsjIN9Po9DN5ljTuuM-bfEkQCLcBGAsYHQ/h809/SpectralSOURCE_PyQt5_02.png \"Interfaz gráfica en PyQt5 de SpectralSOURCE: SOURCE\")\n![app menu](https://lh3.googleusercontent.com/-knDSFv9BLo0/X2k_mfiaPXI/AAAAAAAABLo/HB4L1ikK0VYjmnaKSitKG-54jYMbU8SmACLcBGAsYHQ/h809/SpectralSOURCE_PyQt5_03.png \"Interfaz gráfica en PyQt5 de SpectralSOURCE: ResultadoGRAF - Avanzado\")\n![app menu](https://lh3.googleusercontent.com/-djVoGL9b4Ec/XyH71kGMjOI/AAAAAAAABIU/ATulrltTFwobK3YjrNVyZCFN4Mma0p_lQCLcBGAsYHQ/h864/Figura-2.png \"Principales espectros de desplazamiento para el sismo de Arequipa del 23 de junio del 2001 - Perú\")\n\n## RECOMENDACIONES\n- Utilizar registros en formato SEED (IRIS hasta el 2019). Si no es posible, utilizar archivos SAC más archivos de polos y zeros (IRIS desde el 2020).\n- Utilizar registros de estaciones de banda ancha en la componente vertical (BHZ) de la red internacional IRIS a una distancia epicentral de 30°-90°.\n- Utilizar señales con 1 min antes del primer arribo de la onda P y 5 min después de S.\n- Utilizar una distribución de GNU/Linux con escritorio KDE Plasma 5.12 o superior.\n\n## REQUISITOS MÍNIMOS\n- rdseed 5.3.1 o superior (https://github.com/iris-edu-legacy/rdseed)\n- SAC 101.6a o superior (https://ds.iris.edu/ds/nodes/dmc/forms/sac/)\n- sac2xy (https://github.com/msthorne/SACTOOLS)\n- Shell y C-Shell\n- ps2eps\n- MATLAB 8.5 (2015a)\n- python3\n- python3-pyqt5\n- GNU Linux (Kernel 4.15) 64-bit\n\n## ¿CÓMO DESCARGAR?\n- Para obtener la última versión estable, descargue desde la pestaña [[Releases](https://github.com/nestor-ld93/SpectralSOURCE/releases)].\n- Para obtener la última versión candidata a estable, descargue desde el botón [Code] o ejecute en un terminal:\n`git clone https://github.com/nestor-ld93/SpectralSOURCE`\n\n## ¿CÓMO EJECUTAR?\n1. Descargar las señales de la red internacional IRIS (un archivo en formato SEED). También es válido descargar señales en formato SAC (little-endian o big-endian) comprimidos en TAR (contiene archivos SAC mas archivos de polos y zeros).\n1. Copiar el archivo SEED en la misma ubicación donde se encuentra el programa **I_Pre-procesamiento.csh**. O en su defecto, descomprimir el contenido del archivo TAR y copiar los archivos SAC y SACPZ en el directorio donde se encuentra el programa **I_Pre-procesamiento.csh**.\n1. Ejecutar en un terminal: `./SpectralSOURCE.py` e ingresar el archivo SEED (o marcar el check \"`Utilizar archivos SAC y PZ`\"), el filtro a utilizar y el tipo de gráficos (registros de ondas) a generar. Por el momento, los parámetros adicionales se encuentran desactivados.\n1. Verificar los parámetros e iniciar \"`Spectral (Parte 1)`\".\n1. Picar manualmente el primer arribo de la onda P (proceso iterativo).\n1. Decidir si eliminar estaciones corruptas y proceder.\n1. Iniciar \"`Spectral (Parte 2)`\".\n1. Ingresar a la pestaña \"`Principal: SOURCE`\" y ejecutar \"`1. Listar archivos`\".\n1. Ejecutar \"`2. SOURCE.m`\" (se abrirá matlab) e identificar manualmente la parte plana del espectro y picar la frecuencia esquina (proceso iterativo). Esperar a que Matlab se cierre automáticamente.\n1. Seleccionar el tipo de falla (utilizado para las relaciones de escalamiento de Papazachos) y ejecutar \"`3. Resultado.m`\" (se abrirá matlab en modo línea de comandos). Esperar a que Matlab muestre los resultados.\n1. Si el usuario lo desea, puede visualizar los resultados en la interfaz gráfica haciendo clic en \"`Mostrar resultados ...`\".\n1. Seleccionar el formato de gráficos (EPS o PNG) para los espectros de desplazamiento y ejecutar \"`4. ResultadoGRAF`\".\n1. Ejecutar \"`5. Ordenar gráficos`\".\n\n## NOTAS IMPORTANTES\n- Las estaciones registradoras muchas veces presentan señales corruptas, aprovechar la opción **Eliminar estaciones corruptas (separar con \";\")** para eliminarlas. Ingresar únicamente el nombre de las estaciones con el separador \";\". Por ejemplo: `RAR;XMAS;RSSD` (no incluir símbolos adicionales al inicio ni al final). Esta opción eliminará los archivos de las estaciones y su conexión con el contenido de **HIPO_IRIS.txt**.\n- Por defecto, **SOURCE** trabaja con el modelo PREM **([PREM]_1s_IDV.txt)**, este modelo puede ser reemplazado por otro según sea el caso (realizar las modificaciones necesarias al programa para acoplar otro modelo).\n- El archivo **[HIPO_IRIS].txt** contiene los parámetros de cada estación además de los parámetros hipocentrales del sismo. Estos datos son obtenidos del cabecero de los archivos SAC, por lo tanto, puede contener errores (sobre todo la profundidad del evento. Línea 1, columna 7). De ser necesario, modificar **[HIPO_IRIS].txt**.\n- La pestaña \"`Principal: SOURCE`\" contiene los elementos \"`Limpiar resultados y reiniciar`\" y \"`Limpiar todos los archivos y reiniciar`\". La primera opción únicamente elimina los resultados de **SOURCE** y permite empezar nuevamente \"`1. Listar archivos`\"; mientras que el segundo, elimina todos los archivos necesarios para empezar **SOURCE** y se deberá iniciar nuevamente con **SPECTRAL**.\n- La pestaña \"`Principal: SOURCE`\" contiene el elemento \"`Tengo los archivos txt de salida`\", dicha opción habilitará **SOURCE** desde \"`3. Resultado.m`\" siempre y cuando se copien los archivos \"[HIPO_IRIS].txt\", \"[LISTA_xy].txt\", \"[SALIDA_ESTACIONES].txt\" y \"[SALIDA_FINAL].txt\" en la carpeta **SOURCE**.\n- IRIS finalizó la distribución de archivos seed a finales del 2019. Como alternativa rápida, es posible la solicitud de archivos SAC más polos y zeros en un fichero TAR. **SpectralSOURCE** considera la nueva nomenclatura (2020 del IRIS) de nombres de archivos SAC mas polos y zeros. **No renombrar ningún archivo del IRIS, SpectralSOURCE lo realizará**.\n\n## LISTA DE CAMBIOS\n- (v1.0.0) [03/04/2020] Lanzamiento inicial.\n- (v1.1.0) [04/04/2020] Cambios a varios archivos para enlazarlos al programa principal: **SpectralSOURCE.py**\n- (v1.1.0) [04/04/2020] Se añadió una interfaz gráfica en PyQt5.\n- (v1.1.1) [06/04/2020] Corrección en el esquema de colores.\n- (v1.1.1) [06/04/2020] Con Plasma 5.12 o superior, es posible tener el modo claro o modo oscuro dependiendo del esquema de colores seleccionado.\n- (v1.1.2) [16/04/2020] Correcciones a los enlaces externos.\n- (v1.2.0) [20/07/2020] Agregado nueva opción para utilizar directamente archivos SAC y SACPZ descargados del IRIS.\n- (v1.2.0) [20/07/2020] Script **1.1_renombrar_BHZ_v1.0.sh** mejorado para tener en cuenta la nueva nomenclatura (nombre de archivos SAC) del IRIS al utilizar señales descargadas en formato SAC little-endian o big-endian.\n- (v1.2.0) [20/07/2020] Se reemplazó los TextEdit por SpinBox de los filtros.\n- (v1.2.0) [20/07/2020] Se agregaron detalles a los gráficos finales.\n- (v1.2.1) [29/07/2020] Se reemplazó Distancia epicentral (km) por Azimuth (°) en **HIPO_IRIS.txt** (columna 4).\n- (v1.2.1) [29/07/2020] Los gráficos finales incluyen el valor del Azimuth para cada estación.\n- (v1.2.1) [29/07/2020] Agregado Distancia epicentral (°) y Azimuth (°) en **[SALIDA_ESTACIONES].txt**.\n- (v1.2.1) [29/07/2020] Si IRIS no proporciona un valor de magnitud (archivos SAC) será reemplazado por NaN.\n- (v1.3.0) [29/09/2020] Mejora en el renderizado vectorial.\n- (v1.3.0) [29/09/2020] 'ResultadoGRAF.m' es más personalizable.\n- (v1.3.0) [29/09/2020] Añadida nueva pestaña: 'ResultadoGRAF - Avanzado'. Ahora es posible personalizar algunas opciones de los gráficos finales.\n",
        "createdAt": "2020-04-03T21:20:37.000Z",
        "updatedAt": "2023-02-09T15:26:09.000Z",
        "language": "Python",
        "homepage": "",
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "readmeUrl": "https://raw.githubusercontent.com/nestor-ld93/SpectralSOURCE/master/README.md",
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "JohnR0s/MX-Earthquakes",
        "url": "https://github.com/JohnR0s/MX-Earthquakes",
        "description": "This repository contains an analysis of seismic data from the National Seismological System of the National Autonomous University of Mexico (UNAM).",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2024-03-03T05:23:01.000Z",
        "updatedAt": "2024-03-03T05:27:15.000Z",
        "language": "Jupyter Notebook",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    },
    {
        "source": "GitHub",
        "name": "McCio/belgium-seismic-lockdown",
        "url": "https://github.com/McCio/belgium-seismic-lockdown",
        "description": "Belgium seisimc surveys analysis for lockdown influence",
        "stars": 0,
        "forks": 0,
        "readme": "",
        "createdAt": "2020-06-15T21:36:09.000Z",
        "updatedAt": "2020-06-15T21:48:48.000Z",
        "language": "TeX",
        "homepage": null,
        "amountPublications": {},
        "agentQueryTerm": {
            "zenodo": "",
            "openAlex": "",
            "openCitations": "",
            "dataCite": "",
            "rsd": ""
        },
        "publications": []
    }
]